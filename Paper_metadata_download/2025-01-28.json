[
    {
        "paper": {
            "id": "2501.15368",
            "authors": [
                {
                    "_id": "67986c6822990ae89bb71fb9",
                    "user": {
                        "_id": "6797cc0ff386b10d1609e3ff",
                        "avatarUrl": "/avatars/3ec1020e974ed01f60a46150501171da.svg",
                        "isPro": false,
                        "fullname": "Yadong Li",
                        "user": "AdamLee1",
                        "type": "user"
                    },
                    "name": "Yadong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T10:29:02.659Z",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fba",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fbb",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fbc",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fbd",
                    "name": "Song Chen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fbe",
                    "name": "Tianpeng Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fbf",
                    "name": "Zehuan Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc0",
                    "name": "Lijun Liu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc1",
                    "name": "Lingfeng Ming",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc2",
                    "name": "Guosheng Dong",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc3",
                    "name": "Da Pan",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc4",
                    "name": "Chong Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc5",
                    "name": "Yuanbo Fang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc6",
                    "name": "Dongdong Kuang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc7",
                    "user": {
                        "_id": "65e88bfa7b458aa68925ea89",
                        "avatarUrl": "/avatars/6cfd5c3fceda7d3f2cb52639fc6b1597.svg",
                        "isPro": false,
                        "fullname": "Wang Ming Rui",
                        "user": "reiiichan",
                        "type": "user"
                    },
                    "name": "Mingrui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:55:18.208Z",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc8",
                    "name": "Chenglin Zhu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fc9",
                    "name": "Youwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fca",
                    "name": "Hongyu Guo",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fcb",
                    "name": "Fengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fcc",
                    "user": {
                        "_id": "65e71ef39cf349af2940b317",
                        "avatarUrl": "/avatars/fc1cd8d3510946fc947d67b16b51834b.svg",
                        "isPro": false,
                        "fullname": "Yuran Wang",
                        "user": "Ryann829",
                        "type": "user"
                    },
                    "name": "Yuran Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-28T13:52:53.610Z",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fcd",
                    "name": "Bowen Ding",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fce",
                    "name": "Wei Song",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fcf",
                    "name": "Xu Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd0",
                    "name": "Yuqi Huo",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd1",
                    "name": "Zheng Liang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd2",
                    "name": "Shusen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd3",
                    "name": "Xin Wu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd4",
                    "name": "Shuai Zhao",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd5",
                    "name": "Linchu Xiong",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd6",
                    "name": "Yozhen Wu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd7",
                    "name": "Jiahui Ye",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd8",
                    "name": "Wenhao Lu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fd9",
                    "name": "Bowen Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fda",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fdb",
                    "name": "Yaqi Zhou",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fdc",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fdd",
                    "name": "Lei Su",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fde",
                    "name": "Hongda Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fdf",
                    "name": "Fuzhong Chen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe0",
                    "name": "Xuezhen Dong",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe1",
                    "name": "Na Nie",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe2",
                    "name": "Zhiying Wu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe3",
                    "name": "Bin Xiao",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe4",
                    "name": "Ting Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe5",
                    "name": "Shunya Dang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe6",
                    "name": "Ping Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe7",
                    "name": "Yijia Sun",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe8",
                    "name": "Jincheng Wu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fe9",
                    "name": "Jinjie Yang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fea",
                    "name": "Xionghai Lin",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71feb",
                    "name": "Zhi Ma",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fec",
                    "name": "Kegeng Wu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fed",
                    "name": "Jia li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fee",
                    "name": "Aiyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fef",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff0",
                    "name": "Jianqiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff1",
                    "name": "Xiaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff2",
                    "name": "Guangwei Ai",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff3",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff4",
                    "name": "Yicong Chen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff5",
                    "name": "Xiaoqin Huang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff6",
                    "name": "Kun Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff7",
                    "name": "Wenjing Luo",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff8",
                    "name": "Yifei Duan",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ff9",
                    "name": "Lingling Zhu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ffa",
                    "name": "Ran Xiao",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ffb",
                    "name": "Zhe Su",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ffc",
                    "name": "Jiani Pu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ffd",
                    "name": "Dian Wang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71ffe",
                    "name": "Xu Jia",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb71fff",
                    "name": "Tianyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72000",
                    "name": "Mengyu Ai",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72001",
                    "name": "Mang Wang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72002",
                    "name": "Yujing Qiao",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72003",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72004",
                    "name": "Yanjun Shen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72005",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72006",
                    "name": "Miao Zhen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72007",
                    "name": "Yijie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72008",
                    "name": "Mingyang Chen",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72009",
                    "name": "Fei Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb7200a",
                    "name": "Chenzheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb7200b",
                    "name": "Keer Lu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb7200c",
                    "name": "Yaqi Zhao",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb7200d",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb7200e",
                    "name": "Youquan Li",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb7200f",
                    "name": "Yanzhao Qin",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72010",
                    "name": "Linzhuang Sun",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72011",
                    "name": "Jianhua Xu",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72012",
                    "name": "Haoze Sun",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72013",
                    "name": "Mingan Lin",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72014",
                    "name": "Zenan Zhou",
                    "hidden": false
                },
                {
                    "_id": "67986c6822990ae89bb72015",
                    "user": {
                        "_id": "6501587887b370a56ad2608e",
                        "avatarUrl": "/avatars/6779baaa8ed9032de55a2f78e1f52e20.svg",
                        "isPro": false,
                        "fullname": "Wei-Peng Chen",
                        "user": "whenfra",
                        "type": "user"
                    },
                    "name": "Weipeng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:54:48.451Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T02:19:03.000Z",
            "title": "Baichuan-Omni-1.5 Technical Report",
            "summary": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\nomni-modal understanding capabilities but also provides end-to-end audio\ngeneration capabilities. To achieve fluent and high-quality interaction across\nmodalities without compromising the capabilities of any modality, we\nprioritized optimizing three key aspects. First, we establish a comprehensive\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\nacoustic information from audio, enabling seamless integration and enhanced\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\nthat progressively integrates multimodal alignment and multitask fine-tuning,\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\nto leading models such as Qwen2-VL-72B across various multimodal medical\nbenchmarks.",
            "upvotes": 36,
            "discussionId": "67986c6b22990ae89bb720aa"
        },
        "publishedAt": "2025-01-28T00:34:49.721Z",
        "title": "Baichuan-Omni-1.5 Technical Report",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15368.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.15383",
            "authors": [
                {
                    "_id": "67986c83b5e71350993d28eb",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28ec",
                    "user": {
                        "_id": "6583ab7983a9e1460c67d876",
                        "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
                        "isPro": false,
                        "fullname": "bowen",
                        "user": "bowenYu",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:56:10.598Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28ed",
                    "name": "Chengyuan Li",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28ee",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:56:44.491Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28ef",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f0",
                    "name": "Haoyan Huang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f1",
                    "name": "Jiandong Jiang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f2",
                    "user": {
                        "_id": "654bead777401b47e6424f88",
                        "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
                        "isPro": false,
                        "fullname": "Jianhong Tu",
                        "user": "ToviTu",
                        "type": "user"
                    },
                    "name": "Jianhong Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:58:35.581Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f3",
                    "name": "Jianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f4",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f5",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:57:31.261Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f6",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f7",
                    "user": {
                        "_id": "65b0b3957e5d5a4ecc750de0",
                        "avatarUrl": "/avatars/e0d79d3265ca4ad5c5411feb01043fb4.svg",
                        "isPro": false,
                        "fullname": "Kexin Yang",
                        "user": "dawn0929",
                        "type": "user"
                    },
                    "name": "Kexin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:57:56.435Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f8",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28f9",
                    "name": "Mei Li",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28fa",
                    "user": {
                        "_id": "636a390037d9329b4a007009",
                        "avatarUrl": "/avatars/a3c9117e104d4667e39e20ec83dc5cd6.svg",
                        "isPro": false,
                        "fullname": "Minmin Sun",
                        "user": "minminsun",
                        "type": "user"
                    },
                    "name": "Minmin Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:58:27.795Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28fb",
                    "name": "Qin Zhu",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28fc",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28fd",
                    "name": "Tao He",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28fe",
                    "name": "Weijia Xu",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d28ff",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2900",
                    "user": {
                        "_id": "63f4c99721eb234ab73dd112",
                        "avatarUrl": "/avatars/162e92d7aeb7de1c6ebf4d6e2bff33f5.svg",
                        "isPro": false,
                        "fullname": "yu wenyuan",
                        "user": "liuxinyijian",
                        "type": "user"
                    },
                    "name": "Wenyuan Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T13:58:11.021Z",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2901",
                    "name": "Xiafei Qiu",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2902",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2903",
                    "name": "Xinlong Yang",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2904",
                    "name": "Yong Li",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2905",
                    "name": "Zhiying Xu",
                    "hidden": false
                },
                {
                    "_id": "67986c83b5e71350993d2906",
                    "name": "Zipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T03:47:25.000Z",
            "title": "Qwen2.5-1M Technical Report",
            "summary": "We introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such as long data synthesis,\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes a\nlength extrapolation method that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement a sparse attention method along with chunked\nprefill optimization for deployment scenarios and a sparsity refinement method\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, including kernel optimization, pipeline parallelism, and\nscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\ntokens of context. This framework provides an efficient and powerful solution\nfor developing applications that require long-context processing using\nopen-source models.\n  The Qwen2.5-1M series currently includes the open-source models\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\nimproved in long-context tasks without compromising performance in\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\ncontexts eight times longer.",
            "upvotes": 26,
            "discussionId": "67986c84b5e71350993d2974"
        },
        "publishedAt": "2025-01-28T00:35:46.871Z",
        "title": "Qwen2.5-1M Technical Report",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15383.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.16142",
            "authors": [
                {
                    "_id": "67986cbc7dbf69e4e38539b7",
                    "name": "Scott Fujimoto",
                    "hidden": false
                },
                {
                    "_id": "67986cbc7dbf69e4e38539b8",
                    "user": {
                        "_id": "64b6df54dce8f1fbb8ac9ed7",
                        "avatarUrl": "/avatars/82ca21cb9c8bacde071769bf4a888375.svg",
                        "isPro": false,
                        "fullname": "Pierluca D'Oro",
                        "user": "pierluca",
                        "type": "user"
                    },
                    "name": "Pierluca D'Oro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:09:37.496Z",
                    "hidden": false
                },
                {
                    "_id": "67986cbc7dbf69e4e38539b9",
                    "name": "Amy Zhang",
                    "hidden": false
                },
                {
                    "_id": "67986cbc7dbf69e4e38539ba",
                    "user": {
                        "_id": "6344cf73ee1504dbcd5bdfe7",
                        "avatarUrl": "/avatars/6dd2bf1f9c5679e5c8c85d62c9836aac.svg",
                        "isPro": false,
                        "fullname": "Yuandong Tian",
                        "user": "tydsh",
                        "type": "user"
                    },
                    "name": "Yuandong Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:10:10.676Z",
                    "hidden": false
                },
                {
                    "_id": "67986cbc7dbf69e4e38539bb",
                    "name": "Michael Rabbat",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T15:36:37.000Z",
            "title": "Towards General-Purpose Model-Free Reinforcement Learning",
            "summary": "Reinforcement learning (RL) promises a framework for near-universal\nproblem-solving. In practice however, RL algorithms are often tailored to\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\nchoices. Recently, powerful model-based RL methods have shown impressive\ngeneral results across benchmarks but come at the cost of increased complexity\nand slow run times, limiting their broader applicability. In this paper, we\nattempt to find a unifying model-free deep RL algorithm that can address a\ndiverse class of domains and problem settings. To achieve this, we leverage\nmodel-based representations that approximately linearize the value function,\ntaking advantage of the denser task objectives used by model-based RL while\navoiding the costs associated with planning or simulated trajectories. We\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\nsingle set of hyperparameters and show a competitive performance against\ndomain-specific and general baselines, providing a concrete step towards\nbuilding general-purpose model-free deep RL algorithms.",
            "upvotes": 13,
            "discussionId": "67986cbf7dbf69e4e3853a89"
        },
        "publishedAt": "2025-01-28T00:36:09.186Z",
        "title": "Towards General-Purpose Model-Free Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16142.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.15570",
            "authors": [
                {
                    "_id": "679843ae7d7b7f8196c61ab7",
                    "user": {
                        "_id": "63a00aa29f1f2baab2034cf8",
                        "avatarUrl": "/avatars/818d104f45cbce2c47d443756fa806c8.svg",
                        "isPro": false,
                        "fullname": "Yueyu Lin",
                        "user": "yueyulin",
                        "type": "user"
                    },
                    "name": "Lin Yueyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:10:34.376Z",
                    "hidden": false
                },
                {
                    "_id": "679843ae7d7b7f8196c61ab8",
                    "name": "Li Zhiyuan",
                    "hidden": false
                },
                {
                    "_id": "679843ae7d7b7f8196c61ab9",
                    "user": {
                        "_id": "64087a0992033c15073afb8c",
                        "avatarUrl": "/avatars/9c590ab5c6526edce5084169ec7bde2e.svg",
                        "isPro": false,
                        "fullname": "peteryue",
                        "user": "peteryue",
                        "type": "user"
                    },
                    "name": "Peter Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:11:02.358Z",
                    "hidden": false
                },
                {
                    "_id": "679843ae7d7b7f8196c61aba",
                    "user": {
                        "_id": "6176b32847ee6431f632981e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
                        "isPro": false,
                        "fullname": "IvanD",
                        "user": "xiaol",
                        "type": "user"
                    },
                    "name": "Liu Xiao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-28T02:44:02.658Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T15:56:56.000Z",
            "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
            "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\nhttps://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside},\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
            "upvotes": 12,
            "discussionId": "679843af7d7b7f8196c61b21"
        },
        "publishedAt": "2025-01-28T03:02:56.062Z",
        "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15570.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "fullname": "IvanD",
            "name": "xiaol",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 81
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.15907",
            "authors": [
                {
                    "_id": "6798a917a8b0d165e39e17f5",
                    "user": {
                        "_id": "61a7569eaf0333e76eb428a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a7569eaf0333e76eb428a8/zwseNheR4Hx0DtCmf_v5H.jpeg",
                        "isPro": false,
                        "fullname": "HarryHe11",
                        "user": "HarryHe",
                        "type": "user"
                    },
                    "name": "Haorui He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-28T13:52:52.095Z",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17f6",
                    "user": {
                        "_id": "64b77dd308e2452d18ddd279",
                        "avatarUrl": "/avatars/258f21fa20a3a187050d80c6088a1f50.svg",
                        "isPro": false,
                        "fullname": "shangzengqiang",
                        "user": "clatter-1",
                        "type": "user"
                    },
                    "name": "Zengqiang Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:12:12.565Z",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17f7",
                    "name": "Chaoren Wang",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17f8",
                    "name": "Xuyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17f9",
                    "user": {
                        "_id": "66b5f38a080d890d1727a2a4",
                        "avatarUrl": "/avatars/4d73017ce888437225d994d8ba370e5d.svg",
                        "isPro": false,
                        "fullname": "guyicheng",
                        "user": "guyicheng",
                        "type": "user"
                    },
                    "name": "Yicheng Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:12:57.932Z",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17fa",
                    "name": "Hua Hua",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17fb",
                    "name": "Liwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17fc",
                    "name": "Chen Yang",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17fd",
                    "user": {
                        "_id": "6635a711a5243c9638f5e4df",
                        "avatarUrl": "/avatars/08651622fc1fd5089551b510be8c4530.svg",
                        "isPro": false,
                        "fullname": "Jiaqi Li",
                        "user": "jiaqili3",
                        "type": "user"
                    },
                    "name": "Jiaqi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:14:30.597Z",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17fe",
                    "name": "Peiyang Shi",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e17ff",
                    "user": {
                        "_id": "63072d60cd148dbc5e49f4dd",
                        "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
                        "isPro": false,
                        "fullname": "Yuancheng Wang",
                        "user": "Hecheng0625",
                        "type": "user"
                    },
                    "name": "Yuancheng Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:14:50.950Z",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e1800",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e1801",
                    "user": {
                        "_id": "65fbe8eb030389a29b87446f",
                        "avatarUrl": "/avatars/6d3ba153c41945e566b7c2c2d6af6da6.svg",
                        "isPro": false,
                        "fullname": "pengyuan zhang",
                        "user": "pengyuan2024",
                        "type": "user"
                    },
                    "name": "Pengyuan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:14:57.414Z",
                    "hidden": false
                },
                {
                    "_id": "6798a917a8b0d165e39e1802",
                    "name": "Zhizheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T09:59:20.000Z",
            "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation",
            "summary": "Recent advancements in speech generation have been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexplored\nin-the-wild data that capture spontaneous human speech in real-world contexts.\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate that Emilia\nsignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiverse speaker timbre and speaking styles of real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvance speech generation research and validates the effectiveness of Emilia\nfor both multilingual and crosslingual speech generation.",
            "upvotes": 11,
            "discussionId": "6798a919a8b0d165e39e187d"
        },
        "publishedAt": "2025-01-28T05:40:25.750Z",
        "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15907.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61a7569eaf0333e76eb428a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a7569eaf0333e76eb428a8/zwseNheR4Hx0DtCmf_v5H.jpeg",
            "fullname": "HarryHe11",
            "name": "HarryHe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.15369",
            "authors": [
                {
                    "_id": "6798706dabdc35456a92212d",
                    "user": {
                        "_id": "65019cc870367843160fbb33",
                        "avatarUrl": "/avatars/f5455482fe9efbdeeea1bd3a3c119f02.svg",
                        "isPro": false,
                        "fullname": "ZhengChuanyang",
                        "user": "BillionZheng",
                        "type": "user"
                    },
                    "name": "Chuanyang Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:16:05.976Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T02:34:58.000Z",
            "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
            "summary": "We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\ni.e., ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.",
            "upvotes": 9,
            "discussionId": "6798706eabdc35456a92215a"
        },
        "publishedAt": "2025-01-28T00:51:51.263Z",
        "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15369.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.14723",
            "authors": [
                {
                    "_id": "67991502dc9404d4424ce38c",
                    "name": "Ryan Ehrlich",
                    "hidden": false
                },
                {
                    "_id": "67991502dc9404d4424ce38d",
                    "name": "Bradley Brown",
                    "hidden": false
                },
                {
                    "_id": "67991502dc9404d4424ce38e",
                    "name": "Jordan Juravsky",
                    "hidden": false
                },
                {
                    "_id": "67991502dc9404d4424ce38f",
                    "name": "Ronald Clark",
                    "hidden": false
                },
                {
                    "_id": "67991502dc9404d4424ce390",
                    "name": "Christopher Ré",
                    "hidden": false
                },
                {
                    "_id": "67991502dc9404d4424ce391",
                    "user": {
                        "_id": "66ba4e72c9b2ab14b3707be0",
                        "avatarUrl": "/avatars/97ef14d683ed2d0115a9c4694b9763dc.svg",
                        "isPro": false,
                        "fullname": "Azalia Mirhoseini",
                        "user": "am34",
                        "type": "user"
                    },
                    "name": "Azalia Mirhoseini",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-28T17:33:55.071Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-24T18:58:40.000Z",
            "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
            "summary": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
            "upvotes": 5,
            "discussionId": "67991503dc9404d4424ce3e7"
        },
        "publishedAt": "2025-01-28T15:00:51.189Z",
        "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60799bed489fc71534e91bf3/6odkURJDAikOXG2gFhzC0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14723.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60799bed489fc71534e91bf3",
            "avatarUrl": "/avatars/0f57ee357b29fed39f253f28e39abf6b.svg",
            "fullname": "Brown",
            "name": "Bradley",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2403.09193",
            "authors": [
                {
                    "_id": "674c73752f5974eb9a7ed124",
                    "user": {
                        "_id": "6266c07e7a1f5a1562c4113b",
                        "avatarUrl": "/avatars/f20e6d735ff52e2941c2240fda42c422.svg",
                        "isPro": false,
                        "fullname": "Paul Gavrikov",
                        "user": "paulgavrikov",
                        "type": "user"
                    },
                    "name": "Paul Gavrikov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-02T08:50:16.337Z",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed125",
                    "name": "Jovita Lukasik",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed126",
                    "name": "Steffen Jung",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed127",
                    "user": {
                        "_id": "673bbe0d7dfcdedd52619ec2",
                        "avatarUrl": "/avatars/531a44f05d0c738bbe3e028c76c2e948.svg",
                        "isPro": false,
                        "fullname": "Robert Geirhos",
                        "user": "rgeirhos",
                        "type": "user"
                    },
                    "name": "Robert Geirhos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-17T17:22:38.274Z",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed128",
                    "name": "Bianca Lamm",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed129",
                    "name": "Muhammad Jehanzeb Mirza",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed12a",
                    "name": "Margret Keuper",
                    "hidden": false
                },
                {
                    "_id": "674c73752f5974eb9a7ed12b",
                    "name": "Janis Keuper",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-03-14T09:07:14.000Z",
            "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer\n  Them?",
            "summary": "Vision language models (VLMs) have drastically changed the computer vision\nmodel landscape in only a few years, opening an exciting array of new\napplications from zero-shot image classification, over to image captioning, and\nvisual question answering. Unlike pure vision models, they offer an intuitive\nway to access visual content through language prompting. The wide applicability\nof such models encourages us to ask whether they also align with human vision -\nspecifically, how far they adopt human-induced visual biases through multimodal\nfusion, or whether they simply inherit biases from pure vision models. One\nimportant visual bias is the texture vs. shape bias, or the dominance of local\nover global information. In this paper, we study this bias in a wide range of\npopular VLMs. Interestingly, we find that VLMs are often more shape-biased than\ntheir vision encoders, indicating that visual biases are modulated to some\nextent through text in multimodal models. If text does indeed influence visual\nbiases, this suggests that we may be able to steer visual biases not just\nthrough visual input but also through language: a hypothesis that we confirm\nthrough extensive experiments. For instance, we are able to steer shape bias\nfrom as low as 49% to as high as 72% through prompting alone. For now, the\nstrong human bias towards shape (96%) remains out of reach for all tested VLMs.",
            "upvotes": 5,
            "discussionId": "674c73762f5974eb9a7ed1a1"
        },
        "publishedAt": "2025-01-28T14:06:36.924Z",
        "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09193.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 738
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.16295",
            "authors": [
                {
                    "_id": "67986cd6bdc99911a989b0a5",
                    "name": "Weixin Liang",
                    "hidden": false
                },
                {
                    "_id": "67986cd6bdc99911a989b0a6",
                    "user": {
                        "_id": "6532e347b66f4bf689cf269a",
                        "avatarUrl": "/avatars/76b5dddf80a24d3ef5c68b702280da82.svg",
                        "isPro": false,
                        "fullname": "Junhong Shen",
                        "user": "sjunhongs",
                        "type": "user"
                    },
                    "name": "Junhong Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:18:15.019Z",
                    "hidden": false
                },
                {
                    "_id": "67986cd6bdc99911a989b0a7",
                    "user": {
                        "_id": "65a76ff1e504d9738d636217",
                        "avatarUrl": "/avatars/26bf5e3f19057835ee95d72c24904d77.svg",
                        "isPro": false,
                        "fullname": "Genghan Zhang",
                        "user": "Genghan",
                        "type": "user"
                    },
                    "name": "Genghan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:18:21.072Z",
                    "hidden": false
                },
                {
                    "_id": "67986cd6bdc99911a989b0a8",
                    "name": "Ning Dong",
                    "hidden": false
                },
                {
                    "_id": "67986cd6bdc99911a989b0a9",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "67986cd6bdc99911a989b0aa",
                    "name": "Lili Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T18:35:05.000Z",
            "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with\n  Modality-Aware Sparsity",
            "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers for sequential modeling, but their inability to leverage\nmodality-specific features limits their performance in multi-modal pretraining.\nHere, we propose Mixture-of-Mamba, a novel SSM architecture that introduces\nmodality-aware sparsity through modality-specific parameterization of the Mamba\nblock. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996;\n2024), we extend the benefits of modality-aware sparsity to SSMs while\npreserving their computational efficiency. We evaluate Mixture-of-Mamba across\nthree multi-modal pretraining settings: Transfusion (interleaved text and\ncontinuous image tokens with diffusion loss), Chameleon (interleaved text and\ndiscrete image tokens), and an extended three-modality framework incorporating\nspeech. Mixture-of-Mamba consistently reaches the same loss values at earlier\ntraining steps with significantly reduced computational costs. In the\nTransfusion setting, Mixture-of-Mamba achieves equivalent image loss using only\n34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting,\nMixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at\nthe 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the\nthree-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the\n1.4B scale. Our ablation study highlights the synergistic effects of decoupling\nprojection components, where joint decoupling yields greater gains than\nindividual modifications. These results establish modality-aware sparsity as a\nversatile and effective design principle, extending its impact from\nTransformers to SSMs and setting new benchmarks in multi-modal pretraining. Our\ncode can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba",
            "upvotes": 5,
            "discussionId": "67986cd7bdc99911a989b0ea"
        },
        "publishedAt": "2025-01-28T00:36:31.841Z",
        "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16295.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.12370",
            "authors": [
                {
                    "_id": "6798d09d208ffebef5bcfa47",
                    "name": "Samira Abnar",
                    "hidden": false
                },
                {
                    "_id": "6798d09d208ffebef5bcfa48",
                    "user": {
                        "_id": "64b1a4f64dd3e24895daa236",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b1a4f64dd3e24895daa236/lzlZ4DBOw-YVspNttAEY3.jpeg",
                        "isPro": false,
                        "fullname": "Harshay Shah",
                        "user": "harshay",
                        "type": "user"
                    },
                    "name": "Harshay Shah",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:16:56.095Z",
                    "hidden": false
                },
                {
                    "_id": "6798d09d208ffebef5bcfa49",
                    "user": {
                        "_id": "64c3726f2a5eaefd000cdedd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c3726f2a5eaefd000cdedd/iwFifH1sWQy7agW3eTmNQ.png",
                        "isPro": false,
                        "fullname": "Dan Busbridge",
                        "user": "dbusbridge",
                        "type": "user"
                    },
                    "name": "Dan Busbridge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:17:03.153Z",
                    "hidden": false
                },
                {
                    "_id": "6798d09d208ffebef5bcfa4a",
                    "name": "Alaaeldin Mohamed Elnouby Ali",
                    "hidden": false
                },
                {
                    "_id": "6798d09d208ffebef5bcfa4b",
                    "name": "Josh Susskind",
                    "hidden": false
                },
                {
                    "_id": "6798d09d208ffebef5bcfa4c",
                    "user": {
                        "_id": "6737e918edaf7e05e4b35791",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uk--P2mE2Jeg1QoxmXSTS.png",
                        "isPro": false,
                        "fullname": "Vimal Thilak",
                        "user": "vimalthilak",
                        "type": "user"
                    },
                    "name": "Vimal Thilak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T14:17:43.503Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-21T18:51:15.000Z",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for\n  Mixture-of-Experts Language Models",
            "summary": "Scaling the capacity of language models has consistently proven to be a\nreliable approach for improving performance and unlocking new capabilities.\nCapacity can be primarily defined by two dimensions: the number of model\nparameters and the compute per example. While scaling typically involves\nincreasing both, the precise interplay between these factors and their combined\ncontribution to overall capacity remains not fully understood. We explore this\nrelationship in the context of sparse Mixture-of-Experts (MoEs), which allow\nscaling the number of parameters without proportionally increasing the FLOPs\nper example. We investigate how varying the sparsity level, i.e., the fraction\nof inactive parameters, impacts model's performance during pretraining and\ndownstream few-shot evaluation. We find that under different constraints (e.g.,\nparameter size and total training compute), there is an optimal level of\nsparsity that improves both training efficiency and model performance. These\nresults provide a better understanding of the impact of sparsity in scaling\nlaws for MoEs and complement existing works in this area, offering insights for\ndesigning more efficient architectures.",
            "upvotes": 4,
            "discussionId": "6798d09e208ffebef5bcfa9c"
        },
        "publishedAt": "2025-01-28T07:42:14.777Z",
        "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12370.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "651e96991b97c9f33d26bde6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
            "fullname": "Elie Bakouch",
            "name": "eliebak",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 80
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.15420",
            "authors": [
                {
                    "_id": "67992c274c3dbd12f9c75abb",
                    "user": {
                        "_id": "65571135bfb62d747abc8129",
                        "avatarUrl": "/avatars/5f4542daa34597f17e6280b9cce18c91.svg",
                        "isPro": false,
                        "fullname": "Hugging",
                        "user": "ChenDRAG",
                        "type": "user"
                    },
                    "name": "Huayu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-28T19:24:41.148Z",
                    "hidden": false
                },
                {
                    "_id": "67992c274c3dbd12f9c75abc",
                    "name": "Kai Jiang",
                    "hidden": false
                },
                {
                    "_id": "67992c274c3dbd12f9c75abd",
                    "name": "Kaiwen Zheng",
                    "hidden": false
                },
                {
                    "_id": "67992c274c3dbd12f9c75abe",
                    "user": {
                        "_id": "65fcad0ba0d7adc40b54fac2",
                        "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
                        "isPro": false,
                        "fullname": "Jianfei Chen",
                        "user": "surfingtomchen",
                        "type": "user"
                    },
                    "name": "Jianfei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:43:24.216Z",
                    "hidden": true
                },
                {
                    "_id": "67992c274c3dbd12f9c75abf",
                    "name": "Hang Su",
                    "hidden": false
                },
                {
                    "_id": "67992c274c3dbd12f9c75ac0",
                    "name": "Jun Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T06:48:05.000Z",
            "title": "Visual Generation Without Guidance",
            "summary": "Classifier-Free Guidance (CFG) has been a default technique in various visual\ngenerative models, yet it requires inference from both conditional and\nunconditional models during sampling. We propose to build visual models that\nare free from guided sampling. The resulting algorithm, Guidance-Free Training\n(GFT), matches the performance of CFG while reducing sampling to a single\nmodel, halving the computational cost. Unlike previous distillation-based\napproaches that rely on pretrained CFG networks, GFT enables training directly\nfrom scratch. GFT is simple to implement. It retains the same maximum\nlikelihood objective as CFG and differs mainly in the parameterization of\nconditional models. Implementing GFT requires only minimal modifications to\nexisting codebases, as most design choices and hyperparameters are directly\ninherited from CFG. Our extensive experiments across five distinct visual\nmodels demonstrate the effectiveness and versatility of GFT. Across domains of\ndiffusion, autoregressive, and masked-prediction modeling, GFT consistently\nachieves comparable or even lower FID scores, with similar diversity-fidelity\ntrade-offs compared with CFG baselines, all while being guidance-free. Code\nwill be available at https://github.com/thu-ml/GFT.",
            "upvotes": 3,
            "discussionId": "67992c2a4c3dbd12f9c75b9a"
        },
        "publishedAt": "2025-01-28T14:17:04.887Z",
        "title": "Visual Generation Without Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15420.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65571135bfb62d747abc8129",
            "avatarUrl": "/avatars/5f4542daa34597f17e6280b9cce18c91.svg",
            "fullname": "Hugging",
            "name": "ChenDRAG",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.15427",
            "authors": [
                {
                    "_id": "67984dfa6e816a0edaa8d7b1",
                    "user": {
                        "_id": "657cd228138b7e391444a65d",
                        "avatarUrl": "/avatars/c7c984ae483144fab627aa2c54d91d0f.svg",
                        "isPro": false,
                        "fullname": "Xiaoyang Wang",
                        "user": "xywang1",
                        "type": "user"
                    },
                    "name": "Xiaoyang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:41:12.925Z",
                    "hidden": false
                },
                {
                    "_id": "67984dfa6e816a0edaa8d7b2",
                    "user": {
                        "_id": "64ed478bec06efeb03034933",
                        "avatarUrl": "/avatars/cd7dc3165831e90cb36d39d41c3c8157.svg",
                        "isPro": false,
                        "fullname": "Hongming Zhang",
                        "user": "Hongming98",
                        "type": "user"
                    },
                    "name": "Hongming Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:41:33.772Z",
                    "hidden": false
                },
                {
                    "_id": "67984dfa6e816a0edaa8d7b3",
                    "name": "Tao Ge",
                    "hidden": false
                },
                {
                    "_id": "67984dfa6e816a0edaa8d7b4",
                    "user": {
                        "_id": "5feab3a28a3201f8e554c969",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                        "isPro": false,
                        "fullname": "Wenhao Yu",
                        "user": "wyu1",
                        "type": "user"
                    },
                    "name": "Wenhao Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:42:23.986Z",
                    "hidden": false
                },
                {
                    "_id": "67984dfa6e816a0edaa8d7b5",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "67984dfa6e816a0edaa8d7b6",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T07:07:01.000Z",
            "title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale\n  Synthetic Personas",
            "summary": "Customizable role-playing in large language models (LLMs), also known as\ncharacter generalization, is gaining increasing attention for its versatility\nand cost-efficiency in developing and deploying role-playing dialogue agents.\nThis study explores a large-scale data synthesis approach to equip LLMs with\ncharacter generalization capabilities. We begin by synthesizing large-scale\ncharacter profiles using personas from Persona Hub and then explore two\nstrategies: response rewriting and response generation, to create\ncharacter-aligned instructional responses. To validate the effectiveness of our\nsynthetic instruction tuning data for character generalization, we perform\nsupervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing\nmodel strengthens the original LLaMA-3 8B Instruct model and achieves\nperformance comparable to GPT-4o models on role-playing dialogue. We release\nour synthetic characters and instruction-tuning dialogues to support public\nresearch.",
            "upvotes": 3,
            "discussionId": "67984dfb6e816a0edaa8d7de"
        },
        "publishedAt": "2025-01-28T12:39:34.021Z",
        "title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15427.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "657cd228138b7e391444a65d",
            "avatarUrl": "/avatars/c7c984ae483144fab627aa2c54d91d0f.svg",
            "fullname": "Xiaoyang Wang",
            "name": "xywang1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.16273",
            "authors": [
                {
                    "_id": "67984addd46e4d88ee27f43f",
                    "user": {
                        "_id": "67984a3a02ff123f680a15c6",
                        "avatarUrl": "/avatars/3595c8962e1739325ba03ead8f76d2e9.svg",
                        "isPro": false,
                        "fullname": "Mohamed Elfeki",
                        "user": "melfeki11",
                        "type": "user"
                    },
                    "name": "Mohamed Elfeki",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-28T10:10:17.395Z",
                    "hidden": false
                },
                {
                    "_id": "67984addd46e4d88ee27f440",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "67984addd46e4d88ee27f441",
                    "name": "Chad Voegele",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T18:06:36.000Z",
            "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
            "summary": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.",
            "upvotes": 2,
            "discussionId": "67984addd46e4d88ee27f47f"
        },
        "publishedAt": "2025-01-28T12:09:18.563Z",
        "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16273.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67984a3a02ff123f680a15c6",
            "avatarUrl": "/avatars/3595c8962e1739325ba03ead8f76d2e9.svg",
            "fullname": "Mohamed Elfeki",
            "name": "melfeki11",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.14912",
            "authors": [
                {
                    "_id": "67986d764fccd4b95149db0b",
                    "user": {
                        "_id": "65555c6c6947208b77271f1e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CQPL5xB-Y-F89BxJrB7tg.png",
                        "isPro": false,
                        "fullname": "Juan Ramírez",
                        "user": "juanramirezneilson",
                        "type": "user"
                    },
                    "name": "Juan Ramirez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T16:53:24.742Z",
                    "hidden": false
                },
                {
                    "_id": "67986d764fccd4b95149db0c",
                    "user": {
                        "_id": "6508b3f15b34509e16c78fea",
                        "avatarUrl": "/avatars/f663439c405354504b27f5ffab5c401a.svg",
                        "isPro": false,
                        "fullname": "Ignacio Hounie",
                        "user": "ihounie",
                        "type": "user"
                    },
                    "name": "Ignacio Hounie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:40:24.939Z",
                    "hidden": false
                },
                {
                    "_id": "67986d764fccd4b95149db0d",
                    "user": {
                        "_id": "65a04adddb5d37ad5e6b9d29",
                        "avatarUrl": "/avatars/071a137c2c1aad0699ee1b8b001e4a58.svg",
                        "isPro": false,
                        "fullname": "Elenter",
                        "user": "juanelenter",
                        "type": "user"
                    },
                    "name": "Juan Elenter",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:40:32.041Z",
                    "hidden": false
                },
                {
                    "_id": "67986d764fccd4b95149db0e",
                    "name": "Jose Gallego-Posada",
                    "hidden": false
                },
                {
                    "_id": "67986d764fccd4b95149db0f",
                    "name": "Meraj Hashemizadeh",
                    "hidden": false
                },
                {
                    "_id": "67986d764fccd4b95149db10",
                    "user": {
                        "_id": "6724c42c055ce33014feaec2",
                        "avatarUrl": "/avatars/8854289d890b555ca9562f95edeab784.svg",
                        "isPro": false,
                        "fullname": "Alejandro Ribeiro Prieto",
                        "user": "Prieto",
                        "type": "user"
                    },
                    "name": "Alejandro Ribeiro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T19:24:31.422Z",
                    "hidden": false
                },
                {
                    "_id": "67986d764fccd4b95149db11",
                    "user": {
                        "_id": "6406bcaca577649430c6bff4",
                        "avatarUrl": "/avatars/77b386172b0d86d40cddd7a7a8744491.svg",
                        "isPro": false,
                        "fullname": "Simon Lacoste-Julien",
                        "user": "slacoste",
                        "type": "user"
                    },
                    "name": "Simon Lacoste-Julien",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-28T16:53:32.537Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-24T20:39:38.000Z",
            "title": "Feasible Learning",
            "summary": "We introduce Feasible Learning (FL), a sample-centric learning paradigm where\nmodels are trained by solving a feasibility problem that bounds the loss for\neach training sample. In contrast to the ubiquitous Empirical Risk Minimization\n(ERM) framework, which optimizes for average performance, FL demands\nsatisfactory performance on every individual data point. Since any model that\nmeets the prescribed performance threshold is a valid FL solution, the choice\nof optimization algorithm and its dynamics play a crucial role in shaping the\nproperties of the resulting solutions. In particular, we study a primal-dual\napproach which dynamically re-weights the importance of each sample during\ntraining. To address the challenge of setting a meaningful threshold in\npractice, we introduce a relaxation of FL that incorporates slack variables of\nminimal norm. Our empirical analysis, spanning image classification, age\nregression, and preference optimization in large language models, demonstrates\nthat models trained via FL can learn from data while displaying improved tail\nbehavior compared to ERM, with only a marginal impact on average performance.",
            "upvotes": 2,
            "discussionId": "67986d784fccd4b95149db6b"
        },
        "publishedAt": "2025-01-28T00:39:11.423Z",
        "title": "Feasible Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14912.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5842
        },
        "isAuthorParticipating": false
    }
]