[
    {
        "paper": {
            "id": "2501.17703",
            "authors": [
                {
                    "_id": "679ae76cf211c66bd702f5d5",
                    "user": {
                        "_id": "636a35eff8d9af4aea181608",
                        "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
                        "isPro": false,
                        "fullname": "yubo",
                        "user": "ubowang",
                        "type": "user"
                    },
                    "name": "Yubo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T08:39:49.375Z",
                    "hidden": false
                },
                {
                    "_id": "679ae76cf211c66bd702f5d6",
                    "user": {
                        "_id": "6230d750d93e84e233882dbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Yue",
                        "user": "yuexiang96",
                        "type": "user"
                    },
                    "name": "Xiang Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T15:17:01.780Z",
                    "hidden": false
                },
                {
                    "_id": "679ae76cf211c66bd702f5d7",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-30T02:43:59.302Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-29T15:20:30.000Z",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than\n  Learning to Imitate",
            "summary": "Supervised Fine-Tuning (SFT) is commonly used to train language models to\nimitate annotated responses for given instructions. In this paper, we challenge\nthis paradigm and propose Critique Fine-Tuning (CFT), a strategy where models\nlearn to critique noisy responses rather than simply imitate correct ones.\nInspired by human learning processes that emphasize critical thinking, CFT\nencourages deeper analysis and nuanced understanding-traits often overlooked by\nstandard SFT. To validate the effectiveness of CFT, we construct a 50K-sample\ndataset from WebInstruct, using GPT-4o as the teacher to generate critiques in\nthe form of (input=[query; noisy response], output=critique). CFT on this\ndataset yields a consistent 4-10% improvement over SFT on six math benchmarks\nwith different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We\nfurther expand to MetaMath and NuminaMath datasets and observe similar gains\nover SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K\nsamples-matches or outperforms competitive models such as AceMath and\nQwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples.\nAblation studies show that CFT is robust to the source of noisy response and\nteacher critique model. Through these findings, we argue that critique-based\ntraining offers a more effective alternative to advance the reasoning of\nlanguage models.",
            "upvotes": 28,
            "discussionId": "679ae770f211c66bd702f697"
        },
        "publishedAt": "2025-01-29T21:51:11.227Z",
        "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17703.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "636a35eff8d9af4aea181608",
            "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
            "fullname": "yubo",
            "name": "ubowang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.17195",
            "authors": [
                {
                    "_id": "679ae7655c55250b48483742",
                    "user": {
                        "_id": "62571e9e0e0c97db812e3afb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662586587273-62571e9e0e0c97db812e3afb.jpeg",
                        "isPro": false,
                        "fullname": "Andrei Alexandru",
                        "user": "inwaves",
                        "type": "user"
                    },
                    "name": "Andrei Alexandru",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T15:17:07.941Z",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483743",
                    "user": {
                        "_id": "66e184e86048d62cd8fb4e52",
                        "avatarUrl": "/avatars/dc459c692fe9fce0911fa1229df0aeee.svg",
                        "isPro": false,
                        "fullname": "Antonia Calvi",
                        "user": "NinaCalvi",
                        "type": "user"
                    },
                    "name": "Antonia Calvi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:40:54.827Z",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483744",
                    "name": "Henry Broomfield",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483745",
                    "name": "Jackson Golden",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483746",
                    "user": {
                        "_id": "659fc8832cb13cede03047bb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659fc8832cb13cede03047bb/Wo_LjryGEJFnxXrOcokfE.jpeg",
                        "isPro": true,
                        "fullname": "kyle",
                        "user": "kaikaidai",
                        "type": "user"
                    },
                    "name": "Kyle Dai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T15:17:03.347Z",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483747",
                    "name": "Mathias Leys",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483748",
                    "user": {
                        "_id": "66d08d5c952f5e4e64bd6be0",
                        "avatarUrl": "/avatars/2fc3a6e3813718f0c001fb26337dab45.svg",
                        "isPro": false,
                        "fullname": "Maurice",
                        "user": "MauriceBurg",
                        "type": "user"
                    },
                    "name": "Maurice Burger",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T15:17:05.147Z",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b48483749",
                    "name": "Max Bartolo",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b4848374a",
                    "name": "Roman Engeler",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b4848374b",
                    "user": {
                        "_id": "633c4fb100732349209f2aad",
                        "avatarUrl": "/avatars/b44ccae4fb097284730291e4fcc47a24.svg",
                        "isPro": false,
                        "fullname": "Sashank Pisupati",
                        "user": "spisupat",
                        "type": "user"
                    },
                    "name": "Sashank Pisupati",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T15:17:06.579Z",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b4848374c",
                    "name": "Toby Drane",
                    "hidden": false
                },
                {
                    "_id": "679ae7655c55250b4848374d",
                    "name": "Young Sun Park",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T15:09:08.000Z",
            "title": "Atla Selene Mini: A General Purpose Evaluation Model",
            "summary": "We introduce Atla Selene Mini, a state-of-the-art small language\nmodel-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that\noutperforms the best SLMJs and GPT-4o-mini on overall performance across 11\nout-of-distribution benchmarks, spanning absolute scoring, classification, and\npairwise preference tasks. It is the highest-scoring 8B generative model on\nRewardBench, surpassing strong baselines like GPT-4o and specialized judges. To\nachieve this, we develop a principled data curation strategy that augments\npublic datasets with synthetically generated critiques and ensures high quality\nthrough filtering and dataset ablations. We train our model on a combined\ndirect preference optimization (DPO) and supervised fine-tuning (SFT) loss, and\nproduce a highly promptable evaluator that excels in real-world scenarios.\nSelene Mini shows dramatically improved zero-shot agreement with human expert\nevaluations on financial and medical industry datasets. It is also robust to\nvariations in prompt format. Preliminary results indicate that Selene Mini is\nthe top-ranking evaluator in a live, community-driven Judge Arena. We release\nthe model weights on HuggingFace\n(https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage\nwidespread community adoption.",
            "upvotes": 24,
            "discussionId": "679ae76b5c55250b484838e0"
        },
        "publishedAt": "2025-01-29T21:44:37.041Z",
        "title": "Atla Selene Mini: A General Purpose Evaluation Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17195.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5874
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.14334",
            "authors": [
                {
                    "_id": "679a7546805383520ce065af",
                    "user": {
                        "_id": "644156da1a80f6d83cb1667c",
                        "avatarUrl": "/avatars/106d30a576b0fb58118ac4333b17260b.svg",
                        "isPro": false,
                        "fullname": "Clement Desroches",
                        "user": "clementdesroches",
                        "type": "user"
                    },
                    "name": "Clément Desroches",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-29T21:06:17.418Z",
                    "hidden": false
                },
                {
                    "_id": "679a7546805383520ce065b0",
                    "user": {
                        "_id": "66221f6295e8f09a668f07f0",
                        "avatarUrl": "/avatars/f7c943996c814630ab5dcfaaaba01a83.svg",
                        "isPro": false,
                        "fullname": "Martin Chauvin",
                        "user": "Neyri56",
                        "type": "user"
                    },
                    "name": "Martin Chauvin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T09:38:17.235Z",
                    "hidden": false
                },
                {
                    "_id": "679a7546805383520ce065b1",
                    "name": "Louis Ladan",
                    "hidden": false
                },
                {
                    "_id": "679a7546805383520ce065b2",
                    "name": "Caroline Vateau",
                    "hidden": false
                },
                {
                    "_id": "679a7546805383520ce065b3",
                    "name": "Simon Gosset",
                    "hidden": false
                },
                {
                    "_id": "679a7546805383520ce065b4",
                    "name": "Philippe Cordier",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-24T08:58:49.000Z",
            "title": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts",
            "summary": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company's AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals.",
            "upvotes": 14,
            "discussionId": "679a7548805383520ce065f5"
        },
        "publishedAt": "2025-01-30T03:05:08.789Z",
        "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14334.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644156da1a80f6d83cb1667c",
            "avatarUrl": "/avatars/106d30a576b0fb58118ac4333b17260b.svg",
            "fullname": "Clement Desroches",
            "name": "clementdesroches",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.17749",
            "authors": [
                {
                    "_id": "679ae5eab898ac90bf4480b6",
                    "user": {
                        "_id": "657b3a44de028a439ea2ed9d",
                        "avatarUrl": "/avatars/9f05e8eb6809a0ce1b50cd1fc9b5a044.svg",
                        "isPro": false,
                        "fullname": "Aitor Arrieta",
                        "user": "aitorarrieta",
                        "type": "user"
                    },
                    "name": "Aitor Arrieta",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-30T08:45:20.561Z",
                    "hidden": false
                },
                {
                    "_id": "679ae5eab898ac90bf4480b7",
                    "name": "Miriam Ugarte",
                    "hidden": false
                },
                {
                    "_id": "679ae5eab898ac90bf4480b8",
                    "user": {
                        "_id": "65001514f322f9156663f096",
                        "avatarUrl": "/avatars/e8712f60d4e8b7c70ac02c532ad547ef.svg",
                        "isPro": false,
                        "fullname": "Pablo Valle",
                        "user": "pablovalle",
                        "type": "user"
                    },
                    "name": "Pablo Valle",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:39:30.629Z",
                    "hidden": false
                },
                {
                    "_id": "679ae5eab898ac90bf4480b9",
                    "user": {
                        "_id": "63527de67e4cc3135fd16651",
                        "avatarUrl": "/avatars/5eb8076d448d0b6746e256c24e1440e0.svg",
                        "isPro": false,
                        "fullname": "José Antonio Parejo Maestre",
                        "user": "japarejo",
                        "type": "user"
                    },
                    "name": "José Antonio Parejo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:39:06.958Z",
                    "hidden": false
                },
                {
                    "_id": "679ae5eab898ac90bf4480ba",
                    "user": {
                        "_id": "6790d642a1863df579840ae3",
                        "avatarUrl": "/avatars/a10a6f4af327c1bb67513c56d7f84820.svg",
                        "isPro": false,
                        "fullname": "Sergio Segura",
                        "user": "ssegura",
                        "type": "user"
                    },
                    "name": "Sergio Segura",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-30T02:37:35.516Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-29T16:36:53.000Z",
            "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the\n  Pre-Deployment Evaluation",
            "summary": "Large Language Models (LLMs) have become an integral part of our daily lives.\nHowever, they impose certain risks, including those that can harm individuals'\nprivacy, perpetuate biases and spread misinformation. These risks highlight the\nneed for robust safety mechanisms, ethical guidelines, and thorough testing to\nensure their responsible deployment. Safety of LLMs is a key property that\nneeds to be thoroughly tested prior the model to be deployed and accessible to\nthe general users. This paper reports the external safety testing experience\nconducted by researchers from Mondragon University and University of Seville on\nOpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing\nprogram. In particular, we apply our tool, ASTRAL, to automatically and\nsystematically generate up to date unsafe test inputs (i.e., prompts) that\nhelps us test and assess different safety categories of LLMs. We automatically\ngenerate and execute a total of 10,080 unsafe test input on a early o3-mini\nbeta version. After manually verifying the test cases classified as unsafe by\nASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We\nhighlight key insights and findings uncovered during the pre-deployment\nexternal testing phase of OpenAI's latest LLM.",
            "upvotes": 8,
            "discussionId": "679ae5f0b898ac90bf44826c"
        },
        "publishedAt": "2025-01-29T21:38:42.464Z",
        "title": "Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17749.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5874
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.17433",
            "authors": [
                {
                    "_id": "679b1319f87b99a2a7c41e36",
                    "user": {
                        "_id": "67325283b318faa97f7ae5f7",
                        "avatarUrl": "/avatars/2f83452768148b323c540c43ad695ee6.svg",
                        "isPro": false,
                        "fullname": "TianshengHuang",
                        "user": "TianshengHuang",
                        "type": "user"
                    },
                    "name": "Tiansheng Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-30T08:39:47.548Z",
                    "hidden": false
                },
                {
                    "_id": "679b1319f87b99a2a7c41e37",
                    "user": {
                        "_id": "6539cab119c3ef6679794706",
                        "avatarUrl": "/avatars/a88691ff5a547c7a1384edcc615c8209.svg",
                        "isPro": false,
                        "fullname": "Sihao Hu",
                        "user": "SihaoHu",
                        "type": "user"
                    },
                    "name": "Sihao Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:39:58.723Z",
                    "hidden": false
                },
                {
                    "_id": "679b1319f87b99a2a7c41e38",
                    "user": {
                        "_id": "647615b995a4dc98e58c24f2",
                        "avatarUrl": "/avatars/7f73999246526c1aef4d019d5f5595ad.svg",
                        "isPro": false,
                        "fullname": "Fatih Ilhan",
                        "user": "tawreos",
                        "type": "user"
                    },
                    "name": "Fatih Ilhan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:40:06.004Z",
                    "hidden": false
                },
                {
                    "_id": "679b1319f87b99a2a7c41e39",
                    "user": {
                        "_id": "65aae89948c718a57434db6f",
                        "avatarUrl": "/avatars/6c0fae8dafad9b9265098a9bc3bfc102.svg",
                        "isPro": false,
                        "fullname": "selim tekin",
                        "user": "sftekin25",
                        "type": "user"
                    },
                    "name": "Selim Furkan Tekin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:40:16.339Z",
                    "hidden": false
                },
                {
                    "_id": "679b1319f87b99a2a7c41e3a",
                    "user": {
                        "_id": "65c998005e17dbeaf147db84",
                        "avatarUrl": "/avatars/6fb47b1e095971b93ff7dcd10369f926.svg",
                        "isPro": false,
                        "fullname": "Ling Liu",
                        "user": "ling1119",
                        "type": "user"
                    },
                    "name": "Ling Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-30T09:40:37.075Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-29T06:24:58.000Z",
            "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing\n  Guardrail Moderation",
            "summary": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus",
            "upvotes": 6,
            "discussionId": "679b131bf87b99a2a7c41ede"
        },
        "publishedAt": "2025-01-30T01:30:18.013Z",
        "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67325283b318faa97f7ae5f7/1hJo5gEfGEXAwYB5a6yWY.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67325283b318faa97f7ae5f7/8SaMXA1izw5vcfwtU2Nhj.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17433.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67325283b318faa97f7ae5f7",
            "avatarUrl": "/avatars/2f83452768148b323c540c43ad695ee6.svg",
            "fullname": "TianshengHuang",
            "name": "TianshengHuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.15654",
            "authors": [
                {
                    "_id": "679a7a7ea3ffd2887d76a1e7",
                    "user": {
                        "_id": "637519809ef4e3b87a5e88fb",
                        "avatarUrl": "/avatars/4465c9194c17f9b5e5a5a5e88d4a4656.svg",
                        "isPro": false,
                        "fullname": "Jenna Russell",
                        "user": "jjrussell10",
                        "type": "user"
                    },
                    "name": "Jenna Russell",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-29T21:06:15.282Z",
                    "hidden": false
                },
                {
                    "_id": "679a7a7ea3ffd2887d76a1e8",
                    "name": "Marzena Karpinska",
                    "hidden": false
                },
                {
                    "_id": "679a7a7ea3ffd2887d76a1e9",
                    "name": "Mohit Iyyer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-26T19:31:34.000Z",
            "title": "People who frequently use ChatGPT for writing tasks are accurate and\n  robust detectors of AI-generated text",
            "summary": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext.",
            "upvotes": 5,
            "discussionId": "679a7a82a3ffd2887d76a32d"
        },
        "publishedAt": "2025-01-30T09:31:27.980Z",
        "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15654.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637519809ef4e3b87a5e88fb",
            "avatarUrl": "/avatars/4465c9194c17f9b5e5a5a5e88d4a4656.svg",
            "fullname": "Jenna Russell",
            "name": "jjrussell10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.15891",
            "authors": [
                {
                    "_id": "679c23c74ca5036d02b91927",
                    "name": "Hailong Guo",
                    "hidden": false
                },
                {
                    "_id": "679c23c74ca5036d02b91928",
                    "name": "Bohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "679c23c74ca5036d02b91929",
                    "name": "Yiren Song",
                    "hidden": false
                },
                {
                    "_id": "679c23c74ca5036d02b9192a",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "679c23c74ca5036d02b9192b",
                    "name": "Chuang Zhang",
                    "hidden": false
                },
                {
                    "_id": "679c23c74ca5036d02b9192c",
                    "name": "Jiaming Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T09:33:23.000Z",
            "title": "Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile\n  Virtual Clothing Tasks",
            "summary": "Image-based virtual try-on (VTON) aims to generate a virtual try-on result by\ntransferring an input garment onto a target person's image. However, the\nscarcity of paired garment-model data makes it challenging for existing methods\nto achieve high generalization and quality in VTON. Also, it limits the ability\nto generate mask-free try-ons. To tackle the data scarcity problem, approaches\nsuch as Stable Garment and MMTryon use a synthetic data strategy, effectively\nincreasing the amount of paired data on the model side. However, existing\nmethods are typically limited to performing specific try-on tasks and lack\nuser-friendliness. To enhance the generalization and controllability of VTON\ngeneration, we propose Any2AnyTryon, which can generate try-on results based on\ndifferent textual instructions and model garment images to meet various needs,\neliminating the reliance on masks, poses, or other conditions. Specifically, we\nfirst construct the virtual try-on dataset LAION-Garment, the largest known\nopen-source garment try-on dataset. Then, we introduce adaptive position\nembedding, which enables the model to generate satisfactory outfitted model\nimages or garment images based on input images of different sizes and\ncategories, significantly enhancing the generalization and controllability of\nVTON generation. In our experiments, we demonstrate the effectiveness of our\nAny2AnyTryon and compare it with existing methods. The results show that\nAny2AnyTryon enables flexible, controllable, and high-quality image-based\nvirtual try-on generation.https://logn-2024.github.io/Any2anyTryonProjectPage/",
            "upvotes": 4,
            "discussionId": "679c23cd4ca5036d02b91afd"
        },
        "publishedAt": "2025-01-30T20:14:03.298Z",
        "title": "Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "fullname": "bohan zeng",
            "name": "zbhpku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]