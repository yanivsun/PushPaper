[
    {
        "paper": {
            "id": "2502.02492",
            "authors": [
                {
                    "_id": "67a2ec904ea0e3138ac966f2",
                    "user": {
                        "_id": "6181c72cdcc1df2c9de8a4d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
                        "isPro": false,
                        "fullname": "Hila Chefer",
                        "user": "Hila",
                        "type": "user"
                    },
                    "name": "Hila Chefer",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-05T04:44:03.218Z",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f3",
                    "user": {
                        "_id": "6345b71843f4f2d2ed113355",
                        "avatarUrl": "/avatars/a497669a4c53a724c4f6ea615d1dda59.svg",
                        "isPro": false,
                        "fullname": "Uriel Singer",
                        "user": "urielsinger",
                        "type": "user"
                    },
                    "name": "Uriel Singer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-05T16:53:54.046Z",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f4",
                    "name": "Amit Zohar",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f5",
                    "name": "Yuval Kirstain",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f6",
                    "name": "Adam Polyak",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f7",
                    "name": "Yaniv Taigman",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f8",
                    "name": "Lior Wolf",
                    "hidden": false
                },
                {
                    "_id": "67a2ec904ea0e3138ac966f9",
                    "name": "Shelly Sheynin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T17:07:10.000Z",
            "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\n  Generation in Video Models",
            "summary": "Despite tremendous recent progress, generative video models still struggle to\ncapture real-world motion, dynamics, and physics. We show that this limitation\narises from the conventional pixel reconstruction objective, which biases\nmodels toward appearance fidelity at the expense of motion coherence. To\naddress this, we introduce VideoJAM, a novel framework that instills an\neffective motion prior to video generators, by encouraging the model to learn a\njoint appearance-motion representation. VideoJAM is composed of two\ncomplementary units. During training, we extend the objective to predict both\nthe generated pixels and their corresponding motion from a single learned\nrepresentation. During inference, we introduce Inner-Guidance, a mechanism that\nsteers the generation toward coherent motion by leveraging the model's own\nevolving motion prediction as a dynamic guidance signal. Notably, our framework\ncan be applied to any video model with minimal adaptations, requiring no\nmodifications to the training data or scaling of the model. VideoJAM achieves\nstate-of-the-art performance in motion coherence, surpassing highly competitive\nproprietary models while also enhancing the perceived visual quality of the\ngenerations. These findings emphasize that appearance and motion can be\ncomplementary and, when effectively integrated, enhance both the visual quality\nand the coherence of video generation. Project website:\nhttps://hila-chefer.github.io/videojam-paper.github.io/",
            "upvotes": 27,
            "discussionId": "67a2ec934ea0e3138ac9678e"
        },
        "publishedAt": "2025-02-04T23:46:17.626Z",
        "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "fullname": "Hila Chefer",
            "name": "Hila",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.01362",
            "authors": [
                {
                    "_id": "67a2ad6ac7caec9bf5a45e61",
                    "user": {
                        "_id": "672503c59f68afdd63cc81a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672503c59f68afdd63cc81a2/lw4ApCTwAKgt_uUyfSVRH.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Gushchin",
                        "user": "ngushchin",
                        "type": "user"
                    },
                    "name": "Nikita Gushchin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-05T10:14:26.177Z",
                    "hidden": false
                },
                {
                    "_id": "67a2ad6ac7caec9bf5a45e62",
                    "user": {
                        "_id": "656a2e59b4020389028dc85f",
                        "avatarUrl": "/avatars/6fda3bddc3cecba2894233bebb3de968.svg",
                        "isPro": false,
                        "fullname": "David Li",
                        "user": "kekchpek",
                        "type": "user"
                    },
                    "name": "David Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-05T10:14:24.236Z",
                    "hidden": false
                },
                {
                    "_id": "67a2ad6ac7caec9bf5a45e63",
                    "user": {
                        "_id": "64a42977250bfdecd9570a9e",
                        "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
                        "isPro": false,
                        "fullname": "Daniil Selikhanovych",
                        "user": "apryc1",
                        "type": "user"
                    },
                    "name": "Daniil Selikhanovych",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-05T10:17:09.668Z",
                    "hidden": false
                },
                {
                    "_id": "67a2ad6ac7caec9bf5a45e64",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                },
                {
                    "_id": "67a2ad6ac7caec9bf5a45e65",
                    "user": {
                        "_id": "62b6cc49752323892323bc04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
                        "isPro": true,
                        "fullname": "Dmitry Baranchuk",
                        "user": "dbaranchuk",
                        "type": "user"
                    },
                    "name": "Dmitry Baranchuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-05T10:17:26.518Z",
                    "hidden": false
                },
                {
                    "_id": "67a2ad6ac7caec9bf5a45e66",
                    "user": {
                        "_id": "67a31c9ae5b870d5157657db",
                        "avatarUrl": "/avatars/ca5fd356e3656e1beacb5a28ecaad5be.svg",
                        "isPro": false,
                        "fullname": "Alexander Korotin",
                        "user": "akorotin",
                        "type": "user"
                    },
                    "name": "Alexander Korotin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-05T10:17:33.816Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T13:56:03.000Z",
            "title": "Inverse Bridge Matching Distillation",
            "summary": "Learning diffusion bridge models is easy; making them fast and practical is\nan art. Diffusion bridge models (DBMs) are a promising extension of diffusion\nmodels for applications in image-to-image translation. However, like many\nmodern diffusion and flow models, DBMs suffer from the problem of slow\ninference. To address it, we propose a novel distillation technique based on\nthe inverse bridge matching formulation and derive the tractable objective to\nsolve it in practice. Unlike previously developed DBM distillation techniques,\nthe proposed method can distill both conditional and unconditional types of\nDBMs, distill models in a one-step generator, and use only the corrupted images\nfor training. We evaluate our approach for both conditional and unconditional\ntypes of bridge matching on a wide set of setups, including super-resolution,\nJPEG restoration, sketch-to-image, and other tasks, and show that our\ndistillation technique allows us to accelerate the inference of DBMs from 4x to\n100x and even provide better generation quality than used teacher model\ndepending on particular setup.",
            "upvotes": 22,
            "discussionId": "67a2ad70c7caec9bf5a45fb0"
        },
        "publishedAt": "2025-02-05T03:01:40.464Z",
        "title": "Inverse Bridge Matching Distillation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672503c59f68afdd63cc81a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672503c59f68afdd63cc81a2/lw4ApCTwAKgt_uUyfSVRH.jpeg",
            "fullname": "Nikita Gushchin",
            "name": "ngushchin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.01718",
            "authors": [
                {
                    "_id": "67a2d995c97974764a8c294c",
                    "name": "Huaye Zeng",
                    "hidden": false
                },
                {
                    "_id": "67a2d995c97974764a8c294d",
                    "user": {
                        "_id": "62567c86d444a9b5a0ec51c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
                        "isPro": false,
                        "fullname": "Dongfu Jiang",
                        "user": "DongfuJiang",
                        "type": "user"
                    },
                    "name": "Dongfu Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-05T10:14:14.136Z",
                    "hidden": false
                },
                {
                    "_id": "67a2d995c97974764a8c294e",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "67a2d995c97974764a8c294f",
                    "user": {
                        "_id": "65358802a920f38780b3248a",
                        "avatarUrl": "/avatars/9415510b598079973c2b0436ad12db9c.svg",
                        "isPro": false,
                        "fullname": "Ping Nie",
                        "user": "pingnieuk",
                        "type": "user"
                    },
                    "name": "Ping Nie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-05T10:18:51.897Z",
                    "hidden": false
                },
                {
                    "_id": "67a2d995c97974764a8c2950",
                    "name": "Xiaotong Chen",
                    "hidden": false
                },
                {
                    "_id": "67a2d995c97974764a8c2951",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T18:46:04.000Z",
            "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
            "summary": "Most progress in recent coder models has been driven by supervised\nfine-tuning (SFT), while the potential of reinforcement learning (RL) remains\nlargely unexplored, primarily due to the lack of reliable reward data/model in\nthe code domain. In this paper, we address this challenge by leveraging\nautomated large-scale test-case synthesis to enhance code model training.\nSpecifically, we design a pipeline that generates extensive (question,\ntest-cases) pairs from existing code data. Using these test cases, we construct\npreference pairs based on pass rates over sampled programs to train reward\nmodels with Bradley-Terry loss. It shows an average of 10-point improvement for\nLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through\nbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.\nFurthermore, we conduct reinforcement learning with both reward models and\ntest-case pass rewards, leading to consistent improvements across HumanEval,\nMBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\ntraining can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\%\nfor merely 80 optimization steps. We believe our results highlight the huge\npotential of reinforcement learning in coder models.",
            "upvotes": 15,
            "discussionId": "67a2d996c97974764a8c29a1"
        },
        "publishedAt": "2025-02-04T22:23:07.858Z",
        "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5952
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.02584",
            "authors": [
                {
                    "_id": "67a2d59fd5ad3369a66ff394",
                    "name": "Zongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "67a2d59fd5ad3369a66ff395",
                    "name": "Yao Tang",
                    "hidden": false
                },
                {
                    "_id": "67a2d59fd5ad3369a66ff396",
                    "name": "Xingcheng Yao",
                    "hidden": false
                },
                {
                    "_id": "67a2d59fd5ad3369a66ff397",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "67a2d59fd5ad3369a66ff398",
                    "name": "Ziniu Hu",
                    "hidden": false
                },
                {
                    "_id": "67a2d59fd5ad3369a66ff399",
                    "name": "Yizhou Sun",
                    "hidden": false
                },
                {
                    "_id": "67a2d59fd5ad3369a66ff39a",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T18:58:31.000Z",
            "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
            "summary": "Language agents have become a promising solution to complex interactive\ntasks. One of the key ingredients to the success of language agents is the\nreward model on the trajectory of the agentic workflow, which provides valuable\nguidance during training or inference. However, due to the lack of annotations\nof intermediate interactions, most existing works use an outcome reward model\nto optimize policies across entire trajectories. This may lead to sub-optimal\npolicies and hinder the overall performance. To address this, we propose QLASS\n(Q-guided Language Agent Stepwise Search), to automatically generate\nannotations by estimating Q-values in a stepwise manner for open language\nagents. By introducing a reasoning tree and performing process reward modeling,\nQLASS provides effective intermediate guidance for each step. With the stepwise\nguidance, we propose a Q-guided generation strategy to enable language agents\nto better adapt to long-term value, resulting in significant performance\nimprovement during model inference on complex interactive agent tasks. Notably,\neven with almost half the annotated data, QLASS retains strong performance,\ndemonstrating its efficiency in handling limited supervision. We also\nempirically demonstrate that QLASS can lead to more effective decision making\nthrough qualitative analysis. We will release our code and data.",
            "upvotes": 12,
            "discussionId": "67a2d5a0d5ad3369a66ff3d4"
        },
        "publishedAt": "2025-02-04T22:08:25.652Z",
        "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634e4670a51d5df8c2d92fce",
            "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
            "fullname": "Da Yin",
            "name": "DaYin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.02508",
            "authors": [
                {
                    "_id": "67a2d1f9bc9d072d9459e857",
                    "user": {
                        "_id": "6553c985a7aded0380b5f928",
                        "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
                        "isPro": false,
                        "fullname": "Maohao Shen",
                        "user": "maohaos2",
                        "type": "user"
                    },
                    "name": "Maohao Shen",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-05T03:00:33.470Z",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e858",
                    "name": "Guangtao Zeng",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e859",
                    "name": "Zhenting Qi",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e85a",
                    "name": "Zhang-Wei Hong",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e85b",
                    "name": "Zhenfang Chen",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e85c",
                    "name": "Wei Lu",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e85d",
                    "name": "Gregory Wornell",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e85e",
                    "name": "Subhro Das",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e85f",
                    "name": "David Cox",
                    "hidden": false
                },
                {
                    "_id": "67a2d1f9bc9d072d9459e860",
                    "name": "Chuang Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T17:26:58.000Z",
            "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search",
            "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced.",
            "upvotes": 9,
            "discussionId": "67a2d1fcbc9d072d9459e91b"
        },
        "publishedAt": "2025-02-04T21:55:09.693Z",
        "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60ad0de755f970745d4ec28d",
            "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
            "fullname": "GtZeng",
            "name": "chaoscodes",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.02589",
            "authors": [
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f0",
                    "user": {
                        "_id": "65ca9b1743207e438a95e90c",
                        "avatarUrl": "/avatars/8f7bde1c44d8e665a29ee08ce7fedfa4.svg",
                        "isPro": false,
                        "fullname": "Xueqing Deng",
                        "user": "xdeng77",
                        "type": "user"
                    },
                    "name": "Xueqing Deng",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-05T18:27:02.349Z",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f1",
                    "name": "Qihang Yu",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f2",
                    "name": "Ali Athar",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f3",
                    "name": "Chenglin Yang",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f4",
                    "name": "Linjie Yang",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f5",
                    "name": "Xiaojie Jin",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f6",
                    "name": "Xiaohui Shen",
                    "hidden": false
                },
                {
                    "_id": "67a3ad7447edcbb9e1f1e2f7",
                    "name": "Liang-Chieh Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T18:59:46.000Z",
            "title": "COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for\n  Fine-Grained Understanding and Generation",
            "summary": "This paper introduces the COCONut-PanCap dataset, created to enhance panoptic\nsegmentation and grounded image captioning. Building upon the COCO dataset with\nadvanced COCONut panoptic masks, this dataset aims to overcome limitations in\nexisting image-text datasets that often lack detailed, scene-comprehensive\ndescriptions. The COCONut-PanCap dataset incorporates fine-grained,\nregion-level captions grounded in panoptic segmentation masks, ensuring\nconsistency and improving the detail of generated captions. Through\nhuman-edited, densely annotated descriptions, COCONut-PanCap supports improved\ntraining of vision-language models (VLMs) for image understanding and\ngenerative models for text-to-image tasks. Experimental results demonstrate\nthat COCONut-PanCap significantly boosts performance across understanding and\ngeneration tasks, offering complementary benefits to large-scale datasets. This\ndataset sets a new benchmark for evaluating models on joint panoptic\nsegmentation and grounded captioning tasks, addressing the need for\nhigh-quality, detailed image-text annotations in multi-modal learning.",
            "upvotes": 7,
            "discussionId": "67a3ad7647edcbb9e1f1e378"
        },
        "publishedAt": "2025-02-05T13:27:36.138Z",
        "title": "COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02589.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ca9b1743207e438a95e90c",
            "avatarUrl": "/avatars/8f7bde1c44d8e665a29ee08ce7fedfa4.svg",
            "fullname": "Xueqing Deng",
            "name": "xdeng77",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.01941",
            "authors": [
                {
                    "_id": "67a2e2a02dd2adbc88755a47",
                    "user": {
                        "_id": "63024676056ec3a2a8714b24",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Liu",
                        "user": "Dominic789654",
                        "type": "user"
                    },
                    "name": "Xiang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-05T10:12:48.427Z",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a48",
                    "name": "Zhenheng Tang",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a49",
                    "name": "Hong Chen",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a4a",
                    "name": "Peijie Dong",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a4b",
                    "name": "Zeyu Li",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a4c",
                    "name": "Xiuze Zhou",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a4d",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a4e",
                    "name": "Xuming Hu",
                    "hidden": false
                },
                {
                    "_id": "67a2e2a02dd2adbc88755a4f",
                    "name": "Xiaowen Chu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T02:23:06.000Z",
            "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
            "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of 17.4%-43.3%. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only 9.67%-25.53% performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves 9%-18%\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
            "upvotes": 7,
            "discussionId": "67a2e2a22dd2adbc88755ab4"
        },
        "publishedAt": "2025-02-04T23:04:25.888Z",
        "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "fullname": "Xiang Liu",
            "name": "Dominic789654",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.00674",
            "authors": [
                {
                    "_id": "67a362c9b9a2bb11fdba4b9f",
                    "name": "Wenzhe Li",
                    "hidden": false
                },
                {
                    "_id": "67a362c9b9a2bb11fdba4ba0",
                    "name": "Yong Lin",
                    "hidden": false
                },
                {
                    "_id": "67a362c9b9a2bb11fdba4ba1",
                    "name": "Mengzhou Xia",
                    "hidden": false
                },
                {
                    "_id": "67a362c9b9a2bb11fdba4ba2",
                    "name": "Chi Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-02T05:23:29.000Z",
            "title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models\n  Beneficial?",
            "summary": "Ensembling outputs from diverse sources is a straightforward yet effective\napproach to boost performance. Mixture-of-Agents (MoA) is one such popular\nensemble method that aggregates outputs from multiple different Large Language\nModels (LLMs). This paper raises the question in the context of language\nmodels: is mixing different LLMs truly beneficial? We propose Self-MoA -- an\nensemble method that aggregates outputs from only the single top-performing\nLLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms\nstandard MoA that mixes different LLMs in a large number of scenarios: Self-MoA\nachieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an\naverage of 3.8% improvement across various benchmarks, including MMLU, CRUX,\nand MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0\ndirectly achieves the new state-of-the-art performance on the leaderboard. To\nunderstand the effectiveness of Self-MoA, we systematically investigate the\ntrade-off between diversity and quality of outputs under various MoA settings.\nWe confirm that the MoA performance is rather sensitive to the quality, and\nmixing different LLMs often lowers the average quality of the models. To\ncomplement the study, we identify the scenarios where mixing different LLMs\ncould be helpful. This paper further introduces a sequential version of\nSelf-MoA, that is capable of aggregating a large number of LLM outputs\non-the-fly over multiple rounds, and is as effective as aggregating all outputs\nat once.",
            "upvotes": 6,
            "discussionId": "67a362cab9a2bb11fdba4bdc"
        },
        "publishedAt": "2025-02-05T08:09:02.787Z",
        "title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00674.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "fullname": "Asankhaya Sharma",
            "name": "codelion",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 51
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.19066",
            "authors": [
                {
                    "_id": "67a0f59c5685d37e28880943",
                    "user": {
                        "_id": "64bcc06fb567ae97c3272d3d",
                        "avatarUrl": "/avatars/bcb61fe9e575154d84913a1501971f1a.svg",
                        "isPro": false,
                        "fullname": "kim",
                        "user": "dahyekim",
                        "type": "user"
                    },
                    "name": "Dahye Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-03T16:59:53.099Z",
                    "hidden": false
                },
                {
                    "_id": "67a0f59c5685d37e28880944",
                    "name": "Deepti Ghadiyaram",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-31T11:52:47.000Z",
            "title": "Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable\n  Generations",
            "summary": "Despite the remarkable progress in text-to-image generative models, they are\nprone to adversarial attacks and inadvertently generate unsafe, unethical\ncontent. Existing approaches often rely on fine-tuning models to remove\nspecific concepts, which is computationally expensive, lack scalability, and/or\ncompromise generation quality. In this work, we propose a novel framework\nleveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable\nconcept manipulation in diffusion models. Specifically, we first identify\ninterpretable monosemantic concepts in the latent space of text embeddings and\nleverage them to precisely steer the generation away or towards a given concept\n(e.g., nudity) or to introduce a new concept (e.g., photographic style).\nThrough extensive experiments, we demonstrate that our approach is very simple,\nrequires no retraining of the base model nor LoRA adapters, does not compromise\nthe generation quality, and is robust to adversarial prompt manipulations. Our\nmethod yields an improvement of 20.01% in unsafe concept removal,\nis effective in style manipulation, and is sim5x faster than\ncurrent state-of-the-art.",
            "upvotes": 6,
            "discussionId": "67a0f5a05685d37e28880a1e"
        },
        "publishedAt": "2025-02-05T07:44:45.130Z",
        "title": "Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable Generations",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19066.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bcc06fb567ae97c3272d3d",
            "avatarUrl": "/avatars/bcb61fe9e575154d84913a1501971f1a.svg",
            "fullname": "kim",
            "name": "dahyekim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.01720",
            "authors": [
                {
                    "_id": "67a2fddb4044bf1c86f765a3",
                    "user": {
                        "_id": "62f6a894c3372328414c7021",
                        "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
                        "isPro": false,
                        "fullname": "Nupur Kumari",
                        "user": "nupurkmr9",
                        "type": "user"
                    },
                    "name": "Nupur Kumari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-05T13:37:30.825Z",
                    "hidden": false
                },
                {
                    "_id": "67a2fddb4044bf1c86f765a4",
                    "name": "Xi Yin",
                    "hidden": false
                },
                {
                    "_id": "67a2fddb4044bf1c86f765a5",
                    "name": "Jun-Yan Zhu",
                    "hidden": false
                },
                {
                    "_id": "67a2fddb4044bf1c86f765a6",
                    "name": "Ishan Misra",
                    "hidden": false
                },
                {
                    "_id": "67a2fddb4044bf1c86f765a7",
                    "name": "Samaneh Azadi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T18:59:41.000Z",
            "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
            "summary": "Customization of text-to-image models enables users to insert custom concepts\nand generate the concepts in unseen settings. Existing methods either rely on\ncostly test-time optimization or train encoders on single-image training\ndatasets without multi-image supervision, leading to worse image quality. We\npropose a simple approach that addresses both limitations. We first leverage\nexisting text-to-image models and 3D datasets to create a high-quality\nSynthetic Customization Dataset (SynCD) consisting of multiple images of the\nsame object in different lighting, backgrounds, and poses. We then propose a\nnew encoder architecture based on shared attention mechanisms that better\nincorporate fine-grained visual details from input images. Finally, we propose\na new inference technique that mitigates overexposure issues during inference\nby normalizing the text and image guidance vectors. Through extensive\nexperiments, we show that our model, trained on the synthetic dataset with the\nproposed encoder and inference algorithm, outperforms existing tuning-free\nmethods on standard customization benchmarks.",
            "upvotes": 2,
            "discussionId": "67a2fde34044bf1c86f767ba"
        },
        "publishedAt": "2025-02-05T00:59:11.275Z",
        "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f6a894c3372328414c7021",
            "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
            "fullname": "Nupur Kumari",
            "name": "nupurkmr9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2501.19389",
            "authors": [
                {
                    "_id": "67a2a05be5b870d51558fc00",
                    "user": {
                        "_id": "671ff4124b2e5a664aae01e1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8PQkyFF-fc9W2K8uArMXn.png",
                        "isPro": false,
                        "fullname": "Wenzhi Fang",
                        "user": "wenzhifang",
                        "type": "user"
                    },
                    "name": "Wenzhi Fang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-04T23:18:52.300Z",
                    "hidden": false
                },
                {
                    "_id": "67a2a05be5b870d51558fc01",
                    "name": "Dong-Jun Han",
                    "hidden": false
                },
                {
                    "_id": "67a2a05be5b870d51558fc02",
                    "name": "Liangqi Yuan",
                    "hidden": false
                },
                {
                    "_id": "67a2a05be5b870d51558fc03",
                    "name": "Seyyedali Hosseinalipour",
                    "hidden": false
                },
                {
                    "_id": "67a2a05be5b870d51558fc04",
                    "name": "Christopher G. Brinton",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-31T18:44:35.000Z",
            "title": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models",
            "summary": "Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines.",
            "upvotes": 1,
            "discussionId": "67a2a05ce5b870d51558fc57"
        },
        "publishedAt": "2025-02-05T15:45:57.451Z",
        "title": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19389.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "671ff4124b2e5a664aae01e1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8PQkyFF-fc9W2K8uArMXn.png",
            "fullname": "Wenzhi Fang",
            "name": "wenzhifang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.01839",
            "authors": [
                {
                    "_id": "67a394a6049991184002e7f4",
                    "user": {
                        "_id": "66824dacdd73c6dd2996c166",
                        "avatarUrl": "/avatars/7c43ccca705bfb608c8d46b68f62a89d.svg",
                        "isPro": false,
                        "fullname": "Eric",
                        "user": "ericzhao28",
                        "type": "user"
                    },
                    "name": "Eric Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-05T16:53:58.892Z",
                    "hidden": false
                },
                {
                    "_id": "67a394a6049991184002e7f5",
                    "name": "Pranjal Awasthi",
                    "hidden": false
                },
                {
                    "_id": "67a394a6049991184002e7f6",
                    "name": "Sreenivas Gollapudi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T21:31:07.000Z",
            "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling\n  Verification",
            "summary": "Sampling-based search, a simple paradigm for utilizing test-time compute,\ninvolves generating multiple candidate responses and selecting the best one --\ntypically by verifying each response for correctness. In this paper, we study\nthe scaling trends governing sampling-based search. Among our findings is that\nsimply scaling up a minimalist implementation that uses only random sampling\nand direct self-verification results in sustained performance improvements\nthat, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities\npast that of o1-Preview on popular benchmarks. We partially attribute the\nscalability of sampling-based search to a phenomenon of implicit scaling, where\nsampling a larger pool of responses in turn improves verification accuracy. We\nfurther identify two useful principles for improving self-verification\ncapabilities with test-time compute: (1) comparing across responses provides\nhelpful signals about the locations of errors and hallucinations, and (2)\ndifferent model output styles are useful for different contexts -- chains of\nthought are useful for reasoning but harder to verify. We also find that,\nthough accurate verification can be elicited, frontier models demonstrate\nremarkably weak out-of-box verification capabilities and introduce a benchmark\nto measure progress on these deficiencies.",
            "upvotes": 1,
            "discussionId": "67a394a7049991184002e82d"
        },
        "publishedAt": "2025-02-05T12:16:39.189Z",
        "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01839.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66824dacdd73c6dd2996c166",
            "avatarUrl": "/avatars/7c43ccca705bfb608c8d46b68f62a89d.svg",
            "fullname": "Eric",
            "name": "ericzhao28",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.00840",
            "authors": [
                {
                    "_id": "67a3e9fc2955dee2f54fb307",
                    "name": "Jiawen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb308",
                    "name": "Kejia Chen",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb309",
                    "user": {
                        "_id": "6433707307bad11484af1d2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
                        "isPro": false,
                        "fullname": "Lipeng (Tony) He",
                        "user": "ttttonyhe",
                        "type": "user"
                    },
                    "name": "Lipeng He",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-05T22:45:18.208Z",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb30a",
                    "name": "Jian Lou",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb30b",
                    "name": "Dan Li",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb30c",
                    "name": "Zunlei Feng",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb30d",
                    "name": "Mingli Song",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb30e",
                    "name": "Jian Liu",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb30f",
                    "name": "Kui Ren",
                    "hidden": false
                },
                {
                    "_id": "67a3e9fc2955dee2f54fb310",
                    "name": "Xiaohu Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-02T16:25:48.000Z",
            "title": "Activation Approximations Can Incur Safety Vulnerabilities Even in\n  Aligned LLMs: Comprehensive Analysis and Defense",
            "summary": "Large Language Models (LLMs) have showcased remarkable capabilities across\nvarious domains. Accompanying the evolving capabilities and expanding\ndeployment scenarios of LLMs, their deployment challenges escalate due to their\nsheer scale and the advanced yet complex activation designs prevalent in\nnotable model series, such as Llama, Gemma, and Mistral. These challenges have\nbecome particularly pronounced in resource-constrained deployment scenarios,\nwhere mitigating inference efficiency bottlenecks is imperative. Among various\nrecent efforts, activation approximation has emerged as a promising avenue for\npursuing inference efficiency, sometimes considered indispensable in\napplications such as private inference. Despite achieving substantial speedups\nwith minimal impact on utility, even appearing sound and practical for\nreal-world deployment, the safety implications of activation approximations\nremain unclear. In this work, we fill this critical gap in LLM safety by\nconducting the first systematic safety evaluation of activation approximations.\nOur safety vetting spans seven sota techniques across three popular categories,\nrevealing consistent safety degradation across ten safety-aligned LLMs.",
            "upvotes": 0,
            "discussionId": "67a3e9fe2955dee2f54fb36e"
        },
        "publishedAt": "2025-02-05T17:48:01.059Z",
        "title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00840.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6433707307bad11484af1d2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433707307bad11484af1d2a/w5zB-zstJzY561n6q7m4D.jpeg",
            "fullname": "Lipeng (Tony) He",
            "name": "ttttonyhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]