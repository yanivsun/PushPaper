[
    {
        "paper": {
            "id": "2502.03032",
            "authors": [
                {
                    "_id": "67a59c4e7ffacd843a56404a",
                    "user": {
                        "_id": "634c5f8cfb80cc6bcaf42c03",
                        "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
                        "isPro": false,
                        "fullname": "Daniil Laptev",
                        "user": "dlaptev",
                        "type": "user"
                    },
                    "name": "Daniil Laptev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:04.546Z",
                    "hidden": false
                },
                {
                    "_id": "67a59c4e7ffacd843a56404b",
                    "user": {
                        "_id": "60b364e7f88532cd79eaff7b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Balagansky",
                        "user": "elephantmipt",
                        "type": "user"
                    },
                    "name": "Nikita Balagansky",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:02.693Z",
                    "hidden": false
                },
                {
                    "_id": "67a59c4e7ffacd843a56404c",
                    "name": "Yaroslav Aksenov",
                    "hidden": false
                },
                {
                    "_id": "67a59c4e7ffacd843a56404d",
                    "user": {
                        "_id": "62a9c8edc19f92ae443ab37f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                        "isPro": false,
                        "fullname": "Daniil Gavrilov",
                        "user": "kefirski",
                        "type": "user"
                    },
                    "name": "Daniil Gavrilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:06.718Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T09:39:34.000Z",
            "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language\n  Models",
            "summary": "We introduce a new approach to systematically map features discovered by\nsparse autoencoder across consecutive layers of large language models,\nextending earlier work that examined inter-layer feature links. By using a\ndata-free cosine similarity technique, we trace how specific features persist,\ntransform, or first appear at each stage. This method yields granular flow\ngraphs of feature evolution, enabling fine-grained interpretability and\nmechanistic insights into model computations. Crucially, we demonstrate how\nthese cross-layer feature maps facilitate direct steering of model behavior by\namplifying or suppressing chosen features, achieving targeted thematic control\nin text generation. Together, our findings highlight the utility of a causal,\ncross-layer interpretability framework that not only clarifies how features\ndevelop through forward passes but also provides new means for transparent\nmanipulation of large language models.",
            "upvotes": 41,
            "discussionId": "67a59c4f7ffacd843a56408f"
        },
        "publishedAt": "2025-02-07T01:29:53.798Z",
        "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03032.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "fullname": "Daniil Gavrilov",
            "name": "kefirski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.03621",
            "authors": [
                {
                    "_id": "67a59e5298f41a0460ee5282",
                    "user": {
                        "_id": "6301d8324ccccaa23d3864f4",
                        "avatarUrl": "/avatars/148b1b1d1460e26f03a1f2ce0feacf78.svg",
                        "isPro": false,
                        "fullname": "Danah Yatim",
                        "user": "DanahY",
                        "type": "user"
                    },
                    "name": "Danah Yatim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:14.464Z",
                    "hidden": false
                },
                {
                    "_id": "67a59e5298f41a0460ee5283",
                    "user": {
                        "_id": "62627f3c02cd5952e013c843",
                        "avatarUrl": "/avatars/1d76689d75d670630b6fa0307309c31f.svg",
                        "isPro": false,
                        "fullname": "Rafail Fridman",
                        "user": "RafailFridman",
                        "type": "user"
                    },
                    "name": "Rafail Fridman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:21.549Z",
                    "hidden": false
                },
                {
                    "_id": "67a59e5298f41a0460ee5284",
                    "user": {
                        "_id": "62e29044a133a252b5cf70b2",
                        "avatarUrl": "/avatars/6d09ddcba9bc47c309150a8d77815891.svg",
                        "isPro": false,
                        "fullname": "Omer Bar-Tal",
                        "user": "omerbartal",
                        "type": "user"
                    },
                    "name": "Omer Bar-Tal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:28.382Z",
                    "hidden": false
                },
                {
                    "_id": "67a59e5298f41a0460ee5285",
                    "user": {
                        "_id": "631cddec68f7da9ad24f6fc7",
                        "avatarUrl": "/avatars/7d4f1ce805e5889ca6594bd4a93f2583.svg",
                        "isPro": false,
                        "fullname": "Tali Dekel",
                        "user": "talidekel",
                        "type": "user"
                    },
                    "name": "Tali Dekel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:34.275Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T21:14:55.000Z",
            "title": "DynVFX: Augmenting Real Videos with Dynamic Content",
            "summary": "We present a method for augmenting real-world videos with newly generated\ndynamic content. Given an input video and a simple user-provided text\ninstruction describing the desired content, our method synthesizes dynamic\nobjects or complex scene effects that naturally interact with the existing\nscene over time. The position, appearance, and motion of the new content are\nseamlessly integrated into the original footage while accounting for camera\nmotion, occlusions, and interactions with other dynamic objects in the scene,\nresulting in a cohesive and realistic output video. We achieve this via a\nzero-shot, training-free framework that harnesses a pre-trained text-to-video\ndiffusion transformer to synthesize the new content and a pre-trained Vision\nLanguage Model to envision the augmented scene in detail. Specifically, we\nintroduce a novel inference-based method that manipulates features within the\nattention mechanism, enabling accurate localization and seamless integration of\nthe new content while preserving the integrity of the original scene. Our\nmethod is fully automated, requiring only a simple user instruction. We\ndemonstrate its effectiveness on a wide range of edits applied to real-world\nvideos, encompassing diverse objects and scenarios involving both camera and\nobject motion.",
            "upvotes": 18,
            "discussionId": "67a59e5798f41a0460ee5389"
        },
        "publishedAt": "2025-02-07T00:48:49.217Z",
        "title": "DynVFX: Augmenting Real Videos with Dynamic Content",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03621.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "fullname": "Hila Chefer",
            "name": "Hila",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04153",
            "authors": [
                {
                    "_id": "67a57b1fdea89ffe80d9fe56",
                    "user": {
                        "_id": "66c89152d33e34fbc29497d7",
                        "avatarUrl": "/avatars/bbddabf6532393951c4759e5915a065b.svg",
                        "isPro": false,
                        "fullname": "KaikaiAn",
                        "user": "kkk-an",
                        "type": "user"
                    },
                    "name": "Kaikai An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:18.320Z",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe57",
                    "name": "Li Sheng",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe58",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:31:05.333Z",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe59",
                    "user": {
                        "_id": "637c99bbfe115289cfedfb44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
                        "isPro": false,
                        "fullname": "ssz",
                        "user": "ssz1111",
                        "type": "user"
                    },
                    "name": "Shuzheng Si",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:16.229Z",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe5a",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe5b",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "67a57b1fdea89ffe80d9fe5c",
                    "name": "Baobao Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T15:39:16.000Z",
            "title": "UltraIF: Advancing Instruction Following from the Wild",
            "summary": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF.",
            "upvotes": 15,
            "discussionId": "67a57b1fdea89ffe80d9fe93"
        },
        "publishedAt": "2025-02-06T22:27:51.425Z",
        "title": "UltraIF: Advancing Instruction Following from the Wild",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04153.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66c89152d33e34fbc29497d7",
            "avatarUrl": "/avatars/bbddabf6532393951c4759e5915a065b.svg",
            "fullname": "KaikaiAn",
            "name": "kkk-an",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04313",
            "authors": [
                {
                    "_id": "67a5b9107897c8f5406155e0",
                    "user": {
                        "_id": "6506832221ac448013f94995",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
                        "isPro": false,
                        "fullname": "Shashwat Goel",
                        "user": "shash42",
                        "type": "user"
                    },
                    "name": "Shashwat Goel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:39:36.508Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e1",
                    "user": {
                        "_id": "6728c6113d35dd53cfe9f30c",
                        "avatarUrl": "/avatars/7f93b9d41446cce382f63c78ca5059a1.svg",
                        "isPro": false,
                        "fullname": "Joschka Strüber",
                        "user": "Klingspor",
                        "type": "user"
                    },
                    "name": "Joschka Struber",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:39:43.250Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e2",
                    "user": {
                        "_id": "671b49503fd1d03dc69194b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnkR0j1VaWClUcumXcgjQ.png",
                        "isPro": false,
                        "fullname": "Ilze Amanda Auzina",
                        "user": "iaa01",
                        "type": "user"
                    },
                    "name": "Ilze Amanda Auzina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:04.242Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e3",
                    "name": "Karuna K Chandra",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e4",
                    "name": "Ponnurangam Kumaraguru",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e5",
                    "user": {
                        "_id": "61dc997715b47073db1620dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1641847245435-61dc997715b47073db1620dc.jpeg",
                        "isPro": false,
                        "fullname": "Douwe Kiela",
                        "user": "douwekiela",
                        "type": "user"
                    },
                    "name": "Douwe Kiela",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:22.153Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e6",
                    "user": {
                        "_id": "6464a0d41683d3c81f51924a",
                        "avatarUrl": "/avatars/bfa89f568302fa34a641e0d8744bf8b5.svg",
                        "isPro": false,
                        "fullname": "Ameya Prabhu",
                        "user": "AmeyaPrabhu",
                        "type": "user"
                    },
                    "name": "Ameya Prabhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:34.763Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e7",
                    "name": "Matthias Bethge",
                    "hidden": false
                },
                {
                    "_id": "67a5b9107897c8f5406155e8",
                    "user": {
                        "_id": "63d86dbf3130cadcaf8bdd11",
                        "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                        "isPro": false,
                        "fullname": "Jonas Geiping",
                        "user": "JonasGeiping",
                        "type": "user"
                    },
                    "name": "Jonas Geiping",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:40:49.233Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:56:01.000Z",
            "title": "Great Models Think Alike and this Undermines AI Oversight",
            "summary": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight.",
            "upvotes": 14,
            "discussionId": "67a5b9137897c8f540615673"
        },
        "publishedAt": "2025-02-07T02:46:29.675Z",
        "title": "Great Models Think Alike and this Undermines AI Oversight",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6506832221ac448013f94995/pXBCc2dpWXCw6JinTbiFP.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04313.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6506832221ac448013f94995",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
            "fullname": "Shashwat Goel",
            "name": "shash42",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04328",
            "authors": [
                {
                    "_id": "67a586fad177de2eeba7de7b",
                    "user": {
                        "_id": "64f001bfabd9fb1914398bd5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
                        "isPro": false,
                        "fullname": "liuzuyan",
                        "user": "Zuyan",
                        "type": "user"
                    },
                    "name": "Zuyan Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:10.679Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7c",
                    "user": {
                        "_id": "652965773a416e1f2173443b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
                        "isPro": false,
                        "fullname": "Yuhao Dong",
                        "user": "THUdyh",
                        "type": "user"
                    },
                    "name": "Yuhao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:37:45.556Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7d",
                    "name": "Jiahui Wang",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7e",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:14.852Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de7f",
                    "user": {
                        "_id": "63673bb9d0ee6e2662be0ec1",
                        "avatarUrl": "/avatars/1b8976785d64bc4e3f7159ccdb7f06c5.svg",
                        "isPro": false,
                        "fullname": "Qingqiao Hu",
                        "user": "WinstonHu",
                        "type": "user"
                    },
                    "name": "Winston Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:23.468Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de80",
                    "user": {
                        "_id": "66c44203ea476bea05e9fcd7",
                        "avatarUrl": "/avatars/b061eebec609446e669f5ad6365959f9.svg",
                        "isPro": false,
                        "fullname": "lu",
                        "user": "jiwenlu",
                        "type": "user"
                    },
                    "name": "Jiwen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:29.456Z",
                    "hidden": false
                },
                {
                    "_id": "67a586fad177de2eeba7de81",
                    "user": {
                        "_id": "63e4865354f51ea342d45d78",
                        "avatarUrl": "/avatars/2e7eccc878751331ca8b282f53e38899.svg",
                        "isPro": false,
                        "fullname": "Yongming Rao",
                        "user": "raoyongming",
                        "type": "user"
                    },
                    "name": "Yongming Rao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:35.766Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:59:55.000Z",
            "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment",
            "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.",
            "upvotes": 14,
            "discussionId": "67a586fbd177de2eeba7deae"
        },
        "publishedAt": "2025-02-07T00:54:43.254Z",
        "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04328.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64f001bfabd9fb1914398bd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg",
            "fullname": "liuzuyan",
            "name": "Zuyan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.03544",
            "authors": [
                {
                    "_id": "67a589ebb16fabcdd2dea1eb",
                    "name": "Yuri Chervonyi",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1ec",
                    "name": "Trieu H. Trinh",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1ed",
                    "name": "Miroslav Olšák",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1ee",
                    "name": "Xiaomeng Yang",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1ef",
                    "name": "Hoang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1f0",
                    "user": {
                        "_id": "60cc0c3494ab6115ab6ecf12",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1623985197562-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Marcelo Menegali",
                        "user": "mmenegali",
                        "type": "user"
                    },
                    "name": "Marcelo Menegali",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:41:38.940Z",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1f1",
                    "name": "Junehyuk Jung",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1f2",
                    "name": "Vikas Verma",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1f3",
                    "name": "Quoc V. Le",
                    "hidden": false
                },
                {
                    "_id": "67a589ebb16fabcdd2dea1f4",
                    "user": {
                        "_id": "65ee0b97306927c125d65779",
                        "avatarUrl": "/avatars/637129308a95efdf8faac9fb81a66589.svg",
                        "isPro": false,
                        "fullname": "Thang Luong",
                        "user": "lmthang",
                        "type": "user"
                    },
                    "name": "Thang Luong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:42:08.011Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T19:02:03.000Z",
            "title": "Gold-medalist Performance in Solving Olympiad Geometry with\n  AlphaGeometry2",
            "summary": "We present AlphaGeometry2, a significantly improved version of AlphaGeometry\nintroduced in Trinh et al. (2024), which has now surpassed an average gold\nmedalist in solving Olympiad geometry problems. To achieve this, we first\nextend the original AlphaGeometry language to tackle harder problems involving\nmovements of objects, and problems containing linear equations of angles,\nratios, and distances. This, together with other additions, has markedly\nimproved the coverage rate of the AlphaGeometry language on International Math\nOlympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process\nof AlphaGeometry2 has also been greatly improved through the use of Gemini\narchitecture for better language modeling, and a novel knowledge-sharing\nmechanism that combines multiple search trees. Together with further\nenhancements to the symbolic engine and synthetic data generation, we have\nsignificantly boosted the overall solving rate of AlphaGeometry2 to 84% for\nall geometry problems over the last 25 years, compared to 54%\npreviously. AlphaGeometry2 was also part of the system that achieved\nsilver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not\nleast, we report progress towards using AlphaGeometry2 as a part of a fully\nautomated system that reliably solves geometry problems directly from natural\nlanguage input.",
            "upvotes": 14,
            "discussionId": "67a589ecb16fabcdd2dea259"
        },
        "publishedAt": "2025-02-06T23:20:09.641Z",
        "title": "Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03544.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5973
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04235",
            "authors": [
                {
                    "_id": "67a56af6d7c26c7497a86308",
                    "user": {
                        "_id": "64b764bffdb702b3d8640610",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b764bffdb702b3d8640610/lpHg0AX_NOmzw-ZxeOa1s.png",
                        "isPro": false,
                        "fullname": "haoxintong",
                        "user": "haoxintong",
                        "type": "user"
                    },
                    "name": "Xintong Hao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-07T04:41:11.249Z",
                    "hidden": false
                },
                {
                    "_id": "67a56af6d7c26c7497a86309",
                    "user": {
                        "_id": "645604eebabbbbd3486dc615",
                        "avatarUrl": "/avatars/17a5ca8274e2bfc8f183a4af9878a930.svg",
                        "isPro": false,
                        "fullname": "shenke",
                        "user": "shenke18",
                        "type": "user"
                    },
                    "name": "Ke Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T12:56:30.330Z",
                    "hidden": false
                },
                {
                    "_id": "67a56af6d7c26c7497a8630a",
                    "name": "Chenggang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T17:19:55.000Z",
            "title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus\n  Expansion",
            "summary": "Despite the remarkable capabilities of large language models across various\ntasks, their continued scaling faces a critical challenge: the scarcity of\nhigh-quality pretraining data. While model architectures continue to evolve,\nthe natural language data struggles to scale up. To tackle this bottleneck, we\npropose MAssive Genre-Audience~(MAGA) reformulation\nmethod, which systematic synthesizes diverse, contextually-rich pretraining\ndata from existing corpus. This work makes three main contributions: (1) We\npropose MAGA reformulation method, a lightweight and scalable approach for\npretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We\nevaluate MAGACorpus with different data budget scaling strategies,\ndemonstrating consistent improvements across various model sizes (134M-13B),\nestablishing the necessity for next-generation large-scale synthetic\npretraining language models. (3) Through comprehensive analysis, we investigate\nprompt engineering's impact on synthetic training collapse and reveal\nlimitations in conventional collapse detection metrics using validation losses.\nOur work shows that MAGA can substantially expand training datasets while\nmaintaining quality, offering a reliably pathway for scaling models beyond data\nlimitations.",
            "upvotes": 13,
            "discussionId": "67a56af8d7c26c7497a86359"
        },
        "publishedAt": "2025-02-07T00:56:20.873Z",
        "title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04235.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b764bffdb702b3d8640610",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b764bffdb702b3d8640610/lpHg0AX_NOmzw-ZxeOa1s.png",
            "fullname": "haoxintong",
            "name": "haoxintong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.02358",
            "authors": [
                {
                    "_id": "67a43546f6caedc30f9d8c71",
                    "user": {
                        "_id": "659faf1d874e583fed79d09b",
                        "avatarUrl": "/avatars/178a18686426908b9496ce71f6550655.svg",
                        "isPro": false,
                        "fullname": "Ziyan Guo",
                        "user": "ZiyanGuo",
                        "type": "user"
                    },
                    "name": "Ziyan Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-06T14:15:01.599Z",
                    "hidden": false
                },
                {
                    "_id": "67a43546f6caedc30f9d8c72",
                    "user": {
                        "_id": "65fbc3c6f52ac1107f5b1677",
                        "avatarUrl": "/avatars/b8373c039c3d978510b89d057bd9b5e8.svg",
                        "isPro": false,
                        "fullname": "Zeyu Hu",
                        "user": "zeyuhu",
                        "type": "user"
                    },
                    "name": "Zeyu Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:38:48.101Z",
                    "hidden": false
                },
                {
                    "_id": "67a43546f6caedc30f9d8c73",
                    "name": "Na Zhao",
                    "hidden": false
                },
                {
                    "_id": "67a43546f6caedc30f9d8c74",
                    "name": "De Wen Soh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T14:43:26.000Z",
            "title": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm",
            "summary": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion. Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions. In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/.",
            "upvotes": 13,
            "discussionId": "67a43547f6caedc30f9d8c9b"
        },
        "publishedAt": "2025-02-06T23:38:19.926Z",
        "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02358.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "659faf1d874e583fed79d09b",
            "avatarUrl": "/avatars/178a18686426908b9496ce71f6550655.svg",
            "fullname": "Ziyan Guo",
            "name": "ZiyanGuo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04306",
            "authors": [
                {
                    "_id": "67a57f334e50b2956b13f4e0",
                    "user": {
                        "_id": "6730dc8df84c8aac97451e57",
                        "avatarUrl": "/avatars/4f2cf5363b17744daca41d2a18ddfeb8.svg",
                        "isPro": false,
                        "fullname": "Yinjie Wang",
                        "user": "yinjiewang",
                        "type": "user"
                    },
                    "name": "Yinjie Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-07T03:34:13.176Z",
                    "hidden": false
                },
                {
                    "_id": "67a57f334e50b2956b13f4e1",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67a57f334e50b2956b13f4e2",
                    "user": {
                        "_id": "6338790e76421c054310c96b",
                        "avatarUrl": "/avatars/112e3d88d155bc998a89fef6f33af64d.svg",
                        "isPro": false,
                        "fullname": "Guohao Li",
                        "user": "lightaime",
                        "type": "user"
                    },
                    "name": "Guohao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:43:05.909Z",
                    "hidden": false
                },
                {
                    "_id": "67a57f334e50b2956b13f4e3",
                    "user": {
                        "_id": "6599415e8c8ac79295e0b5e3",
                        "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
                        "isPro": false,
                        "fullname": "Mengdi Wang",
                        "user": "Edify-Kd2024",
                        "type": "user"
                    },
                    "name": "Mengdi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:42:45.269Z",
                    "hidden": false
                },
                {
                    "_id": "67a57f334e50b2956b13f4e4",
                    "name": "Bryon Aragam",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:47:49.000Z",
            "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference\n  Optimization",
            "summary": "Recent research has leveraged large language model multi-agent systems for\ncomplex problem-solving while trying to reduce the manual effort required to\nbuild them, driving the development of automated agent workflow optimization\nmethods. However, existing methods remain inflexible due to representational\nlimitations, a lack of adaptability, and poor scalability when relying on\ndiscrete optimization techniques. We address these challenges with ScoreFlow, a\nsimple yet high-performance framework that leverages efficient gradient-based\noptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel\nvariant of the direct preference optimization method that accounts for\nquantitative feedback. Across six benchmarks spanning question answering,\ncoding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over\nexisting baselines. Moreover, it empowers smaller models to outperform larger\nones with lower inference costs. Project:\nhttps://github.com/Gen-Verse/ScoreFlow",
            "upvotes": 11,
            "discussionId": "67a57f354e50b2956b13f53d"
        },
        "publishedAt": "2025-02-06T22:34:42.483Z",
        "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04306.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04128",
            "authors": [
                {
                    "_id": "67a5894db16fabcdd2de5459",
                    "user": {
                        "_id": "645f172d7c6bff8577353d1a",
                        "avatarUrl": "/avatars/a83682e1343809257b082b78d58c582a.svg",
                        "isPro": false,
                        "fullname": "ZhenYE",
                        "user": "ZhenYe234",
                        "type": "user"
                    },
                    "name": "Zhen Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:08.787Z",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de545a",
                    "name": "Xinfa Zhu",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de545b",
                    "name": "Chi-Min Chan",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de545c",
                    "name": "Xinsheng Wang",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de545d",
                    "name": "Xu Tan",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de545e",
                    "name": "Jiahe Lei",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de545f",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5460",
                    "name": "Haohe Liu",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5461",
                    "name": "Yizhu Jin",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5462",
                    "name": "Zheqi DAI",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5463",
                    "name": "Hongzhan Lin",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5464",
                    "name": "Jianyi Chen",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5465",
                    "name": "Xingjian Du",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5466",
                    "name": "Liumeng Xue",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5467",
                    "name": "Yunlin Chen",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5468",
                    "name": "Zhifei Li",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de5469",
                    "name": "Lei Xie",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de546a",
                    "name": "Qiuqiang Kong",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de546b",
                    "name": "Yike Guo",
                    "hidden": false
                },
                {
                    "_id": "67a5894db16fabcdd2de546c",
                    "user": {
                        "_id": "6628adb14277eae0da5eee28",
                        "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
                        "isPro": true,
                        "fullname": "HKUST Audio",
                        "user": "HKUST-Audio",
                        "type": "user"
                    },
                    "name": "Wei Xue",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-07T04:17:17.888Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T15:04:00.000Z",
            "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis",
            "summary": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.",
            "upvotes": 10,
            "discussionId": "67a5894db16fabcdd2de54d3"
        },
        "publishedAt": "2025-02-06T23:17:40.725Z",
        "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04128.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5973
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04320",
            "authors": [
                {
                    "_id": "67a6431d0fdd5543151da7d2",
                    "name": "Alec Helbling",
                    "hidden": false
                },
                {
                    "_id": "67a6431d0fdd5543151da7d3",
                    "name": "Tuna Han Salih Meral",
                    "hidden": false
                },
                {
                    "_id": "67a6431d0fdd5543151da7d4",
                    "name": "Ben Hoover",
                    "hidden": false
                },
                {
                    "_id": "67a6431d0fdd5543151da7d5",
                    "name": "Pinar Yanardag",
                    "hidden": false
                },
                {
                    "_id": "67a6431d0fdd5543151da7d6",
                    "name": "Duen Horng Chau",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:59:00.000Z",
            "title": "ConceptAttention: Diffusion Transformers Learn Highly Interpretable\n  Features",
            "summary": "Do the rich representations of multi-modal diffusion transformers (DiTs)\nexhibit unique properties that enhance their interpretability? We introduce\nConceptAttention, a novel method that leverages the expressive power of DiT\nattention layers to generate high-quality saliency maps that precisely locate\ntextual concepts within images. Without requiring additional training,\nConceptAttention repurposes the parameters of DiT attention layers to produce\nhighly contextualized concept embeddings, contributing the major discovery that\nperforming linear projections in the output space of DiT attention layers\nyields significantly sharper saliency maps compared to commonly used\ncross-attention mechanisms. Remarkably, ConceptAttention even achieves\nstate-of-the-art performance on zero-shot image segmentation benchmarks,\noutperforming 11 other zero-shot interpretability methods on the\nImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our\nwork contributes the first evidence that the representations of multi-modal DiT\nmodels like Flux are highly transferable to vision tasks like segmentation,\neven outperforming multi-modal foundation models like CLIP.",
            "upvotes": 9,
            "discussionId": "67a643200fdd5543151da869"
        },
        "publishedAt": "2025-02-07T12:46:43.929Z",
        "title": "ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64f8b03f83807928d25e766f/t0642OHdPxXymRKmI5l-g.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04320.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64f8b03f83807928d25e766f",
            "avatarUrl": "/avatars/68fd4ee967a1673a1d78a7581be8b3da.svg",
            "fullname": "Tuna Han Salih Meral",
            "name": "tmeral",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.00473",
            "authors": [
                {
                    "_id": "67a5f635c20315f5e3f16f62",
                    "user": {
                        "_id": "66348bf4e1555067669870fa",
                        "avatarUrl": "/avatars/8b8bbc7dff7d9a0a02b0960084bc95ab.svg",
                        "isPro": false,
                        "fullname": "白立忱",
                        "user": "Indulge-Bai",
                        "type": "user"
                    },
                    "name": "Lichen Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T12:56:27.462Z",
                    "hidden": false
                },
                {
                    "_id": "67a5f635c20315f5e3f16f63",
                    "name": "Masashi Sugiyama",
                    "hidden": false
                },
                {
                    "_id": "67a5f635c20315f5e3f16f64",
                    "name": "Zeke Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-01T16:00:08.000Z",
            "title": "Weak-to-Strong Diffusion with Reflection",
            "summary": "The goal of diffusion generative models is to align the learned distribution\nwith the real data distribution through gradient score matching. However,\ninherent limitations in training data quality, modeling strategies, and\narchitectural design lead to inevitable gap between generated outputs and real\ndata. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel\nframework that utilizes the estimated difference between existing weak and\nstrong models (i.e., weak-to-strong difference) to approximate the gap between\nan ideal model and a strong model. By employing a reflective operation that\nalternates between denoising and inversion with weak-to-strong difference, we\ntheoretically understand that W2SD steers latent variables along sampling\ntrajectories toward regions of the real data distribution. W2SD is highly\nflexible and broadly applicable, enabling diverse improvements through the\nstrategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,\ngood experts vs. bad experts in MoE). Extensive experiments demonstrate that\nW2SD significantly improves human preference, aesthetic quality, and prompt\nadherence, achieving SOTA performance across various modalities (e.g., image,\nvideo), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For\nexample, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to\n90% over the original results. Moreover, the performance gains achieved by W2SD\nmarkedly outweigh its additional computational overhead, while the cumulative\nimprovements from different weak-to-strong difference further solidify its\npractical utility and deployability.",
            "upvotes": 8,
            "discussionId": "67a5f638c20315f5e3f17086"
        },
        "publishedAt": "2025-02-07T07:08:30.818Z",
        "title": "Weak-to-Strong Diffusion with Reflection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00473.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66348bf4e1555067669870fa",
            "avatarUrl": "/avatars/8b8bbc7dff7d9a0a02b0960084bc95ab.svg",
            "fullname": "白立忱",
            "name": "Indulge-Bai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04299",
            "authors": [
                {
                    "_id": "67a591234020a3bfdb8cb2e5",
                    "user": {
                        "_id": "64770e86d7cf39f2e937ae9a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg",
                        "isPro": false,
                        "fullname": "Jinbo Xing",
                        "user": "Doubiiu",
                        "type": "user"
                    },
                    "name": "Jinbo Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:43:19.911Z",
                    "hidden": false
                },
                {
                    "_id": "67a591234020a3bfdb8cb2e6",
                    "name": "Long Mai",
                    "hidden": false
                },
                {
                    "_id": "67a591234020a3bfdb8cb2e7",
                    "user": {
                        "_id": "6372fab1bd1595ae66a62543",
                        "avatarUrl": "/avatars/783bdae07b2663eebeea4c7919a87c91.svg",
                        "isPro": false,
                        "fullname": "Cusuh Ham",
                        "user": "cusuh",
                        "type": "user"
                    },
                    "name": "Cusuh Ham",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:43:42.324Z",
                    "hidden": false
                },
                {
                    "_id": "67a591234020a3bfdb8cb2e8",
                    "user": {
                        "_id": "644a717e75fce8ebef4e4955",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zLga4NZBohFPlv50dcAo9.png",
                        "isPro": false,
                        "fullname": "Jiahui Huang",
                        "user": "heiwang1997",
                        "type": "user"
                    },
                    "name": "Jiahui Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:44:01.034Z",
                    "hidden": true
                },
                {
                    "_id": "67a591234020a3bfdb8cb2e9",
                    "user": {
                        "_id": "633bd831d5935998f74c4156",
                        "avatarUrl": "/avatars/feb4976ad10dd678ccad2652acf8a611.svg",
                        "isPro": false,
                        "fullname": "Aniruddha Mahapatra",
                        "user": "aniruddha26398",
                        "type": "user"
                    },
                    "name": "Aniruddha Mahapatra",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:44:08.344Z",
                    "hidden": false
                },
                {
                    "_id": "67a591234020a3bfdb8cb2ea",
                    "name": "Chi-Wing Fu",
                    "hidden": false
                },
                {
                    "_id": "67a591234020a3bfdb8cb2eb",
                    "user": {
                        "_id": "65574f0fc4865c852d5eec15",
                        "avatarUrl": "/avatars/1e03db4f2de4959dee620c577fbbb063.svg",
                        "isPro": false,
                        "fullname": "Tien-Tsin Wong",
                        "user": "ttwong",
                        "type": "user"
                    },
                    "name": "Tien-Tsin Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:44:26.835Z",
                    "hidden": false
                },
                {
                    "_id": "67a591234020a3bfdb8cb2ec",
                    "name": "Feng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:41:04.000Z",
            "title": "MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video\n  Generation",
            "summary": "This paper presents a method that allows users to design cinematic video\nshots in the context of image-to-video generation. Shot design, a critical\naspect of filmmaking, involves meticulously planning both camera movements and\nobject motions in a scene. However, enabling intuitive shot design in modern\nimage-to-video generation systems presents two main challenges: first,\neffectively capturing user intentions on the motion design, where both camera\nmovements and scene-space object motions must be specified jointly; and second,\nrepresenting motion information that can be effectively utilized by a video\ndiffusion model to synthesize the image animations. To address these\nchallenges, we introduce MotionCanvas, a method that integrates user-driven\ncontrols into image-to-video (I2V) generation models, allowing users to control\nboth object and camera motions in a scene-aware manner. By connecting insights\nfrom classical computer graphics and contemporary video generation techniques,\nwe demonstrate the ability to achieve 3D-aware motion control in I2V synthesis\nwithout requiring costly 3D-related training data. MotionCanvas enables users\nto intuitively depict scene-space motion intentions, and translates them into\nspatiotemporal motion-conditioning signals for video diffusion models. We\ndemonstrate the effectiveness of our method on a wide range of real-world image\ncontent and shot-design scenarios, highlighting its potential to enhance the\ncreative workflows in digital content creation and adapt to various image and\nvideo editing applications.",
            "upvotes": 8,
            "discussionId": "67a5912b4020a3bfdb8cb4d5"
        },
        "publishedAt": "2025-02-06T23:50:54.836Z",
        "title": "MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04299.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5973
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.03860",
            "authors": [
                {
                    "_id": "67a5880c886a1e223b1d57ec",
                    "user": {
                        "_id": "63e08acbf351dc0745749d56",
                        "avatarUrl": "/avatars/8e2d5ce9db5bd8008ac2ad80f6025553.svg",
                        "isPro": false,
                        "fullname": "Bo Pang",
                        "user": "bpucla",
                        "type": "user"
                    },
                    "name": "Bo Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:46:17.484Z",
                    "hidden": false
                },
                {
                    "_id": "67a5880c886a1e223b1d57ed",
                    "user": {
                        "_id": "63a3ff69f91ad3ea5703841d",
                        "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
                        "isPro": false,
                        "fullname": "Hanze Dong",
                        "user": "hendrydong",
                        "type": "user"
                    },
                    "name": "Hanze Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:46:02.107Z",
                    "hidden": false
                },
                {
                    "_id": "67a5880c886a1e223b1d57ee",
                    "user": {
                        "_id": "631983d5cb116eab31df5821",
                        "avatarUrl": "/avatars/6a42c842a9439241ead2ace1d79fc32c.svg",
                        "isPro": false,
                        "fullname": "Jiacheng Xu",
                        "user": "jcxu",
                        "type": "user"
                    },
                    "name": "Jiacheng Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:45:54.892Z",
                    "hidden": false
                },
                {
                    "_id": "67a5880c886a1e223b1d57ef",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "67a5880c886a1e223b1d57f0",
                    "user": {
                        "_id": "649bc93758d8b19de0c7785f",
                        "avatarUrl": "/avatars/3ed9473aee23d99f4ee949d3705089ea.svg",
                        "isPro": false,
                        "fullname": "Yingbo Zhou",
                        "user": "yingbozhou",
                        "type": "user"
                    },
                    "name": "Yingbo Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:45:09.887Z",
                    "hidden": false
                },
                {
                    "_id": "67a5880c886a1e223b1d57f1",
                    "user": {
                        "_id": "649dbcc4e0fff1ed099dc80a",
                        "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
                        "isPro": false,
                        "fullname": "Caiming Xiong",
                        "user": "cxiong",
                        "type": "user"
                    },
                    "name": "Caiming Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:45:02.543Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T08:19:59.000Z",
            "title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without\n  Distillation",
            "summary": "Large language models (LLMs), such as o1 from OpenAI, have demonstrated\nremarkable reasoning capabilities. o1 generates a long chain-of-thought\n(LongCoT) before answering a question. LongCoT allows LLMs to analyze problems,\ndevise plans, reflect, and backtrack effectively. These actions empower LLM to\nsolve complex problems. After the release of o1, many teams have attempted to\nreplicate its LongCoT and reasoning capabilities. In terms of methods, they\nprimarily rely on knowledge distillation with data from existing models with\nLongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving\nsignificant uncertainties on systematically developing such reasoning\nabilities. In terms of data domains, these works focus narrowly on math while a\nfew others include coding, limiting their generalizability. This paper\nintroduces a novel approach to enable LLM's LongCoT capacity without\ndistillation from o1-like models or expensive human annotations, where we\nbootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three\nstages: 1) LongCoT data bootstrapping with in-context learning on a standard\ninstruct model; 2) LongCoT supervised finetuning; 3) online training to further\nrefine LongCoT capacities. In BOLT, only a few in-context examples need to be\nconstructed during the bootstrapping stage; in our experiments, we created 10\nexamples, demonstrating the feasibility of this approach. We use\nLlama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various\nmodel scales (7B, 8B, 70B). We achieve impressive performance on a variety of\nbenchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which\nevaluate diverse task-solving and reasoning capabilities.",
            "upvotes": 7,
            "discussionId": "67a5880e886a1e223b1d58ca"
        },
        "publishedAt": "2025-02-06T23:12:15.874Z",
        "title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03860.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5973
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.00989",
            "authors": [
                {
                    "_id": "67a5c7601e6db426653ebc3d",
                    "name": "Kanika Goswami",
                    "hidden": false
                },
                {
                    "_id": "67a5c7601e6db426653ebc3e",
                    "user": {
                        "_id": "65c16444d4c3b8dff2f0d78d",
                        "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
                        "isPro": false,
                        "fullname": "Puneet Mathur",
                        "user": "puneetm",
                        "type": "user"
                    },
                    "name": "Puneet Mathur",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:47:20.093Z",
                    "hidden": false
                },
                {
                    "_id": "67a5c7601e6db426653ebc3f",
                    "user": {
                        "_id": "62a3ab83e4dd6252344d27cd",
                        "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
                        "isPro": false,
                        "fullname": "Ryan A. Rossi",
                        "user": "ryanrossi",
                        "type": "user"
                    },
                    "name": "Ryan Rossi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:47:32.496Z",
                    "hidden": false
                },
                {
                    "_id": "67a5c7601e6db426653ebc40",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T10:30:50.575Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T02:00:51.000Z",
            "title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual\n  Attribution",
            "summary": "Large Language Models (LLMs) can perform chart question-answering tasks but\noften generate unverified hallucinated responses. Existing answer attribution\nmethods struggle to ground responses in source charts due to limited\nvisual-semantic context, complex visual-text alignment requirements, and\ndifficulties in bounding box prediction across complex layouts. We present\nChartCitor, a multi-agent framework that provides fine-grained bounding box\ncitations by identifying supporting evidence within chart images. The system\norchestrates LLM agents to perform chart-to-table extraction, answer\nreformulation, table augmentation, evidence retrieval through pre-filtering and\nre-ranking, and table-to-chart mapping. ChartCitor outperforms existing\nbaselines across different chart types. Qualitative user studies show that\nChartCitor helps increase user trust in Generative AI by providing enhanced\nexplainability for LLM-assisted chart QA and enables professionals to be more\nproductive.",
            "upvotes": 6,
            "discussionId": "67a5c7621e6db426653ebc8a"
        },
        "publishedAt": "2025-02-07T03:42:17.799Z",
        "title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00989.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04270",
            "authors": [
                {
                    "_id": "67a5882fa8e877ef10b8d1fd",
                    "user": {
                        "_id": "664187fa1cd689758847f44b",
                        "avatarUrl": "/avatars/501ed1d5bcffd7466fd8b8c8d3b758f0.svg",
                        "isPro": false,
                        "fullname": "Yunzhen Feng",
                        "user": "Coolfyz",
                        "type": "user"
                    },
                    "name": "Yunzhen Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:49:26.187Z",
                    "hidden": false
                },
                {
                    "_id": "67a5882fa8e877ef10b8d1fe",
                    "user": {
                        "_id": "625de0717341c641426e7932",
                        "avatarUrl": "/avatars/9deb06fc565a80002c3ae75c6f4cd9e7.svg",
                        "isPro": false,
                        "fullname": "Ariel Kwiatkowski",
                        "user": "RedTachyon",
                        "type": "user"
                    },
                    "name": "Ariel Kwiatkowski",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:49:33.321Z",
                    "hidden": false
                },
                {
                    "_id": "67a5882fa8e877ef10b8d1ff",
                    "user": {
                        "_id": "6424123d3fa01ecba6fd94e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/B-1YSkTJMVBBQDX3WVxIL.jpeg",
                        "isPro": false,
                        "fullname": "Kunhao Zheng",
                        "user": "Kunhao",
                        "type": "user"
                    },
                    "name": "Kunhao Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:49:41.052Z",
                    "hidden": false
                },
                {
                    "_id": "67a5882fa8e877ef10b8d200",
                    "user": {
                        "_id": "65ce30e06da01df536eded5a",
                        "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg",
                        "isPro": false,
                        "fullname": "Julia Kempe",
                        "user": "Knykny",
                        "type": "user"
                    },
                    "name": "Julia Kempe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:49:48.009Z",
                    "hidden": false
                },
                {
                    "_id": "67a5882fa8e877ef10b8d201",
                    "user": {
                        "_id": "66494b428d50b4b0efceab9c",
                        "avatarUrl": "/avatars/ac7293aafaf15759d53cf62f4e1ae874.svg",
                        "isPro": false,
                        "fullname": "Yaqi Duan",
                        "user": "duanyq",
                        "type": "user"
                    },
                    "name": "Yaqi Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:49:54.220Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:09:00.000Z",
            "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
            "summary": "As large language models increasingly drive real-world applications, aligning\nthem with human values becomes paramount. Reinforcement Learning from Human\nFeedback (RLHF) has emerged as a key technique, translating preference data\ninto reward models when oracle human values remain inaccessible. In practice,\nRLHF mostly relies on approximate reward models, which may not consistently\nguide the policy toward maximizing the underlying human values. We propose\nPolicy-Interpolated Learning for Aligned Feedback (PILAF), a novel response\nsampling strategy for preference labeling that explicitly aligns preference\nlearning with maximizing the underlying oracle reward. PILAF is theoretically\ngrounded, demonstrating optimality from both an optimization and a statistical\nperspective. The method is straightforward to implement and demonstrates strong\nperformance in iterative and online RLHF settings where feedback curation is\ncritical.",
            "upvotes": 6,
            "discussionId": "67a58830a8e877ef10b8d226"
        },
        "publishedAt": "2025-02-06T23:13:23.158Z",
        "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04270.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65cbfa6c968742be942e6cba",
            "avatarUrl": "/avatars/1a6cc0983edc28fa92178d3abc283ba1.svg",
            "fullname": "Feng",
            "name": "Yunzhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.00988",
            "authors": [
                {
                    "_id": "67a5e076b94446dfc848533b",
                    "name": "Kanika Goswami",
                    "hidden": false
                },
                {
                    "_id": "67a5e076b94446dfc848533c",
                    "name": "Puneet Mathur",
                    "hidden": false
                },
                {
                    "_id": "67a5e076b94446dfc848533d",
                    "name": "Ryan Rossi",
                    "hidden": false
                },
                {
                    "_id": "67a5e076b94446dfc848533e",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T10:30:03.421Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T02:00:29.000Z",
            "title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via\n  Multimodal Feedback",
            "summary": "Scientific data visualization is pivotal for transforming raw data into\ncomprehensible visual representations, enabling pattern recognition,\nforecasting, and the presentation of data-driven insights. However, novice\nusers often face difficulties due to the complexity of selecting appropriate\ntools and mastering visualization techniques. Large Language Models (LLMs) have\nrecently demonstrated potential in assisting code generation, though they\nstruggle with accuracy and require iterative debugging. In this paper, we\npropose PlotGen, a novel multi-agent framework aimed at automating the creation\nof precise scientific visualizations. PlotGen orchestrates multiple LLM-based\nagents, including a Query Planning Agent that breaks down complex user requests\ninto executable steps, a Code Generation Agent that converts pseudocode into\nexecutable Python code, and three retrieval feedback agents - a Numeric\nFeedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that\nleverage multimodal LLMs to iteratively refine the data accuracy, textual\nlabels, and visual correctness of generated plots via self-reflection.\nExtensive experiments show that PlotGen outperforms strong baselines, achieving\na 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user\ntrust in LLM-generated visualizations and improved novice productivity due to a\nreduction in debugging time needed for plot errors.",
            "upvotes": 4,
            "discussionId": "67a5e077b94446dfc8485375"
        },
        "publishedAt": "2025-02-07T05:29:23.184Z",
        "title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.00988.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.19085",
            "authors": [
                {
                    "_id": "67a5b65fe7798ca5b7473a45",
                    "user": {
                        "_id": "663486a1f64712540644cb68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
                        "isPro": true,
                        "fullname": "Alessandro",
                        "user": "Devy1",
                        "type": "user"
                    },
                    "name": "Alessandro Giagnorio",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:57:46.514Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b65fe7798ca5b7473a46",
                    "user": {
                        "_id": "65a7cb0fc5ffe1d019a21cb3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GShcO1DwVNzlIUr3n1ifi.jpeg",
                        "isPro": false,
                        "fullname": "Alberto Martín López ",
                        "user": "AML14",
                        "type": "user"
                    },
                    "name": "Alberto Martin-Lopez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:46:32.306Z",
                    "hidden": false
                },
                {
                    "_id": "67a5b65fe7798ca5b7473a47",
                    "user": {
                        "_id": "6638bea59e57161faac814e7",
                        "avatarUrl": "/avatars/91375b88945af50e51b7229a789a31b8.svg",
                        "isPro": false,
                        "fullname": "Gabriele Bavota",
                        "user": "gbavota",
                        "type": "user"
                    },
                    "name": "Gabriele Bavota",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:46:38.124Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-31T12:23:28.000Z",
            "title": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet",
            "summary": "The advent of Large Language Models (LLMs) has significantly advanced the\nfield of automated code generation. LLMs rely on large and diverse datasets to\nlearn syntax, semantics, and usage patterns of programming languages. For\nlow-resource languages (i.e., niche programming languages characterized by the\nscarcity of training data), the limited availability of such data hampers the\nmodels' ability to generalize effectively, resulting in poorer code generation\nperformance as compared to high-resource languages. For this reason, there is a\nquest for techniques able to close this performance gap. We present an\nempirical study investigating the effectiveness of several approaches for\nboosting LLMs' performance on low-resource languages, namely: (i) a classic\nfine-tuning, which is however capped in size by the scarcity of training data;\n(ii) three variants of in-context learning, with prompts crafted to provide the\nLLM with additional information about the low-resource language (e.g., few-shot\nexamples showcasing features of the targeted language); and (iii) a\npre-training objective teaching the model how to translate between high- and\nlow-resource languages. The context of our study are two low-resource languages\n(R and Racket) and six LLMs having different architectures and sizes. Our\nfindings reveal that a fine-tuning is usually the best choice for smaller LLMs,\npossibly due to the fact that even a small dataset is sufficient to train their\nlimited number of parameters. With the increase in size of the models,\nin-context learning becomes more and more effective, representing a safe and\ncheap bet (i.e., it always helps, but with different magnitudes). Differently,\nvery large LLMs may deteriorate their performance on low-resource languages\nwhen fine-tuning is performed, possibly due to the lack of enough data needed\nto effectively update their weights.",
            "upvotes": 4,
            "discussionId": "67a5b660e7798ca5b7473a6b"
        },
        "publishedAt": "2025-02-07T05:25:27.744Z",
        "title": "Enhancing Code Generation for Low-Resource Languages: No Silver Bullet",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.19085.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "663486a1f64712540644cb68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
            "fullname": "Alessandro",
            "name": "Devy1",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.03639",
            "authors": [
                {
                    "_id": "67a59193f86e1b9d7ae7cd55",
                    "user": {
                        "_id": "65a47b4d60cc6b04c9ebb0ff",
                        "avatarUrl": "/avatars/b35ae99eab95e95a327c30b6d3ad6c83.svg",
                        "isPro": false,
                        "fullname": "Yunuo Chen",
                        "user": "yunuoch",
                        "type": "user"
                    },
                    "name": "Yunuo Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:50:19.710Z",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd56",
                    "user": {
                        "_id": "63f54aa73aa49d8cb97b84bc",
                        "avatarUrl": "/avatars/c73c5870039611ab9162daad46a1ba20.svg",
                        "isPro": false,
                        "fullname": "junli cao",
                        "user": "jlcao2",
                        "type": "user"
                    },
                    "name": "Junli Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:50:25.916Z",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd57",
                    "user": {
                        "_id": "66b01ee8e53bbad918362856",
                        "avatarUrl": "/avatars/293529589a91dd7a95909d66727db224.svg",
                        "isPro": false,
                        "fullname": "Anil Kag",
                        "user": "anilkagak2",
                        "type": "user"
                    },
                    "name": "Anil Kag",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:50:32.227Z",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd58",
                    "user": {
                        "_id": "636c0c1a15cd58e915bb8139",
                        "avatarUrl": "/avatars/7c675ac6a7d303d3425e498c4e939eb0.svg",
                        "isPro": false,
                        "fullname": "Vidit Goel",
                        "user": "vidit98",
                        "type": "user"
                    },
                    "name": "Vidit Goel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:50:37.829Z",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd59",
                    "name": "Sergei Korolev",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd5a",
                    "user": {
                        "_id": "655683727be68c0961673f45",
                        "avatarUrl": "/avatars/cddca36c041fa04860a4d42c0feaa07f.svg",
                        "isPro": false,
                        "fullname": "Chenfanfu Jiang",
                        "user": "cffjiang",
                        "type": "user"
                    },
                    "name": "Chenfanfu Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:50:50.619Z",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd5b",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "67a59193f86e1b9d7ae7cd5c",
                    "name": "Jian Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T21:49:06.000Z",
            "title": "Towards Physical Understanding in Video Generation: A 3D Point\n  Regularization Approach",
            "summary": "We present a novel video generation framework that integrates 3-dimensional\ngeometry and dynamic awareness. To achieve this, we augment 2D videos with 3D\npoint trajectories and align them in pixel space. The resulting 3D-aware video\ndataset, PointVid, is then used to fine-tune a latent diffusion model, enabling\nit to track 2D objects with 3D Cartesian coordinates. Building on this, we\nregularize the shape and motion of objects in the video to eliminate undesired\nartifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality\nof generated RGB videos and alleviate common issues like object morphing, which\nare prevalent in current video models due to a lack of shape awareness. With\nour 3D augmentation and regularization, our model is capable of handling\ncontact-rich scenarios such as task-oriented videos. These videos involve\ncomplex interactions of solids, where 3D information is essential for\nperceiving deformation and contact. Furthermore, our model improves the overall\nquality of video generation by promoting the 3D consistency of moving objects\nand reducing abrupt changes in shape and motion.",
            "upvotes": 4,
            "discussionId": "67a59195f86e1b9d7ae7cd97"
        },
        "publishedAt": "2025-02-06T23:52:49.331Z",
        "title": "Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03639.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5973
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04295",
            "authors": [
                {
                    "_id": "67a57d32bc587f5b57a3f24f",
                    "name": "Yuanye Liu",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f250",
                    "user": {
                        "_id": "62abdf657b037eafffc48808",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Jiahang Xu",
                        "user": "Jiahang",
                        "type": "user"
                    },
                    "name": "Jiahang Xu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-07T03:25:39.760Z",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f251",
                    "user": {
                        "_id": "62b0009c72043b05d29492b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                        "isPro": false,
                        "fullname": "Li Lyna Zhang",
                        "user": "lynazhang",
                        "type": "user"
                    },
                    "name": "Li Lyna Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:47:55.350Z",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f252",
                    "name": "Qi Chen",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f253",
                    "name": "Xuan Feng",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f254",
                    "name": "Yang Chen",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f255",
                    "name": "Zhongxin Guo",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f256",
                    "name": "Yuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "67a57d32bc587f5b57a3f257",
                    "name": "Cheng Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:36:44.000Z",
            "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format\n  Integrated Prompt Optimization",
            "summary": "Large Language Models (LLMs) have shown significant capability across various\ntasks, with their real-world effectiveness often driven by prompt design. While\nrecent research has focused on optimizing prompt content, the role of prompt\nformatting, a critical but often overlooked dimension, has received limited\nsystematic investigation. In this paper, we introduce Content-Format Integrated\nPrompt Optimization (CFPO), an innovative methodology that jointly optimizes\nboth prompt content and formatting through an iterative refinement process.\nCFPO leverages natural language mutations to explore content variations and\nemploys a dynamic format exploration strategy that systematically evaluates\ndiverse format options. Our extensive evaluations across multiple tasks and\nopen-source LLMs demonstrate that CFPO demonstrates measurable performance\nimprovements compared to content-only optimization methods. This highlights the\nimportance of integrated content-format optimization and offers a practical,\nmodel-agnostic approach to enhancing LLM performance. Code will be available at\nhttps://github.com/HenryLau7/CFPO.",
            "upvotes": 4,
            "discussionId": "67a57d33bc587f5b57a3f29d"
        },
        "publishedAt": "2025-02-06T22:27:24.284Z",
        "title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04295.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62abdf657b037eafffc48808",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg",
            "fullname": "Jiahang Xu",
            "name": "Jiahang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04322",
            "authors": [
                {
                    "_id": "67a5a9357415f9155e9b4b58",
                    "name": "Yik Siu Chan",
                    "hidden": false
                },
                {
                    "_id": "67a5a9357415f9155e9b4b59",
                    "user": {
                        "_id": "64698ed0dcbb937d56b9dd02",
                        "avatarUrl": "/avatars/835ce9bf6e2cd1d4b7a709cf41a884e2.svg",
                        "isPro": false,
                        "fullname": "Edward Ri",
                        "user": "narutatsuri",
                        "type": "user"
                    },
                    "name": "Narutatsu Ri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:57:58.519Z",
                    "hidden": true
                },
                {
                    "_id": "67a5a9357415f9155e9b4b5a",
                    "user": {
                        "_id": "64bf072bae436c8813494ba3",
                        "avatarUrl": "/avatars/afb96d2bbf90411f4b1a030ebebff300.svg",
                        "isPro": false,
                        "fullname": "Yuxin Xiao",
                        "user": "YuxinXiao",
                        "type": "user"
                    },
                    "name": "Yuxin Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-07T09:58:00.910Z",
                    "hidden": false
                },
                {
                    "_id": "67a5a9357415f9155e9b4b5b",
                    "name": "Marzyeh Ghassemi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:59:02.000Z",
            "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions",
            "summary": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.",
            "upvotes": 3,
            "discussionId": "67a5a9367415f9155e9b4bbb"
        },
        "publishedAt": "2025-02-07T01:37:25.953Z",
        "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04322.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64bf072bae436c8813494ba3",
            "avatarUrl": "/avatars/afb96d2bbf90411f4b1a030ebebff300.svg",
            "fullname": "Yuxin Xiao",
            "name": "YuxinXiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04296",
            "authors": [
                {
                    "_id": "67a57a4637e2abc28667ec1b",
                    "user": {
                        "_id": "63151385b031f7b1c7c0871c",
                        "avatarUrl": "/avatars/0088eb929866face5f95218943e3f478.svg",
                        "isPro": false,
                        "fullname": "Lirui Wang",
                        "user": "liruiw",
                        "type": "user"
                    },
                    "name": "Lirui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:51:11.182Z",
                    "hidden": false
                },
                {
                    "_id": "67a57a4637e2abc28667ec1c",
                    "name": "Kevin Zhao",
                    "hidden": false
                },
                {
                    "_id": "67a57a4637e2abc28667ec1d",
                    "user": {
                        "_id": "6747a05a736eaadf2eec50ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mF6_-m3GRm5OfG2HDNorC.jpeg",
                        "isPro": false,
                        "fullname": "Chaoqi Liu",
                        "user": "chaoqi-liu",
                        "type": "user"
                    },
                    "name": "Chaoqi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:51:27.451Z",
                    "hidden": false
                },
                {
                    "_id": "67a57a4637e2abc28667ec1e",
                    "user": {
                        "_id": "63e58e3a006a775275e59e41",
                        "avatarUrl": "/avatars/75262a35b27a2ae1939df9118120d99e.svg",
                        "isPro": false,
                        "fullname": "Xinlei Chen",
                        "user": "endernewton",
                        "type": "user"
                    },
                    "name": "Xinlei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-07T10:51:33.905Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:38:26.000Z",
            "title": "Learning Real-World Action-Video Dynamics with Heterogeneous Masked\n  Autoregression",
            "summary": "We propose Heterogeneous Masked Autoregression (HMA) for modeling\naction-video dynamics to generate high-quality data and evaluation in scaling\nrobot learning. Building interactive video world models and policies for\nrobotics is difficult due to the challenge of handling diverse settings while\nmaintaining computational efficiency to run in real time. HMA uses\nheterogeneous pre-training from observations and action sequences across\ndifferent robotic embodiments, domains, and tasks. HMA uses masked\nautoregression to generate quantized or soft tokens for video predictions.\n\\ourshort achieves better visual fidelity and controllability than the previous\nrobotic video generation models with 15 times faster speed in the real world.\nAfter post-training, this model can be used as a video simulator from low-level\naction inputs for evaluating policies and generating synthetic data. See this\nlink https://liruiw.github.io/hma for more information.",
            "upvotes": 2,
            "discussionId": "67a57a4737e2abc28667ec58"
        },
        "publishedAt": "2025-02-06T22:17:36.193Z",
        "title": "Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04296.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63151385b031f7b1c7c0871c",
            "avatarUrl": "/avatars/0088eb929866face5f95218943e3f478.svg",
            "fullname": "Lirui Wang",
            "name": "liruiw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    }
]