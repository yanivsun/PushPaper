[
    {
        "paper": {
            "id": "2502.05173",
            "authors": [
                {
                    "_id": "67a97a47174028234b74f687",
                    "user": {
                        "_id": "62eb70462f0f5e54df42f778",
                        "avatarUrl": "/avatars/456049dba67638d3cdb330cdf383f272.svg",
                        "isPro": false,
                        "fullname": "Xilin Wei",
                        "user": "Wiselnn",
                        "type": "user"
                    },
                    "name": "Xilin Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T13:12:02.432Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f688",
                    "user": {
                        "_id": "64f033ef82c6eea604c4da8b",
                        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                        "isPro": false,
                        "fullname": "Liu Xiaoran",
                        "user": "LiuXR",
                        "type": "user"
                    },
                    "name": "Xiaoran Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:59.999Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f689",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:50:02.011Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f68a",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f68b",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f68c",
                    "user": {
                        "_id": "65000bef18830fabea469fdd",
                        "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
                        "isPro": false,
                        "fullname": "Cao Yuhang",
                        "user": "yhcao",
                        "type": "user"
                    },
                    "name": "Yuhang Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:01:29.622Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f68d",
                    "name": "Jian Tong",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f68e",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:01:41.685Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f68f",
                    "user": {
                        "_id": "6491cd52b1e5d3444528edb1",
                        "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg",
                        "isPro": false,
                        "fullname": "Qipeng Guo",
                        "user": "QipengGuo",
                        "type": "user"
                    },
                    "name": "Qipeng Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:01:48.883Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f690",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f691",
                    "user": {
                        "_id": "61457b8deff2c9fdb4de4988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
                        "isPro": false,
                        "fullname": "Xipeng Qiu",
                        "user": "xpqiu",
                        "type": "user"
                    },
                    "name": "Xipeng Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:02:04.834Z",
                    "hidden": false
                },
                {
                    "_id": "67a97a47174028234b74f692",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:02:10.781Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:56:04.000Z",
            "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
            "summary": "While Rotary Position Embedding (RoPE) and its variants are widely adopted\nfor their long-context capabilities, the extension of the 1D RoPE to video,\nwith its complex spatio-temporal structure, remains an open challenge. This\nwork first introduces a comprehensive analysis that identifies four key\ncharacteristics essential for the effective adaptation of RoPE to video, which\nhave not been fully considered in prior work. As part of our analysis, we\nintroduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors)\ntask, which adds periodic distractors into V-NIAH. The V-NIAH-D task\ndemonstrates that previous RoPE variants, lacking appropriate temporal\ndimension allocation, are easily misled by distractors. Based on our analysis,\nwe introduce VideoRoPE, with a 3D structure designed to\npreserve spatio-temporal relationships. VideoRoPE features\nlow-frequency temporal allocation to mitigate periodic oscillations, a\ndiagonal layout to maintain spatial symmetry, and adjustable\ntemporal spacing to decouple temporal and spatial indexing. VideoRoPE\nconsistently surpasses previous RoPE variants, across diverse downstream tasks\nsuch as long video retrieval, video understanding, and video hallucination. Our\ncode will be available at\nhttps://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.",
            "upvotes": 50,
            "discussionId": "67a97a4a174028234b74f707"
        },
        "publishedAt": "2025-02-09T23:03:21.947Z",
        "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05173.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04507",
            "authors": [
                {
                    "_id": "67a98cd1b8b21202c9004628",
                    "user": {
                        "_id": "63565cc56d7fcf1bedb7d347",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
                        "isPro": false,
                        "fullname": "Zhang Peiyuan",
                        "user": "PY007",
                        "type": "user"
                    },
                    "name": "Peiyuan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:07:27.309Z",
                    "hidden": false
                },
                {
                    "_id": "67a98cd1b8b21202c9004629",
                    "user": {
                        "_id": "65416817271d3bc4d70f6745",
                        "avatarUrl": "/avatars/55cc24918c62ab39540c4df813b026ef.svg",
                        "isPro": false,
                        "fullname": "Yongqi Chen",
                        "user": "BrianChen1129",
                        "type": "user"
                    },
                    "name": "Yongqi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:48.410Z",
                    "hidden": false
                },
                {
                    "_id": "67a98cd1b8b21202c900462a",
                    "name": "Runlong Su",
                    "hidden": false
                },
                {
                    "_id": "67a98cd1b8b21202c900462b",
                    "name": "Hangliang Ding",
                    "hidden": false
                },
                {
                    "_id": "67a98cd1b8b21202c900462c",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "67a98cd1b8b21202c900462d",
                    "name": "Zhenghong Liu",
                    "hidden": false
                },
                {
                    "_id": "67a98cd1b8b21202c900462e",
                    "user": {
                        "_id": "6320ddd4a023aad6a768db54",
                        "avatarUrl": "/avatars/96b449a5cf6b2c7a818d7e7dc8c2e821.svg",
                        "isPro": false,
                        "fullname": "Hao Zhang",
                        "user": "haozhang",
                        "type": "user"
                    },
                    "name": "Hao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:08:01.920Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T21:17:09.000Z",
            "title": "Fast Video Generation with Sliding Tile Attention",
            "summary": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art\nvideo generation, but suffer from prohibitive compute cost -- when generating\njust a 5-second 720P video, attention alone takes 800 out of 945 seconds of\ntotal inference time. This paper introduces sliding tile attention (STA) to\naddress this challenge. STA leverages the observation that attention scores in\npretrained video diffusion models predominantly concentrate within localized 3D\nwindows. By sliding and attending over the local spatial-temporal region, STA\neliminates redundancy from full attention. Unlike traditional token-wise\nsliding window attention (SWA), STA operates tile-by-tile with a novel\nhardware-aware sliding window design, preserving expressiveness while being\nhardware-efficient. With careful kernel-level optimizations, STA offers the\nfirst efficient 2D/3D sliding-window-like attention implementation, achieving\n58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over\nFlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading\nvideo DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s\nwithout quality degradation, requiring no training. Enabling finetuning further\nlowers latency to 268s with only a 0.09% drop on VBench.",
            "upvotes": 39,
            "discussionId": "67a98cd7b8b21202c90047c5"
        },
        "publishedAt": "2025-02-10T00:22:26.568Z",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04507.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63565cc56d7fcf1bedb7d347",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
            "fullname": "Zhang Peiyuan",
            "name": "PY007",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 80
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04896",
            "authors": [
                {
                    "_id": "67a983ea9b72585dd12587fb",
                    "user": {
                        "_id": "6412a33900634c4fe9873652",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
                        "isPro": false,
                        "fullname": "Shoufa Chen",
                        "user": "ShoufaChen",
                        "type": "user"
                    },
                    "name": "Shoufa Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:52.136Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd12587fc",
                    "user": {
                        "_id": "620f126891e167b068fa76f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f126891e167b068fa76f8/NaPyS5lFjgZYJZrWaf0OI.jpeg",
                        "isPro": false,
                        "fullname": "ChongjianGE",
                        "user": "RhettGee",
                        "type": "user"
                    },
                    "name": "Chongjian Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:54:30.233Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd12587fd",
                    "name": "Yuqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd12587fe",
                    "name": "Yida Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd12587ff",
                    "user": {
                        "_id": "656971db2f7ea4b5ac238169",
                        "avatarUrl": "/avatars/29eca045338f1b9a272c42cf10a62823.svg",
                        "isPro": false,
                        "fullname": "Fengda Zhu",
                        "user": "zhufengdaaa",
                        "type": "user"
                    },
                    "name": "Fengda Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:55:24.292Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258800",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258801",
                    "name": "Hongxiang Hao",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258802",
                    "name": "Hui Wu",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258803",
                    "user": {
                        "_id": "6673e67d65b9964067706db9",
                        "avatarUrl": "/avatars/45018a5fffa77643b7a6d476f6063151.svg",
                        "isPro": false,
                        "fullname": "Zhichao Lai",
                        "user": "sgcc-chao",
                        "type": "user"
                    },
                    "name": "Zhichao Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:53:38.146Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258804",
                    "user": {
                        "_id": "64832c6675779e269260e98e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64832c6675779e269260e98e/r-d14egc7wRBBY7_pD9dr.jpeg",
                        "isPro": false,
                        "fullname": "Yifei Hu",
                        "user": "yifeihu",
                        "type": "user"
                    },
                    "name": "Yifei Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:53:30.624Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258805",
                    "user": {
                        "_id": "63f89398da440a47e9f6b782",
                        "avatarUrl": "/avatars/6e2b4994a59b38add1332cc07b0ff3de.svg",
                        "isPro": false,
                        "fullname": "Ting-Che Lin",
                        "user": "dronchego",
                        "type": "user"
                    },
                    "name": "Ting-Che Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:53:19.492Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258806",
                    "user": {
                        "_id": "6424ffce46d202ad3d918a67",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg",
                        "isPro": false,
                        "fullname": "Shilong Zhang",
                        "user": "shilongz",
                        "type": "user"
                    },
                    "name": "Shilong Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:53:12.376Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258807",
                    "name": "Fu Li",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258808",
                    "name": "Chuan Li",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258809",
                    "name": "Xing Wang",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd125880a",
                    "name": "Yanghua Peng",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd125880b",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd125880c",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd125880d",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd125880e",
                    "user": {
                        "_id": "661a80af3557013b638061d5",
                        "avatarUrl": "/avatars/4c551aeb223e257a5fc45b5b6c7ded49.svg",
                        "isPro": false,
                        "fullname": "Zehuan Yuan",
                        "user": "sweetrabor",
                        "type": "user"
                    },
                    "name": "Zehuan Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:52:06.140Z",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd125880f",
                    "name": "Bingyue Peng",
                    "hidden": false
                },
                {
                    "_id": "67a983ea9b72585dd1258810",
                    "user": {
                        "_id": "66dbf16d7ec0e5f42175dbcb",
                        "avatarUrl": "/avatars/d28477ac9f02b633300cd51dea78704f.svg",
                        "isPro": false,
                        "fullname": "liuxiaobing",
                        "user": "xiaobinggg",
                        "type": "user"
                    },
                    "name": "Xiaobing Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T15:51:52.195Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T13:03:55.000Z",
            "title": "Goku: Flow Based Video Generative Foundation Models",
            "summary": "This paper introduces Goku, a state-of-the-art family of joint\nimage-and-video generation models leveraging rectified flow Transformers to\nachieve industry-leading performance. We detail the foundational elements\nenabling high-quality visual generation, including the data curation pipeline,\nmodel architecture design, flow formulation, and advanced infrastructure for\nefficient and robust large-scale training. The Goku models demonstrate superior\nperformance in both qualitative and quantitative evaluations, setting new\nbenchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and\n83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for\ntext-to-video tasks. We believe that this work provides valuable insights and\npractical advancements for the research community in developing joint\nimage-and-video generation models.",
            "upvotes": 34,
            "discussionId": "67a983ee9b72585dd125890f"
        },
        "publishedAt": "2025-02-09T23:43:39.239Z",
        "title": "Goku: Flow Based Video Generative Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04896.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05003",
            "authors": [
                {
                    "_id": "67a9b1a69a99341e859c488d",
                    "user": {
                        "_id": "623753b5eddd7763adc9346a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
                        "isPro": false,
                        "fullname": "Andrei Panferov",
                        "user": "BlackSamorez",
                        "type": "user"
                    },
                    "name": "Andrei Panferov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-10T08:09:18.686Z",
                    "hidden": false
                },
                {
                    "_id": "67a9b1a69a99341e859c488e",
                    "name": "Jiale Chen",
                    "hidden": false
                },
                {
                    "_id": "67a9b1a69a99341e859c488f",
                    "user": {
                        "_id": "632a2e325f2ff1958c0103be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg",
                        "isPro": false,
                        "fullname": "Soroush Tabesh",
                        "user": "soroushtabesh",
                        "type": "user"
                    },
                    "name": "Soroush Tabesh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:37.573Z",
                    "hidden": false
                },
                {
                    "_id": "67a9b1a69a99341e859c4890",
                    "name": "Roberto L. Castro",
                    "hidden": false
                },
                {
                    "_id": "67a9b1a69a99341e859c4891",
                    "user": {
                        "_id": "6526b8ebba9a8279c139616b",
                        "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
                        "isPro": false,
                        "fullname": "Mahdi Nikdan",
                        "user": "mnikdan97",
                        "type": "user"
                    },
                    "name": "Mahdi Nikdan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:50:25.944Z",
                    "hidden": false
                },
                {
                    "_id": "67a9b1a69a99341e859c4892",
                    "user": {
                        "_id": "64ef52c2718f94ae8e78a5e7",
                        "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
                        "isPro": false,
                        "fullname": "Alistarh",
                        "user": "d-alistarh",
                        "type": "user"
                    },
                    "name": "Dan Alistarh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:35.449Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T15:23:34.000Z",
            "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
            "summary": "One approach to reducing the massive costs of large language models (LLMs) is\nthe use of quantized or sparse representations for training or deployment.\nWhile post-training compression methods are very popular, the question of\nobtaining even more accurate compressed models by directly training over such\nrepresentations, i.e., Quantization-Aware Training (QAT), is still open: for\nexample, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at\nwhich models can be trained using QAT, while staying accuracy-competitive with\nstandard FP16/BF16 precision, at 8-bits weights and activations.\n  We advance this state-of-the-art via a new method called QuEST, which is\nPareto-competitive with FP16, i.e., it provides better accuracy at lower model\nsize, while training models with weights and activations in 4-bits or less.\nMoreover, QuEST allows stable training with 1-bit weights and activations.\nQuEST achieves this by improving two key aspects of QAT methods: (1) accurate\nand fast quantization of the (continuous) distributions of weights and\nactivations via Hadamard normalization and MSE-optimal fitting; (2) a new trust\ngradient estimator based on the idea of explicitly minimizing the error between\nthe noisy gradient computed over quantized states and the \"true\" (but unknown)\nfull-precision gradient. Experiments on Llama-type architectures show that\nQuEST induces stable scaling laws across the entire range of hardware-supported\nprecisions, and can be extended to sparse representations. We provide GPU\nkernel support showing that models produced by QuEST can be executed\nefficiently. Our code is available at https://github.com/IST-DASLab/QuEST.",
            "upvotes": 28,
            "discussionId": "67a9b1a79a99341e859c48c7"
        },
        "publishedAt": "2025-02-10T03:00:12.065Z",
        "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05003.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64ef52c2718f94ae8e78a5e7",
            "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
            "fullname": "Alistarh",
            "name": "d-alistarh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05171",
            "authors": [
                {
                    "_id": "67a97e27495b23306cd5ea56",
                    "user": {
                        "_id": "63d86dbf3130cadcaf8bdd11",
                        "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                        "isPro": false,
                        "fullname": "Jonas Geiping",
                        "user": "JonasGeiping",
                        "type": "user"
                    },
                    "name": "Jonas Geiping",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:10:13.470Z",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea57",
                    "user": {
                        "_id": "65255f1073a043e50d043641",
                        "avatarUrl": "/avatars/257085f01c439d7c84787a4e6d085b3d.svg",
                        "isPro": true,
                        "fullname": "Sean McLeish",
                        "user": "smcleish",
                        "type": "user"
                    },
                    "name": "Sean McLeish",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T13:11:57.835Z",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea58",
                    "user": {
                        "_id": "63e2b1ec282ee5f9624cfbcb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e2b1ec282ee5f9624cfbcb/4SVTp93cvRevacoJgiXzS.jpeg",
                        "isPro": false,
                        "fullname": "Neel Jain",
                        "user": "nsjain",
                        "type": "user"
                    },
                    "name": "Neel Jain",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:10:34.976Z",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea59",
                    "user": {
                        "_id": "63d98af1897746d6496177df",
                        "avatarUrl": "/avatars/c5d0031c796a3c11bcb0d01b959168dc.svg",
                        "isPro": false,
                        "fullname": "John Kirchenbauer",
                        "user": "jwkirchenbauer",
                        "type": "user"
                    },
                    "name": "John Kirchenbauer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:10:40.754Z",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea5a",
                    "name": "Siddharth Singh",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea5b",
                    "name": "Brian R. Bartoldson",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea5c",
                    "user": {
                        "_id": "65cb79db6427380bc21261e2",
                        "avatarUrl": "/avatars/a003eb5d0955417329c1a4170ae65879.svg",
                        "isPro": false,
                        "fullname": "Bhavya Kailkhura",
                        "user": "bhavyakailkhura",
                        "type": "user"
                    },
                    "name": "Bhavya Kailkhura",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:11:04.907Z",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea5d",
                    "user": {
                        "_id": "6361d9ce6bd72c97d005b4db",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6361d9ce6bd72c97d005b4db/vjKaW2JFavVffoRxwgFwn.jpeg",
                        "isPro": false,
                        "fullname": "Abhinav Bhatele",
                        "user": "bhatele",
                        "type": "user"
                    },
                    "name": "Abhinav Bhatele",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:11:12.069Z",
                    "hidden": false
                },
                {
                    "_id": "67a97e27495b23306cd5ea5e",
                    "user": {
                        "_id": "6381ca7d65dc156aba0b933d",
                        "avatarUrl": "/avatars/84dfdca8e1cd6fbf50d6fb2a6f1b488d.svg",
                        "isPro": false,
                        "fullname": "Tom Goldstein",
                        "user": "tomgoldstein",
                        "type": "user"
                    },
                    "name": "Tom Goldstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:11:20.220Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:55:02.000Z",
            "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach",
            "summary": "We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.",
            "upvotes": 27,
            "discussionId": "67a97e29495b23306cd5eae5"
        },
        "publishedAt": "2025-02-09T23:19:16.714Z",
        "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05171.png",
        "numComments": 9,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05176",
            "authors": [
                {
                    "_id": "67a9889dc1fbde5146aba8b1",
                    "user": {
                        "_id": "65d70288ca16ef9ba7f72542",
                        "avatarUrl": "/avatars/8ceec128b7e7be6d1b4c615b9eced98d.svg",
                        "isPro": false,
                        "fullname": "Chung-Ho Wu",
                        "user": "kkennethwu",
                        "type": "user"
                    },
                    "name": "Chung-Ho Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:08:15.192Z",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b2",
                    "name": "Yang-Jung Chen",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b3",
                    "name": "Ying-Huan Chen",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b4",
                    "name": "Jie-Ying Lee",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b5",
                    "user": {
                        "_id": "67173302fd698e5b2a9c91dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7_5xqQEShNkkKpGbjFIjG.png",
                        "isPro": false,
                        "fullname": "Bo-Hsu Ke",
                        "user": "Hentci",
                        "type": "user"
                    },
                    "name": "Bo-Hsu Ke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:08:43.078Z",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b6",
                    "name": "Chun-Wei Tuan Mu",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b7",
                    "user": {
                        "_id": "665d84f05fdfe8f923fb0fe2",
                        "avatarUrl": "/avatars/71fa629eda3d34d5d854055f2a905b53.svg",
                        "isPro": false,
                        "fullname": "Yichuan Huang",
                        "user": "yichuan-huang",
                        "type": "user"
                    },
                    "name": "Yi-Chuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:09:08.174Z",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b8",
                    "user": {
                        "_id": "66b9d0996f861799b80b457a",
                        "avatarUrl": "/avatars/31d4989c5e0983283a6a8e8a152b82e6.svg",
                        "isPro": false,
                        "fullname": "CY Lin",
                        "user": "chinyanglin",
                        "type": "user"
                    },
                    "name": "Chin-Yang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:09:19.714Z",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8b9",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:50.370Z",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8ba",
                    "user": {
                        "_id": "65cd7863b2e8d2486a01bd49",
                        "avatarUrl": "/avatars/33a9c978924e69bcc5db1e620ff3c0f7.svg",
                        "isPro": false,
                        "fullname": "YenYu Lin",
                        "user": "Yenyu",
                        "type": "user"
                    },
                    "name": "Yen-Yu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:09:38.418Z",
                    "hidden": false
                },
                {
                    "_id": "67a9889dc1fbde5146aba8bb",
                    "user": {
                        "_id": "6459d5da3b6fafd9664807ab",
                        "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                        "isPro": false,
                        "fullname": "Yu-Lun Liu",
                        "user": "yulunliu",
                        "type": "user"
                    },
                    "name": "Yu-Lun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:09:49.464Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:59:55.000Z",
            "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based\n  360° Unbounded Scene Inpainting",
            "summary": "Three-dimensional scene inpainting is crucial for applications from virtual\nreality to architectural visualization, yet existing methods struggle with view\nconsistency and geometric accuracy in 360{\\deg} unbounded scenes. We present\nAuraFusion360, a novel reference-based method that enables high-quality object\nremoval and hole filling in 3D scenes represented by Gaussian Splatting. Our\napproach introduces (1) depth-aware unseen mask generation for accurate\nocclusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot\nmethod for accurate initial point placement without requiring additional\ntraining, and (3) SDEdit-based detail enhancement for multi-view coherence. We\nalso introduce 360-USID, the first comprehensive dataset for 360{\\deg}\nunbounded scene inpainting with ground truth. Extensive experiments demonstrate\nthat AuraFusion360 significantly outperforms existing methods, achieving\nsuperior perceptual quality while maintaining geometric accuracy across\ndramatic viewpoint changes. See our project page for video results and the\ndataset at https://kkennethwu.github.io/aurafusion360/.",
            "upvotes": 23,
            "discussionId": "67a988a4c1fbde5146abaa3b"
        },
        "publishedAt": "2025-02-10T00:05:28.205Z",
        "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KMKt5j_3UB0zDhxjSiyxI.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05176.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05163",
            "authors": [
                {
                    "_id": "67a9604851169a582d14c113",
                    "user": {
                        "_id": "642f4c789b2484d7d8551a93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
                        "isPro": true,
                        "fullname": "Yihe Deng",
                        "user": "ydeng9",
                        "type": "user"
                    },
                    "name": "Yihe Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:50:06.136Z",
                    "hidden": false
                },
                {
                    "_id": "67a9604851169a582d14c114",
                    "user": {
                        "_id": "60a53adbf9b53404e7806277",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621441193991-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Yu Yang",
                        "user": "yuyang",
                        "type": "user"
                    },
                    "name": "Yu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:12:23.986Z",
                    "hidden": false
                },
                {
                    "_id": "67a9604851169a582d14c115",
                    "user": {
                        "_id": "64e7bb81b159a6f87be99459",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e7bb81b159a6f87be99459/cxvzoEHg1YATnPJ9d3PTg.jpeg",
                        "isPro": false,
                        "fullname": "Junkai Zhang",
                        "user": "JunkaiZ",
                        "type": "user"
                    },
                    "name": "Junkai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:11:40.494Z",
                    "hidden": false
                },
                {
                    "_id": "67a9604851169a582d14c116",
                    "user": {
                        "_id": "62fa0ffe0697d224219a0cb7",
                        "avatarUrl": "/avatars/f0ef59e1c0cf4ab4fe5cee08d488bd03.svg",
                        "isPro": false,
                        "fullname": "Wei Wang",
                        "user": "WeiWang",
                        "type": "user"
                    },
                    "name": "Wei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:11:53.569Z",
                    "hidden": false
                },
                {
                    "_id": "67a9604851169a582d14c117",
                    "user": {
                        "_id": "6493236b70d925ae8050a1bf",
                        "avatarUrl": "/avatars/b16069de1445cfa8608567175deaa2ae.svg",
                        "isPro": false,
                        "fullname": "Bo Li",
                        "user": "BoLi-aisecure",
                        "type": "user"
                    },
                    "name": "Bo Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:12:08.449Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:45:03.000Z",
            "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM\n  Guardrails",
            "summary": "The rapid advancement of large language models (LLMs) has increased the need\nfor guardrail models to ensure responsible use, particularly in detecting\nunsafe and illegal content. While substantial safety data exist in English,\nmultilingual guardrail modeling remains underexplored due to the scarcity of\nopen-source safety data in other languages. To address this gap, we propose a\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\nguardrail model co-evolve adversarially to produce high-quality synthetic data\nfor multilingual guardrail training. We theoretically formalize this\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\nEmpirical evaluations show that our model \\ours outperforms state-of-the-art\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\nbenchmarks while being 4.5x faster at inference with a significantly smaller\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\nparticularly in addressing the imbalance for lower-resource languages in a\ncollected real dataset. Ablation studies emphasize the critical role of\nsynthetic data generation in bridging the imbalance in open-source data between\nEnglish and other languages. These findings establish a scalable and efficient\napproach to synthetic data generation, paving the way for improved multilingual\nguardrail models to enhance LLM safety. Code, model, and data will be\nopen-sourced at https://github.com/yihedeng9/DuoGuard.",
            "upvotes": 18,
            "discussionId": "67a9604951169a582d14c14d"
        },
        "publishedAt": "2025-02-10T00:43:32.191Z",
        "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05163.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642f4c789b2484d7d8551a93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
            "fullname": "Yihe Deng",
            "name": "ydeng9",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04403",
            "authors": [
                {
                    "_id": "67a97c7542d4d2f92ee57d20",
                    "name": "David Abel",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d21",
                    "user": {
                        "_id": "6602bfb4a44fc523256912b0",
                        "avatarUrl": "/avatars/d23fd4d654fa490389dd6dfb37c0e834.svg",
                        "isPro": false,
                        "fullname": "Andre Barreto",
                        "user": "andrebarreto",
                        "type": "user"
                    },
                    "name": "André Barreto",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:12:50.057Z",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d22",
                    "user": {
                        "_id": "64929b5d53b71f9cf934dcb8",
                        "avatarUrl": "/avatars/462add1d4f423f831481acf53217f900.svg",
                        "isPro": false,
                        "fullname": "Michael Bowling",
                        "user": "Alkaroth",
                        "type": "user"
                    },
                    "name": "Michael Bowling",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:13:08.978Z",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d23",
                    "name": "Will Dabney",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d24",
                    "name": "Shi Dong",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d25",
                    "name": "Steven Hansen",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d26",
                    "name": "Anna Harutyunyan",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d27",
                    "name": "Khimya Khetarpal",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d28",
                    "user": {
                        "_id": "6313f69d54e6e5d9f0fb9f10",
                        "avatarUrl": "/avatars/12114d479bb6dc394d29f988944f6d47.svg",
                        "isPro": false,
                        "fullname": "Clare Lyle",
                        "user": "justclarifying",
                        "type": "user"
                    },
                    "name": "Clare Lyle",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:13:37.970Z",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d29",
                    "user": {
                        "_id": "64b9310403124195cd9778ec",
                        "avatarUrl": "/avatars/57c594d3d0f97d3010b15b6a0806451c.svg",
                        "isPro": false,
                        "fullname": "Razvan Pascanu",
                        "user": "razp",
                        "type": "user"
                    },
                    "name": "Razvan Pascanu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:13:44.165Z",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d2a",
                    "name": "Georgios Piliouras",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d2b",
                    "name": "Doina Precup",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d2c",
                    "name": "Jonathan Richens",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d2d",
                    "name": "Mark Rowland",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d2e",
                    "name": "Tom Schaul",
                    "hidden": false
                },
                {
                    "_id": "67a97c7542d4d2f92ee57d2f",
                    "name": "Satinder Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T08:34:57.000Z",
            "title": "Agency Is Frame-Dependent",
            "summary": "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
            "upvotes": 15,
            "discussionId": "67a97c7642d4d2f92ee57d77"
        },
        "publishedAt": "2025-02-09T23:11:57.959Z",
        "title": "Agency Is Frame-Dependent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04403.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04728",
            "authors": [
                {
                    "_id": "67a97d1c02da0cdf059cb0d8",
                    "user": {
                        "_id": "62a80fe3ac97233f1625235a",
                        "avatarUrl": "/avatars/e9b10b2b86d39912a880b07da69fb144.svg",
                        "isPro": false,
                        "fullname": "Zhouliang Yu",
                        "user": "zhouliang",
                        "type": "user"
                    },
                    "name": "Zhouliang Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:16:00.654Z",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0d9",
                    "user": {
                        "_id": "632ff46bf242a8532b713381",
                        "avatarUrl": "/avatars/72e96e0dd7d7b4fce64a07def170174f.svg",
                        "isPro": false,
                        "fullname": "yuhuanyuan",
                        "user": "yuhuanyuan",
                        "type": "user"
                    },
                    "name": "Yuhuan Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:16:06.566Z",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0da",
                    "name": "Tim Z. Xiao",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0db",
                    "name": "Fuxiang Frank Xia",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0dc",
                    "user": {
                        "_id": "641a6895fb5ffff5ac79d593",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/dFR_ofjbqCrcqGa9R3MMq.jpeg",
                        "isPro": false,
                        "fullname": "Jie Fu",
                        "user": "bigaidream",
                        "type": "user"
                    },
                    "name": "Jie Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T13:12:00.097Z",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0dd",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:56.250Z",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0de",
                    "name": "Ge Lin",
                    "hidden": false
                },
                {
                    "_id": "67a97d1c02da0cdf059cb0df",
                    "user": {
                        "_id": "648905d1a15c43c791d4381f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
                        "isPro": false,
                        "fullname": "Weiyang Liu",
                        "user": "wy1iu",
                        "type": "user"
                    },
                    "name": "Weiyang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:16:29.068Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T07:52:25.000Z",
            "title": "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
            "summary": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
            "upvotes": 12,
            "discussionId": "67a97d1d02da0cdf059cb11a"
        },
        "publishedAt": "2025-02-09T23:17:42.258Z",
        "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04728.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05179",
            "authors": [
                {
                    "_id": "67a9901cc0310368e2488929",
                    "user": {
                        "_id": "6424ffce46d202ad3d918a67",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg",
                        "isPro": false,
                        "fullname": "Shilong Zhang",
                        "user": "shilongz",
                        "type": "user"
                    },
                    "name": "Shilong Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:14:33.747Z",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e248892a",
                    "user": {
                        "_id": "6538cc7f43d9189cdcbd1e6a",
                        "avatarUrl": "/avatars/6d06005601aeb665de37cc93f1fd03d3.svg",
                        "isPro": false,
                        "fullname": "wenboli",
                        "user": "wenboli",
                        "type": "user"
                    },
                    "name": "Wenbo Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:14:43.524Z",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e248892b",
                    "user": {
                        "_id": "6412a33900634c4fe9873652",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
                        "isPro": false,
                        "fullname": "Shoufa Chen",
                        "user": "ShoufaChen",
                        "type": "user"
                    },
                    "name": "Shoufa Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:46.264Z",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e248892c",
                    "user": {
                        "_id": "620f126891e167b068fa76f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f126891e167b068fa76f8/NaPyS5lFjgZYJZrWaf0OI.jpeg",
                        "isPro": false,
                        "fullname": "ChongjianGE",
                        "user": "RhettGee",
                        "type": "user"
                    },
                    "name": "Chongjian Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:14:54.589Z",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e248892d",
                    "user": {
                        "_id": "640dc9bf8512ec51d7f0ac1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dc9bf8512ec51d7f0ac1a/sT4rdEoQbzfW6D3xDVdqt.jpeg",
                        "isPro": false,
                        "fullname": "peizesun",
                        "user": "peizesun",
                        "type": "user"
                    },
                    "name": "Peize Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:15:01.448Z",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e248892e",
                    "name": "Yida Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e248892f",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e2488930",
                    "user": {
                        "_id": "661a80af3557013b638061d5",
                        "avatarUrl": "/avatars/4c551aeb223e257a5fc45b5b6c7ded49.svg",
                        "isPro": false,
                        "fullname": "Zehuan Yuan",
                        "user": "sweetrabor",
                        "type": "user"
                    },
                    "name": "Zehuan Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:15:29.017Z",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e2488931",
                    "name": "Binyue Peng",
                    "hidden": false
                },
                {
                    "_id": "67a9901cc0310368e2488932",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:59:59.000Z",
            "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution\n  Video Generation",
            "summary": "DiT diffusion models have achieved great success in text-to-video generation,\nleveraging their scalability in model capacity and data scale. High content and\nmotion fidelity aligned with text prompts, however, often require large model\nparameters and a substantial number of function evaluations (NFEs). Realistic\nand visually appealing details are typically reflected in high resolution\noutputs, further amplifying computational demands especially for single stage\nDiT models. To address these challenges, we propose a novel two stage\nframework, FlashVideo, which strategically allocates model capacity and NFEs\nacross stages to balance generation fidelity and quality. In the first stage,\nprompt fidelity is prioritized through a low resolution generation process\nutilizing large parameters and sufficient NFEs to enhance computational\nefficiency. The second stage establishes flow matching between low and high\nresolutions, effectively generating fine details with minimal NFEs.\nQuantitative and visual results demonstrate that FlashVideo achieves\nstate-of-the-art high resolution video generation with superior computational\nefficiency. Additionally, the two-stage design enables users to preview the\ninitial output before committing to full resolution generation, thereby\nsignificantly reducing computational costs and wait times as well as enhancing\ncommercial viability .",
            "upvotes": 11,
            "discussionId": "67a9901ec0310368e24889c2"
        },
        "publishedAt": "2025-02-10T00:35:37.019Z",
        "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05179.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04959",
            "authors": [
                {
                    "_id": "67a9f4900b97667e0a82ad3d",
                    "user": {
                        "_id": "65a5358ddb5c00652ef24c8d",
                        "avatarUrl": "/avatars/d50b6297584c9b4c2ccd93e64477b940.svg",
                        "isPro": false,
                        "fullname": "Daniel Marczak",
                        "user": "danielm1405",
                        "type": "user"
                    },
                    "name": "Daniel Marczak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T13:11:49.393Z",
                    "hidden": false
                },
                {
                    "_id": "67a9f4900b97667e0a82ad3e",
                    "name": "Simone Magistri",
                    "hidden": false
                },
                {
                    "_id": "67a9f4900b97667e0a82ad3f",
                    "user": {
                        "_id": "6763f031c0a39e58c57ed9f9",
                        "avatarUrl": "/avatars/2e864838a571f52b5316f90d60b763f1.svg",
                        "isPro": false,
                        "fullname": "Sebastian Cygert",
                        "user": "cygerts",
                        "type": "user"
                    },
                    "name": "Sebastian Cygert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:17:02.946Z",
                    "hidden": false
                },
                {
                    "_id": "67a9f4900b97667e0a82ad40",
                    "name": "Bartłomiej Twardowski",
                    "hidden": false
                },
                {
                    "_id": "67a9f4900b97667e0a82ad41",
                    "name": "Andrew D. Bagdanov",
                    "hidden": false
                },
                {
                    "_id": "67a9f4900b97667e0a82ad42",
                    "name": "Joost van de Weijer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T14:22:56.000Z",
            "title": "No Task Left Behind: Isotropic Model Merging with Common and\n  Task-Specific Subspaces",
            "summary": "Model merging integrates the weights of multiple task-specific models into a\nsingle multi-task model. Despite recent interest in the problem, a significant\nperformance gap between the combined and single-task models remains. In this\npaper, we investigate the key characteristics of task matrices -- weight update\nmatrices applied to a pre-trained model -- that enable effective merging. We\nshow that alignment between singular components of task-specific and merged\nmatrices strongly correlates with performance improvement over the pre-trained\nmodel. Based on this, we propose an isotropic merging framework that flattens\nthe singular value spectrum of task matrices, enhances alignment, and reduces\nthe performance gap. Additionally, we incorporate both common and task-specific\nsubspaces to further improve alignment and performance. Our proposed approach\nachieves state-of-the-art performance across multiple scenarios, including\nvarious sets of tasks and model scales. This work advances the understanding of\nmodel merging dynamics, offering an effective methodology to merge models\nwithout requiring additional training. Code is available at\nhttps://github.com/danielm1405/iso-merging .",
            "upvotes": 8,
            "discussionId": "67a9f4920b97667e0a82adeb"
        },
        "publishedAt": "2025-02-10T07:46:25.333Z",
        "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04959.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a5358ddb5c00652ef24c8d",
            "avatarUrl": "/avatars/d50b6297584c9b4c2ccd93e64477b940.svg",
            "fullname": "Daniel Marczak",
            "name": "danielm1405",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04520",
            "authors": [
                {
                    "_id": "67a97eea96d822bc6e13a1bb",
                    "user": {
                        "_id": "64323dd503d81fa4d26deaf9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
                        "isPro": false,
                        "fullname": "Letian Peng",
                        "user": "KomeijiForce",
                        "type": "user"
                    },
                    "name": "Letian Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:17:52.581Z",
                    "hidden": false
                },
                {
                    "_id": "67a97eea96d822bc6e13a1bc",
                    "user": {
                        "_id": "6546d644bab28a482e1956c3",
                        "avatarUrl": "/avatars/b35e53afd1acf56534338b7788b49ee1.svg",
                        "isPro": false,
                        "fullname": "Chenyang An",
                        "user": "chenyang-an",
                        "type": "user"
                    },
                    "name": "Chenyang An",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:18:00.824Z",
                    "hidden": false
                },
                {
                    "_id": "67a97eea96d822bc6e13a1bd",
                    "user": {
                        "_id": "660ee5df35d092e3fc2a3685",
                        "avatarUrl": "/avatars/a7e0472fb7ea49973f74e3eea13dc964.svg",
                        "isPro": false,
                        "fullname": "Shibo Hao",
                        "user": "Shibo-UCSD",
                        "type": "user"
                    },
                    "name": "Shibo Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:18:07.047Z",
                    "hidden": false
                },
                {
                    "_id": "67a97eea96d822bc6e13a1be",
                    "user": {
                        "_id": "668640a1369b09d564b75509",
                        "avatarUrl": "/avatars/ef70bfdaae307a602f0ce0a0753596c7.svg",
                        "isPro": false,
                        "fullname": "CHENGYU_DONG",
                        "user": "sakuraCY",
                        "type": "user"
                    },
                    "name": "Chengyu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:18:12.963Z",
                    "hidden": false
                },
                {
                    "_id": "67a97eea96d822bc6e13a1bf",
                    "user": {
                        "_id": "660655119e3555d648f6c6b5",
                        "avatarUrl": "/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg",
                        "isPro": false,
                        "fullname": "Jingbo Shang",
                        "user": "shangjingbo",
                        "type": "user"
                    },
                    "name": "Jingbo Shang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:54.200Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T21:44:30.000Z",
            "title": "Linear Correlation in LM's Compositional Generalization and\n  Hallucination",
            "summary": "The generalization of language models (LMs) is undergoing active debates,\ncontrasting their potential for general intelligence with their struggles with\nbasic knowledge composition (e.g., reverse/transition curse). This paper\nuncovers the phenomenon of linear correlations in LMs during knowledge\ncomposition. For explanation, there exists a linear transformation between\ncertain related knowledge that maps the next token prediction logits from one\nprompt to another, e.g., \"X lives in the city of\" rightarrow \"X lives in the\ncountry of\" for every given X. This mirrors the linearity in human knowledge\ncomposition, such as Paris rightarrow France. Our findings indicate that the\nlinear transformation is resilient to large-scale fine-tuning, generalizing\nupdated knowledge when aligned with real-world relationships, but causing\nhallucinations when it deviates. Empirical results suggest that linear\ncorrelation can serve as a potential identifier of LM's generalization.\nFinally, we show such linear correlations can be learned with a single\nfeedforward network and pre-trained vocabulary representations, indicating LM\ngeneralization heavily relies on the latter.",
            "upvotes": 8,
            "discussionId": "67a97eea96d822bc6e13a1e7"
        },
        "publishedAt": "2025-02-09T23:22:06.784Z",
        "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04520.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04416",
            "authors": [
                {
                    "_id": "67a970920d2e1d1311d04053",
                    "user": {
                        "_id": "6527c063e86758eb6ca800a1",
                        "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
                        "isPro": false,
                        "fullname": "JarvisPei",
                        "user": "Eleven-P",
                        "type": "user"
                    },
                    "name": "Zehua Pei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:50:03.881Z",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d04054",
                    "user": {
                        "_id": "65392c3429f8a911550fb9d8",
                        "avatarUrl": "/avatars/f12cd0ace82072817baea0d72f158de5.svg",
                        "isPro": false,
                        "fullname": "LANCHENG ZOU",
                        "user": "culczou",
                        "type": "user"
                    },
                    "name": "Lancheng Zou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T13:12:04.847Z",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d04055",
                    "name": "Hui-Ling Zhen",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d04056",
                    "name": "Xianzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d04057",
                    "user": {
                        "_id": "67444dc518f0b9f39c48aa2f",
                        "avatarUrl": "/avatars/c911660165cc4c0abf6e0dcf6fa46034.svg",
                        "isPro": false,
                        "fullname": "liuwulong",
                        "user": "long202005589",
                        "type": "user"
                    },
                    "name": "Wulong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:18:49.935Z",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d04058",
                    "user": {
                        "_id": "6751cc1807c0a99c402af739",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GPK4RlHBRFxbgrSKfkxUz.png",
                        "isPro": false,
                        "fullname": "Sinno Pan",
                        "user": "SinnoPan",
                        "type": "user"
                    },
                    "name": "Sinno Jialin Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:19:06.682Z",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d04059",
                    "name": "Mingxuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "67a970920d2e1d1311d0405a",
                    "name": "Bei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T14:05:30.000Z",
            "title": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
            "summary": "Large language models (LLMs) achieve impressive performance by scaling model\nparameters, but this comes with significant inference overhead. Feed-forward\nnetworks (FFNs), which dominate LLM parameters, exhibit high activation\nsparsity in hidden neurons. To exploit this, researchers have proposed using a\nmixture-of-experts (MoE) architecture, where only a subset of parameters is\nactivated. However, existing approaches often require extensive training data\nand resources, limiting their practicality. We propose CMoE (Carved MoE), a\nnovel framework to efficiently carve MoE models from dense models. CMoE\nachieves remarkable performance through efficient expert grouping and\nlightweight adaptation. First, neurons are grouped into shared and routed\nexperts based on activation rates. Next, we construct a routing mechanism\nwithout training from scratch, incorporating a differentiable routing process\nand load balancing. Using modest data, CMoE produces a well-designed, usable\nMoE from a 7B dense model within five minutes. With lightweight fine-tuning, it\nachieves high-performance recovery in under an hour. We make our code publicly\navailable at https://github.com/JarvisPei/CMoE.",
            "upvotes": 7,
            "discussionId": "67a970970d2e1d1311d040ff"
        },
        "publishedAt": "2025-02-10T05:25:07.375Z",
        "title": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04416.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527c063e86758eb6ca800a1",
            "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
            "fullname": "JarvisPei",
            "name": "Eleven-P",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04363",
            "authors": [
                {
                    "_id": "67a98180d0dc1ed664297368",
                    "name": "Bosung Kim",
                    "hidden": false
                },
                {
                    "_id": "67a98180d0dc1ed664297369",
                    "name": "Kyuhwan Lee",
                    "hidden": false
                },
                {
                    "_id": "67a98180d0dc1ed66429736a",
                    "name": "Isu Jeong",
                    "hidden": false
                },
                {
                    "_id": "67a98180d0dc1ed66429736b",
                    "user": {
                        "_id": "65c9911ad3870f24084060f9",
                        "avatarUrl": "/avatars/889ff75133203f9ed5b3c46cc67fb068.svg",
                        "isPro": false,
                        "fullname": "Jungmin Cheon",
                        "user": "Ruyan2",
                        "type": "user"
                    },
                    "name": "Jungmin Cheon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:24:07.147Z",
                    "hidden": false
                },
                {
                    "_id": "67a98180d0dc1ed66429736c",
                    "name": "Yeojin Lee",
                    "hidden": false
                },
                {
                    "_id": "67a98180d0dc1ed66429736d",
                    "name": "Seulki Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T05:42:29.000Z",
            "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for\n  Mobile Devices",
            "summary": "We present On-device Sora, a first pioneering solution for diffusion-based\non-device text-to-video generation that operates efficiently on\nsmartphone-grade devices. Building on Open-Sora, On-device Sora applies three\nnovel techniques to address the challenges of diffusion-based text-to-video\ngeneration on computation- and memory-limited mobile devices. First, Linear\nProportional Leap (LPL) reduces the excessive denoising steps required in video\ndiffusion through an efficient leap-based approach. Second, Temporal Dimension\nToken Merging (TDTM) minimizes intensive token-processing computation in\nattention layers by merging consecutive tokens along the temporal dimension.\nThird, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions\nlarge models into smaller blocks and loads them into memory for concurrent\nmodel inference, effectively addressing the challenges of limited device\nmemory. We implement On-device Sora on the iPhone 15 Pro, and the experimental\nevaluations demonstrate that it is capable of generating high-quality videos on\nthe device, comparable to those produced by Open-Sora running on high-end GPUs.\nThese results show that On-device Sora enables efficient and high-quality video\ngeneration on resource-constrained mobile devices, expanding accessibility,\nensuring user privacy, reducing dependence on cloud infrastructure, and\nlowering associated costs. We envision the proposed On-device Sora as a\nsignificant first step toward democratizing state-of-the-art generative\ntechnologies, enabling video generation capabilities on commodity mobile and\nembedded devices. The code implementation is publicly available at an GitHub\nrepository: https://github.com/eai-lab/On-device-Sora.",
            "upvotes": 7,
            "discussionId": "67a98185d0dc1ed664297491"
        },
        "publishedAt": "2025-02-09T23:33:13.185Z",
        "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04363.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04404",
            "authors": [
                {
                    "_id": "67a97bc5500b3bcf5babc5e8",
                    "user": {
                        "_id": "64bb3d1eb1a618880956da76",
                        "avatarUrl": "/avatars/ec393b5eee8a3ccec61107b4aa63c4d9.svg",
                        "isPro": false,
                        "fullname": "Xiao-Wen Yang",
                        "user": "yangxw",
                        "type": "user"
                    },
                    "name": "Xiao-Wen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:57.842Z",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5e9",
                    "name": "Xuan-Yi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5ea",
                    "name": "Wen-Da Wei",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5eb",
                    "user": {
                        "_id": "65542c8c9bd4907a067050b2",
                        "avatarUrl": "/avatars/031a2fdcc7d73da4f88fbcfca6ad3920.svg",
                        "isPro": false,
                        "fullname": "Zhang Dingchu",
                        "user": "zhangdingchu",
                        "type": "user"
                    },
                    "name": "Ding-Chu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:34:44.713Z",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5ec",
                    "user": {
                        "_id": "640731714dc5f2846c945251",
                        "avatarUrl": "/avatars/a15695d306f05dd10a7b7f636af6a4f5.svg",
                        "isPro": false,
                        "fullname": "Jie-Jing Shao",
                        "user": "shjj",
                        "type": "user"
                    },
                    "name": "Jie-Jing Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:25:55.507Z",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5ed",
                    "name": "Zhi Zhou",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5ee",
                    "user": {
                        "_id": "63fc116b1b4b1bd4e707d198",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fc116b1b4b1bd4e707d198/kM1pL6_FUVwM2PXpNV160.jpeg",
                        "isPro": false,
                        "fullname": "Lan-Zhe Guo",
                        "user": "Guolz",
                        "type": "user"
                    },
                    "name": "Lan-Zhe Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:25:47.379Z",
                    "hidden": false
                },
                {
                    "_id": "67a97bc5500b3bcf5babc5ef",
                    "name": "Yu-Feng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T08:52:43.000Z",
            "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of\n  Language Models",
            "summary": "The integration of slow-thinking mechanisms into large language models (LLMs)\noffers a promising way toward achieving Level 2 AGI Reasoners, as exemplified\nby systems like OpenAI's o1. However, several significant challenges remain,\nincluding inefficient overthinking and an overreliance on auxiliary reward\nmodels. We point out that these limitations stem from LLMs' inability to\ninternalize the search process, a key component of effective reasoning. A\ncritical step toward addressing this issue is enabling LLMs to autonomously\ndetermine when and where to backtrack, a fundamental operation in traditional\nsearch algorithms. To this end, we propose a self-backtracking mechanism that\nequips LLMs with the ability to backtrack during both training and inference.\nThis mechanism not only enhances reasoning ability but also efficiency by\ntransforming slow-thinking processes into fast-thinking through\nself-improvement. Empirical evaluations demonstrate that our proposal\nsignificantly enhances the reasoning capabilities of LLMs, achieving a\nperformance gain of over 40 percent compared to the optimal-path supervised\nfine-tuning method. We believe this study introduces a novel and promising\npathway for developing more advanced and robust Reasoners.",
            "upvotes": 7,
            "discussionId": "67a97bc7500b3bcf5babc64e"
        },
        "publishedAt": "2025-02-09T23:09:01.160Z",
        "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04404.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.03738",
            "authors": [
                {
                    "_id": "67a8d049406cb5a65f847eb1",
                    "name": "Feng Wang",
                    "hidden": false
                },
                {
                    "_id": "67a8d049406cb5a65f847eb2",
                    "user": {
                        "_id": "6100e69a393be1b5c4c83867",
                        "avatarUrl": "/avatars/1b87098cffb9c50345789808daea4f68.svg",
                        "isPro": false,
                        "fullname": "Yaodong Yu",
                        "user": "yaodongyu",
                        "type": "user"
                    },
                    "name": "Yaodong Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:22:16.391Z",
                    "hidden": false
                },
                {
                    "_id": "67a8d049406cb5a65f847eb3",
                    "name": "Guoyizhe Wei",
                    "hidden": false
                },
                {
                    "_id": "67a8d049406cb5a65f847eb4",
                    "name": "Wei Shao",
                    "hidden": false
                },
                {
                    "_id": "67a8d049406cb5a65f847eb5",
                    "user": {
                        "_id": "66c7fb4ce2c92fe5b132f314",
                        "avatarUrl": "/avatars/22d915fa339a70803c5c748255250256.svg",
                        "isPro": false,
                        "fullname": "Yuyin Zhou",
                        "user": "RitaCoding",
                        "type": "user"
                    },
                    "name": "Yuyin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:22:41.354Z",
                    "hidden": false
                },
                {
                    "_id": "67a8d049406cb5a65f847eb6",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "67a8d049406cb5a65f847eb7",
                    "user": {
                        "_id": "645eb61da3c5cd8a16efffff",
                        "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
                        "isPro": false,
                        "fullname": "Cihang Xie",
                        "user": "cihangxie",
                        "type": "user"
                    },
                    "name": "Cihang Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-10T16:22:31.527Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T03:01:38.000Z",
            "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
            "summary": "Since the introduction of Vision Transformer (ViT), patchification has long\nbeen regarded as a de facto image tokenization approach for plain visual\narchitectures. By compressing the spatial size of images, this approach can\neffectively shorten the token sequence and reduce the computational cost of\nViT-like plain architectures. In this work, we aim to thoroughly examine the\ninformation loss caused by this patchification-based compressive encoding\nparadigm and how it affects visual understanding. We conduct extensive patch\nsize scaling experiments and excitedly observe an intriguing scaling law in\npatchification: the models can consistently benefit from decreased patch sizes\nand attain improved predictive performance, until it reaches the minimum patch\nsize of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable\nacross different vision tasks, various input scales, and diverse architectures\nsuch as ViT and the recent Mamba models. Moreover, as a by-product, we discover\nthat with smaller patches, task-specific decoder heads become less critical for\ndense prediction. In the experiments, we successfully scale up the visual\nsequence to an exceptional length of 50,176 tokens, achieving a competitive\ntest accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We\nhope this study can provide insights and theoretical foundations for future\nworks of building non-compressive vision models. Code is available at\nhttps://github.com/wangf3014/Patch_Scaling.",
            "upvotes": 6,
            "discussionId": "67a8d04a406cb5a65f847ed3"
        },
        "publishedAt": "2025-02-10T02:34:31.480Z",
        "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03738.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 754
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05178",
            "authors": [
                {
                    "_id": "67a99dfe98423dca45d8f659",
                    "user": {
                        "_id": "638fe91639f7e2a7f9d2a8c6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
                        "isPro": false,
                        "fullname": "Yue Zhao",
                        "user": "zhaoyue-zephyrus",
                        "type": "user"
                    },
                    "name": "Yue Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:43.493Z",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f65a",
                    "name": "Fuzhao Xue",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f65b",
                    "name": "Scott Reed",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f65c",
                    "name": "Linxi Fan",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f65d",
                    "name": "Yuke Zhu",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f65e",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f65f",
                    "name": "Zhiding Yu",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f660",
                    "name": "Philipp Krähenbühl",
                    "hidden": false
                },
                {
                    "_id": "67a99dfe98423dca45d8f661",
                    "name": "De-An Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:59:57.000Z",
            "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive\n  Multimodal Understanding and Generation",
            "summary": "We introduce Quantized Language-Image Pretraining (QLIP), a visual\ntokenization method that combines state-of-the-art reconstruction quality with\nstate-of-the-art zero-shot image understanding. QLIP trains a\nbinary-spherical-quantization-based autoencoder with reconstruction and\nlanguage-image alignment objectives. We are the first to show that the two\nobjectives do not need to be at odds. We balance the two loss terms dynamically\nduring training and show that a two-stage training pipeline effectively mixes\nthe large-batch requirements of image-language pre-training with the memory\nbottleneck imposed by the reconstruction objective. We validate the\neffectiveness of QLIP for multimodal understanding and text-conditioned image\ngeneration with a single model. Specifically, QLIP serves as a drop-in\nreplacement for the visual encoder for LLaVA and the image tokenizer for\nLlamaGen with comparable or even better performance. Finally, we demonstrate\nthat QLIP enables a unified mixed-modality auto-regressive model for\nunderstanding and generation.",
            "upvotes": 6,
            "discussionId": "67a99dfe98423dca45d8f691"
        },
        "publishedAt": "2025-02-10T01:35:35.818Z",
        "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "fullname": "Yue Zhao",
            "name": "zhaoyue-zephyrus",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05092",
            "authors": [
                {
                    "_id": "67aa05c5ffb9f6b5b2f658b2",
                    "name": "Rohit Saxena",
                    "hidden": false
                },
                {
                    "_id": "67aa05c5ffb9f6b5b2f658b3",
                    "name": "Aryo Pradipta Gema",
                    "hidden": false
                },
                {
                    "_id": "67aa05c5ffb9f6b5b2f658b4",
                    "user": {
                        "_id": "61001311e043e15c13412d30",
                        "avatarUrl": "/avatars/eea1e4c39decee282f2940d122090491.svg",
                        "isPro": false,
                        "fullname": "Pasquale Minervini",
                        "user": "pminervini",
                        "type": "user"
                    },
                    "name": "Pasquale Minervini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T15:51:20.711Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T17:11:23.000Z",
            "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs",
            "summary": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) ClockQA,\nwhich comprises various types of clock styles-standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks-paired with time related\nquestions; and 2) CalendarQA, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.",
            "upvotes": 5,
            "discussionId": "67aa05c6ffb9f6b5b2f658fb"
        },
        "publishedAt": "2025-02-10T08:59:36.230Z",
        "title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05092.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "657ccbf2869d5bb0e53b482f",
            "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
            "fullname": "Rohit Saxena",
            "name": "rohitsaxena",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04689",
            "authors": [
                {
                    "_id": "67a9b911b1f5eece682d7961",
                    "user": {
                        "_id": "64510a21f800611f94f0d9f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
                        "isPro": false,
                        "fullname": "Yuwei Yin",
                        "user": "yuweiyin",
                        "type": "user"
                    },
                    "name": "Yuwei Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:32.672Z",
                    "hidden": false
                },
                {
                    "_id": "67a9b911b1f5eece682d7962",
                    "name": "Giuseppe Carenini",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T06:30:33.000Z",
            "title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning",
            "summary": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.",
            "upvotes": 4,
            "discussionId": "67a9b911b1f5eece682d798c"
        },
        "publishedAt": "2025-02-10T03:30:51.974Z",
        "title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64510a21f800611f94f0d9f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
            "fullname": "Yuwei Yin",
            "name": "yuweiyin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.03512",
            "authors": [
                {
                    "_id": "67a9a7cb6be3ca4a7ede471e",
                    "name": "Amitava Das",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede471f",
                    "name": "Yaswanth Narsupalli",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede4720",
                    "name": "Gurpreet Singh",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede4721",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede4722",
                    "name": "Vasu Sharma",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede4723",
                    "name": "Suranjana Trivedy",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede4724",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-10T09:49:39.550Z",
                    "hidden": false
                },
                {
                    "_id": "67a9a7cb6be3ca4a7ede4725",
                    "name": "Amit Sheth",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T18:46:20.000Z",
            "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing\n  Multi-Objective Optimization based DPO for Text-to-Image Alignment",
            "summary": "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that\ngenerated visuals not only accurately encapsulate user intents but also conform\nto stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini\nfiasco, where misaligned outputs triggered significant public backlash,\nunderscore the critical need for robust alignment mechanisms. In contrast,\nLarge Language Models (LLMs) have achieved notable success in alignment.\nBuilding on these advancements, researchers are eager to apply similar\nalignment techniques, such as Direct Preference Optimization (DPO), to T2I\nsystems to enhance image generation fidelity and reliability.\n  We present YinYangAlign, an advanced benchmarking framework that\nsystematically quantifies the alignment fidelity of T2I systems, addressing six\nfundamental and inherently contradictory design objectives. Each pair\nrepresents fundamental tensions in image generation, such as balancing\nadherence to user prompts with creative modifications or maintaining diversity\nalongside visual coherence. YinYangAlign includes detailed axiom datasets\nfeaturing human prompts, aligned (chosen) responses, misaligned (rejected)\nAI-generated outputs, and explanations of the underlying contradictions.",
            "upvotes": 4,
            "discussionId": "67a9a7cf6be3ca4a7ede47d5"
        },
        "publishedAt": "2025-02-10T02:21:52.370Z",
        "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03512.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04350",
            "authors": [
                {
                    "_id": "67a97a77d163c9e6ea2bdb85",
                    "name": "Yongchao Chen",
                    "hidden": false
                },
                {
                    "_id": "67a97a77d163c9e6ea2bdb86",
                    "name": "Yilun Hao",
                    "hidden": false
                },
                {
                    "_id": "67a97a77d163c9e6ea2bdb87",
                    "name": "Yueying Liu",
                    "hidden": false
                },
                {
                    "_id": "67a97a77d163c9e6ea2bdb88",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a97a77d163c9e6ea2bdb89",
                    "name": "Chuchu Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T15:53:59.000Z",
            "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
            "summary": "Existing methods fail to effectively steer Large Language Models (LLMs)\nbetween textual reasoning and code generation, leaving symbolic computing\ncapabilities underutilized. We introduce CodeSteer, an effective method for\nguiding LLM code/text generation. We construct a comprehensive benchmark\nSymBench comprising 37 symbolic tasks with adjustable complexity and also\nsynthesize datasets of 12k multi-round guidance/generation trajectories and\n5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly\ndesigned multi-round supervised fine-tuning (SFT) and direct preference\noptimization (DPO). The resulting model, CodeSteerLLM, augmented with the\nproposed symbolic and self-answer checkers, effectively guides the code/text\ngeneration of larger models. Augmenting GPT-4o with CodeSteer raises its\naverage performance score from 53.3 to 86.4, even outperforming the existing\nbest LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all\n37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates\nsuperior generalizability, providing an average 41.8 performance boost on\nClaude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic\ncomputing to maintain strong performance on highly complex tasks. Models,\nDatasets, and Codes are available at\nhttps://github.com/yongchao98/CodeSteer-v1.0.",
            "upvotes": 4,
            "discussionId": "67a97a79d163c9e6ea2bdc0c"
        },
        "publishedAt": "2025-02-09T23:03:14.294Z",
        "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04350.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6008
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04327",
            "authors": [
                {
                    "_id": "67aa44a0927861da2b7a3479",
                    "user": {
                        "_id": "64d1161315b26cc7f70f37e6",
                        "avatarUrl": "/avatars/c020b7d2c6c2bb1ca289a9cf0c4eaf00.svg",
                        "isPro": false,
                        "fullname": "Oleh Rybkin",
                        "user": "orybkin",
                        "type": "user"
                    },
                    "name": "Oleh Rybkin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-10T18:25:37.833Z",
                    "hidden": false
                },
                {
                    "_id": "67aa44a0927861da2b7a347a",
                    "name": "Michal Nauman",
                    "hidden": false
                },
                {
                    "_id": "67aa44a0927861da2b7a347b",
                    "name": "Preston Fu",
                    "hidden": false
                },
                {
                    "_id": "67aa44a0927861da2b7a347c",
                    "name": "Charlie Snell",
                    "hidden": false
                },
                {
                    "_id": "67aa44a0927861da2b7a347d",
                    "name": "Pieter Abbeel",
                    "hidden": false
                },
                {
                    "_id": "67aa44a0927861da2b7a347e",
                    "name": "Sergey Levine",
                    "hidden": false
                },
                {
                    "_id": "67aa44a0927861da2b7a347f",
                    "name": "Aviral Kumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T18:59:47.000Z",
            "title": "Value-Based Deep RL Scales Predictably",
            "summary": "Scaling data and compute is critical to the success of machine learning.\nHowever, scaling demands predictability: we want methods to not only perform\nwell with more compute or data, but also have their performance be predictable\nfrom small-scale runs, without running the large-scale experiment. In this\npaper, we show that value-based off-policy RL methods are predictable despite\ncommunity lore regarding their pathological behavior. First, we show that data\nand compute requirements to attain a given performance level lie on a Pareto\nfrontier, controlled by the updates-to-data (UTD) ratio. By estimating this\nfrontier, we can predict this data requirement when given more compute, and\nthis compute requirement when given more data. Second, we determine the optimal\nallocation of a total resource budget across data and compute for a given\nperformance and use it to determine hyperparameters that maximize performance\nfor a given budget. Third, this scaling behavior is enabled by first estimating\npredictable relationships between hyperparameters, which is used to manage\neffects of overfitting and plasticity loss unique to RL. We validate our\napproach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI\ngym, and IsaacGym, when extrapolating to higher levels of data, compute,\nbudget, or performance.",
            "upvotes": 3,
            "discussionId": "67aa44a1927861da2b7a34bc"
        },
        "publishedAt": "2025-02-10T13:27:42.383Z",
        "title": "Value-Based Deep RL Scales Predictably",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64d1161315b26cc7f70f37e6/BNKgENZKBSBIAAhDgUpwL.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04327.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "64d1161315b26cc7f70f37e6",
            "avatarUrl": "/avatars/c020b7d2c6c2bb1ca289a9cf0c4eaf00.svg",
            "fullname": "Oleh Rybkin",
            "name": "orybkin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04376",
            "authors": [
                {
                    "_id": "67a998fe495b23306cdbf51d",
                    "name": "Lingxiang Hu",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf51e",
                    "name": "Shurun Yuan",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf51f",
                    "name": "Xiaoting Qin",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf520",
                    "name": "Jue Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf521",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf522",
                    "name": "Dongmei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf523",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "67a998fe495b23306cdbf524",
                    "name": "Qi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T16:25:43.000Z",
            "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
            "summary": "In contemporary workplaces, meetings are essential for exchanging ideas and\nensuring team alignment but often face challenges such as time consumption,\nscheduling conflicts, and inefficient participation. Recent advancements in\nLarge Language Models (LLMs) have demonstrated their strong capabilities in\nnatural language generation and reasoning, prompting the question: can LLMs\neffectively delegate participants in meetings? To explore this, we develop a\nprototype LLM-powered meeting delegate system and create a comprehensive\nbenchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o\nmaintain balanced performance between active and cautious engagement\nstrategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini\n1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\%\nof responses address at least one key point from the ground-truth. However,\nimprovements are needed to reduce irrelevant or repetitive content and enhance\ntolerance for transcription errors commonly found in real-world settings.\nAdditionally, we implement the system in practical settings and collect\nreal-world feedback from demos. Our findings underscore the potential and\nchallenges of utilizing LLMs as meeting delegates, offering valuable insights\ninto their practical application for alleviating the burden of meetings.",
            "upvotes": 3,
            "discussionId": "67a99900495b23306cdbf57e"
        },
        "publishedAt": "2025-02-10T01:15:52.070Z",
        "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04376.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662b0bc9c709a61df8291c0f",
            "avatarUrl": "/avatars/16dd4d945e9fbef5ac889a8087101ded.svg",
            "fullname": "Xiaoting Qin",
            "name": "XiaotingQin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.03771",
            "authors": [
                {
                    "_id": "67aaa0ebe37429ebdbd113cf",
                    "name": "Luis Gaspar Schroeder",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d0",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d1",
                    "name": "Alejandro Cuadron",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d2",
                    "name": "Mark Zhao",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d3",
                    "name": "Stephan Krusche",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d4",
                    "name": "Alfons Kemper",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d5",
                    "name": "Matei Zaharia",
                    "hidden": false
                },
                {
                    "_id": "67aaa0ebe37429ebdbd113d6",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T04:16:20.000Z",
            "title": "Adaptive Semantic Prompt Caching with VectorQ",
            "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
            "upvotes": 2,
            "discussionId": "67aaa0ebe37429ebdbd113fb"
        },
        "publishedAt": "2025-02-10T19:59:41.241Z",
        "title": "Adaptive Semantic Prompt Caching with VectorQ",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03771.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652a656d1a3250bbfe3bb92d",
            "avatarUrl": "/avatars/a1c25150d55c493edd9a7f81287fc449.svg",
            "fullname": "Alejandro Cuadron Lafuente",
            "name": "AlexCuadron",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2501.12387",
            "authors": [
                {
                    "_id": "67a1596a167bea74d5057f25",
                    "name": "Qianqian Wang",
                    "hidden": false
                },
                {
                    "_id": "67a1596a167bea74d5057f26",
                    "name": "Yifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67a1596a167bea74d5057f27",
                    "name": "Aleksander Holynski",
                    "hidden": false
                },
                {
                    "_id": "67a1596a167bea74d5057f28",
                    "name": "Alexei A. Efros",
                    "hidden": false
                },
                {
                    "_id": "67a1596a167bea74d5057f29",
                    "name": "Angjoo Kanazawa",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-21T18:59:23.000Z",
            "title": "Continuous 3D Perception Model with Persistent State",
            "summary": "We present a unified framework capable of solving a broad range of 3D tasks.\nOur approach features a stateful recurrent model that continuously updates its\nstate representation with each new observation. Given a stream of images, this\nevolving state can be used to generate metric-scale pointmaps (per-pixel 3D\npoints) for each new input in an online fashion. These pointmaps reside within\na common coordinate system, and can be accumulated into a coherent, dense scene\nreconstruction that updates as new images arrive. Our model, called CUT3R\n(Continuous Updating Transformer for 3D Reconstruction), captures rich priors\nof real-world scenes: not only can it predict accurate pointmaps from image\nobservations, but it can also infer unseen regions of the scene by probing at\nvirtual, unobserved views. Our method is simple yet highly flexible, naturally\naccepting varying lengths of images that may be either video streams or\nunordered photo collections, containing both static and dynamic content. We\nevaluate our method on various 3D/4D tasks and demonstrate competitive or\nstate-of-the-art performance in each. Project Page: https://cut3r.github.io/",
            "upvotes": 1,
            "discussionId": "67a1596d167bea74d5057fa9"
        },
        "publishedAt": "2025-02-10T14:43:39.581Z",
        "title": "Continuous 3D Perception Model with Persistent State",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12387.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 754
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.02909",
            "authors": [
                {
                    "_id": "67aa91fd5f845ebfe01d7769",
                    "name": "Dinithi Jayasuriya",
                    "hidden": false
                },
                {
                    "_id": "67aa91fd5f845ebfe01d776a",
                    "name": "Sina Tayebati",
                    "hidden": false
                },
                {
                    "_id": "67aa91fd5f845ebfe01d776b",
                    "name": "Davide Ettori",
                    "hidden": false
                },
                {
                    "_id": "67aa91fd5f845ebfe01d776c",
                    "user": {
                        "_id": "647a45aeccb84c6180b41b54",
                        "avatarUrl": "/avatars/cd0db59a1b7f49f53f65751a8efc1033.svg",
                        "isPro": false,
                        "fullname": "Ranganath Krishnan",
                        "user": "ranganathkrishnan",
                        "type": "user"
                    },
                    "name": "Ranganath Krishnan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-10T23:55:43.302Z",
                    "hidden": false
                },
                {
                    "_id": "67aa91fd5f845ebfe01d776d",
                    "name": "Amit Ranjan Trivedi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T06:11:55.000Z",
            "title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in\n  LLMs",
            "summary": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs.",
            "upvotes": 0,
            "discussionId": "67aa91ff5f845ebfe01d77fc"
        },
        "publishedAt": "2025-02-10T18:55:57.167Z",
        "title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02909.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655ec30b12fb73960ceb048f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655ec30b12fb73960ceb048f/q7zVSStJWBywrtPoL2ChO.png",
            "fullname": "Sina Tayebati",
            "name": "sinatayebati",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.02692",
            "authors": [
                {
                    "_id": "67aa915d2e821999a96f8d85",
                    "name": "Amit Ranjan Trivedi",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d86",
                    "name": "Sina Tayebati",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d87",
                    "name": "Hemant Kumawat",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d88",
                    "name": "Nastaran Darabi",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d89",
                    "name": "Divake Kumar",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d8a",
                    "name": "Adarsh Kumar Kosta",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d8b",
                    "name": "Yeshwanth Venkatesha",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d8c",
                    "name": "Dinithi Jayasuriya",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d8d",
                    "name": "Nethmi Jayasinghe",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d8e",
                    "name": "Priyadarshini Panda",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d8f",
                    "name": "Saibal Mukhopadhyay",
                    "hidden": false
                },
                {
                    "_id": "67aa915d2e821999a96f8d90",
                    "name": "Kaushik Roy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-04T20:13:58.000Z",
            "title": "Intelligent Sensing-to-Action for Robust Autonomy at the Edge:\n  Opportunities and Challenges",
            "summary": "Autonomous edge computing in robotics, smart cities, and autonomous vehicles\nrelies on the seamless integration of sensing, processing, and actuation for\nreal-time decision-making in dynamic environments. At its core is the\nsensing-to-action loop, which iteratively aligns sensor inputs with\ncomputational models to drive adaptive control strategies. These loops can\nadapt to hyper-local conditions, enhancing resource efficiency and\nresponsiveness, but also face challenges such as resource constraints,\nsynchronization delays in multi-modal data fusion, and the risk of cascading\nerrors in feedback loops. This article explores how proactive, context-aware\nsensing-to-action and action-to-sensing adaptations can enhance efficiency by\ndynamically adjusting sensing and computation based on task demands, such as\nsensing a very limited part of the environment and predicting the rest. By\nguiding sensing through control actions, action-to-sensing pathways can improve\ntask relevance and resource use, but they also require robust monitoring to\nprevent cascading errors and maintain reliability. Multi-agent sensing-action\nloops further extend these capabilities through coordinated sensing and actions\nacross distributed agents, optimizing resource use via collaboration.\nAdditionally, neuromorphic computing, inspired by biological systems, provides\nan efficient framework for spike-based, event-driven processing that conserves\nenergy, reduces latency, and supports hierarchical control--making it ideal for\nmulti-agent optimization. This article highlights the importance of end-to-end\nco-design strategies that align algorithmic models with hardware and\nenvironmental dynamics and improve cross-layer interdependencies to improve\nthroughput, precision, and adaptability for energy-efficient edge autonomy in\ncomplex environments.",
            "upvotes": 0,
            "discussionId": "67aa91602e821999a96f8e79"
        },
        "publishedAt": "2025-02-10T18:54:04.415Z",
        "title": "Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02692.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655ec30b12fb73960ceb048f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655ec30b12fb73960ceb048f/q7zVSStJWBywrtPoL2ChO.png",
            "fullname": "Sina Tayebati",
            "name": "sinatayebati",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]