[
    {
        "paper": {
            "id": "2502.06394",
            "authors": [
                {
                    "_id": "67aafead3711ca5b760f324c",
                    "user": {
                        "_id": "61ade264f602880813dbe10b",
                        "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
                        "isPro": false,
                        "fullname": "Daniil Moskovskiy",
                        "user": "etomoscow",
                        "type": "user"
                    },
                    "name": "Daniil Moskovskiy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:17.448Z",
                    "hidden": false
                },
                {
                    "_id": "67aafead3711ca5b760f324d",
                    "user": {
                        "_id": "634c72e6fe1bfa967d6c2b5c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c72e6fe1bfa967d6c2b5c/WFWIAlWl-FsiJRyGxQTTx.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Sushko",
                        "user": "chameleon-lizard",
                        "type": "user"
                    },
                    "name": "Nikita Sushko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:21.453Z",
                    "hidden": false
                },
                {
                    "_id": "67aafead3711ca5b760f324e",
                    "user": {
                        "_id": "5dfa8e07da6d0311fd3d5430",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
                        "isPro": false,
                        "fullname": "Sergey Pletenev",
                        "user": "memyprokotow",
                        "type": "user"
                    },
                    "name": "Sergey Pletenev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T09:59:47.063Z",
                    "hidden": false
                },
                {
                    "_id": "67aafead3711ca5b760f324f",
                    "user": {
                        "_id": "662f8d645c4db70c77a203b0",
                        "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
                        "isPro": false,
                        "fullname": "Elena Tutubalina",
                        "user": "tlenusik",
                        "type": "user"
                    },
                    "name": "Elena Tutubalina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T09:59:50.003Z",
                    "hidden": false
                },
                {
                    "_id": "67aafead3711ca5b760f3250",
                    "user": {
                        "_id": "605473729d7c1d4d81b7e52b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
                        "isPro": false,
                        "fullname": "Alexander Panchenko",
                        "user": "apanc",
                        "type": "user"
                    },
                    "name": "Alexander Panchenko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:54:34.688Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T12:30:25.000Z",
            "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data\n  Annotators",
            "summary": "Existing approaches to multilingual text detoxification are hampered by the\nscarcity of parallel multilingual datasets. In this work, we introduce a\npipeline for the generation of multilingual parallel detoxification data. We\nalso introduce SynthDetoxM, a manually collected and synthetically generated\nmultilingual parallel text detoxification dataset comprising 16,000\nhigh-quality detoxification sentence pairs across German, French, Spanish and\nRussian. The data was sourced from different toxicity evaluation datasets and\nthen rewritten with nine modern open-source LLMs in few-shot setting. Our\nexperiments demonstrate that models trained on the produced synthetic datasets\nhave superior performance to those trained on the human-annotated\nMultiParaDetox dataset even in data limited setting. Models trained on\nSynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our\ndataset and code to help further research in multilingual text detoxification.",
            "upvotes": 70,
            "discussionId": "67aafeae3711ca5b760f3280"
        },
        "publishedAt": "2025-02-11T03:03:12.135Z",
        "title": "SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06394.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61ade264f602880813dbe10b",
            "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
            "fullname": "Daniil Moskovskiy",
            "name": "etomoscow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06703",
            "authors": [
                {
                    "_id": "67aabf93c0f8648f68c68ce4",
                    "user": {
                        "_id": "667187ba9ab144eb3ac43a1b",
                        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
                        "isPro": false,
                        "fullname": "Runze Liu",
                        "user": "RyanLiu112",
                        "type": "user"
                    },
                    "name": "Runze Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:22.940Z",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68ce5",
                    "user": {
                        "_id": "67ab05fe4c6ca2d5db4c0c52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
                        "isPro": false,
                        "fullname": "Junqi Gao",
                        "user": "ChetKao",
                        "type": "user"
                    },
                    "name": "Junqi Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:54:46.128Z",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68ce6",
                    "name": "Jian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68ce7",
                    "user": {
                        "_id": "60bc94cd85a3ab33829b6211",
                        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "iseesaw",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:18.725Z",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68ce8",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68ce9",
                    "user": {
                        "_id": "645d9c3058f9ee315148116d",
                        "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
                        "isPro": false,
                        "fullname": "Biqing Qi",
                        "user": "jackqi7",
                        "type": "user"
                    },
                    "name": "Biqing Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:55:23.328Z",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68cea",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67aabf93c0f8648f68c68ceb",
                    "user": {
                        "_id": "669f614b59adf5b56e05bce3",
                        "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
                        "isPro": false,
                        "fullname": "BowenZhou",
                        "user": "bowenZhou",
                        "type": "user"
                    },
                    "name": "Bowen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:55:11.315Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T17:30:23.000Z",
            "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time\n  Scaling",
            "summary": "Test-Time Scaling (TTS) is an important method for improving the performance\nof Large Language Models (LLMs) by using additional computation during the\ninference phase. However, current studies do not systematically analyze how\npolicy models, Process Reward Models (PRMs), and problem difficulty influence\nTTS. This lack of analysis limits the understanding and practical use of TTS\nmethods. In this paper, we focus on two core questions: (1) What is the optimal\napproach to scale test-time computation across different policy models, PRMs,\nand problem difficulty levels? (2) To what extent can extended computation\nimprove the performance of LLMs on complex tasks, and can smaller language\nmodels outperform larger ones through this approach? Through comprehensive\nexperiments on MATH-500 and challenging AIME24 tasks, we have the following\nobservations: (1) The compute-optimal TTS strategy is highly dependent on the\nchoice of policy model, PRM, and problem difficulty. (2) With our\ncompute-optimal TTS strategy, extremely small policy models can outperform\nlarger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500.\nMoreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM\nsurpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher\ninference efficiency. These findings show the significance of adapting TTS\nstrategies to the specific characteristics of each task and model and indicate\nthat TTS is a promising approach for enhancing the reasoning abilities of LLMs.",
            "upvotes": 66,
            "discussionId": "67aabf94c0f8648f68c68d19"
        },
        "publishedAt": "2025-02-11T00:36:11.270Z",
        "title": "Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06703.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6032
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06781",
            "authors": [
                {
                    "_id": "67aacd7e078cdf445284f9f6",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9f7",
                    "user": {
                        "_id": "650ab54e23196fb2d86b486b",
                        "avatarUrl": "/avatars/e0506393589695b553ec9ee3fe99b93a.svg",
                        "isPro": false,
                        "fullname": "SongYang Gao",
                        "user": "Wizardcoast",
                        "type": "user"
                    },
                    "name": "Songyang Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:56:04.271Z",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9f8",
                    "user": {
                        "_id": "64ee366d1667d8973af3e8ca",
                        "avatarUrl": "/avatars/7ff57a0d2ec93a8b9aec980a2f6e94fd.svg",
                        "isPro": false,
                        "fullname": "Yuzhe Gu",
                        "user": "Tracygu",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:56:15.905Z",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9f9",
                    "user": {
                        "_id": "64e8505321540e1da3226b54",
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:40.279Z",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9fa",
                    "user": {
                        "_id": "64070c5c4dc5f2846c925e93",
                        "avatarUrl": "/avatars/ac2d7c1cd4ecccd6a88b85767c963ec7.svg",
                        "isPro": false,
                        "fullname": "Gao Jianfei",
                        "user": "pppppM",
                        "type": "user"
                    },
                    "name": "Jianfei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:56:31.173Z",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9fb",
                    "name": "Kuikun Liu",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9fc",
                    "name": "Ziyi Wang",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9fd",
                    "name": "Shuaibin Li",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9fe",
                    "name": "Qian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284f9ff",
                    "name": "Haian Huang",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa00",
                    "name": "Weihan Cao",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa01",
                    "name": "Jiangning Liu",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa02",
                    "name": "Hongwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa03",
                    "name": "Junnan Liu",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa04",
                    "user": {
                        "_id": "630716d11801ecc7d2595021",
                        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                        "isPro": false,
                        "fullname": "Songyang Zhang",
                        "user": "zsytony",
                        "type": "user"
                    },
                    "name": "Songyang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:37.733Z",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa05",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "67aacd7e078cdf445284fa06",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:57:29.000Z",
            "title": "Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning",
            "summary": "Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough Outcome REwArd-based reinforcement\nLearning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture researchhttps://github.com/InternLM/OREAL.",
            "upvotes": 36,
            "discussionId": "67aacd7f078cdf445284fa4b"
        },
        "publishedAt": "2025-02-10T23:18:11.727Z",
        "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06781.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "fullname": "yuzhe gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06060",
            "authors": [
                {
                    "_id": "67ab1314385da1f07cda1271",
                    "user": {
                        "_id": "63abbf74ad514ca8d14a0548",
                        "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
                        "isPro": false,
                        "fullname": "Bidipta Sarkar",
                        "user": "bidiptas",
                        "type": "user"
                    },
                    "name": "Bidipta Sarkar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T09:51:17.933Z",
                    "hidden": false
                },
                {
                    "_id": "67ab1314385da1f07cda1272",
                    "name": "Warren Xia",
                    "hidden": false
                },
                {
                    "_id": "67ab1314385da1f07cda1273",
                    "name": "C. Karen Liu",
                    "hidden": false
                },
                {
                    "_id": "67ab1314385da1f07cda1274",
                    "name": "Dorsa Sadigh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T22:44:45.000Z",
            "title": "Training Language Models for Social Deduction with Multi-Agent\n  Reinforcement Learning",
            "summary": "Communicating in natural language is a powerful tool in multi-agent settings,\nas it enables independent agents to share information in partially observable\nsettings and allows zero-shot coordination with humans. However, most prior\nworks are limited as they either rely on training with large amounts of human\ndemonstrations or lack the ability to generate natural and useful communication\nstrategies. In this work, we train language models to have productive\ndiscussions about their environment in natural language without any human\ndemonstrations. We decompose the communication problem into listening and\nspeaking. Our key idea is to leverage the agent's goal to predict useful\ninformation about the world as a dense reward signal that guides communication.\nSpecifically, we improve a model's listening skills by training them to predict\ninformation about the environment based on discussions, and we simultaneously\nimprove a model's speaking skills with multi-agent reinforcement learning by\nrewarding messages based on their influence on other agents. To investigate the\nrole and necessity of communication in complex social settings, we study an\nembodied social deduction game based on Among Us, where the key question to\nanswer is the identity of an adversarial imposter. We analyze emergent\nbehaviors due to our technique, such as accusing suspects and providing\nevidence, and find that it enables strong discussions, doubling the win rates\ncompared to standard RL. We release our code and models at\nhttps://socialdeductionllm.github.io/",
            "upvotes": 22,
            "discussionId": "67ab1315385da1f07cda12a5"
        },
        "publishedAt": "2025-02-11T04:08:55.672Z",
        "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06060.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63abbf74ad514ca8d14a0548",
            "avatarUrl": "/avatars/b1357b73b8f9a8ff9908710ad64154ef.svg",
            "fullname": "Bidipta Sarkar",
            "name": "bidiptas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05664",
            "authors": [
                {
                    "_id": "67ab56dc0bc5f6a94eb49892",
                    "user": {
                        "_id": "63fe51dcc0ec83fda436d558",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fe51dcc0ec83fda436d558/22wrFA08OxRLIsRVxPts0.jpeg",
                        "isPro": false,
                        "fullname": "Md. Ashraful Islam",
                        "user": "ashraful",
                        "type": "user"
                    },
                    "name": "Md. Ashraful Islam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T14:25:21.564Z",
                    "hidden": false
                },
                {
                    "_id": "67ab56dc0bc5f6a94eb49893",
                    "name": "Mohammed Eunus Ali",
                    "hidden": false
                },
                {
                    "_id": "67ab56dc0bc5f6a94eb49894",
                    "name": "Md Rizwan Parvez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-08T18:43:59.000Z",
            "title": "CODESIM: Multi-Agent Code Generation and Problem Solving through\n  Simulation-Driven Planning and Debugging",
            "summary": "Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).",
            "upvotes": 15,
            "discussionId": "67ab56de0bc5f6a94eb49918"
        },
        "publishedAt": "2025-02-11T09:36:24.937Z",
        "title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05664.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fe51dcc0ec83fda436d558",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fe51dcc0ec83fda436d558/22wrFA08OxRLIsRVxPts0.jpeg",
            "fullname": "Md. Ashraful Islam",
            "name": "ashraful",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06049",
            "authors": [
                {
                    "_id": "67aac01bd7b18841e7c266df",
                    "user": {
                        "_id": "6489e10ca13f65198dc6e122",
                        "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
                        "isPro": false,
                        "fullname": "Kang",
                        "user": "JaxonK",
                        "type": "user"
                    },
                    "name": "Jikun Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T15:26:38.660Z",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e0",
                    "name": "Wenqi Wu",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e1",
                    "name": "Filippos Christianos",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e2",
                    "user": {
                        "_id": "636c1e4415cd58e915bc45df",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c1e4415cd58e915bc45df/KnPgdPe0G5ngvXaCBua6R.jpeg",
                        "isPro": false,
                        "fullname": "Alex J. Chan",
                        "user": "XanderJC",
                        "type": "user"
                    },
                    "name": "Alex J. Chan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T15:26:36.110Z",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e3",
                    "name": "Fraser Greenlee",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e4",
                    "name": "George Thomas",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e5",
                    "name": "Marvin Purtorab",
                    "hidden": false
                },
                {
                    "_id": "67aac01bd7b18841e7c266e6",
                    "name": "Andy Toulis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T22:11:42.000Z",
            "title": "LM2: Large Memory Models",
            "summary": "This paper introduces the Large Memory Model (LM2), a decoder-only\nTransformer architecture enhanced with an auxiliary memory module that aims to\naddress the limitations of standard Transformers in multi-step reasoning,\nrelational argumentation, and synthesizing information distributed over long\ncontexts. The proposed LM2 incorporates a memory module that acts as a\ncontextual representation repository, interacting with input tokens via cross\nattention and updating through gating mechanisms. To preserve the Transformers\ngeneral-purpose capabilities, LM2 maintains the original information flow while\nintegrating a complementary memory pathway. Experimental results on the\nBABILong benchmark demonstrate that the LM2model outperforms both the\nmemory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3%\non average across tasks. LM2 exhibits exceptional capabilities in multi-hop\ninference, numerical reasoning, and large-context question-answering. On the\nMMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model,\ndemonstrating that its memory module does not degrade performance on general\ntasks. Further, in our analysis, we explore the memory interpretability,\neffectiveness of memory modules, and test-time behavior. Our findings emphasize\nthe importance of explicit memory in enhancing Transformer architectures.",
            "upvotes": 14,
            "discussionId": "67aac01dd7b18841e7c26739"
        },
        "publishedAt": "2025-02-10T22:13:17.117Z",
        "title": "LM2: Large Memory Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06049.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6489e10ca13f65198dc6e122",
            "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
            "fullname": "Kang",
            "name": "JaxonK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05415",
            "authors": [
                {
                    "_id": "67aaea0a0acaa007694aed73",
                    "user": {
                        "_id": "65708920806dee337da0eef5",
                        "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
                        "isPro": false,
                        "fullname": "xuchenkai",
                        "user": "UnhurriedDawn",
                        "type": "user"
                    },
                    "name": "Chenkai Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:28.861Z",
                    "hidden": false
                },
                {
                    "_id": "67aaea0a0acaa007694aed74",
                    "user": {
                        "_id": "6644548a3a16452261cdb173",
                        "avatarUrl": "/avatars/4643db904204e3a60202a29e8c884139.svg",
                        "isPro": false,
                        "fullname": "wangxu",
                        "user": "asunalove",
                        "type": "user"
                    },
                    "name": "Xu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:26.432Z",
                    "hidden": false
                },
                {
                    "_id": "67aaea0a0acaa007694aed75",
                    "name": "Zhenyi Liao",
                    "hidden": false
                },
                {
                    "_id": "67aaea0a0acaa007694aed76",
                    "name": "Yishun Li",
                    "hidden": false
                },
                {
                    "_id": "67aaea0a0acaa007694aed77",
                    "name": "Tianqi Hou",
                    "hidden": false
                },
                {
                    "_id": "67aaea0a0acaa007694aed78",
                    "user": {
                        "_id": "64bba541da140e461924dfed",
                        "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
                        "isPro": false,
                        "fullname": "zhijie deng",
                        "user": "zhijie3",
                        "type": "user"
                    },
                    "name": "Zhijie Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:24.089Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-08T02:52:25.000Z",
            "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and\n  Generation",
            "summary": "There has been increasing research interest in building unified multimodal\nunderstanding and generation models, among which Show-o stands as a notable\nrepresentative, demonstrating great promise for both text-to-image and\nimage-to-text generation. The inference of Show-o involves progressively\ndenoising image tokens and autoregressively decoding text tokens, and hence,\nunfortunately, suffers from inefficiency issues from both sides. This paper\nintroduces Show-o Turbo to bridge the gap. We first identify a unified\ndenoising perspective for the generation of images and text in Show-o based on\nthe parallel decoding of text tokens. We then propose to extend consistency\ndistillation (CD), a qualified approach for shortening the denoising process of\ndiffusion models, to the multimodal denoising trajectories of Show-o. We\nintroduce a trajectory segmentation strategy and a curriculum learning\nprocedure to improve the training convergence. Empirically, in text-to-image\ngeneration, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps\nwithout using classifier-free guidance (CFG), outperforming that of the\noriginal Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo\nexhibits a 1.5x speedup without significantly sacrificing performance. The code\nis available at https://github.com/zhijie-group/Show-o-Turbo.",
            "upvotes": 13,
            "discussionId": "67aaea100acaa007694aeea5"
        },
        "publishedAt": "2025-02-11T02:09:27.778Z",
        "title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05415.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "fullname": "zhijie deng",
            "name": "zhijie3",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05609",
            "authors": [
                {
                    "_id": "67aacaaaa03eecbc2d72835f",
                    "user": {
                        "_id": "64ec4c04c782d648d28d70fc",
                        "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
                        "isPro": false,
                        "fullname": "Sukmin Cho",
                        "user": "zomss",
                        "type": "user"
                    },
                    "name": "Sukmin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:43.377Z",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728360",
                    "name": "Sangjin Choi",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728361",
                    "user": {
                        "_id": "64d1e70a84f205869017703b",
                        "avatarUrl": "/avatars/215d0d4db5f79cb74df4d888b18c6a0d.svg",
                        "isPro": false,
                        "fullname": "Taeho Hwang",
                        "user": "doubleyyh",
                        "type": "user"
                    },
                    "name": "Taeho Hwang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:54:45.737Z",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728362",
                    "name": "Jeongyeon Seo",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728363",
                    "name": "Soyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728364",
                    "name": "Huije Lee",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728365",
                    "name": "Hoyun Song",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728366",
                    "name": "Jong C. Park",
                    "hidden": false
                },
                {
                    "_id": "67aacaaaa03eecbc2d728367",
                    "name": "Youngjin Kwon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-08T15:32:53.000Z",
            "title": "Lossless Acceleration of Large Language Models with Hierarchical\n  Drafting based on Temporal Locality in Speculative Decoding",
            "summary": "Accelerating inference in Large Language Models (LLMs) is critical for\nreal-time interactions, as they have been widely incorporated into real-world\nservices. Speculative decoding, a fully algorithmic solution, has gained\nattention for improving inference speed by drafting and verifying tokens,\nthereby generating multiple tokens in a single forward pass. However, current\ndrafting strategies usually require significant fine-tuning or have\ninconsistent performance across tasks. To address these challenges, we propose\nHierarchy Drafting (HD), a novel lossless drafting approach that organizes\nvarious token sources into multiple databases in a hierarchical framework based\non temporal locality. In the drafting step, HD sequentially accesses multiple\ndatabases to obtain draft tokens from the highest to the lowest locality,\nensuring consistent acceleration across diverse tasks and minimizing drafting\nlatency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters\ndemonstrate that HD outperforms existing database drafting methods, achieving\nrobust inference speedups across model sizes, tasks, and temperatures.",
            "upvotes": 12,
            "discussionId": "67aacaaca03eecbc2d728394"
        },
        "publishedAt": "2025-02-10T22:58:41.471Z",
        "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05609.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64ec4c04c782d648d28d70fc",
            "avatarUrl": "/avatars/6975526fcf4b513cc934b5bc45370a48.svg",
            "fullname": "Sukmin Cho",
            "name": "zomss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06786",
            "authors": [
                {
                    "_id": "67aae91b83b1182df7c0cf54",
                    "name": "Pranav Nair",
                    "hidden": false
                },
                {
                    "_id": "67aae91b83b1182df7c0cf55",
                    "name": "Puranjay Datta",
                    "hidden": false
                },
                {
                    "_id": "67aae91b83b1182df7c0cf56",
                    "name": "Jeff Dean",
                    "hidden": false
                },
                {
                    "_id": "67aae91b83b1182df7c0cf57",
                    "name": "Prateek Jain",
                    "hidden": false
                },
                {
                    "_id": "67aae91b83b1182df7c0cf58",
                    "name": "Aditya Kusupati",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:59:10.000Z",
            "title": "Matryoshka Quantization",
            "summary": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to 10% more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.",
            "upvotes": 11,
            "discussionId": "67aae91d83b1182df7c0cff6"
        },
        "publishedAt": "2025-02-11T01:07:50.116Z",
        "title": "Matryoshka Quantization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06786.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6032
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06772",
            "authors": [
                {
                    "_id": "67aac8adfe33f6d8d695bc40",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T14:25:31.970Z",
                    "hidden": false
                },
                {
                    "_id": "67aac8adfe33f6d8d695bc41",
                    "name": "Zhaochen Yu",
                    "hidden": false
                },
                {
                    "_id": "67aac8adfe33f6d8d695bc42",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "67aac8adfe33f6d8d695bc43",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:51:47.000Z",
            "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
            "summary": "We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux",
            "upvotes": 11,
            "discussionId": "67aac8affe33f6d8d695bcbd"
        },
        "publishedAt": "2025-02-10T22:49:56.390Z",
        "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06772.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06788",
            "authors": [
                {
                    "_id": "67aac64de37429ebdbdafc40",
                    "name": "Haiwen Diao",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc41",
                    "name": "Xiaotong Li",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc42",
                    "name": "Yufeng Cui",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc43",
                    "user": {
                        "_id": "6458b59c7a7e192202df8fa0",
                        "avatarUrl": "/avatars/33ee716477e5686da8723d01e199cd27.svg",
                        "isPro": false,
                        "fullname": "Yueze Wang",
                        "user": "yzwang",
                        "type": "user"
                    },
                    "name": "Yueze Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T14:25:34.818Z",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc44",
                    "name": "Haoge Deng",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc45",
                    "user": {
                        "_id": "6565bc5ee5aac326bfc98e39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vIfHy9Y1yAK6A96UCHNBH.jpeg",
                        "isPro": false,
                        "fullname": "Ting Pan",
                        "user": "PhyscalX",
                        "type": "user"
                    },
                    "name": "Ting Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:09.401Z",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc46",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc47",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "67aac64de37429ebdbdafc48",
                    "name": "Xinlong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:59:58.000Z",
            "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
            "summary": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.",
            "upvotes": 9,
            "discussionId": "67aac64ee37429ebdbdafc96"
        },
        "publishedAt": "2025-02-10T22:40:39.442Z",
        "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06788.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4a717aa03b6520839e9b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
            "fullname": "Haiwen Diao",
            "name": "Paranioar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.03628",
            "authors": [
                {
                    "_id": "67aab82e6024056209d727a8",
                    "name": "Zhuowei Li",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727a9",
                    "name": "Haizhou Shi",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727aa",
                    "name": "Yunhe Gao",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727ab",
                    "name": "Di Liu",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727ac",
                    "name": "Zhenting Wang",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727ad",
                    "name": "Yuxiao Chen",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727ae",
                    "name": "Ting Liu",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727af",
                    "name": "Long Zhao",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727b0",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "67aab82e6024056209d727b1",
                    "name": "Dimitris N. Metaxas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T21:34:02.000Z",
            "title": "The Hidden Life of Tokens: Reducing Hallucination of Large\n  Vision-Language Models via Visual Information Steering",
            "summary": "Large Vision-Language Models (LVLMs) can reason effectively over both textual\nand visual inputs, but they tend to hallucinate syntactically coherent yet\nvisually ungrounded contents. In this paper, we investigate the internal\ndynamics of hallucination by examining the tokens logits rankings throughout\nthe generation process, revealing three key patterns in how LVLMs process\ninformation: (1) gradual visual information loss -- visually grounded tokens\ngradually become less favored throughout generation, and (2) early excitation\n-- semantically meaningful tokens achieve peak activation in the layers earlier\nthan the final layer. (3) hidden genuine information -- visually grounded\ntokens though not being eventually decided still retain relatively high\nrankings at inference. Based on these insights, we propose VISTA (Visual\nInformation Steering with Token-logit Augmentation), a training-free\ninference-time intervention framework that reduces hallucination while\npromoting genuine information. VISTA works by combining two complementary\napproaches: reinforcing visual information in activation space and leveraging\nearly layer activations to promote semantically meaningful decoding. Compared\nto existing methods, VISTA requires no external supervision and is applicable\nto various decoding strategies. Extensive experiments show that VISTA on\naverage reduces hallucination by abount 40% on evaluated open-ended generation\ntask, and it consistently outperforms existing methods on four benchmarks\nacross four architectures under three decoding strategies.",
            "upvotes": 9,
            "discussionId": "67aab82f6024056209d727f6"
        },
        "publishedAt": "2025-02-10T21:38:53.032Z",
        "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03628.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64dfcc62e8b6f3f3baa950e0",
            "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
            "fullname": "Zhenting Wang",
            "name": "ztwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06782",
            "authors": [
                {
                    "_id": "67aae76c71a9983f50e134ef",
                    "name": "Dongyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f0",
                    "name": "Shicheng Li",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f1",
                    "name": "Yutong Liu",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f2",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f3",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f4",
                    "name": "Xinyue Li",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f5",
                    "name": "Qi Qin",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f6",
                    "name": "Yufei Liu",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f7",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f8",
                    "name": "Zhongyu Li",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134f9",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134fa",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134fb",
                    "name": "Yuewen Cao",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134fc",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134fd",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134fe",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e134ff",
                    "name": "Qibin Hou",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e13500",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "67aae76c71a9983f50e13501",
                    "name": "Peng Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:58:11.000Z",
            "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT",
            "summary": "Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
            "upvotes": 8,
            "discussionId": "67aae76e71a9983f50e1357d"
        },
        "publishedAt": "2025-02-11T01:00:25.383Z",
        "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6032
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06764",
            "authors": [
                {
                    "_id": "67aac6052c02e43558b6b4b0",
                    "name": "Kiwhan Song",
                    "hidden": false
                },
                {
                    "_id": "67aac6052c02e43558b6b4b1",
                    "name": "Boyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "67aac6052c02e43558b6b4b2",
                    "name": "Max Simchowitz",
                    "hidden": false
                },
                {
                    "_id": "67aac6052c02e43558b6b4b3",
                    "name": "Yilun Du",
                    "hidden": false
                },
                {
                    "_id": "67aac6052c02e43558b6b4b4",
                    "name": "Russ Tedrake",
                    "hidden": false
                },
                {
                    "_id": "67aac6052c02e43558b6b4b5",
                    "name": "Vincent Sitzmann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:44:25.000Z",
            "title": "History-Guided Video Diffusion",
            "summary": "Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Website: https://boyuan.space/history-guidance",
            "upvotes": 8,
            "discussionId": "67aac6072c02e43558b6b543"
        },
        "publishedAt": "2025-02-11T00:55:33.866Z",
        "title": "History-Guided Video Diffusion",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06764.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6032
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05957",
            "authors": [
                {
                    "_id": "67aaecec114e64d6e15e7f41",
                    "name": "Jiabin Tang",
                    "hidden": false
                },
                {
                    "_id": "67aaecec114e64d6e15e7f42",
                    "name": "Tianyu Fan",
                    "hidden": false
                },
                {
                    "_id": "67aaecec114e64d6e15e7f43",
                    "name": "Chao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T16:53:56.000Z",
            "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
            "summary": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce MetaChain-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, MetaChain comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, MetaChain\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate MetaChain's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.",
            "upvotes": 7,
            "discussionId": "67aaecef114e64d6e15e802c"
        },
        "publishedAt": "2025-02-11T01:33:35.134Z",
        "title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05957.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643b751cc5f633a7fa84b325",
            "avatarUrl": "/avatars/a094b856cf3d51eb78d16a14361def62.svg",
            "fullname": "Tang",
            "name": "Jiabin99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06023",
            "authors": [
                {
                    "_id": "67aac3a9ef5570c0c9047095",
                    "user": {
                        "_id": "640f6299ef5c6dcac8b1df52",
                        "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
                        "isPro": false,
                        "fullname": "Amir",
                        "user": "sahsaeedi",
                        "type": "user"
                    },
                    "name": "Amir Saeidi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-11T03:31:48.492Z",
                    "hidden": false
                },
                {
                    "_id": "67aac3a9ef5570c0c9047096",
                    "name": "Yiran Luo",
                    "hidden": false
                },
                {
                    "_id": "67aac3a9ef5570c0c9047097",
                    "name": "Agneet Chatterjee",
                    "hidden": false
                },
                {
                    "_id": "67aac3a9ef5570c0c9047098",
                    "name": "Shamanthak Hegde",
                    "hidden": false
                },
                {
                    "_id": "67aac3a9ef5570c0c9047099",
                    "name": "Bimsara Pathiraja",
                    "hidden": false
                },
                {
                    "_id": "67aac3a9ef5570c0c904709a",
                    "name": "Yezhou Yang",
                    "hidden": false
                },
                {
                    "_id": "67aac3a9ef5570c0c904709b",
                    "name": "Chitta Baral",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T20:34:43.000Z",
            "title": "Dual Caption Preference Optimization for Diffusion Models",
            "summary": "Recent advancements in human preference optimization, originally developed\nfor Large Language Models (LLMs), have shown significant potential in improving\ntext-to-image diffusion models. These methods aim to learn the distribution of\npreferred samples while distinguishing them from less preferred ones. However,\nexisting preference datasets often exhibit overlap between these distributions,\nleading to a conflict distribution. Additionally, we identified that input\nprompts contain irrelevant information for less preferred images, limiting the\ndenoising network's ability to accurately predict noise in preference\noptimization methods, known as the irrelevant prompt issue. To address these\nchallenges, we propose Dual Caption Preference Optimization (DCPO), a novel\napproach that utilizes two distinct captions to mitigate irrelevant prompts. To\ntackle conflict distribution, we introduce the Pick-Double Caption dataset, a\nmodified version of Pick-a-Pic v2 with separate captions for preferred and less\npreferred images. We further propose three different strategies for generating\ndistinct captions: captioning, perturbation, and hybrid methods. Our\nexperiments show that DCPO significantly improves image quality and relevance\nto prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO,\nand MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval,\nCLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.",
            "upvotes": 7,
            "discussionId": "67aac3b1ef5570c0c9047264"
        },
        "publishedAt": "2025-02-10T22:33:17.468Z",
        "title": "Dual Caption Preference Optimization for Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06023.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640f6299ef5c6dcac8b1df52",
            "avatarUrl": "/avatars/022f21183abc8a8b5ce1b198d3ba96dc.svg",
            "fullname": "Amir",
            "name": "sahsaeedi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05795",
            "authors": [
                {
                    "_id": "67ab189a8087b66340398b01",
                    "user": {
                        "_id": "643ce831a16fa581f3f826c9",
                        "avatarUrl": "/avatars/2f2dffb660eee3d3c7029dd7305f5226.svg",
                        "isPro": false,
                        "fullname": "Wenfang Sun",
                        "user": "lmsdss",
                        "type": "user"
                    },
                    "name": "Wenfang Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T14:25:29.711Z",
                    "hidden": false
                },
                {
                    "_id": "67ab189a8087b66340398b02",
                    "name": "Xinyuan Song",
                    "hidden": false
                },
                {
                    "_id": "67ab189a8087b66340398b03",
                    "user": {
                        "_id": "64245f2c089d5fae56b4549a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                        "isPro": false,
                        "fullname": "Pengxiang Li",
                        "user": "pengxiang",
                        "type": "user"
                    },
                    "name": "Pengxiang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T09:51:15.671Z",
                    "hidden": false
                },
                {
                    "_id": "67ab189a8087b66340398b04",
                    "name": "Lu Yin",
                    "hidden": false
                },
                {
                    "_id": "67ab189a8087b66340398b05",
                    "name": "Yefeng Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ab189a8087b66340398b06",
                    "name": "Shiwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T07:03:36.000Z",
            "title": "The Curse of Depth in Large Language Models",
            "summary": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language\nModels(LLMs) where nearly half of the layers are less effective than expected.\nWe first confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling, which scales the variance\nof output of the layer normalization inversely by the square root of its depth.\nThis simple modification mitigates the output variance explosion of deeper\nTransformer layers, improving their contribution. Our experimental results,\nspanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling\nsignificantly enhances LLM pre-training performance compared to Pre-LN.\nMoreover, this improvement seamlessly carries over to supervised fine-tuning.\nAll these gains can be attributed to the fact that LayerNorm Scaling enables\ndeeper layers to contribute more effectively during training.",
            "upvotes": 6,
            "discussionId": "67ab189b8087b66340398b3b"
        },
        "publishedAt": "2025-02-11T04:30:30.043Z",
        "title": "The Curse of Depth in Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05795.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06155",
            "authors": [
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c19",
                    "user": {
                        "_id": "643a451ee2b979ae6141329d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a451ee2b979ae6141329d/HN3M5vyroanQoUEiXJFyB.jpeg",
                        "isPro": false,
                        "fullname": "Hangliang Ding",
                        "user": "foreverpiano",
                        "type": "user"
                    },
                    "name": "Hangliang Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:29.115Z",
                    "hidden": false
                },
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c1a",
                    "name": "Dacheng Li",
                    "hidden": false
                },
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c1b",
                    "name": "Runlong Su",
                    "hidden": false
                },
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c1c",
                    "name": "Peiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c1d",
                    "user": {
                        "_id": "64bba541da140e461924dfed",
                        "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
                        "isPro": false,
                        "fullname": "zhijie deng",
                        "user": "zhijie3",
                        "type": "user"
                    },
                    "name": "Zhijie Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:25.471Z",
                    "hidden": false
                },
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c1e",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "67aab9b4a2bf5e5ea03d4c1f",
                    "name": "Hao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T05:00:56.000Z",
            "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention\n  Tile",
            "summary": "Despite the promise of synthesizing high-fidelity videos, Diffusion\nTransformers (DiTs) with 3D full attention suffer from expensive inference due\nto the complexity of attention computation and numerous sampling steps. For\nexample, the popular Open-Sora-Plan model consumes more than 9 minutes for\ngenerating a single video of 29 frames. This paper addresses the inefficiency\nissue from two aspects: 1) Prune the 3D full attention based on the redundancy\nwithin video data; We identify a prevalent tile-style repetitive pattern in the\n3D attention maps for video data, and advocate a new family of sparse 3D\nattention that holds a linear complexity w.r.t. the number of video frames. 2)\nShorten the sampling process by adopting existing multi-step consistency\ndistillation; We split the entire sampling trajectory into several segments and\nperform consistency distillation within each one to activate few-step\ngeneration capacities. We further devise a three-stage training pipeline to\nconjoin the low-complexity attention and few-step generation capacities.\nNotably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into\nan efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video\ngeneration with a marginal performance trade-off in VBench. In addition, we\ndemonstrate that our approach is amenable to distributed inference, achieving\nan additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
            "upvotes": 6,
            "discussionId": "67aab9bca2bf5e5ea03d4e3c"
        },
        "publishedAt": "2025-02-10T22:09:58.181Z",
        "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06155.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63565cc56d7fcf1bedb7d347",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
            "fullname": "Zhang Peiyuan",
            "name": "PY007",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 82
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04370",
            "authors": [
                {
                    "_id": "67aafd90141fac22732a79b3",
                    "user": {
                        "_id": "6425318d175bd2952281065e",
                        "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
                        "isPro": false,
                        "fullname": "ZhenglinZhou",
                        "user": "zhenglin",
                        "type": "user"
                    },
                    "name": "Zhenglin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:53:27.764Z",
                    "hidden": false
                },
                {
                    "_id": "67aafd90141fac22732a79b4",
                    "user": {
                        "_id": "66c48f944511077b9ff5ce9d",
                        "avatarUrl": "/avatars/a4977246bc951e9da0cb2301bedd8249.svg",
                        "isPro": false,
                        "fullname": "Xiaobo Xia",
                        "user": "XiaoboXia1997",
                        "type": "user"
                    },
                    "name": "Xiaobo Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:53:52.391Z",
                    "hidden": false
                },
                {
                    "_id": "67aafd90141fac22732a79b5",
                    "name": "Fan Ma",
                    "hidden": false
                },
                {
                    "_id": "67aafd90141fac22732a79b6",
                    "user": {
                        "_id": "64ad04020fb9b20dbabbd30e",
                        "avatarUrl": "/avatars/a6bae4a3a4bcd6b54c33860fe14c7923.svg",
                        "isPro": false,
                        "fullname": "Hehe Fan",
                        "user": "hehefan",
                        "type": "user"
                    },
                    "name": "Hehe Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:54:14.222Z",
                    "hidden": false
                },
                {
                    "_id": "67aafd90141fac22732a79b7",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "67aafd90141fac22732a79b8",
                    "user": {
                        "_id": "6570ae84c4993b8fb96f41a8",
                        "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
                        "isPro": false,
                        "fullname": "Tat-Seng Chua",
                        "user": "chuats",
                        "type": "user"
                    },
                    "name": "Tat-Seng Chua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-11T15:54:04.147Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T11:03:08.000Z",
            "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via\n  Direct Preference Optimization",
            "summary": "Text-to-3D generation automates 3D content creation from textual\ndescriptions, which offers transformative potential across various fields.\nHowever, existing methods often struggle to align generated content with human\npreferences, limiting their applicability and flexibility. To address these\nlimitations, in this paper, we propose DreamDPO, an optimization-based\nframework that integrates human preferences into the 3D generation process,\nthrough direct preference optimization. Practically, DreamDPO first constructs\npairwise examples, then compare their alignment with human preferences using\nreward or large multimodal models, and lastly optimizes the 3D representation\nwith a preference-driven loss function. By leveraging pairwise comparison to\nreflect preferences, DreamDPO reduces reliance on precise pointwise quality\nevaluations while enabling fine-grained controllability through\npreference-guided optimization. Experiments demonstrate that DreamDPO achieves\ncompetitive results, and provides higher-quality and more controllable 3D\ncontent compared to existing methods. The code and models will be open-sourced.",
            "upvotes": 5,
            "discussionId": "67aafd94141fac22732a7adc"
        },
        "publishedAt": "2025-02-11T02:46:33.870Z",
        "title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6425318d175bd2952281065e/R7cMLIsmYovAMtL1vhsDn.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04370.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6425318d175bd2952281065e",
            "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
            "fullname": "ZhenglinZhou",
            "name": "zhenglin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06527",
            "authors": [
                {
                    "_id": "67aae4128d478dcb4b39a097",
                    "name": "D. She",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a098",
                    "name": "Mushui Liu",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a099",
                    "name": "Jingxuan Pang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a09a",
                    "name": "Jin Wang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a09b",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a09c",
                    "name": "Wanggui He",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a09d",
                    "name": "Guanghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a09e",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a09f",
                    "name": "Qihan Huang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a0a0",
                    "name": "Haobin Tang",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a0a1",
                    "name": "Yunlong Yu",
                    "hidden": false
                },
                {
                    "_id": "67aae4128d478dcb4b39a0a2",
                    "name": "Siming Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T14:50:32.000Z",
            "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for\n  Zero-Shot Customized Video Diffusion Transformers",
            "summary": "Customized generation has achieved significant progress in image synthesis,\nyet personalized video generation remains challenging due to temporal\ninconsistencies and quality degradation. In this paper, we introduce\nCustomVideoX, an innovative framework leveraging the video diffusion\ntransformer for personalized video generation from a reference image.\nCustomVideoX capitalizes on pre-trained video networks by exclusively training\nthe LoRA parameters to extract reference features, ensuring both efficiency and\nadaptability. To facilitate seamless interaction between the reference image\nand video content, we propose 3D Reference Attention, which enables direct and\nsimultaneous engagement of reference image features with all video frames\nacross spatial and temporal dimensions. To mitigate the excessive influence of\nreference image features and textual guidance on generated video content during\ninference, we implement the Time-Aware Reference Attention Bias (TAB) strategy,\ndynamically modulating reference bias over different time steps. Additionally,\nwe introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly\nactivated regions of key entity tokens with reference feature injection by\nadjusting attention bias. To thoroughly evaluate personalized video generation,\nwe establish a new benchmark, VideoBench, comprising over 50 objects and 100\nprompts for extensive assessment. Experimental results show that CustomVideoX\nsignificantly outperforms existing methods in terms of video consistency and\nquality.",
            "upvotes": 5,
            "discussionId": "67aae4178d478dcb4b39a1e7"
        },
        "publishedAt": "2025-02-11T00:46:11.168Z",
        "title": "CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06527.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6032
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05431",
            "authors": [
                {
                    "_id": "67aac392385da1f07cc7fcbd",
                    "user": {
                        "_id": "64f58b970b24e548a85522bc",
                        "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
                        "isPro": false,
                        "fullname": "Xinyu Yang",
                        "user": "Hanyuezhuohua",
                        "type": "user"
                    },
                    "name": "Xinyu Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:13.131Z",
                    "hidden": false
                },
                {
                    "_id": "67aac392385da1f07cc7fcbe",
                    "name": "Tianqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67aac392385da1f07cc7fcbf",
                    "name": "Beidi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-08T03:41:16.000Z",
            "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
            "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n(APE), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5times\nspeedup by reducing 28times prefilling time for a 128K-length context.",
            "upvotes": 5,
            "discussionId": "67aac393385da1f07cc7fd17"
        },
        "publishedAt": "2025-02-10T22:29:36.102Z",
        "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05431.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "fullname": "Xinyu Yang",
            "name": "Hanyuezhuohua",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06635",
            "authors": [
                {
                    "_id": "67aac0ba91e6f5eb5476ea76",
                    "name": "Qingshui Gu",
                    "hidden": false
                },
                {
                    "_id": "67aac0ba91e6f5eb5476ea77",
                    "name": "Shu Li",
                    "hidden": false
                },
                {
                    "_id": "67aac0ba91e6f5eb5476ea78",
                    "user": {
                        "_id": "64ab99dcb76bfd863eba64c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
                        "isPro": false,
                        "fullname": "TY.Zheng",
                        "user": "aaabiao",
                        "type": "user"
                    },
                    "name": "Tianyu Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T07:55:15.968Z",
                    "hidden": false
                },
                {
                    "_id": "67aac0ba91e6f5eb5476ea79",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T16:31:37.000Z",
            "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building\n  a Chinese-Centric LLM",
            "summary": "Steel-LLM is a Chinese-centric language model developed from scratch with the\ngoal of creating a high-quality, open-source model despite limited\ncomputational resources. Launched in March 2024, the project aimed to train a\n1-billion-parameter model on a large-scale dataset, prioritizing transparency\nand the sharing of practical insights to assist others in the community. The\ntraining process primarily focused on Chinese data, with a small proportion of\nEnglish data included, addressing gaps in existing open-source LLMs by\nproviding a more detailed and practical account of the model-building journey.\nSteel-LLM has demonstrated competitive performance on benchmarks such as CEVAL\nand CMMLU, outperforming early models from larger institutions. This paper\nprovides a comprehensive summary of the project's key contributions, including\ndata collection, model design, training methodologies, and the challenges\nencountered along the way, offering a valuable resource for researchers and\npractitioners looking to develop their own LLMs. The model checkpoints and\ntraining script are available at https://github.com/zhanshijinwat/Steel-LLM.",
            "upvotes": 4,
            "discussionId": "67aac0bb91e6f5eb5476eab8"
        },
        "publishedAt": "2025-02-10T22:20:38.168Z",
        "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06635.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "fullname": "TY.Zheng",
            "name": "aaabiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06282",
            "authors": [
                {
                    "_id": "67ab6c9867a1607ab478b975",
                    "user": {
                        "_id": "65780c60411e14898b8da93e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fvjP6qICiuR09LV8Xzahb.png",
                        "isPro": false,
                        "fullname": "Haiduo Huang",
                        "user": "Hhaiduo",
                        "type": "user"
                    },
                    "name": "Haiduo Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T15:33:50.773Z",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b976",
                    "name": "Fuwei Yang",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b977",
                    "name": "Zhenhua Liu",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b978",
                    "name": "Yixing Xu",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b979",
                    "name": "Jinze Li",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b97a",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b97b",
                    "name": "Xuanwu Yin",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b97c",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b97d",
                    "name": "Pengju Ren",
                    "hidden": false
                },
                {
                    "_id": "67ab6c9867a1607ab478b97e",
                    "name": "Emad Barsoum",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T09:24:06.000Z",
            "title": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE",
            "summary": "Speculative decoding (SD) accelerates large language model inference by using\na smaller draft model to predict multiple tokens, which are then verified in\nparallel by the larger target model. However, the limited capacity of the draft\nmodel often necessitates tree-based sampling to improve prediction accuracy,\nwhere multiple candidates are generated at each step. We identify a key\nlimitation in this approach: the candidates at the same step are derived from\nthe same representation, limiting diversity and reducing overall effectiveness.\nTo address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where\nindependent experts generate diverse predictions, effectively decoupling\ncorrelations among candidates. Furthermore, we introduce a hybrid inference\nstrategy, combining autoregressive decoding for initial tokens with parallel\ndecoding for subsequent stages, and enhance the latter with contrastive\nmechanism in features to improve accuracy. Our method significantly boosts\nprediction accuracy and achieves higher inference speedups. Extensive\nexperiments across diverse models validate the effectiveness and robustness of\nour approach, establishing a new SOTA in speculative decoding. Our codes are\navailable at https://github.com/haiduo/Jakiro.",
            "upvotes": 1,
            "discussionId": "67ab6c9967a1607ab478b9d0"
        },
        "publishedAt": "2025-02-11T20:30:51.808Z",
        "title": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06282.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65780c60411e14898b8da93e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fvjP6qICiuR09LV8Xzahb.png",
            "fullname": "Haiduo Huang",
            "name": "Hhaiduo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06776",
            "authors": [
                {
                    "_id": "67aae05bb893603a0b4b241d",
                    "name": "Brandon Trabucco",
                    "hidden": false
                },
                {
                    "_id": "67aae05bb893603a0b4b241e",
                    "name": "Gunnar Sigurdsson",
                    "hidden": false
                },
                {
                    "_id": "67aae05bb893603a0b4b241f",
                    "name": "Robinson Piramuthu",
                    "hidden": false
                },
                {
                    "_id": "67aae05bb893603a0b4b2420",
                    "name": "Ruslan Salakhutdinov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:54:05.000Z",
            "title": "Towards Internet-Scale Training For Agents",
            "summary": "The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.",
            "upvotes": 1,
            "discussionId": "67aae05cb893603a0b4b2480"
        },
        "publishedAt": "2025-02-11T14:26:46.017Z",
        "title": "Towards Internet-Scale Training For Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06776.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632d8b2e1d8a018adf4f98f1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632d8b2e1d8a018adf4f98f1/vGvpkxyGLNQSJmEONR2uX.jpeg",
            "fullname": "Brandon Trabucco",
            "name": "btrabucco",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2411.18676",
            "authors": [
                {
                    "_id": "67ab837b02329ca8f809ceae",
                    "name": "Sathwik Karnik",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceaf",
                    "name": "Zhang-Wei Hong",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceb0",
                    "name": "Nishant Abhangi",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceb1",
                    "name": "Yen-Chen Lin",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceb2",
                    "name": "Tsun-Hsuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceb3",
                    "name": "Christophe Dupuy",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceb4",
                    "name": "Rahul Gupta",
                    "hidden": false
                },
                {
                    "_id": "67ab837b02329ca8f809ceb5",
                    "name": "Pulkit Agrawal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-27T18:57:26.000Z",
            "title": "Embodied Red Teaming for Auditing Robotic Foundation Models",
            "summary": "Language-conditioned robot models have the potential to enable robots to\nperform a wide range of tasks based on natural language instructions. However,\nassessing their safety and effectiveness remains challenging because it is\ndifficult to test all the different ways a single task can be phrased. Current\nbenchmarks have two key limitations: they rely on a limited set of\nhuman-generated instructions, missing many challenging cases, and focus only on\ntask performance without assessing safety, such as avoiding damage. To address\nthese gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method\nthat generates diverse and challenging instructions to test these models. ERT\nuses automated red teaming techniques with Vision Language Models (VLMs) to\ncreate contextually grounded, difficult instructions. Experimental results show\nthat state-of-the-art language-conditioned robot models fail or behave unsafely\non ERT-generated instructions, underscoring the shortcomings of current\nbenchmarks in evaluating real-world performance and safety. Code and videos are\navailable at: https://s-karnik.github.io/embodied-red-team-project-page.",
            "upvotes": 0,
            "discussionId": "67ab837d02329ca8f809cef0"
        },
        "publishedAt": "2025-02-11T12:06:30.185Z",
        "title": "Embodied Red Teaming for Auditing Robotic Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.18676.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6032
        },
        "isAuthorParticipating": false
    }
]