[
    {
        "paper": {
            "id": "2502.06329",
            "authors": [
                {
                    "_id": "67ab4174757d2eb190af0375",
                    "user": {
                        "_id": "621d6f532165dc431641e438",
                        "avatarUrl": "/avatars/56ccef10a8426d7160ef3586a771bd63.svg",
                        "isPro": false,
                        "fullname": "Kiran Kamble",
                        "user": "kiranr",
                        "type": "user"
                    },
                    "name": "Kiran Kamble",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:16:55.367Z",
                    "hidden": false
                },
                {
                    "_id": "67ab4174757d2eb190af0376",
                    "name": "Melisa Russak",
                    "hidden": false
                },
                {
                    "_id": "67ab4174757d2eb190af0377",
                    "name": "Dmytro Mozolevskyi",
                    "hidden": false
                },
                {
                    "_id": "67ab4174757d2eb190af0378",
                    "user": {
                        "_id": "6320a906a023aad6a7670e99",
                        "avatarUrl": "/avatars/48071559b0c7660bf6861cfe008b3006.svg",
                        "isPro": false,
                        "fullname": "Muayad Sayed Ali",
                        "user": "muayad",
                        "type": "user"
                    },
                    "name": "Muayad Ali",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:16:53.157Z",
                    "hidden": false
                },
                {
                    "_id": "67ab4174757d2eb190af0379",
                    "name": "Mateusz Russak",
                    "hidden": false
                },
                {
                    "_id": "67ab4174757d2eb190af037a",
                    "name": "Waseem AlShikh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T10:29:28.000Z",
            "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
            "summary": "We propose a new long-context financial benchmark, FailSafeQA, designed to\ntest the robustness and context-awareness of LLMs against six variations in\nhuman-interface interactions in LLM-based query-answer systems within finance.\nWe concentrate on two case studies: Query Failure and Context Failure. In the\nQuery Failure scenario, we perturb the original query to vary in domain\nexpertise, completeness, and linguistic accuracy. In the Context Failure case,\nwe simulate the uploads of degraded, irrelevant, and empty documents. We employ\nthe LLM-as-a-Judge methodology with Qwen2.5-72B-Instruct and use fine-grained\nrating criteria to define and calculate Robustness, Context Grounding, and\nCompliance scores for 24 off-the-shelf models. The results suggest that\nalthough some models excel at mitigating input perturbations, they must balance\nrobust answering with the ability to refrain from hallucinating. Notably,\nPalmyra-Fin-128k-Instruct, recognized as the most compliant model, maintained\nstrong baseline performance but encountered challenges in sustaining robust\npredictions in 17% of test cases. On the other hand, the most robust model,\nOpenAI o3-mini, fabricated information in 41% of tested cases. The results\ndemonstrate that even high-performing models have significant room for\nimprovement and highlight the role of FailSafeQA as a tool for developing LLMs\noptimized for dependability in financial applications. The dataset is available\nat: https://huggingface.co/datasets/Writer/FailSafeQA",
            "upvotes": 103,
            "discussionId": "67ab4175757d2eb190af03ca"
        },
        "publishedAt": "2025-02-12T02:51:41.003Z",
        "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06329.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60e61b3969bd0df25c9375da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
            "fullname": "Melisa Russak",
            "name": "melisa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 27
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06807",
            "authors": [
                {
                    "_id": "67ac1b080686a1e0690741ce",
                    "name": "OpenAI",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d0",
                    "name": "Ahmed El-Kishky",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d1",
                    "name": "Alexander Wei",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d2",
                    "name": "Andre Saraiva",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d3",
                    "name": "Borys Minaev",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d4",
                    "name": "Daniel Selsam",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d5",
                    "name": "David Dohan",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d6",
                    "name": "Francis Song",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d7",
                    "name": "Hunter Lightman",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d8",
                    "name": "Ignasi Clavera",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741d9",
                    "name": "Jakub Pachocki",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741da",
                    "name": "Jerry Tworek",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741db",
                    "name": "Lorenz Kuhn",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741dc",
                    "name": "Lukasz Kaiser",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741dd",
                    "name": "Mark Chen",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741de",
                    "name": "Max Schwarzer",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741df",
                    "name": "Mostafa Rohaninejad",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e0",
                    "name": "Nat McAleese",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e1",
                    "name": "o3 contributors",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e2",
                    "name": "Oleg MÃ¼rk",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e3",
                    "name": "Rhythm Garg",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e4",
                    "name": "Rui Shu",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e5",
                    "name": "Szymon Sidor",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e6",
                    "name": "Vineet Kosaraju",
                    "hidden": false
                },
                {
                    "_id": "67ac1b080686a1e0690741e7",
                    "name": "Wenda Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-03T23:00:15.000Z",
            "title": "Competitive Programming with Large Reasoning Models",
            "summary": "We show that reinforcement learning applied to large language models (LLMs)\nsignificantly boosts performance on complex coding and reasoning tasks.\nAdditionally, we compare two general-purpose reasoning models - OpenAI o1 and\nan early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses\nhand-engineered inference strategies designed for competing in the 2024\nInternational Olympiad in Informatics (IOI). We competed live at IOI 2024 with\no1-ioi and, using hand-crafted test-time strategies, placed in the 49th\npercentile. Under relaxed competition constraints, o1-ioi achieved a gold\nmedal. However, when evaluating later models such as o3, we find that o3\nachieves gold without hand-crafted domain-specific strategies or relaxed\nconstraints. Our findings show that although specialized pipelines such as\no1-ioi yield solid improvements, the scaled-up, general-purpose o3 model\nsurpasses those results without relying on hand-crafted inference heuristics.\nNotably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces\nrating on par with elite human competitors. Overall, these results indicate\nthat scaling general-purpose reinforcement learning, rather than relying on\ndomain-specific techniques, offers a robust path toward state-of-the-art AI in\nreasoning domains, such as competitive programming.",
            "upvotes": 41,
            "discussionId": "67ac1b090686a1e069074208"
        },
        "publishedAt": "2025-02-11T22:53:19.310Z",
        "title": "Competitive Programming with Large Reasoning Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05878",
            "authors": [
                {
                    "_id": "67aab4a98642da4d695cf045",
                    "name": "Mengxi Xiao",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf046",
                    "name": "Zihao Jiang",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf047",
                    "name": "Lingfei Qian",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf048",
                    "name": "Zhengyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf049",
                    "name": "Yueru He",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf04a",
                    "name": "Yijing Xu",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf04b",
                    "name": "Yuecheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf04c",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf04d",
                    "name": "Ruey-Ling Weng",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf04e",
                    "name": "Min Peng",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf04f",
                    "name": "Jimin Huang",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf050",
                    "name": "Sophia Ananiadou",
                    "hidden": false
                },
                {
                    "_id": "67aab4a98642da4d695cf051",
                    "name": "Qianqian Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T12:26:05.000Z",
            "title": "Retrieval-augmented Large Language Models for Financial Time Series\n  Forecasting",
            "summary": "Stock movement prediction, a fundamental task in financial time-series\nforecasting, requires identifying and retrieving critical influencing factors\nfrom vast amounts of time-series data. However, existing text-trained or\nnumeric similarity-based retrieval methods fall short in handling complex\nfinancial analysis. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework for financial time-series forecasting, featuring\nthree key innovations: a fine-tuned 1B parameter large language model\n(StockLLM) as the backbone, a novel candidate selection method leveraging LLM\nfeedback, and a training objective that maximizes similarity between queries\nand historically significant sequences. This enables our retriever, FinSeer, to\nuncover meaningful patterns while minimizing noise in complex financial data.\nWe also construct new datasets integrating financial indicators and historical\nstock prices to train FinSeer and ensure robust evaluation. Experimental\nresults demonstrate that our RAG framework outperforms bare StockLLM and random\nretrieval, highlighting its effectiveness, while FinSeer surpasses existing\nretrieval methods, achieving an 8\\% higher accuracy on BIGDATA22 and retrieving\nmore impactful sequences. This work underscores the importance of tailored\nretrieval models in financial forecasting and provides a novel framework for\nfuture research.",
            "upvotes": 25,
            "discussionId": "67aab4ac8642da4d695cf101"
        },
        "publishedAt": "2025-02-12T07:10:24.189Z",
        "title": "Retrieval-augmented Large Language Models for Financial Time Series Forecasting",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/Y3YaXroeoJ851uS_hS3j0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05878.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "fullname": "Jimin Huang",
            "name": "jiminHuang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07316",
            "authors": [
                {
                    "_id": "67ac0ab720e98bddc5c19fed",
                    "name": "Junlong Li",
                    "hidden": false
                },
                {
                    "_id": "67ac0ab720e98bddc5c19fee",
                    "name": "Daya Guo",
                    "hidden": false
                },
                {
                    "_id": "67ac0ab720e98bddc5c19fef",
                    "name": "Dejian Yang",
                    "hidden": false
                },
                {
                    "_id": "67ac0ab720e98bddc5c19ff0",
                    "name": "Runxin Xu",
                    "hidden": false
                },
                {
                    "_id": "67ac0ab720e98bddc5c19ff1",
                    "name": "Yu Wu",
                    "hidden": false
                },
                {
                    "_id": "67ac0ab720e98bddc5c19ff2",
                    "name": "Junxian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T07:26:50.000Z",
            "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
            "summary": "Reasoning is a fundamental capability of Large Language Models. While prior\nresearch predominantly focuses on enhancing narrow skills like math or code\ngeneration, improving performance on many other reasoning tasks remains\nchallenging due to sparse and fragmented training data. To address this issue,\nwe propose CodeI/O, a novel approach that systematically condenses diverse\nreasoning patterns inherently embedded in contextually-grounded codes, through\ntransforming the original code into a code input-output prediction format. By\ntraining models to predict inputs/outputs given code and test cases entirely in\nnatural language as Chain-of-Thought (CoT) rationales, we expose them to\nuniversal reasoning primitives -- like logic flow planning, state-space\nsearching, decision tree traversal, and modular decomposition -- while\ndecoupling structured reasoning from code-specific syntax and preserving\nprocedural rigor. Experimental results demonstrate CodeI/O leads to consistent\nimprovements across symbolic, scientific, logic, math & numerical, and\ncommonsense reasoning tasks. By matching the existing ground-truth outputs or\nre-executing the code with predicted inputs, we can verify each prediction and\nfurther enhance the CoTs through multi-turn revision, resulting in CodeI/O++\nand achieving higher performance. Our data and models are available at\nhttps://github.com/hkust-nlp/CodeIO.",
            "upvotes": 23,
            "discussionId": "67ac0ab820e98bddc5c1a039"
        },
        "publishedAt": "2025-02-11T23:00:20.080Z",
        "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "621e40ac944c7e36aaec2369",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e40ac944c7e36aaec2369/Yj-FJRWps3rvsS_B2bnKo.jpeg",
            "fullname": "Junlong Li",
            "name": "lockon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07701",
            "authors": [
                {
                    "_id": "67ac23166def89f9aae56abd",
                    "name": "Hongwei Yi",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56abe",
                    "name": "Shitong Shao",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56abf",
                    "user": {
                        "_id": "66015e8aa4d296af07de538e",
                        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
                        "isPro": false,
                        "fullname": "Ye",
                        "user": "Owen777",
                        "type": "user"
                    },
                    "name": "Tian Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:16:12.141Z",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac0",
                    "name": "Jiantong Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac1",
                    "name": "Qingyu Yin",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac2",
                    "name": "Michael Lingelbach",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac3",
                    "name": "Li Yuan",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac4",
                    "name": "Yonghong Tian",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac5",
                    "name": "Enze Xie",
                    "hidden": false
                },
                {
                    "_id": "67ac23166def89f9aae56ac6",
                    "name": "Daquan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T16:58:15.000Z",
            "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
            "summary": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.",
            "upvotes": 20,
            "discussionId": "67ac23186def89f9aae56b69"
        },
        "publishedAt": "2025-02-11T23:27:13.769Z",
        "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07374",
            "authors": [
                {
                    "_id": "67ac1c6436464325ebe3c6e3",
                    "name": "Dacheng Li",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6e4",
                    "name": "Shiyi Cao",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6e5",
                    "name": "Tyler Griggs",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6e6",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6e7",
                    "name": "Xiangxi Mo",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6e8",
                    "name": "Shishir G. Patil",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6e9",
                    "name": "Matei Zaharia",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6ea",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "67ac1c6436464325ebe3c6eb",
                    "name": "Ion Stoica",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T08:48:48.000Z",
            "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
            "summary": "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
            "upvotes": 20,
            "discussionId": "67ac1c6536464325ebe3c723"
        },
        "publishedAt": "2025-02-11T22:58:37.585Z",
        "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06857",
            "authors": [
                {
                    "_id": "67ac9127184de583cc7daa75",
                    "user": {
                        "_id": "65255f1073a043e50d043641",
                        "avatarUrl": "/avatars/257085f01c439d7c84787a4e6d085b3d.svg",
                        "isPro": true,
                        "fullname": "Sean McLeish",
                        "user": "smcleish",
                        "type": "user"
                    },
                    "name": "Sean McLeish",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-12T12:21:11.370Z",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa76",
                    "name": "John Kirchenbauer",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa77",
                    "name": "David Yu Miller",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa78",
                    "name": "Siddharth Singh",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa79",
                    "name": "Abhinav Bhatele",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa7a",
                    "name": "Micah Goldblum",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa7b",
                    "name": "Ashwinee Panda",
                    "hidden": false
                },
                {
                    "_id": "67ac9127184de583cc7daa7c",
                    "name": "Tom Goldstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T18:09:38.000Z",
            "title": "Gemstones: A Model Suite for Multi-Faceted Scaling Laws",
            "summary": "Scaling laws are typically fit using a family of models with a narrow range\nof frozen hyper-parameter choices. In this work we study scaling laws using a\nwide range of architecture and hyper-parameter choices, and highlight their\nimpact on resulting prescriptions. As a primary artifact of our research, we\nrelease the Gemstones: the most comprehensive open-source scaling law dataset\nto date, consisting of over 4000 checkpoints from transformers with up to 2\nbillion parameters; these models have been trained with different learning\nrates, cooldown schedules, and architectural shapes. Our checkpoints enable\nmore complex studies of scaling, such as a law that predicts language modeling\nperformance as a function of model width and depth. By examining the various\nfacets of our model suite, we find that the prescriptions of scaling laws can\nbe highly sensitive to the experimental design process and the specific model\ncheckpoints used during fitting. Code:\nhttps://github.com/mcleish7/gemstone-scaling-laws",
            "upvotes": 18,
            "discussionId": "67ac9128184de583cc7daaba"
        },
        "publishedAt": "2025-02-12T07:18:08.463Z",
        "title": "Gemstones: A Model Suite for Multi-Faceted Scaling Laws",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06857.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65255f1073a043e50d043641",
            "avatarUrl": "/avatars/257085f01c439d7c84787a4e6d085b3d.svg",
            "fullname": "Sean McLeish",
            "name": "smcleish",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.03492",
            "authors": [
                {
                    "_id": "67a5a8e595df68b0a167c298",
                    "user": {
                        "_id": "622f103fc78da4c7ebd7c887",
                        "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
                        "isPro": false,
                        "fullname": "Xie",
                        "user": "Zhihui",
                        "type": "user"
                    },
                    "name": "Zhihui Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:17:02.682Z",
                    "hidden": false
                },
                {
                    "_id": "67a5a8e595df68b0a167c299",
                    "name": "Jie chen",
                    "hidden": false
                },
                {
                    "_id": "67a5a8e595df68b0a167c29a",
                    "name": "Liyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67a5a8e595df68b0a167c29b",
                    "name": "Weichao Mao",
                    "hidden": false
                },
                {
                    "_id": "67a5a8e595df68b0a167c29c",
                    "name": "Jingjing Xu",
                    "hidden": false
                },
                {
                    "_id": "67a5a8e595df68b0a167c29d",
                    "name": "Lingpeng Kong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-05T02:18:46.000Z",
            "title": "Teaching Language Models to Critique via Reinforcement Learning",
            "summary": "Teaching large language models (LLMs) to critique and refine their outputs is\ncrucial for building systems that can iteratively improve, yet it is\nfundamentally limited by the ability to provide accurate judgments and\nactionable suggestions. In this work, we study LLM critics for code generation\nand propose CTRL, a framework for Critic\nTraining via Reinforcement Learning, which\ntrains a critic model to generate feedback that maximizes correction\nperformance for a fixed generator model without human supervision. Our results\ndemonstrate that critics trained with CTRL significantly enhance\npass rates and mitigate compounding errors across both base and stronger\ngenerator models. Furthermore, we show that these critic models act as accurate\ngenerative reward models and enable test-time scaling through iterative\ncritique-revision, achieving up to 106.1% relative improvements across\nchallenging code generation benchmarks.",
            "upvotes": 16,
            "discussionId": "67a5a8e695df68b0a167c2c6"
        },
        "publishedAt": "2025-02-11T23:55:37.671Z",
        "title": "Teaching Language Models to Critique via Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622f103fc78da4c7ebd7c887",
            "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
            "fullname": "Xie",
            "name": "Zhihui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.07617",
            "authors": [
                {
                    "_id": "67ac1d68c29356f92ed772c5",
                    "name": "Xiao Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d68c29356f92ed772c6",
                    "name": "Ibrahim Alabdulmohsin",
                    "hidden": false
                },
                {
                    "_id": "67ac1d68c29356f92ed772c7",
                    "name": "Daniel Salz",
                    "hidden": false
                },
                {
                    "_id": "67ac1d68c29356f92ed772c8",
                    "name": "Zhe Li",
                    "hidden": false
                },
                {
                    "_id": "67ac1d68c29356f92ed772c9",
                    "name": "Keran Rong",
                    "hidden": false
                },
                {
                    "_id": "67ac1d68c29356f92ed772ca",
                    "name": "Xiaohua Zhai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T15:05:33.000Z",
            "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language\n  Models",
            "summary": "We provide an empirical investigation of the potential of pre-training\nvision-language models on an unprecedented scale: 100 billion examples. We find\nthat model performance tends to saturate at this scale on many common\nWestern-centric classification and retrieval benchmarks, such as COCO Captions.\nNevertheless, tasks of cultural diversity achieve more substantial gains from\nthe 100-billion scale web data, thanks to its coverage of long-tail concepts.\nFurthermore, we analyze the model's multilinguality and show gains in\nlow-resource languages as well. In addition, we observe that reducing the size\nof the pretraining dataset via quality filters like using CLIP, typically used\nto enhance performance, may inadvertently reduce the cultural diversity\nrepresented even in large-scale datasets. Our results highlight that while\ntraditional benchmarks may not benefit significantly from scaling noisy, raw\nweb data to 100 billion examples, this data scale is vital for building truly\ninclusive multimodal systems.",
            "upvotes": 14,
            "discussionId": "67ac1d6ac29356f92ed77354"
        },
        "publishedAt": "2025-02-11T23:03:08.578Z",
        "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07508",
            "authors": [
                {
                    "_id": "67ac2006a6b5a26040fc94f7",
                    "name": "Yang Luo",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94f8",
                    "name": "Xuanlei Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94f9",
                    "name": "Mengzhao Chen",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94fa",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94fb",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94fc",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94fd",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac2006a6b5a26040fc94fe",
                    "name": "Yang You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T12:22:35.000Z",
            "title": "Enhance-A-Video: Better Generated Video for Free",
            "summary": "DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.",
            "upvotes": 13,
            "discussionId": "67ac200ea6b5a26040fc9709"
        },
        "publishedAt": "2025-02-11T23:14:10.293Z",
        "title": "Enhance-A-Video: Better Generated Video for Free",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06589",
            "authors": [
                {
                    "_id": "67ac1d45e6f1e95ccf6de3b7",
                    "user": {
                        "_id": "6471bddd609ae9f56368f132",
                        "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
                        "isPro": true,
                        "fullname": "Yuchen Zhuang",
                        "user": "yczhuang",
                        "type": "user"
                    },
                    "name": "Yuchen Zhuang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-12T04:02:14.866Z",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3b8",
                    "name": "Jingfeng Yang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3b9",
                    "name": "Haoming Jiang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3ba",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3bb",
                    "name": "Kewei Cheng",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3bc",
                    "name": "Sanket Lokegaonkar",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3bd",
                    "name": "Yifan Gao",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3be",
                    "name": "Qing Ping",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3bf",
                    "name": "Tianyi Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c0",
                    "name": "Binxuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c1",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c2",
                    "name": "Zhengyang Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c3",
                    "name": "Pei Chen",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c4",
                    "name": "Ruijie Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c5",
                    "name": "Rongzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c6",
                    "name": "Nasser Zalmout",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c7",
                    "name": "Priyanka Nigam",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c8",
                    "name": "Bing Yin",
                    "hidden": false
                },
                {
                    "_id": "67ac1d45e6f1e95ccf6de3c9",
                    "name": "Chao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T15:54:34.000Z",
            "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training",
            "summary": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.",
            "upvotes": 11,
            "discussionId": "67ac1d46e6f1e95ccf6de419"
        },
        "publishedAt": "2025-02-11T23:04:08.153Z",
        "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "fullname": "Yuchen Zhuang",
            "name": "yczhuang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.07527",
            "authors": [
                {
                    "_id": "67ac1eaac61306b0ac95d2c6",
                    "name": "Yingce Xia",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2c7",
                    "name": "Peiran Jin",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2c8",
                    "name": "Shufang Xie",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2c9",
                    "name": "Liang He",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ca",
                    "name": "Chuan Cao",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2cb",
                    "name": "Renqian Luo",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2cc",
                    "name": "Guoqing Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2cd",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ce",
                    "name": "Zequn Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2cf",
                    "name": "Yuan-Jyue Chen",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d0",
                    "name": "Zekun Guo",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d1",
                    "name": "Yeqi Bai",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d2",
                    "name": "Pan Deng",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d3",
                    "name": "Yaosen Min",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d4",
                    "name": "Ziheng Lu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d5",
                    "name": "Hongxia Hao",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d6",
                    "name": "Han Yang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d7",
                    "name": "Jielan Li",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d8",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2d9",
                    "name": "Jia Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2da",
                    "name": "Jianwei Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2db",
                    "name": "Kehan Wu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2dc",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2dd",
                    "name": "Kaiyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2de",
                    "name": "Qizhi Pei",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2df",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e0",
                    "name": "Xixian Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e1",
                    "name": "Yanting Li",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e2",
                    "name": "Houtian Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e3",
                    "name": "Yeqing Lu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e4",
                    "name": "Mingqian Ma",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e5",
                    "name": "Zun Wang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e6",
                    "name": "Tian Xie",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e7",
                    "name": "Krzysztof Maziarz",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e8",
                    "name": "Marwin Segler",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2e9",
                    "name": "Zhao Yang",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ea",
                    "name": "Zilong Chen",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2eb",
                    "name": "Yu Shi",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ec",
                    "name": "Shuxin Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ed",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ee",
                    "name": "Chen Hu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2ef",
                    "name": "Peggy Dai",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2f0",
                    "name": "Tie-Yan Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2f1",
                    "name": "Haiguang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac1eaac61306b0ac95d2f2",
                    "name": "Tao Qin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T13:08:03.000Z",
            "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
            "summary": "Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.",
            "upvotes": 10,
            "discussionId": "67ac1eabc61306b0ac95d346"
        },
        "publishedAt": "2025-02-11T23:10:26.895Z",
        "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.04223",
            "authors": [
                {
                    "_id": "67ac5e0d653d273eeaf25e59",
                    "name": "Ilia Karmanov",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e5a",
                    "user": {
                        "_id": "67ac5d85a19e34140ea1013b",
                        "avatarUrl": "/avatars/e5b7446787dbbd17553dc9e11b58a0b4.svg",
                        "isPro": false,
                        "fullname": "Amala Sanjay Deshmukh",
                        "user": "amalad",
                        "type": "user"
                    },
                    "name": "Amala Sanjay Deshmukh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:14:49.009Z",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e5b",
                    "name": "Lukas Voegtle",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e5c",
                    "name": "Philipp Fischer",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e5d",
                    "user": {
                        "_id": "64c7a43e0d3d1b209df90b9c",
                        "avatarUrl": "/avatars/1d0d2f129b799a72345b17fd5307aa5e.svg",
                        "isPro": false,
                        "fullname": "Kateryna Chumachenko",
                        "user": "katerynaCh",
                        "type": "user"
                    },
                    "name": "Kateryna Chumachenko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:14:47.025Z",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e5e",
                    "name": "Timo Roman",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e5f",
                    "user": {
                        "_id": "60098ca06e8ac78787773f85",
                        "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
                        "isPro": false,
                        "fullname": "Jarno SeppÃ¤nen",
                        "user": "jseppanen",
                        "type": "user"
                    },
                    "name": "Jarno SeppÃ¤nen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:14:51.062Z",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e60",
                    "name": "Jupinder Parmar",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e61",
                    "name": "Joseph Jennings",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e62",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "67ac5e0d653d273eeaf25e63",
                    "name": "Karan Sapra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T17:07:22.000Z",
            "title": "Ãclair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents",
            "summary": "Optical Character Recognition (OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extract formatted text in reading order, along with bounding boxes and\ntheir corresponding semantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark for\ndocument-level OCR and semantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating its versatility and strength across several evaluation standards.",
            "upvotes": 9,
            "discussionId": "67ac5e0f653d273eeaf25eea"
        },
        "publishedAt": "2025-02-12T04:25:54.558Z",
        "title": "Ãclair -- Extracting Content and Layout with Integrated Reading Order for Documents",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60098ca06e8ac78787773f85/BfZ57W-gCoY32J60tx7dN.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04223.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60098ca06e8ac78787773f85",
            "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
            "fullname": "Jarno SeppÃ¤nen",
            "name": "jseppanen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.03997",
            "authors": [
                {
                    "_id": "67ac206214d5fe7767e7ec4e",
                    "name": "Yu Yuan",
                    "hidden": false
                },
                {
                    "_id": "67ac206214d5fe7767e7ec4f",
                    "user": {
                        "_id": "63eb00a191a1b8ec4fbba2a9",
                        "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
                        "isPro": false,
                        "fullname": "ShizhaoSun",
                        "user": "ShizhaoSun",
                        "type": "user"
                    },
                    "name": "Shizhao Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:16:14.580Z",
                    "hidden": false
                },
                {
                    "_id": "67ac206214d5fe7767e7ec50",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "67ac206214d5fe7767e7ec51",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T11:57:14.000Z",
            "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data\n  Synthesis for Text-Based CAD Editing",
            "summary": "Computer Aided Design (CAD) is indispensable across various industries.\nText-based CAD editing, which automates the modification of CAD models\nbased on textual instructions, holds great potential but remains underexplored.\nExisting methods primarily focus on design variation generation or text-based\nCAD generation, either lacking support for text-based control or neglecting\nexisting CAD models as constraints. We introduce CAD-Editor, the first\nframework for text-based CAD editing. To address the challenge of demanding\ntriplet data with accurate correspondence for training, we propose an automated\ndata synthesis pipeline. This pipeline utilizes design variation models to\ngenerate pairs of original and edited CAD models and employs Large\nVision-Language Models (LVLMs) to summarize their differences into editing\ninstructions. To tackle the composite nature of text-based CAD editing, we\npropose a locate-then-infill framework that decomposes the task into two\nfocused sub-tasks: locating regions requiring modification and infilling these\nregions with appropriate edits. Large Language Models (LLMs) serve as the\nbackbone for both sub-tasks, leveraging their capabilities in natural language\nunderstanding and CAD knowledge. Experiments show that CAD-Editor achieves\nsuperior performance both quantitatively and qualitatively.",
            "upvotes": 8,
            "discussionId": "67ac206314d5fe7767e7ec98"
        },
        "publishedAt": "2025-02-11T23:16:28.213Z",
        "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63eb00a191a1b8ec4fbba2a9",
            "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
            "fullname": "ShizhaoSun",
            "name": "ShizhaoSun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.07531",
            "authors": [
                {
                    "_id": "67ac21acaa680a0f8782d273",
                    "name": "Sixiao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ac21acaa680a0f8782d274",
                    "name": "Zimian Peng",
                    "hidden": false
                },
                {
                    "_id": "67ac21acaa680a0f8782d275",
                    "name": "Yanpeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "67ac21acaa680a0f8782d276",
                    "name": "Yi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ac21acaa680a0f8782d277",
                    "name": "Hang Xu",
                    "hidden": false
                },
                {
                    "_id": "67ac21acaa680a0f8782d278",
                    "name": "Xiangru Huang",
                    "hidden": false
                },
                {
                    "_id": "67ac21acaa680a0f8782d279",
                    "name": "Yanwei Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T13:11:59.000Z",
            "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation",
            "summary": "Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available. Project page: https://sixiaozheng.github.io/VidCRAFT3/.",
            "upvotes": 7,
            "discussionId": "67ac21b2aa680a0f8782d3bd"
        },
        "publishedAt": "2025-02-11T23:21:13.452Z",
        "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05364",
            "authors": [
                {
                    "_id": "67ab99c8bb44ec714c6a4a96",
                    "user": {
                        "_id": "635582fb6b58fa7cc8701580",
                        "avatarUrl": "/avatars/a00791ba70a2de40dacac4582307c0f2.svg",
                        "isPro": false,
                        "fullname": "Julian Killingback",
                        "user": "jfkback",
                        "type": "user"
                    },
                    "name": "Julian Killingback",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:16:46.458Z",
                    "hidden": false
                },
                {
                    "_id": "67ab99c8bb44ec714c6a4a97",
                    "name": "Hansi Zeng",
                    "hidden": false
                },
                {
                    "_id": "67ab99c8bb44ec714c6a4a98",
                    "name": "Hamed Zamani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-07T22:31:38.000Z",
            "title": "Hypencoder: Hypernetworks for Information Retrieval",
            "summary": "The vast majority of retrieval models depend on vector inner products to\nproduce a relevance score between a query and a document. This naturally limits\nthe expressiveness of the relevance score that can be employed. We propose a\nnew paradigm, instead of producing a vector to represent the query we produce a\nsmall neural network which acts as a learned relevance function. This small\nneural network takes in a representation of the document, in this paper we use\na single vector, and produces a scalar relevance score. To produce the little\nneural network we use a hypernetwork, a network that produce the weights of\nother networks, as our query encoder or as we call it a Hypencoder. Experiments\non in-domain search tasks show that Hypencoder is able to significantly\noutperform strong dense retrieval models and has higher metrics then reranking\nmodels and models an order of magnitude larger. Hypencoder is also shown to\ngeneralize well to out-of-domain search tasks. To assess the extent of\nHypencoder's capabilities, we evaluate on a set of hard retrieval tasks\nincluding tip-of-the-tongue retrieval and instruction-following retrieval tasks\nand find that the performance gap widens substantially compared to standard\nretrieval tasks. Furthermore, to demonstrate the practicality of our method we\nimplement an approximate search algorithm and show that our model is able to\nsearch 8.8M documents in under 60ms.",
            "upvotes": 6,
            "discussionId": "67ab99c9bb44ec714c6a4ac1"
        },
        "publishedAt": "2025-02-12T10:35:08.488Z",
        "title": "Hypencoder: Hypernetworks for Information Retrieval",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05364.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635582fb6b58fa7cc8701580",
            "avatarUrl": "/avatars/a00791ba70a2de40dacac4582307c0f2.svg",
            "fullname": "Julian Killingback",
            "name": "jfkback",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06428",
            "authors": [
                {
                    "_id": "67ac99089e12456bdb1d2e9d",
                    "name": "Jian Hu",
                    "hidden": false
                },
                {
                    "_id": "67ac99089e12456bdb1d2e9e",
                    "name": "Zixu Cheng",
                    "hidden": false
                },
                {
                    "_id": "67ac99089e12456bdb1d2e9f",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "67ac99089e12456bdb1d2ea0",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67ac99089e12456bdb1d2ea1",
                    "name": "Shaogang Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T13:03:05.000Z",
            "title": "CoS: Chain-of-Shot Prompting for Long Video Understanding",
            "summary": "Multi-modal Large Language Models (MLLMs) struggle with long videos due to\nthe need for excessive visual tokens. These tokens exceed massively the context\nlength of MLLMs, resulting in filled by redundant task-irrelevant shots. How to\nselect shots is an unsolved critical problem: sparse sampling risks missing key\ndetails, while exhaustive sampling overwhelms the model with irrelevant\ncontent, leading to video misunderstanding. To solve this problem, we propose\nChain-of-Shot prompting (CoS). The key idea is to frame shot selection as\ntest-time visual prompt optimisation, choosing shots adaptive to video\nunderstanding semantic task by optimising shots-task alignment. CoS has two key\nparts: (1) a binary video summary mechanism that performs pseudo temporal\ngrounding, discovering a binary coding to identify task-relevant shots, and (2)\na video co-reasoning module that deploys the binary coding to pair (learning to\nalign) task-relevant positive shots with irrelevant negative shots. It embeds\nthe optimised shot selections into the original video, facilitating a focus on\nrelevant context to optimize long video understanding. Experiments across three\nbaselines and five datasets demonstrate the effectiveness and adaptability of\nCoS. Code given in https://lwpyh.github.io/CoS.",
            "upvotes": 6,
            "discussionId": "67ac990b9e12456bdb1d2efe"
        },
        "publishedAt": "2025-02-12T07:51:18.930Z",
        "title": "CoS: Chain-of-Shot Prompting for Long Video Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e1b6e9501590df0173cbd3",
            "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
            "fullname": "Jian Hu",
            "name": "lwpyh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07490",
            "authors": [
                {
                    "_id": "67ac8bc95cc4f961c9550320",
                    "name": "Xialie Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67ac8bc95cc4f961c9550321",
                    "name": "Zhikai Jia",
                    "hidden": false
                },
                {
                    "_id": "67ac8bc95cc4f961c9550322",
                    "name": "Jianjin Li",
                    "hidden": false
                },
                {
                    "_id": "67ac8bc95cc4f961c9550323",
                    "name": "Zhenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ac8bc95cc4f961c9550324",
                    "name": "Li Shen",
                    "hidden": false
                },
                {
                    "_id": "67ac8bc95cc4f961c9550325",
                    "name": "Zheng Cao",
                    "hidden": false
                },
                {
                    "_id": "67ac8bc95cc4f961c9550326",
                    "name": "Shiwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T11:49:03.000Z",
            "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More",
            "summary": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.",
            "upvotes": 6,
            "discussionId": "67ac8bc95cc4f961c955035f"
        },
        "publishedAt": "2025-02-12T06:55:30.432Z",
        "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07490.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b04d2291e63920a7898c9e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
            "fullname": "Liu",
            "name": "Shiweiliuiiiiiii",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07785",
            "authors": [
                {
                    "_id": "67aceb3315317375eccddc3b",
                    "name": "Yash Kant",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc3c",
                    "name": "Ethan Weber",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc3d",
                    "name": "Jin Kyu Kim",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc3e",
                    "name": "Rawal Khirodkar",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc3f",
                    "name": "Su Zhaoen",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc40",
                    "name": "Julieta Martinez",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc41",
                    "name": "Igor Gilitschenski",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc42",
                    "name": "Shunsuke Saito",
                    "hidden": false
                },
                {
                    "_id": "67aceb3315317375eccddc43",
                    "name": "Timur Bagautdinov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T18:59:59.000Z",
            "title": "Pippo: High-Resolution Multi-View Humans from a Single Image",
            "summary": "We present Pippo, a generative model capable of producing 1K resolution dense\nturnaround videos of a person from a single casually clicked photo. Pippo is a\nmulti-view diffusion transformer and does not require any additional inputs -\ne.g., a fitted parametric model or camera parameters of the input image. We\npre-train Pippo on 3B human images without captions, and conduct multi-view\nmid-training and post-training on studio captured humans. During mid-training,\nto quickly absorb the studio dataset, we denoise several (up to 48) views at\nlow-resolution, and encode target cameras coarsely using a shallow MLP. During\npost-training, we denoise fewer views at high-resolution and use pixel-aligned\ncontrols (e.g., Spatial anchor and Plucker rays) to enable 3D consistent\ngenerations. At inference, we propose an attention biasing technique that\nallows Pippo to simultaneously generate greater than 5 times as many views as\nseen during training. Finally, we also introduce an improved metric to evaluate\n3D consistency of multi-view generations, and show that Pippo outperforms\nexisting works on multi-view human generation from a single image.",
            "upvotes": 4,
            "discussionId": "67aceb3b15317375eccddd5b"
        },
        "publishedAt": "2025-02-12T13:41:46.312Z",
        "title": "Pippo: High-Resolution Multi-View Humans from a Single Image",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638546d0a179f856005ae310/WpddysA9AZ5Y3Fgo_XA75.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07785.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638546d0a179f856005ae310",
            "avatarUrl": "/avatars/f7f2fface336c8e168a1daaf9fd4d40c.svg",
            "fullname": "yashkant",
            "name": "yashkant",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07445",
            "authors": [
                {
                    "_id": "67ac216d602eb9ca8a517be6",
                    "name": "Nurit Cohen-Inger",
                    "hidden": false
                },
                {
                    "_id": "67ac216d602eb9ca8a517be7",
                    "name": "Yehonatan Elisha",
                    "hidden": false
                },
                {
                    "_id": "67ac216d602eb9ca8a517be8",
                    "name": "Bracha Shapira",
                    "hidden": false
                },
                {
                    "_id": "67ac216d602eb9ca8a517be9",
                    "name": "Lior Rokach",
                    "hidden": false
                },
                {
                    "_id": "67ac216d602eb9ca8a517bea",
                    "name": "Seffi Cohen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T10:43:36.000Z",
            "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
            "summary": "Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.",
            "upvotes": 4,
            "discussionId": "67ac216e602eb9ca8a517c1d"
        },
        "publishedAt": "2025-02-11T23:22:50.454Z",
        "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6731e56a07cf693a1104d2cb",
            "avatarUrl": "/avatars/46a3269a19c7e6bfb7004a5da9701459.svg",
            "fullname": "Seffi Cohen",
            "name": "seffico",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06755",
            "authors": [
                {
                    "_id": "67ab7c4f84bf8aaa60cf1d8d",
                    "user": {
                        "_id": "64df8592d27135dd568380b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df8592d27135dd568380b5/1-YkiTAkI11QBbZnVRjJu.jpeg",
                        "isPro": false,
                        "fullname": "Samuel Stevens",
                        "user": "samuelstevens",
                        "type": "user"
                    },
                    "name": "Samuel Stevens",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:16:50.887Z",
                    "hidden": false
                },
                {
                    "_id": "67ab7c4f84bf8aaa60cf1d8e",
                    "name": "Wei-Lun Chao",
                    "hidden": false
                },
                {
                    "_id": "67ab7c4f84bf8aaa60cf1d8f",
                    "name": "Tanya Berger-Wolf",
                    "hidden": false
                },
                {
                    "_id": "67ab7c4f84bf8aaa60cf1d90",
                    "name": "Yu Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T18:32:41.000Z",
            "title": "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision\n  Models",
            "summary": "To truly understand vision models, we must not only interpret their learned\nfeatures but also validate these interpretations through controlled\nexperiments. Current approaches either provide interpretable features without\nthe ability to test their causal influence, or enable model editing without\ninterpretable controls. We present a unified framework using sparse\nautoencoders (SAEs) that bridges this gap, allowing us to discover\nhuman-interpretable visual features and precisely manipulate them to test\nhypotheses about model behavior. By applying our method to state-of-the-art\nvision models, we reveal key differences in the semantic abstractions learned\nby models with different pre-training objectives. We then demonstrate the\npractical usage of our framework through controlled interventions across\nmultiple vision tasks. We show that SAEs can reliably identify and manipulate\ninterpretable visual features without model re-training, providing a powerful\ntool for understanding and controlling vision model behavior. We provide code,\ndemos and models on our project website: https://osu-nlp-group.github.io/SAE-V.",
            "upvotes": 3,
            "discussionId": "67ab7c5784bf8aaa60cf1f4c"
        },
        "publishedAt": "2025-02-12T09:54:39.924Z",
        "title": "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64df8592d27135dd568380b5/xfktZouikPLtypubL69TW.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06755.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64df8592d27135dd568380b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df8592d27135dd568380b5/1-YkiTAkI11QBbZnVRjJu.jpeg",
            "fullname": "Samuel Stevens",
            "name": "samuelstevens",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07776",
            "authors": [
                {
                    "_id": "67ac1f7851c7f3b53ffc4def",
                    "name": "Chenchen Gu",
                    "hidden": false
                },
                {
                    "_id": "67ac1f7851c7f3b53ffc4df0",
                    "name": "Xiang Lisa Li",
                    "hidden": false
                },
                {
                    "_id": "67ac1f7851c7f3b53ffc4df1",
                    "name": "Rohith Kuditipudi",
                    "hidden": false
                },
                {
                    "_id": "67ac1f7851c7f3b53ffc4df2",
                    "name": "Percy Liang",
                    "hidden": false
                },
                {
                    "_id": "67ac1f7851c7f3b53ffc4df3",
                    "user": {
                        "_id": "661595d1b3d0b21da55cde7d",
                        "avatarUrl": "/avatars/ba3fa065536518637d21a5c46cee5dd1.svg",
                        "isPro": false,
                        "fullname": "Tatsu Hashimoto",
                        "user": "thashim",
                        "type": "user"
                    },
                    "name": "Tatsunori Hashimoto",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-12T04:11:36.912Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T18:58:04.000Z",
            "title": "Auditing Prompt Caching in Language Model APIs",
            "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
            "upvotes": 3,
            "discussionId": "67ac1f7851c7f3b53ffc4e1b"
        },
        "publishedAt": "2025-02-11T23:11:49.993Z",
        "title": "Auditing Prompt Caching in Language Model APIs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07640",
            "authors": [
                {
                    "_id": "67acb6af4335fbde70348fc1",
                    "name": "Yong Lin",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc2",
                    "name": "Shange Tang",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc3",
                    "name": "Bohan Lyu",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc4",
                    "name": "Jiayun Wu",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc5",
                    "name": "Hongzhou Lin",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc6",
                    "name": "Kaiyu Yang",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc7",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc8",
                    "name": "Mengzhou Xia",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fc9",
                    "name": "Danqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fca",
                    "name": "Sanjeev Arora",
                    "hidden": false
                },
                {
                    "_id": "67acb6af4335fbde70348fcb",
                    "name": "Chi Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T15:27:35.000Z",
            "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
            "summary": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.",
            "upvotes": 2,
            "discussionId": "67acb6b04335fbde70349021"
        },
        "publishedAt": "2025-02-12T09:56:56.287Z",
        "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07640.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6054
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.05932",
            "authors": [
                {
                    "_id": "67ac4356401012b81050022a",
                    "user": {
                        "_id": "67ac430c4ab9207cc227d23f",
                        "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
                        "isPro": false,
                        "fullname": "Tenglong Liu",
                        "user": "LTL07",
                        "type": "user"
                    },
                    "name": "Tenglong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-12T09:15:09.627Z",
                    "hidden": false
                },
                {
                    "_id": "67ac4356401012b81050022b",
                    "name": "Jianxiong Li",
                    "hidden": false
                },
                {
                    "_id": "67ac4356401012b81050022c",
                    "name": "Yinan Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ac4356401012b81050022d",
                    "name": "Haoyi Niu",
                    "hidden": false
                },
                {
                    "_id": "67ac4356401012b81050022e",
                    "name": "Yixing Lan",
                    "hidden": false
                },
                {
                    "_id": "67ac4356401012b81050022f",
                    "name": "Xin Xu",
                    "hidden": false
                },
                {
                    "_id": "67ac4356401012b810500230",
                    "name": "Xianyuan Zhan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T15:22:38.000Z",
            "title": "Skill Expansion and Composition in Parameter Space",
            "summary": "Humans excel at reusing prior knowledge to address new challenges and\ndeveloping skills while solving problems. This paradigm becomes increasingly\npopular in the development of autonomous agents, as it develops systems that\ncan self-evolve in response to new challenges like human beings. However,\nprevious methods suffer from limited training efficiency when expanding new\nskills and fail to fully leverage prior knowledge to facilitate new task\nlearning. In this paper, we propose Parametric Skill Expansion and Composition\n(PSEC), a new framework designed to iteratively evolve the agents' capabilities\nand efficiently address new challenges by maintaining a manageable skill\nlibrary. This library can progressively integrate skill primitives as\nplug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient\nfinetuning, facilitating efficient and flexible skill expansion. This structure\nalso enables the direct skill compositions in parameter space by merging LoRA\nmodules that encode different skills, leveraging shared information across\nskills to effectively program new skills. Based on this, we propose a\ncontext-aware module to dynamically activate different skills to\ncollaboratively handle new tasks. Empowering diverse applications including\nmulti-objective composition, dynamics shift, and continual policy shift, the\nresults on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC\nexhibits superior capacity to leverage prior knowledge to efficiently tackle\nnew challenges, as well as expand its skill libraries to evolve the\ncapabilities. Project website: https://ltlhuuu.github.io/PSEC/.",
            "upvotes": 2,
            "discussionId": "67ac435b401012b8105003dc"
        },
        "publishedAt": "2025-02-12T04:53:50.325Z",
        "title": "Skill Expansion and Composition in Parameter Space",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05932.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ac430c4ab9207cc227d23f",
            "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
            "fullname": "Tenglong Liu",
            "name": "LTL07",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.04465",
            "authors": [
                {
                    "_id": "67a953844ea315a67e02461d",
                    "user": {
                        "_id": "63195d0582e7eec0eac040e3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
                        "isPro": false,
                        "fullname": "Luca Della Libera",
                        "user": "lucadellalib",
                        "type": "user"
                    },
                    "name": "Luca Della Libera",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-11T10:03:10.257Z",
                    "hidden": false
                },
                {
                    "_id": "67a953844ea315a67e02461e",
                    "name": "Francesco Paissan",
                    "hidden": false
                },
                {
                    "_id": "67a953844ea315a67e02461f",
                    "name": "Cem Subakan",
                    "hidden": false
                },
                {
                    "_id": "67a953844ea315a67e024620",
                    "name": "Mirco Ravanelli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-06T19:24:50.000Z",
            "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
            "summary": "Large language models have revolutionized natural language processing through\nself-supervised pretraining on massive datasets. Inspired by this success,\nresearchers have explored adapting these methods to speech by discretizing\ncontinuous audio into tokens using neural audio codecs. However, existing\napproaches face limitations, including high bitrates, the loss of either\nsemantic or acoustic information, and the reliance on multi-codebook designs\nwhen trying to capture both, which increases architectural complexity for\ndownstream tasks. To address these challenges, we introduce FocalCodec, an\nefficient low-bitrate codec based on focal modulation that utilizes a single\nbinary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec\ndelivers competitive performance in speech resynthesis and voice conversion at\nlower bitrates than the current state-of-the-art, while effectively handling\nmultilingual speech and noisy environments. Evaluation on downstream tasks\nshows that FocalCodec successfully preserves sufficient semantic and acoustic\ninformation, while also being well-suited for generative modeling. Demo\nsamples, code and checkpoints are available at\nhttps://lucadellalib.github.io/focalcodec-web/.",
            "upvotes": 2,
            "discussionId": "67a953854ea315a67e024659"
        },
        "publishedAt": "2025-02-12T01:31:44.368Z",
        "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "fullname": "Luca Della Libera",
            "name": "lucadellalib",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.06884",
            "authors": [
                {
                    "_id": "67ad315adbc8ae7b3ca9f17d",
                    "name": "Sina Tayebati",
                    "hidden": false
                },
                {
                    "_id": "67ad315adbc8ae7b3ca9f17e",
                    "name": "Divake Kumar",
                    "hidden": false
                },
                {
                    "_id": "67ad315adbc8ae7b3ca9f17f",
                    "name": "Nastaran Darabi",
                    "hidden": false
                },
                {
                    "_id": "67ad315adbc8ae7b3ca9f180",
                    "name": "Dinithi Jayasuriya",
                    "hidden": false
                },
                {
                    "_id": "67ad315adbc8ae7b3ca9f181",
                    "name": "Ranganath Krishnan",
                    "hidden": false
                },
                {
                    "_id": "67ad315adbc8ae7b3ca9f182",
                    "name": "Amit Ranjan Trivedi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-08T21:30:41.000Z",
            "title": "Learning Conformal Abstention Policies for Adaptive Risk Management in\n  Large Language and Vision-Language Models",
            "summary": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used\nin safety-critical applications, yet their opaque decision-making complicates\nrisk assessment and reliability. Uncertainty quantification (UQ) helps assess\nprediction confidence and enables abstention when uncertainty is high.\nConformal prediction (CP), a leading UQ method, provides statistical guarantees\nbut relies on static thresholds, which fail to adapt to task complexity and\nevolving data distributions, leading to suboptimal trade-offs in accuracy,\ncoverage, and informativeness. To address this, we propose learnable conformal\nabstention, integrating reinforcement learning (RL) with CP to optimize\nabstention thresholds dynamically. By treating CP thresholds as adaptive\nactions, our approach balances multiple objectives, minimizing prediction set\nsize while maintaining reliable coverage. Extensive evaluations across diverse\nLLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers\n(LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%,\nboosting AUROC for hallucination detection by 22.19%, enhancing\nuncertainty-guided selective generation (AUARC) by 21.17%, and reducing\ncalibration error by 70%-85%. These improvements hold across multiple models\nand datasets while consistently meeting the 90% coverage target, establishing\nour approach as a more effective and flexible solution for reliable\ndecision-making in safety-critical applications. The code is available at:\n{https://github.com/sinatayebati/vlm-uncertainty}.",
            "upvotes": 0,
            "discussionId": "67ad315bdbc8ae7b3ca9f1b7"
        },
        "publishedAt": "2025-02-12T18:40:34.235Z",
        "title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06884.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655ec30b12fb73960ceb048f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655ec30b12fb73960ceb048f/q7zVSStJWBywrtPoL2ChO.png",
            "fullname": "Sina Tayebati",
            "name": "sinatayebati",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]