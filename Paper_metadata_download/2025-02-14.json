[
    {
        "paper": {
            "id": "2502.08910",
            "authors": [
                {
                    "_id": "67aebd48225614bbe7f6f271",
                    "user": {
                        "_id": "62e622d08e0b2dc6707f8794",
                        "avatarUrl": "/avatars/8c47b5c862f82d4258ba707c932f7f87.svg",
                        "isPro": false,
                        "fullname": "Heejun Lee",
                        "user": "gmlwns5176",
                        "type": "user"
                    },
                    "name": "Heejun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:15.423Z",
                    "hidden": false
                },
                {
                    "_id": "67aebd48225614bbe7f6f272",
                    "user": {
                        "_id": "646cae3093badbc8c2e891c7",
                        "avatarUrl": "/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg",
                        "isPro": false,
                        "fullname": "Geon Park",
                        "user": "geonp",
                        "type": "user"
                    },
                    "name": "Geon Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:12.988Z",
                    "hidden": false
                },
                {
                    "_id": "67aebd48225614bbe7f6f273",
                    "user": {
                        "_id": "657ffd9ba2ef32167533f04a",
                        "avatarUrl": "/avatars/e180e063c810c15d02b494727e962b84.svg",
                        "isPro": false,
                        "fullname": "Jaduk Suh",
                        "user": "Losif63",
                        "type": "user"
                    },
                    "name": "Jaduk Suh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:00:46.888Z",
                    "hidden": false
                },
                {
                    "_id": "67aebd48225614bbe7f6f274",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T02:52:01.000Z",
            "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
            "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
            "upvotes": 61,
            "discussionId": "67aebd4a225614bbe7f6f2d6"
        },
        "publishedAt": "2025-02-13T22:57:03.709Z",
        "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646cae3093badbc8c2e891c7/upRSt7mdOUX5vJZTWKG8D.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08910.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "646cae3093badbc8c2e891c7",
            "avatarUrl": "/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg",
            "fullname": "Geon Park",
            "name": "geonp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.08690",
            "authors": [
                {
                    "_id": "67aec0a203bf3301ec29ac39",
                    "user": {
                        "_id": "633e6f07309a99325095dd42",
                        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
                        "isPro": false,
                        "fullname": "Hoigi Seo",
                        "user": "Agorium",
                        "type": "user"
                    },
                    "name": "Hoigi Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:10.420Z",
                    "hidden": false
                },
                {
                    "_id": "67aec0a203bf3301ec29ac3a",
                    "name": "Wongi Jeong",
                    "hidden": false
                },
                {
                    "_id": "67aec0a203bf3301ec29ac3b",
                    "name": "Jae-sun Seo",
                    "hidden": false
                },
                {
                    "_id": "67aec0a203bf3301ec29ac3c",
                    "name": "Se Young Chun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T15:03:26.000Z",
            "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient\n  Text-to-Image Generation",
            "summary": "Large-scale text encoders in text-to-image (T2I) diffusion models have\ndemonstrated exceptional performance in generating high-quality images from\ntextual prompts. Unlike denoising modules that rely on multiple iterative\nsteps, text encoders require only a single forward pass to produce text\nembeddings. However, despite their minimal contribution to total inference time\nand floating-point operations (FLOPs), text encoders demand significantly\nhigher memory usage, up to eight times more than denoising modules. To address\nthis inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet\neffective pruning strategy specifically designed for text encoders in T2I\ndiffusion models. Skrr exploits the inherent redundancy in transformer blocks\nby selectively skipping or reusing certain layers in a manner tailored for T2I\ntasks, thereby reducing memory consumption without compromising performance.\nExtensive experiments demonstrate that Skrr maintains image quality comparable\nto the original model even under high sparsity levels, outperforming existing\nblockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory\nefficiency while preserving performance across multiple evaluation metrics,\nincluding the FID, CLIP, DreamSim, and GenEval scores.",
            "upvotes": 28,
            "discussionId": "67aec0a903bf3301ec29adf3"
        },
        "publishedAt": "2025-02-13T23:10:44.295Z",
        "title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08690.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633e6f07309a99325095dd42",
            "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
            "fullname": "Hoigi Seo",
            "name": "Agorium",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09604",
            "authors": [
                {
                    "_id": "67aeac4f2d48d9bf7728334e",
                    "user": {
                        "_id": "5df84571da6d0311fd3d5407",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650651305661-5df84571da6d0311fd3d5407.png",
                        "isPro": false,
                        "fullname": "Yung-Sung Chuang",
                        "user": "voidism",
                        "type": "user"
                    },
                    "name": "Yung-Sung Chuang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-14T02:37:32.909Z",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf7728334f",
                    "user": {
                        "_id": "639aaf82a4c528850bba2bfe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639aaf82a4c528850bba2bfe/nn23r8bsNiOJzVUxAPfo7.png",
                        "isPro": false,
                        "fullname": "Benjamin Cohen-Wang",
                        "user": "bencw",
                        "type": "user"
                    },
                    "name": "Benjamin Cohen-Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:17.696Z",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283350",
                    "name": "Shannon Zejiang Shen",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283351",
                    "user": {
                        "_id": "6351712b40dffad651f128c7",
                        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
                        "isPro": false,
                        "fullname": "Zhaofeng Wu",
                        "user": "ZhaofengWu",
                        "type": "user"
                    },
                    "name": "Zhaofeng Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:19.691Z",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283352",
                    "name": "Hu Xu",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283353",
                    "name": "Xi Victoria Lin",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283354",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283355",
                    "name": "Shang-Wen Li",
                    "hidden": false
                },
                {
                    "_id": "67aeac4f2d48d9bf77283356",
                    "name": "Wen-tau Yih",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:55:13.000Z",
            "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models",
            "summary": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.",
            "upvotes": 22,
            "discussionId": "67aeac502d48d9bf77283380"
        },
        "publishedAt": "2025-02-13T21:42:37.926Z",
        "title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5df84571da6d0311fd3d5407/YmJO6H2Wa0ZVw31qeHZi0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09604.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5df84571da6d0311fd3d5407",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650651305661-5df84571da6d0311fd3d5407.png",
            "fullname": "Yung-Sung Chuang",
            "name": "voidism",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09619",
            "authors": [
                {
                    "_id": "67aef6212c36e4d8bd23740e",
                    "user": {
                        "_id": "6465fd33dac127ac80f0b334",
                        "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
                        "isPro": false,
                        "fullname": "Jonathan Kahana",
                        "user": "jonkahana",
                        "type": "user"
                    },
                    "name": "Jonathan Kahana",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:41:31.020Z",
                    "hidden": false
                },
                {
                    "_id": "67aef6212c36e4d8bd23740f",
                    "name": "Or Nathan",
                    "hidden": false
                },
                {
                    "_id": "67aef6212c36e4d8bd237410",
                    "user": {
                        "_id": "630dd4218df86f1e5beb2ed7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
                        "isPro": false,
                        "fullname": "Eliahu Horwitz",
                        "user": "Eliahu",
                        "type": "user"
                    },
                    "name": "Eliahu Horwitz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T11:10:52.689Z",
                    "hidden": false
                },
                {
                    "_id": "67aef6212c36e4d8bd237411",
                    "user": {
                        "_id": "646cfc3b4220471ca0c56b20",
                        "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
                        "isPro": false,
                        "fullname": "Yedid Hoshen",
                        "user": "yedid",
                        "type": "user"
                    },
                    "name": "Yedid Hoshen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:41:43.672Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:59:44.000Z",
            "title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights",
            "summary": "With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.",
            "upvotes": 21,
            "discussionId": "67aef6222c36e4d8bd237472"
        },
        "publishedAt": "2025-02-14T02:58:25.756Z",
        "title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09619.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465fd33dac127ac80f0b334",
            "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
            "fullname": "Jonathan Kahana",
            "name": "jonkahana",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09056",
            "authors": [
                {
                    "_id": "67aea8d7926b659c7e959bbc",
                    "user": {
                        "_id": "62d192c2d50433c35eb1b48e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png",
                        "isPro": true,
                        "fullname": "Kunat Pipatanakul",
                        "user": "kunato",
                        "type": "user"
                    },
                    "name": "Kunat Pipatanakul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:04:29.437Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8d7926b659c7e959bbd",
                    "user": {
                        "_id": "615313b0793ef66b3324da1f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
                        "isPro": false,
                        "fullname": "Pittawat Taveekitworachai",
                        "user": "pittawat",
                        "type": "user"
                    },
                    "name": "Pittawat Taveekitworachai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:21.838Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8d7926b659c7e959bbe",
                    "user": {
                        "_id": "63f6a050b4c9a104f4b95755",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f6a050b4c9a104f4b95755/eJQyJkenSz536j-EGcpkH.jpeg",
                        "isPro": false,
                        "fullname": "Potsawee Manakul",
                        "user": "potsawee",
                        "type": "user"
                    },
                    "name": "Potsawee Manakul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:04:35.434Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8d7926b659c7e959bbf",
                    "name": "Kasima Tharnpipitchai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T08:10:45.000Z",
            "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in\n  One Day via Model Merging",
            "summary": "This paper investigates data selection and model merging methodologies aimed\nat incorporating advanced reasoning capabilities such as those of DeepSeek R1\ninto language-specific large language models (LLMs), with a particular focus on\nthe Thai LLM. Our goal is to enhance the reasoning capabilities of\nlanguage-specific LLMs while maintaining their target language abilities.\nDeepSeek R1 excels in reasoning but primarily benefits high-resource languages\nsuch as English and Chinese. However, low-resource languages remain underserved\ndue to the dominance of English-centric training data and model optimizations,\nwhich limit performance in these languages. This limitation results in\nunreliable code-switching and diminished effectiveness on tasks in low-resource\nlanguages. Meanwhile, local and regional LLM initiatives have attempted to\nbridge this gap by developing language-specific LLMs that focus on improving\nlocal linguistic fidelity. We demonstrate that, with only publicly available\ndatasets and a computational budget of $120, it is possible to enhance the\nreasoning capabilities of language-specific LLMs to match the level of DeepSeek\nR1, without compromising their performance on target language tasks.",
            "upvotes": 21,
            "discussionId": "67aea8d8926b659c7e959bee"
        },
        "publishedAt": "2025-02-13T22:01:48.364Z",
        "title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09056.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6091
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09560",
            "authors": [
                {
                    "_id": "67aec4285b9801b819449b84",
                    "name": "Rui Yang",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b85",
                    "user": {
                        "_id": "6700b1f93381f2db06857fb5",
                        "avatarUrl": "/avatars/c8b9ec7c00773c5a4055ba50de0c6b2f.svg",
                        "isPro": false,
                        "fullname": "Hanyang Chen",
                        "user": "Hanyang81",
                        "type": "user"
                    },
                    "name": "Hanyang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:08.365Z",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b86",
                    "name": "Junyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b87",
                    "name": "Mark Zhao",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b88",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b89",
                    "user": {
                        "_id": "66f2b8602ef2817ec3cb65f6",
                        "avatarUrl": "/avatars/c8c5b2706644fb45a75f13af99fa7ae9.svg",
                        "isPro": false,
                        "fullname": "Kangrui Wang",
                        "user": "JamesK2W",
                        "type": "user"
                    },
                    "name": "Kangrui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:47:36.061Z",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b8a",
                    "user": {
                        "_id": "640c3b28c5fa12d61a50cd92",
                        "avatarUrl": "/avatars/81556de3214c848b3c3e118f50fd2968.svg",
                        "isPro": false,
                        "fullname": "Qineng Wang",
                        "user": "Inevitablevalor",
                        "type": "user"
                    },
                    "name": "Qineng Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:46:06.895Z",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b8b",
                    "name": "Teja Venkat Koripella",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b8c",
                    "user": {
                        "_id": "64e10c263209bf4194912319",
                        "avatarUrl": "/avatars/02f1a9e2ce333ff521d901cf83fcdff3.svg",
                        "isPro": false,
                        "fullname": "Marziyeh Movahedi",
                        "user": "Marzimv",
                        "type": "user"
                    },
                    "name": "Marziyeh Movahedi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:46:13.104Z",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b8d",
                    "user": {
                        "_id": "6746a140f2ca2162e3bcfe2b",
                        "avatarUrl": "/avatars/d9d8cfb5f112e6ed7f6152fc230135d3.svg",
                        "isPro": false,
                        "fullname": "Manling Li",
                        "user": "ManlingLi",
                        "type": "user"
                    },
                    "name": "Manling Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:46:50.911Z",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b8e",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b8f",
                    "user": {
                        "_id": "6719d581a6cad13741b8bc7f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
                        "isPro": false,
                        "fullname": "Huan Zhang",
                        "user": "huanzhang12",
                        "type": "user"
                    },
                    "name": "Huan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:47:14.741Z",
                    "hidden": false
                },
                {
                    "_id": "67aec4285b9801b819449b90",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:11:34.000Z",
            "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents",
            "summary": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.",
            "upvotes": 20,
            "discussionId": "67aec42b5b9801b819449bf5"
        },
        "publishedAt": "2025-02-13T23:23:42.492Z",
        "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09560.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "fullname": "Rui Yang",
            "name": "Ray2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09620",
            "authors": [
                {
                    "_id": "67aeec91b1bbfb68824df5d1",
                    "user": {
                        "_id": "6552f1ad5d55ccb20e9142a0",
                        "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg",
                        "isPro": false,
                        "fullname": "Ivan Tang",
                        "user": "IvanTang",
                        "type": "user"
                    },
                    "name": "Yiwen Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:00:57.216Z",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d2",
                    "user": {
                        "_id": "642a8302d651bae3c11b72b1",
                        "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg",
                        "isPro": false,
                        "fullname": "Zoey Guo",
                        "user": "Purple1288",
                        "type": "user"
                    },
                    "name": "Zoey Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:02:40.282Z",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d3",
                    "user": {
                        "_id": "672489e32fd598f719be33ba",
                        "avatarUrl": "/avatars/63a6edf7bf38957e029fa52c1a7f9061.svg",
                        "isPro": false,
                        "fullname": "Zhuhao Wang",
                        "user": "zhuhaow",
                        "type": "user"
                    },
                    "name": "Zhuhao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:02:47.566Z",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d4",
                    "name": "Ray Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d5",
                    "user": {
                        "_id": "6535045a910b844786a6642f",
                        "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg",
                        "isPro": false,
                        "fullname": "Qizhi Chen",
                        "user": "Tavish9",
                        "type": "user"
                    },
                    "name": "Qizhi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:03:12.952Z",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d6",
                    "name": "Junli Liu",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d7",
                    "user": {
                        "_id": "64daecec888b7e9c400f59b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
                        "isPro": false,
                        "fullname": "Delin Qu",
                        "user": "delinqu",
                        "type": "user"
                    },
                    "name": "Delin Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:00:55.263Z",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d8",
                    "name": "Zhigang Wang",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5d9",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5da",
                    "name": "Xuelong Li",
                    "hidden": false
                },
                {
                    "_id": "67aeec91b1bbfb68824df5db",
                    "name": "Bin Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:59:45.000Z",
            "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
            "summary": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL",
            "upvotes": 17,
            "discussionId": "67aeec92b1bbfb68824df61f"
        },
        "publishedAt": "2025-02-14T02:27:45.749Z",
        "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09620.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647d9ab61a1fcad2fdbf2d3d",
            "avatarUrl": "/avatars/48c8aeae8979d2c87df8bde922437d62.svg",
            "fullname": "Ziyu Guo",
            "name": "ZiyuG",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09082",
            "authors": [
                {
                    "_id": "67aee90c208d299238758622",
                    "name": "Xintao Wang",
                    "hidden": true
                },
                {
                    "_id": "67aee90c208d299238758623",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d299238758624",
                    "name": "Yifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d299238758625",
                    "name": "Xinfeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d299238758626",
                    "name": "Rui Xu",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d299238758627",
                    "name": "Jen-tse Huang",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d299238758628",
                    "user": {
                        "_id": "62d62b333bf5e059f7d2b286",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
                        "isPro": false,
                        "fullname": "Siyu Yuan",
                        "user": "siyuyuan",
                        "type": "user"
                    },
                    "name": "Siyu Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:43:00.148Z",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d299238758629",
                    "user": {
                        "_id": "64f06b030ba7caa06a8bf13e",
                        "avatarUrl": "/avatars/a9dce43ecdd0339c438a64e28dcd3fcf.svg",
                        "isPro": false,
                        "fullname": "Haoran Guo",
                        "user": "Haoran-Guo",
                        "type": "user"
                    },
                    "name": "Haoran Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:42:42.152Z",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d29923875862a",
                    "user": {
                        "_id": "606ed1884ffe81d1e03e81e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639375346654-606ed1884ffe81d1e03e81e5.png",
                        "isPro": false,
                        "fullname": "Jiangjie Chen",
                        "user": "jiangjiechen",
                        "type": "user"
                    },
                    "name": "Jiangjie Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:42:33.652Z",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d29923875862b",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d29923875862c",
                    "name": "Yanghua Xiao",
                    "hidden": false
                },
                {
                    "_id": "67aee90c208d29923875862d",
                    "user": {
                        "_id": "620531f8522e40b4a18d872f",
                        "avatarUrl": "/avatars/cd2fba21c499e27dea75e571a3b75228.svg",
                        "isPro": false,
                        "fullname": "Shuchang Zhou",
                        "user": "zsc",
                        "type": "user"
                    },
                    "name": "Shuchang Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:42:04.561Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T08:55:24.000Z",
            "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
            "summary": "Role-playing language agents (RPLAs) have emerged as promising applications\nof large language models (LLMs). However, simulating established characters\npresents a challenging task for RPLAs, due to the lack of authentic character\ndatasets and nuanced evaluation methods using such data. In this paper, we\npresent CoSER, a collection of a high-quality dataset, open models, and an\nevaluation protocol towards effective RPLAs of established characters. The\nCoSER dataset covers 17,966 characters from 771 renowned books. It provides\nauthentic dialogues with real-world intricacies, as well as diverse data types\nsuch as conversation setups, character experiences and internal thoughts.\nDrawing from acting methodology, we introduce given-circumstance acting for\ntraining and evaluating role-playing LLMs, where LLMs sequentially portray\nmultiple characters in book scenes. Using our dataset, we develop CoSER 8B and\nCoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.\nExtensive experiments demonstrate the value of the CoSER dataset for RPLA\ntraining, evaluation and retrieval. Moreover, CoSER 70B exhibits\nstate-of-the-art performance surpassing or matching GPT-4o on our evaluation\nand three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on\nthe InCharacter and LifeChoice benchmarks respectively.",
            "upvotes": 16,
            "discussionId": "67aee90f208d2992387586d1"
        },
        "publishedAt": "2025-02-14T02:50:35.108Z",
        "title": "CoSER: Coordinating LLM-Based Persona Simulation of Established Roles",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09082.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c7bf2c4524c2aea7eac0b3",
            "avatarUrl": "/avatars/03e432e05c0f711cfe32fc07f195e11e.svg",
            "fullname": "Xintao Wang",
            "name": "Neph0s",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.06608",
            "authors": [
                {
                    "_id": "67aebe57f47426f753bc3b07",
                    "user": {
                        "_id": "64d71083a787c9bc7b9f1238",
                        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                        "isPro": false,
                        "fullname": "YG",
                        "user": "Lp256",
                        "type": "user"
                    },
                    "name": "Yangguang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T16:20:05.446Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b08",
                    "user": {
                        "_id": "644dbf6453ad80c6593bf748",
                        "avatarUrl": "/avatars/0e170cf2aa8d7f0f3f83e36f06f023f8.svg",
                        "isPro": false,
                        "fullname": "Zixin Zou",
                        "user": "zouzx",
                        "type": "user"
                    },
                    "name": "Zi-Xin Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:44:17.925Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b09",
                    "user": {
                        "_id": "65ff33a830b872fe2ccc6c1e",
                        "avatarUrl": "/avatars/cf15da8fc5a51f0b3ae0e11e3ff685cf.svg",
                        "isPro": false,
                        "fullname": "Zexiang Liu",
                        "user": "zexiangliu",
                        "type": "user"
                    },
                    "name": "Zexiang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:44:23.987Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b0a",
                    "user": {
                        "_id": "666c4e95dc348adcab8f2836",
                        "avatarUrl": "/avatars/875545d88ba7d935340015a719e6e5f0.svg",
                        "isPro": false,
                        "fullname": "dehuwang",
                        "user": "dehu168",
                        "type": "user"
                    },
                    "name": "Dehu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:44:35.149Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b0b",
                    "name": "Yuan Liang",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b0c",
                    "name": "Zhipeng Yu",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b0d",
                    "user": {
                        "_id": "646b0bbdec9a61e871799339",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b0bbdec9a61e871799339/Xippv-ajkHkrGGAA7caLn.jpeg",
                        "isPro": false,
                        "fullname": "Xingchao Liu",
                        "user": "XCLiu",
                        "type": "user"
                    },
                    "name": "Xingchao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:45:05.465Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b0e",
                    "user": {
                        "_id": "6346aaa3f06b237ba4e297b0",
                        "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
                        "isPro": false,
                        "fullname": "Yuan-Chen Guo",
                        "user": "bennyguo",
                        "type": "user"
                    },
                    "name": "Yuan-Chen Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:45:11.634Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b0f",
                    "user": {
                        "_id": "6476b01cbb7fdd7f425dcefb",
                        "avatarUrl": "/avatars/65efe7067f47a68c204a1ab5a772b939.svg",
                        "isPro": false,
                        "fullname": "dingliang",
                        "user": "dingliang01",
                        "type": "user"
                    },
                    "name": "Ding Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:45:32.067Z",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b10",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67aebe57f47426f753bc3b11",
                    "name": "Yan-Pei Cao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-10T16:07:54.000Z",
            "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models",
            "summary": "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprece- dented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data process- ing, and insufficient exploration of\nadvanced tech- niques in the 3D domain. Current approaches to 3D shape\ngeneration face substantial challenges in terms of output quality,\ngeneralization capa- bility, and alignment with input conditions. We present\nTripoSG, a new streamlined shape diffu- sion paradigm capable of generating\nhigh-fidelity 3D meshes with precise correspondence to input images.\nSpecifically, we propose: 1) A large-scale rectified flow transformer for 3D\nshape generation, achieving state-of-the-art fidelity through training on\nextensive, high-quality data. 2) A hybrid supervised training strategy\ncombining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality\n3D reconstruction performance. 3) A data processing pipeline to generate 2\nmillion high- quality 3D samples, highlighting the crucial rules for data\nquality and quantity in training 3D gen- erative models. Through comprehensive\nexperi- ments, we have validated the effectiveness of each component in our new\nframework. The seamless integration of these parts has enabled TripoSG to\nachieve state-of-the-art performance in 3D shape generation. The resulting 3D\nshapes exhibit en- hanced detail due to high-resolution capabilities and\ndemonstrate exceptional fidelity to input im- ages. Moreover, TripoSG\ndemonstrates improved versatility in generating 3D models from diverse image\nstyles and contents, showcasing strong gen- eralization capabilities. To foster\nprogress and innovation in the field of 3D generation, we will make our model\npublicly available.",
            "upvotes": 15,
            "discussionId": "67aebe5ef47426f753bc3d31"
        },
        "publishedAt": "2025-02-13T22:56:23.567Z",
        "title": "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06608.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "fullname": "YG",
            "name": "Lp256",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.08946",
            "authors": [
                {
                    "_id": "67aeb180cb3be2cefd46ed07",
                    "name": "Mo Yu",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed08",
                    "user": {
                        "_id": "64462d77efa0723f3946356b",
                        "avatarUrl": "/avatars/f9757030d82c69aef933309e0c83ccd0.svg",
                        "isPro": false,
                        "fullname": "Lemao Liu",
                        "user": "lemaoliu",
                        "type": "user"
                    },
                    "name": "Lemao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:53:38.597Z",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed09",
                    "user": {
                        "_id": "64d36ca3036ae0b3756588e3",
                        "avatarUrl": "/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg",
                        "isPro": false,
                        "fullname": "Junjie Wu",
                        "user": "junjiewu",
                        "type": "user"
                    },
                    "name": "Junjie Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:53:53.052Z",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed0a",
                    "name": "Tsz Ting Chung",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed0b",
                    "user": {
                        "_id": "63fe24448b3c5087ff866b39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fe24448b3c5087ff866b39/CaTM4yAfj9tJj53ciQWJk.jpeg",
                        "isPro": false,
                        "fullname": "Shunchi Zhang",
                        "user": "ShunchiZhang",
                        "type": "user"
                    },
                    "name": "Shunchi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:54:03.865Z",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed0c",
                    "name": "Jiangnan Li",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed0d",
                    "name": "Dit-Yan Yeung",
                    "hidden": false
                },
                {
                    "_id": "67aeb180cb3be2cefd46ed0e",
                    "name": "Jie Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T04:00:03.000Z",
            "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding",
            "summary": "In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.",
            "upvotes": 13,
            "discussionId": "67aeb181cb3be2cefd46ed4c"
        },
        "publishedAt": "2025-02-13T21:59:28.400Z",
        "title": "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08946.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6091
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09621",
            "authors": [
                {
                    "_id": "67aee0229e69670f49533146",
                    "user": {
                        "_id": "6349214f8146350b3a4c5cdf",
                        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
                        "isPro": false,
                        "fullname": "Dongzhi Jiang",
                        "user": "CaraJ",
                        "type": "user"
                    },
                    "name": "Dongzhi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:05.736Z",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533147",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533148",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533149",
                    "user": {
                        "_id": "643ff78cdc984afcbbbc3b1a",
                        "avatarUrl": "/avatars/eec5198ce88aaf8156840bec0d190a7f.svg",
                        "isPro": false,
                        "fullname": "Yanwei Li",
                        "user": "YanweiLi",
                        "type": "user"
                    },
                    "name": "Yanwei Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:50:20.626Z",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f4953314a",
                    "name": "Yu Qi",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f4953314b",
                    "user": {
                        "_id": "647c7a4ed412b3b376572a00",
                        "avatarUrl": "/avatars/9cc310fd3f9e3f211475816ed9b0cdaa.svg",
                        "isPro": false,
                        "fullname": "Xinyan Chen",
                        "user": "xy06",
                        "type": "user"
                    },
                    "name": "Xinyan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:50:07.239Z",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f4953314c",
                    "name": "Liuhui Wang",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f4953314d",
                    "user": {
                        "_id": "67aeecf31f297f2bdabd91bf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mZBgh3oudPtx_rgSt8PJ7.png",
                        "isPro": false,
                        "fullname": "Jianhan Jin",
                        "user": "Pala718",
                        "type": "user"
                    },
                    "name": "Jianhan Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:49:46.156Z",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f4953314e",
                    "user": {
                        "_id": "64862c6e19b74d6d646baa88",
                        "avatarUrl": "/avatars/5ca91ff2b7dddb36378edf74067cd9e2.svg",
                        "isPro": false,
                        "fullname": "Claire Guo",
                        "user": "clairerg",
                        "type": "user"
                    },
                    "name": "Claire Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:49:39.014Z",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f4953314f",
                    "name": "Shen Yan",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533150",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533151",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533152",
                    "user": {
                        "_id": "6759af3eccbc8817f9169179",
                        "avatarUrl": "/avatars/49e64c7ccf71b8f25c52783b6ae93620.svg",
                        "isPro": false,
                        "fullname": "Peng Gao",
                        "user": "gaopenghigh",
                        "type": "user"
                    },
                    "name": "Peng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:49:22.182Z",
                    "hidden": false
                },
                {
                    "_id": "67aee0229e69670f49533153",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:49:00.794Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:59:46.000Z",
            "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency",
            "summary": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/",
            "upvotes": 12,
            "discussionId": "67aee0249e69670f495331d8"
        },
        "publishedAt": "2025-02-14T01:34:58.800Z",
        "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09621.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6349214f8146350b3a4c5cdf",
            "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
            "fullname": "Dongzhi Jiang",
            "name": "CaraJ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09100",
            "authors": [
                {
                    "_id": "67aeb0a3d58f4990b384d83e",
                    "name": "Hanmeng Liu",
                    "hidden": false
                },
                {
                    "_id": "67aeb0a3d58f4990b384d83f",
                    "user": {
                        "_id": "6467136a8334813a7ae1d1b0",
                        "avatarUrl": "/avatars/d0fd37532c830e8bef14995148190f9f.svg",
                        "isPro": false,
                        "fullname": "Zhizhang Fu",
                        "user": "HarryFu",
                        "type": "user"
                    },
                    "name": "Zhizhang Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T11:10:54.325Z",
                    "hidden": false
                },
                {
                    "_id": "67aeb0a3d58f4990b384d840",
                    "name": "Mengru Ding",
                    "hidden": false
                },
                {
                    "_id": "67aeb0a3d58f4990b384d841",
                    "user": {
                        "_id": "62e47d1b6a82e063860c587e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e47d1b6a82e063860c587e/jvFt1caSZNWDQTYKZQ9K-.jpeg",
                        "isPro": false,
                        "fullname": "ruoxining",
                        "user": "ruoxining",
                        "type": "user"
                    },
                    "name": "Ruoxi Ning",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-14T06:28:50.414Z",
                    "hidden": false
                },
                {
                    "_id": "67aeb0a3d58f4990b384d842",
                    "name": "Chaoli Zhang",
                    "hidden": false
                },
                {
                    "_id": "67aeb0a3d58f4990b384d843",
                    "name": "Xiaozhang Liu",
                    "hidden": false
                },
                {
                    "_id": "67aeb0a3d58f4990b384d844",
                    "name": "Yue Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T09:19:14.000Z",
            "title": "Logical Reasoning in Large Language Models: A Survey",
            "summary": "With the emergence of advanced reasoning models like OpenAI o3 and\nDeepSeek-R1, large language models (LLMs) have demonstrated remarkable\nreasoning capabilities. However, their ability to perform rigorous logical\nreasoning remains an open question. This survey synthesizes recent advancements\nin logical reasoning within LLMs, a critical area of AI research. It outlines\nthe scope of logical reasoning in LLMs, its theoretical foundations, and the\nbenchmarks used to evaluate reasoning proficiency. We analyze existing\ncapabilities across different reasoning paradigms - deductive, inductive,\nabductive, and analogical - and assess strategies to enhance reasoning\nperformance, including data-centric tuning, reinforcement learning, decoding\nstrategies, and neuro-symbolic approaches. The review concludes with future\ndirections, emphasizing the need for further exploration to strengthen logical\nreasoning in AI systems.",
            "upvotes": 12,
            "discussionId": "67aeb0a4d58f4990b384d871"
        },
        "publishedAt": "2025-02-13T21:55:58.708Z",
        "title": "Logical Reasoning in Large Language Models: A Survey",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09100.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6091
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09042",
            "authors": [
                {
                    "_id": "67aea8c94d4cb38be4a40c55",
                    "user": {
                        "_id": "615313b0793ef66b3324da1f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
                        "isPro": false,
                        "fullname": "Pittawat Taveekitworachai",
                        "user": "pittawat",
                        "type": "user"
                    },
                    "name": "Pittawat Taveekitworachai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:24.073Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8c94d4cb38be4a40c56",
                    "user": {
                        "_id": "63f6a050b4c9a104f4b95755",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f6a050b4c9a104f4b95755/eJQyJkenSz536j-EGcpkH.jpeg",
                        "isPro": false,
                        "fullname": "Potsawee Manakul",
                        "user": "potsawee",
                        "type": "user"
                    },
                    "name": "Potsawee Manakul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:48:21.806Z",
                    "hidden": false
                },
                {
                    "_id": "67aea8c94d4cb38be4a40c57",
                    "name": "Kasima Tharnpipitchai",
                    "hidden": false
                },
                {
                    "_id": "67aea8c94d4cb38be4a40c58",
                    "user": {
                        "_id": "62d192c2d50433c35eb1b48e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png",
                        "isPro": true,
                        "fullname": "Kunat Pipatanakul",
                        "user": "kunato",
                        "type": "user"
                    },
                    "name": "Kunat Pipatanakul",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T12:41:14.906Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T07:55:54.000Z",
            "title": "Typhoon T1: An Open Thai Reasoning Model",
            "summary": "This paper introduces Typhoon T1, an open effort to develop an open Thai\nreasoning model. A reasoning model is a relatively new type of generative model\nbuilt on top of large language models (LLMs). A reasoning model generates a\nlong chain of thought before arriving at a final answer, an approach found to\nimprove performance on complex tasks. However, details on developing such a\nmodel are limited, especially for reasoning models that can generate traces in\na low-resource language. Typhoon T1 presents an open effort that dives into the\ndetails of developing a reasoning model in a more cost-effective way by\nleveraging supervised fine-tuning using open datasets, instead of reinforcement\nlearning. This paper shares the details about synthetic data generation and\ntraining, as well as our dataset and model weights. Additionally, we provide\ninsights gained from developing a reasoning model that generalizes across\ndomains and is capable of generating reasoning traces in a low-resource\nlanguage, using Thai as an example. We hope this open effort provides a\nfoundation for further research in this field.",
            "upvotes": 11,
            "discussionId": "67aea8ca4d4cb38be4a40cab"
        },
        "publishedAt": "2025-02-14T01:29:44.233Z",
        "title": "Typhoon T1: An Open Thai Reasoning Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09042.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "615313b0793ef66b3324da1f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
            "fullname": "Pittawat Taveekitworachai",
            "name": "pittawat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09601",
            "authors": [
                {
                    "_id": "67aed173e6952709b47c0c5c",
                    "user": {
                        "_id": "64396ebc21221ac7411852b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
                        "isPro": false,
                        "fullname": "Xinyin Ma",
                        "user": "horseee",
                        "type": "user"
                    },
                    "name": "Xinyin Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:50:50.673Z",
                    "hidden": false
                },
                {
                    "_id": "67aed173e6952709b47c0c5d",
                    "user": {
                        "_id": "6627cccfded9b7936d5d1d21",
                        "avatarUrl": "/avatars/9216bdd9ed10c226f2d14edce4a10daa.svg",
                        "isPro": false,
                        "fullname": "Guangnian Wan",
                        "user": "bigglesworthnotcat",
                        "type": "user"
                    },
                    "name": "Guangnian Wan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:51:10.430Z",
                    "hidden": false
                },
                {
                    "_id": "67aed173e6952709b47c0c5e",
                    "user": {
                        "_id": "635364b3c41f548fe39db945",
                        "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
                        "isPro": false,
                        "fullname": "Runpeng Yu",
                        "user": "rp-yu",
                        "type": "user"
                    },
                    "name": "Runpeng Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:51:16.707Z",
                    "hidden": false
                },
                {
                    "_id": "67aed173e6952709b47c0c5f",
                    "user": {
                        "_id": "646a1939c37ca1e12308fe81",
                        "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
                        "isPro": false,
                        "fullname": "Gongfan Fang",
                        "user": "Vinnnf",
                        "type": "user"
                    },
                    "name": "Gongfan Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:51:22.345Z",
                    "hidden": false
                },
                {
                    "_id": "67aed173e6952709b47c0c60",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:52:36.000Z",
            "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
            "summary": "Chain-of-Thought significantly enhances a model's reasoning capability, but\nit also comes with a considerable increase in inference costs due to long\nchains. With the observation that the reasoning path can be easily compressed\nunder easy tasks but struggle on hard tasks, we explore the feasibility of\nelastically controlling the length of reasoning paths with only one model,\nthereby reducing the inference overhead of reasoning models dynamically based\non task difficulty. We introduce a new tuning and inference strategy named\nCoT-Valve, designed to allow models to generate reasoning chains of varying\nlengths. To achieve this, we propose to identify a direction in the parameter\nspace that, when manipulated, can effectively control the length of generated\nCoT. Moreover, we show that this property is valuable for compressing the\nreasoning chain. We construct datasets with chains from long to short for the\nsame questions and explore two enhanced strategies for CoT-Valve: (1) a precise\nlength-compressible CoT tuning method, and (2) a progressive chain length\ncompression approach. Our experiments show that CoT-Valve successfully enables\ncontrollability and compressibility of the chain and shows better performance\nthan the prompt-based control. We applied this method to QwQ-32B-Preview,\nreducing reasoning chains on GSM8K from 741 to 225 tokens with a minor\nperformance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with\nonly one additional incorrect answer.",
            "upvotes": 8,
            "discussionId": "67aed174e6952709b47c0ca1"
        },
        "publishedAt": "2025-02-14T00:16:30.034Z",
        "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09601.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64396ebc21221ac7411852b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
            "fullname": "Xinyin Ma",
            "name": "horseee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09390",
            "authors": [
                {
                    "_id": "67aef17da9f929ce0ca3e36b",
                    "user": {
                        "_id": "62d93cd728f9c86a4031562e",
                        "avatarUrl": "/avatars/4619930d15512ec9b80b01c62e986217.svg",
                        "isPro": false,
                        "fullname": "Daniel Fleischer",
                        "user": "danf",
                        "type": "user"
                    },
                    "name": "Daniel Fleischer",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-14T07:32:14.019Z",
                    "hidden": false
                },
                {
                    "_id": "67aef17da9f929ce0ca3e36c",
                    "user": {
                        "_id": "63e0c8875c6964861ebb0c49",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e0c8875c6964861ebb0c49/yzkhPSxgXtJCM62iMBOOK.jpeg",
                        "isPro": false,
                        "fullname": "Moshe Berchansky",
                        "user": "mber",
                        "type": "user"
                    },
                    "name": "Moshe Berchansky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:52:07.600Z",
                    "hidden": false
                },
                {
                    "_id": "67aef17da9f929ce0ca3e36d",
                    "user": {
                        "_id": "666154a2694ec45eaabc19c3",
                        "avatarUrl": "/avatars/b0022ad6dd992c75a51973070b302315.svg",
                        "isPro": false,
                        "fullname": "Gad Markovits",
                        "user": "gadmarkovits",
                        "type": "user"
                    },
                    "name": "Gad Markovits",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:52:13.549Z",
                    "hidden": false
                },
                {
                    "_id": "67aef17da9f929ce0ca3e36e",
                    "user": {
                        "_id": "6013c64031d84a3bb7650038",
                        "avatarUrl": "/avatars/e05ca715004b39e79472399f75010bda.svg",
                        "isPro": false,
                        "fullname": "Moshe Wasserblat",
                        "user": "moshew",
                        "type": "user"
                    },
                    "name": "Moshe Wasserblat",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:52:19.452Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T15:07:20.000Z",
            "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models",
            "summary": "In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.",
            "upvotes": 7,
            "discussionId": "67aef17ea9f929ce0ca3e3bf"
        },
        "publishedAt": "2025-02-14T02:35:53.718Z",
        "title": "SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09390.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d93cd728f9c86a4031562e",
            "avatarUrl": "/avatars/4619930d15512ec9b80b01c62e986217.svg",
            "fullname": "Daniel Fleischer",
            "name": "danf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.08468",
            "authors": [
                {
                    "_id": "67ad5f3fcad644864b4366ca",
                    "user": {
                        "_id": "66add675c7a575aa0e03d5f3",
                        "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
                        "isPro": false,
                        "fullname": "Haonan Chen",
                        "user": "Haon-Chen",
                        "type": "user"
                    },
                    "name": "Haonan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-13T08:21:55.329Z",
                    "hidden": false
                },
                {
                    "_id": "67ad5f3fcad644864b4366cb",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "67ad5f3fcad644864b4366cc",
                    "name": "Nan Yang",
                    "hidden": false
                },
                {
                    "_id": "67ad5f3fcad644864b4366cd",
                    "user": {
                        "_id": "625e62452a7279d3c77b5c38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg",
                        "isPro": false,
                        "fullname": "Yutao Zhu",
                        "user": "yutaozhu94",
                        "type": "user"
                    },
                    "name": "Yutao Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:53:09.223Z",
                    "hidden": false
                },
                {
                    "_id": "67ad5f3fcad644864b4366ce",
                    "user": {
                        "_id": "6639d5c106b25a7ea6f18391",
                        "avatarUrl": "/avatars/788e339472999a9159f77f857817d618.svg",
                        "isPro": false,
                        "fullname": "Ziliang Zhao",
                        "user": "ZillionZhao",
                        "type": "user"
                    },
                    "name": "Ziliang Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:53:01.987Z",
                    "hidden": false
                },
                {
                    "_id": "67ad5f3fcad644864b4366cf",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:52:40.042Z",
                    "hidden": false
                },
                {
                    "_id": "67ad5f3fcad644864b4366d0",
                    "user": {
                        "_id": "66f0bf59e9d50ec57febf751",
                        "avatarUrl": "/avatars/be97941e60064e5dd806c6fe9db3c537.svg",
                        "isPro": false,
                        "fullname": "Zhicheng Dou",
                        "user": "douzc",
                        "type": "user"
                    },
                    "name": "Zhicheng Dou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:52:33.880Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T15:03:33.000Z",
            "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data",
            "summary": "Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.",
            "upvotes": 6,
            "discussionId": "67ad5f3fcad644864b4366f5"
        },
        "publishedAt": "2025-02-13T23:32:15.420Z",
        "title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08468.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66add675c7a575aa0e03d5f3",
            "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
            "fullname": "Haonan Chen",
            "name": "Haon-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05979",
            "authors": [
                {
                    "_id": "67ab5cc2b8d7fe3b96361e31",
                    "name": "Xinyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67ab5cc2b8d7fe3b96361e32",
                    "user": {
                        "_id": "665030b8c08859923b274a55",
                        "avatarUrl": "/avatars/06a3f3cd533caa459f3da1bb5e0f4c1f.svg",
                        "isPro": false,
                        "fullname": "aznukad",
                        "user": "alkxncda",
                        "type": "user"
                    },
                    "name": "Ailing Zeng",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-11T14:21:00.414Z",
                    "hidden": false
                },
                {
                    "_id": "67ab5cc2b8d7fe3b96361e33",
                    "user": {
                        "_id": "6628adb14277eae0da5eee28",
                        "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
                        "isPro": true,
                        "fullname": "HKUST Audio",
                        "user": "HKUST-Audio",
                        "type": "user"
                    },
                    "name": "Wei Xue",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-11T14:21:00.414Z",
                    "hidden": false
                },
                {
                    "_id": "67ab5cc2b8d7fe3b96361e34",
                    "name": "Harry Yang",
                    "hidden": false
                },
                {
                    "_id": "67ab5cc2b8d7fe3b96361e35",
                    "name": "Wenhan Luo",
                    "hidden": false
                },
                {
                    "_id": "67ab5cc2b8d7fe3b96361e36",
                    "name": "Qifeng Liu",
                    "hidden": false
                },
                {
                    "_id": "67ab5cc2b8d7fe3b96361e37",
                    "name": "Yike Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T18:12:25.000Z",
            "title": "VFX Creator: Animated Visual Effect Generation with Controllable\n  Diffusion Transformer",
            "summary": "Crafting magic and illusions is one of the most thrilling aspects of\nfilmmaking, with visual effects (VFX) serving as the powerhouse behind\nunforgettable cinematic experiences. While recent advances in generative\nartificial intelligence have driven progress in generic image and video\nsynthesis, the domain of controllable VFX generation remains relatively\nunderexplored. In this work, we propose a novel paradigm for animated VFX\ngeneration as image animation, where dynamic effects are generated from\nuser-friendly textual descriptions and static reference images.\n  Our work makes two primary contributions: (i) Open-VFX, the first\nhigh-quality VFX video dataset spanning 15 diverse effect categories, annotated\nwith textual descriptions, instance segmentation masks for spatial\nconditioning, and start-end timestamps for temporal control. (ii) VFX Creator,\na simple yet effective controllable VFX generation framework based on a Video\nDiffusion Transformer. The model incorporates a spatial and temporal\ncontrollable LoRA adapter, requiring minimal training videos. Specifically, a\nplug-and-play mask control module enables instance-level spatial manipulation,\nwhile tokenized start-end motion timestamps embedded in the diffusion process,\nalongside the text encoder, allow precise temporal control over effect timing\nand pace.\n  Extensive experiments on the Open-VFX test set demonstrate the superiority of\nthe proposed system in generating realistic and dynamic effects, achieving\nstate-of-the-art performance and generalization ability in both spatial and\ntemporal controllability. Furthermore, we introduce a specialized metric to\nevaluate the precision of temporal control. By bridging traditional VFX\ntechniques with generative approaches, VFX Creator unlocks new possibilities\nfor efficient and high-quality video effect generation, making advanced VFX\naccessible to a broader audience.",
            "upvotes": 3,
            "discussionId": "67ab5cccb8d7fe3b96361ff7"
        },
        "publishedAt": "2025-02-14T08:47:33.396Z",
        "title": "VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05979.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 29
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.08680",
            "authors": [
                {
                    "_id": "67af2b2d1f297f2bdacded89",
                    "user": {
                        "_id": "64cb922ec7f30fbf7b91a9a7",
                        "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
                        "isPro": false,
                        "fullname": "Safal Shrestha",
                        "user": "safal312",
                        "type": "user"
                    },
                    "name": "Safal Shrestha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T12:04:44.693Z",
                    "hidden": false
                },
                {
                    "_id": "67af2b2d1f297f2bdacded8a",
                    "user": {
                        "_id": "64e77b47d96966317b45eeb3",
                        "avatarUrl": "/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg",
                        "isPro": false,
                        "fullname": "Minwu Kim",
                        "user": "guactastesgood",
                        "type": "user"
                    },
                    "name": "Minwu Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T12:00:37.837Z",
                    "hidden": false
                },
                {
                    "_id": "67af2b2d1f297f2bdacded8b",
                    "name": "Keith Ross",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T09:53:10.000Z",
            "title": "Mathematical Reasoning in Large Language Models: Assessing Logical and\n  Arithmetic Errors across Wide Numerical Ranges",
            "summary": "Mathematical reasoning in Large Language Models (LLMs) is often evaluated\nusing benchmarks with limited numerical ranges, failing to reflect real-world\nproblem-solving across diverse scales. Furthermore, most existing evaluation\nmethods only compare model outputs to ground-truth answers, obscuring insights\ninto reasoning processes. To address these limitations, we introduce\nGSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs\nnumerical values in math problems to assess model robustness across varying\nnumerical scales. Additionally, we propose a novel grading methodology that\ndistinguishes between logical and non-logical errors, offering a more precise\nevaluation of reasoning processes beyond computational accuracy. Our\nexperiments with various models reveal a significant increase in logical error\nrates-up to 14 percentage points-as numerical complexity rises, demonstrating a\ngeneral weakness in reasoning with out-of-distribution numerical values.\nMoreover, while models demonstrate high accuracy on standalone arithmetic\ntasks, their performance deteriorates substantially when computations are\nembedded within word problems. These findings provide a comprehensive\nevaluation of LLMs' mathematical reasoning capabilities and inform future\nresearch directions for improving numerical generalization in language models.",
            "upvotes": 2,
            "discussionId": "67af2b2e1f297f2bdacdedd3"
        },
        "publishedAt": "2025-02-14T09:18:18.443Z",
        "title": "Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e77b47d96966317b45eeb3",
            "avatarUrl": "/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg",
            "fullname": "Minwu Kim",
            "name": "guactastesgood",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09614",
            "authors": [
                {
                    "_id": "67af107d6bd28b8bd4e13c38",
                    "user": {
                        "_id": "65b8070ad49f4330ab0ca5f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png",
                        "isPro": false,
                        "fullname": "Xueyi Liu",
                        "user": "xymeow7",
                        "type": "user"
                    },
                    "name": "Xueyi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-14T12:55:06.626Z",
                    "hidden": false
                },
                {
                    "_id": "67af107d6bd28b8bd4e13c39",
                    "name": "Jianibieke Adalibieke",
                    "hidden": false
                },
                {
                    "_id": "67af107d6bd28b8bd4e13c3a",
                    "name": "Qianwei Han",
                    "hidden": false
                },
                {
                    "_id": "67af107d6bd28b8bd4e13c3b",
                    "name": "Yuzhe Qin",
                    "hidden": false
                },
                {
                    "_id": "67af107d6bd28b8bd4e13c3c",
                    "name": "Li Yi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:59:13.000Z",
            "title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References",
            "summary": "We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.",
            "upvotes": 1,
            "discussionId": "67af10806bd28b8bd4e13ce5"
        },
        "publishedAt": "2025-02-14T04:50:27.474Z",
        "title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65b8070ad49f4330ab0ca5f7/Ir-_GtsnqYII8yhrpJRD5.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09614.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b8070ad49f4330ab0ca5f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png",
            "fullname": "Xueyi Liu",
            "name": "xymeow7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.05761",
            "authors": [
                {
                    "_id": "67aee1cd7af05a21a72e793d",
                    "user": {
                        "_id": "648bf9afded4c3eb970eca85",
                        "avatarUrl": "/avatars/a4b7b7fd6c1fca0eac85da7383f58361.svg",
                        "isPro": false,
                        "fullname": "enquan yang",
                        "user": "enquan2022",
                        "type": "user"
                    },
                    "name": "Enquan Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-14T08:01:03.483Z",
                    "hidden": false
                },
                {
                    "_id": "67aee1cd7af05a21a72e793e",
                    "name": "Peng Xing",
                    "hidden": false
                },
                {
                    "_id": "67aee1cd7af05a21a72e793f",
                    "name": "Hanyang Sun",
                    "hidden": false
                },
                {
                    "_id": "67aee1cd7af05a21a72e7940",
                    "name": "Wenbo Guo",
                    "hidden": false
                },
                {
                    "_id": "67aee1cd7af05a21a72e7941",
                    "name": "Yuanwei Ma",
                    "hidden": false
                },
                {
                    "_id": "67aee1cd7af05a21a72e7942",
                    "name": "Zechao Li",
                    "hidden": false
                },
                {
                    "_id": "67aee1cd7af05a21a72e7943",
                    "name": "Dan Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T03:37:54.000Z",
            "title": "3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised\n  Anomaly",
            "summary": "Industrial anomaly detection achieves progress thanks to datasets such as\nMVTec-AD and VisA. However, they suf- fer from limitations in terms of the\nnumber of defect sam- ples, types of defects, and availability of real-world\nscenes. These constraints inhibit researchers from further exploring the\nperformance of industrial detection with higher accuracy. To this end, we\npropose a new large-scale anomaly detection dataset called 3CAD, which is\nderived from real 3C produc- tion lines. Specifically, the proposed 3CAD\nincludes eight different types of manufactured parts, totaling 27,039 high-\nresolution images labeled with pixel-level anomalies. The key features of 3CAD\nare that it covers anomalous regions of different sizes, multiple anomaly\ntypes, and the possibility of multiple anomalous regions and multiple anomaly\ntypes per anomaly image. This is the largest and first anomaly de- tection\ndataset dedicated to 3C product quality control for community exploration and\ndevelopment. Meanwhile, we in- troduce a simple yet effective framework for\nunsupervised anomaly detection: a Coarse-to-Fine detection paradigm with\nRecovery Guidance (CFRG). To detect small defect anoma- lies, the proposed CFRG\nutilizes a coarse-to-fine detection paradigm. Specifically, we utilize a\nheterogeneous distilla- tion model for coarse localization and then fine\nlocaliza- tion through a segmentation model. In addition, to better capture\nnormal patterns, we introduce recovery features as guidance. Finally, we report\nthe results of our CFRG frame- work and popular anomaly detection methods on\nthe 3CAD dataset, demonstrating strong competitiveness and providing a highly\nchallenging benchmark to promote the development of the anomaly detection\nfield. Data and code are available: https://github.com/EnquanYang2022/3CAD.",
            "upvotes": 1,
            "discussionId": "67aee1cf7af05a21a72e799b"
        },
        "publishedAt": "2025-02-14T04:00:29.585Z",
        "title": "3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648bf9afded4c3eb970eca85/n-ufwo6Smo9TdMiTqKG8_.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05761.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648bf9afded4c3eb970eca85",
            "avatarUrl": "/avatars/a4b7b7fd6c1fca0eac85da7383f58361.svg",
            "fullname": "enquan yang",
            "name": "enquan2022",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]