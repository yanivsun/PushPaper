[
    {
        "paper": {
            "id": "2502.10389",
            "authors": [
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff1",
                    "name": "Ziming Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff2",
                    "name": "Yifan Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff3",
                    "name": "Chengruidong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff4",
                    "name": "Yiqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff5",
                    "name": "Lili Qiu",
                    "hidden": false
                },
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff6",
                    "name": "Yang You",
                    "hidden": false
                },
                {
                    "_id": "67b2a89ebe31bfaa7cd2bff7",
                    "name": "Yuqing Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T18:59:36.000Z",
            "title": "Region-Adaptive Sampling for Diffusion Transformers",
            "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
            "upvotes": 45,
            "discussionId": "67b2a8a4be31bfaa7cd2c1ad"
        },
        "publishedAt": "2025-02-16T22:22:08.102Z",
        "title": "Region-Adaptive Sampling for Diffusion Transformers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10389.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62d18eb81e36881a57f29bf4",
            "avatarUrl": "/avatars/104851421b4ee9641daaf15942fa7ea1.svg",
            "fullname": "Yif Yang",
            "name": "Yif29",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09992",
            "authors": [
                {
                    "_id": "67b2c31125f77e5fc242f4f8",
                    "name": "Shen Nie",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4f9",
                    "name": "Fengqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4fa",
                    "name": "Zebin You",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4fb",
                    "name": "Xiaolu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4fc",
                    "name": "Jingyang Ou",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4fd",
                    "name": "Jun Hu",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4fe",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f4ff",
                    "name": "Yankai Lin",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f500",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "67b2c31125f77e5fc242f501",
                    "name": "Chongxuan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T08:23:51.000Z",
            "title": "Large Language Diffusion Models",
            "summary": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs.",
            "upvotes": 40,
            "discussionId": "67b2c31225f77e5fc242f527"
        },
        "publishedAt": "2025-02-17T00:03:18.228Z",
        "title": "Large Language Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09992.png",
        "numComments": 8,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6120
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10248",
            "authors": [
                {
                    "_id": "67b2a72e7a49eaea082b9dcf",
                    "name": "Guoqing Ma",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd0",
                    "name": "Haoyang Huang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd1",
                    "name": "Kun Yan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd2",
                    "name": "Liangyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd3",
                    "name": "Nan Duan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd4",
                    "name": "Shengming Yin",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd5",
                    "name": "Changyi Wan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd6",
                    "name": "Ranchen Ming",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd7",
                    "name": "Xiaoniu Song",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd8",
                    "name": "Xing Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dd9",
                    "name": "Yu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dda",
                    "name": "Deshan Sun",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9ddb",
                    "name": "Deyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9ddc",
                    "name": "Jian Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9ddd",
                    "name": "Kaijun Tan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dde",
                    "name": "Kang An",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9ddf",
                    "name": "Mei Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de0",
                    "name": "Wei Ji",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de1",
                    "name": "Qiling Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de2",
                    "name": "Wen Sun",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de3",
                    "name": "Xin Han",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de4",
                    "name": "Yanan Wei",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de5",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de6",
                    "name": "Aojie Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de7",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de8",
                    "name": "Bizhu Huang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9de9",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dea",
                    "name": "Brian Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9deb",
                    "name": "Changxing Miao",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dec",
                    "name": "Chen Xu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9ded",
                    "name": "Chenfei Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dee",
                    "name": "Chenguang Yu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9def",
                    "name": "Dapeng Shi",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df0",
                    "name": "Dingyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df1",
                    "name": "Enle Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df2",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df3",
                    "name": "Ge Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df4",
                    "name": "Guanzhe Huang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df5",
                    "name": "Gulin Yan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df6",
                    "name": "Haiyang Feng",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df7",
                    "name": "Hao Nie",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df8",
                    "name": "Haonan Jia",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9df9",
                    "name": "Hanpeng Hu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dfa",
                    "name": "Hanqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dfb",
                    "name": "Haolong Yan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dfc",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dfd",
                    "name": "Hongcheng Guo",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dfe",
                    "name": "Huilin Xiong",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9dff",
                    "name": "Huixin Xiong",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e00",
                    "name": "Jiahao Gong",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e01",
                    "name": "Jianchang Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e02",
                    "name": "Jiaoren Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e03",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e04",
                    "name": "Jie Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e05",
                    "name": "Jiashuai Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e06",
                    "name": "Jiashuo Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e07",
                    "name": "Jingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e08",
                    "name": "Junjing Guo",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e09",
                    "name": "Junzhe Lin",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e0a",
                    "name": "Kaixiang Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e0b",
                    "name": "Lei Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e0c",
                    "name": "Lei Xia",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e0d",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e0e",
                    "name": "Liguo Tan",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e0f",
                    "name": "Liwen Huang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e10",
                    "name": "Liying Shi",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e11",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e12",
                    "name": "Mingliang Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e13",
                    "name": "Muhua Cheng",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e14",
                    "name": "Na Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e15",
                    "name": "Qiaohui Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e16",
                    "name": "Qinglin He",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e17",
                    "name": "Qiuyan Liang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e18",
                    "name": "Quan Sun",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e19",
                    "name": "Ran Sun",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e1a",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e1b",
                    "name": "Shaoliang Pang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e1c",
                    "name": "Shiliang Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e1d",
                    "name": "Sitong Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e1e",
                    "name": "Siqi Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e1f",
                    "name": "Shuli Gao",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e20",
                    "name": "Tiancheng Cao",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e21",
                    "name": "Tianyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e22",
                    "name": "Weipeng Ming",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e23",
                    "name": "Wenqing He",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e24",
                    "name": "Xu Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e25",
                    "name": "Xuelin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e26",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e27",
                    "name": "Xiaojia Liu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e28",
                    "name": "Xuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e29",
                    "name": "Yaqi Dai",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e2a",
                    "name": "Yanbo Yu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e2b",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e2c",
                    "name": "Yineng Deng",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e2d",
                    "name": "Yingming Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e2e",
                    "name": "Yilei Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e2f",
                    "name": "Yuanwei Lu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e30",
                    "name": "Yu Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e31",
                    "name": "Yu Luo",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e32",
                    "name": "Yuchu Luo",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e33",
                    "name": "Yuhe Yin",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e34",
                    "name": "Yuheng Feng",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e35",
                    "name": "Yuxiang Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e36",
                    "name": "Zecheng Tang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e37",
                    "name": "Zekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e38",
                    "name": "Zidong Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e39",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e3a",
                    "name": "Jiansheng Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e3b",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e3c",
                    "name": "Shuchang Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e3d",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e3e",
                    "name": "Xinhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e3f",
                    "name": "Yibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e40",
                    "name": "Heung-Yeung Shum",
                    "hidden": false
                },
                {
                    "_id": "67b2a72e7a49eaea082b9e41",
                    "name": "Daxin Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T15:58:10.000Z",
            "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model",
            "summary": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.",
            "upvotes": 36,
            "discussionId": "67b2a7357a49eaea082b9fbf"
        },
        "publishedAt": "2025-02-16T22:50:38.622Z",
        "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10248.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60efceb38432bc401cd0abc8",
            "avatarUrl": "/avatars/c3331d9a46da4afcb90a25691d47aed4.svg",
            "fullname": "tongwang",
            "name": "turrf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.08235",
            "authors": [
                {
                    "_id": "67b078cb1c879c0cbb785d5f",
                    "name": "Alejandro Cuadron",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d60",
                    "name": "Dacheng Li",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d61",
                    "name": "Wenjie Ma",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d62",
                    "name": "Xingyao Wang",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d63",
                    "name": "Yichuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d64",
                    "name": "Siyuan Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d65",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d66",
                    "name": "Luis Gaspar Schroeder",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d67",
                    "name": "Tian Xia",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d68",
                    "name": "Huanzhi Mao",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d69",
                    "name": "Nicholas Thumiger",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d6a",
                    "name": "Aditya Desai",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d6b",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d6c",
                    "name": "Ana Klimovic",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d6d",
                    "name": "Graham Neubig",
                    "hidden": false
                },
                {
                    "_id": "67b078cb1c879c0cbb785d6e",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T09:23:26.000Z",
            "title": "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in\n  Agentic Tasks",
            "summary": "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving\ncapabilities, but their effectiveness in interactive environments can be\nlimited. This paper introduces and analyzes overthinking in LRMs. A phenomenon\nwhere models favor extended internal reasoning chains over environmental\ninteraction. Through experiments on software engineering tasks using SWE Bench\nVerified, we observe three recurring patterns: Analysis Paralysis, Rogue\nActions, and Premature Disengagement. We propose a framework to study these\nbehaviors, which correlates with human expert assessments, and analyze 4018\ntrajectories. We observe that higher overthinking scores correlate with\ndecreased performance, with reasoning models exhibiting stronger tendencies\ntoward overthinking compared to non-reasoning models. Our analysis reveals that\nsimple efforts to mitigate overthinking in agentic environments, such as\nselecting the solution with the lower overthinking score, can improve model\nperformance by almost 30% while reducing computational costs by 43%. These\nresults suggest that mitigating overthinking has strong practical implications.\nWe suggest that by leveraging native function-calling capabilities and\nselective reinforcement learning overthinking tendencies could be mitigated. We\nalso open-source our evaluation framework and dataset to facilitate research in\nthis direction at https://github.com/AlexCuadron/Overthinking.",
            "upvotes": 29,
            "discussionId": "67b078cc1c879c0cbb785dbb"
        },
        "publishedAt": "2025-02-17T17:09:38.653Z",
        "title": "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08235.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652a656d1a3250bbfe3bb92d",
            "avatarUrl": "/avatars/a1c25150d55c493edd9a7f81287fc449.svg",
            "fullname": "Alejandro Cuadron Lafuente",
            "name": "AlexCuadron",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09696",
            "authors": [
                {
                    "_id": "67b2aae22a4cd186392a18b2",
                    "name": "Jonathan Roberts",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b3",
                    "name": "Mohammad Reza Taesiri",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b4",
                    "name": "Ansh Sharma",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b5",
                    "name": "Akash Gupta",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b6",
                    "name": "Samuel Roberts",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b7",
                    "name": "Ioana Croitoru",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b8",
                    "name": "Simion-Vlad Bogolin",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18b9",
                    "name": "Jialu Tang",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18ba",
                    "name": "Florian Langer",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18bb",
                    "name": "Vyas Raina",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18bc",
                    "name": "Vatsal Raina",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18bd",
                    "name": "Hanyi Xiong",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18be",
                    "name": "Vishaal Udandarao",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18bf",
                    "name": "Jingyi Lu",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c0",
                    "name": "Shiyang Chen",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c1",
                    "name": "Sam Purkis",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c2",
                    "name": "Tianshuo Yan",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c3",
                    "name": "Wenye Lin",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c4",
                    "name": "Gyungin Shin",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c5",
                    "name": "Qiaochu Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c6",
                    "name": "Anh Totti Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c7",
                    "name": "Kai Han",
                    "hidden": false
                },
                {
                    "_id": "67b2aae22a4cd186392a18c8",
                    "name": "Samuel Albanie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T18:59:11.000Z",
            "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large\n  Multimodal Models",
            "summary": "Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting\nimages and, by some measures, have poorer spatial cognition than small children\nor animals. Despite this, they attain high scores on many popular visual\nbenchmarks, with headroom rapidly eroded by an ongoing surge of model progress.\nTo address this, there is a pressing need for difficult benchmarks that remain\nrelevant for longer. We take this idea to its limit by introducing ZeroBench-a\nlightweight visual reasoning benchmark that is entirely impossible for\ncontemporary frontier LMMs. Our benchmark consists of 100 manually curated\nquestions and 334 less difficult subquestions. We evaluate 20 LMMs on\nZeroBench, all of which score 0.0%, and rigorously analyse the errors. To\nencourage progress in visual understanding, we publicly release ZeroBench.",
            "upvotes": 24,
            "discussionId": "67b2aae42a4cd186392a195b"
        },
        "publishedAt": "2025-02-16T22:20:53.227Z",
        "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/QJdJ_pJPI20MjNz_q8PTw.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09696.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 60
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10391",
            "authors": [
                {
                    "_id": "67b2ab548191c180b9c4eb83",
                    "name": "Yi-Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb84",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb85",
                    "name": "Haochen Tian",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb86",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb87",
                    "name": "Peiyan Li",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb88",
                    "name": "Jianshu Zeng",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb89",
                    "name": "Wulin Xie",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb8a",
                    "name": "Yang Shi",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb8b",
                    "name": "Huanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb8c",
                    "name": "Junkang Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb8d",
                    "name": "Xue Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb8e",
                    "name": "Yibo Hu",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb8f",
                    "name": "Bin Wen",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb90",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb91",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb92",
                    "name": "Tingting Gao",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb93",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb94",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb95",
                    "name": "Rong Jin",
                    "hidden": false
                },
                {
                    "_id": "67b2ab548191c180b9c4eb96",
                    "name": "Tieniu Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T18:59:51.000Z",
            "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
            "summary": "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing 120k fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n10 distinct dimensions and 27 benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a 19.5% increase in conversational abilities and a\n60% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.",
            "upvotes": 21,
            "discussionId": "67b2ab598191c180b9c4ec10"
        },
        "publishedAt": "2025-02-16T22:51:55.408Z",
        "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/YtpeHGys5Zs3bqPlOGs94.png",
            "https://cdn-uploads.huggingface.co/production/uploads/623d8ca4c29adf5ef6175615/8mE0hOEgm-if-9zaLyMGn.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10391.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09935",
            "authors": [
                {
                    "_id": "67b2e6939edebc815a35eec8",
                    "name": "Łukasz Staniszewski",
                    "hidden": false
                },
                {
                    "_id": "67b2e6939edebc815a35eec9",
                    "name": "Bartosz Cywiński",
                    "hidden": false
                },
                {
                    "_id": "67b2e6939edebc815a35eeca",
                    "name": "Franziska Boenisch",
                    "hidden": false
                },
                {
                    "_id": "67b2e6939edebc815a35eecb",
                    "name": "Kamil Deja",
                    "hidden": false
                },
                {
                    "_id": "67b2e6939edebc815a35eecc",
                    "name": "Adam Dziedzic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T06:11:23.000Z",
            "title": "Precise Parameter Localization for Textual Generation in Diffusion\n  Models",
            "summary": "Novel diffusion models can synthesize photo-realistic images with integrated\nhigh-quality text. Surprisingly, we demonstrate through attention activation\npatching that only less than 1% of diffusion models' parameters, all contained\nin attention layers, influence the generation of textual content within the\nimages. Building on this observation, we improve textual generation efficiency\nand performance by targeting cross and joint attention layers of diffusion\nmodels. We introduce several applications that benefit from localizing the\nlayers responsible for textual content generation. We first show that a\nLoRA-based fine-tuning solely of the localized layers enhances, even more, the\ngeneral text-generation capabilities of large diffusion models while preserving\nthe quality and diversity of the diffusion models' generations. Then, we\ndemonstrate how we can use the localized layers to edit textual content in\ngenerated images. Finally, we extend this idea to the practical use case of\npreventing the generation of toxic text in a cost-free manner. In contrast to\nprior work, our localization approach is broadly applicable across various\ndiffusion model architectures, including U-Net (e.g., LDM and SDXL) and\ntransformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing\ndiverse text encoders (e.g., from CLIP to the large language models like T5).\nProject page available at https://t2i-text-loc.github.io/.",
            "upvotes": 10,
            "discussionId": "67b2e6979edebc815a35efbc"
        },
        "publishedAt": "2025-02-17T03:06:17.932Z",
        "title": "Precise Parameter Localization for Textual Generation in Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09935.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c7c19721bd95f80ed8ed80",
            "avatarUrl": "/avatars/0b1c1ace991e0290118d4f99f619d809.svg",
            "fullname": "Lukasz Staniszewski",
            "name": "lukasz-staniszewski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09955",
            "authors": [
                {
                    "_id": "67b2c1ac0303a07acd3f9443",
                    "name": "Iddo Drori",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9444",
                    "name": "Gaston Longhitano",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9445",
                    "name": "Mao Mao",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9446",
                    "name": "Seunghwan Hyun",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9447",
                    "name": "Yuke Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9448",
                    "name": "Sungjun Park",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9449",
                    "name": "Zachary Meeks",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f944a",
                    "name": "Xin-Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f944b",
                    "name": "Ben Segev",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f944c",
                    "name": "Howard Yong",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f944d",
                    "name": "Nakul Verma",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f944e",
                    "name": "Avi Shporer",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f944f",
                    "name": "Alon Amit",
                    "hidden": false
                },
                {
                    "_id": "67b2c1ac0303a07acd3f9450",
                    "name": "Madeleine Udell",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T07:22:25.000Z",
            "title": "Diverse Inference and Verification for Advanced Reasoning",
            "summary": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant\nprogress in mathematics and coding, yet find challenging advanced tasks such as\nInternational Mathematical Olympiad (IMO) combinatorics problems, Abstraction\nand Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.\nWe use a diverse inference approach that combines multiple models and methods\nat test time. We find that verifying mathematics and code problems, and\nrejection sampling on other problems is simple and effective. We automatically\nverify correctness of solutions to IMO problems by Lean, and ARC puzzles by\ncode, and find that best-of-N effectively answers HLE questions. Our approach\nincreases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,\naccuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that\n948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.\nTest-time simulations, reinforcement learning, and meta-learning with inference\nfeedback improve generalization by adapting agent graph representations and\nvarying prompts, code, and datasets. Our approach is reliable, robust, and\nscalable, and in the spirit of reproducible research, we will make it publicly\navailable upon publication.",
            "upvotes": 10,
            "discussionId": "67b2c1b10303a07acd3f9532"
        },
        "publishedAt": "2025-02-16T23:57:43.710Z",
        "title": "Diverse Inference and Verification for Advanced Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09955.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6120
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07780",
            "authors": [
                {
                    "_id": "67b33f632f3994b7d95b6e77",
                    "name": "Shengkun Tang",
                    "hidden": false
                },
                {
                    "_id": "67b33f632f3994b7d95b6e78",
                    "name": "Oliver Sieberling",
                    "hidden": false
                },
                {
                    "_id": "67b33f632f3994b7d95b6e79",
                    "name": "Eldar Kurtic",
                    "hidden": false
                },
                {
                    "_id": "67b33f632f3994b7d95b6e7a",
                    "name": "Zhiqiang Shen",
                    "hidden": false
                },
                {
                    "_id": "67b33f632f3994b7d95b6e7b",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T18:59:35.000Z",
            "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
            "summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for non-uniform model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\ntraining-aware structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n5times less training data during post-compression training.",
            "upvotes": 8,
            "discussionId": "67b33f642f3994b7d95b6eb1"
        },
        "publishedAt": "2025-02-17T08:54:04.307Z",
        "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07780.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63e76e2bfdb4097ef65e0745",
            "avatarUrl": "/avatars/6d4d94ab6f44e23437488fd9fed2a383.svg",
            "fullname": "Tang",
            "name": "Shengkun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09411",
            "authors": [
                {
                    "_id": "67b33c3f8904ba09caa986fb",
                    "name": "Rotem Shalev-Arkushin",
                    "hidden": false
                },
                {
                    "_id": "67b33c3f8904ba09caa986fc",
                    "name": "Rinon Gal",
                    "hidden": false
                },
                {
                    "_id": "67b33c3f8904ba09caa986fd",
                    "name": "Amit H. Bermano",
                    "hidden": false
                },
                {
                    "_id": "67b33c3f8904ba09caa986fe",
                    "name": "Ohad Fried",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T15:36:12.000Z",
            "title": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation",
            "summary": "Diffusion models enable high-quality and diverse visual content synthesis.\nHowever, they struggle to generate rare or unseen concepts. To address this\nchallenge, we explore the usage of Retrieval-Augmented Generation (RAG) with\nimage generation models. We propose ImageRAG, a method that dynamically\nretrieves relevant images based on a given text prompt, and uses them as\ncontext to guide the generation process. Prior approaches that used retrieved\nimages to improve generation, trained models specifically for retrieval-based\ngeneration. In contrast, ImageRAG leverages the capabilities of existing image\nconditioning models, and does not require RAG-specific training. Our approach\nis highly adaptable and can be applied across different model types, showing\nsignificant improvement in generating rare and fine-grained concepts using\ndifferent base models.\n  Our project page is available at: https://rotem-shalev.github.io/ImageRAG",
            "upvotes": 8,
            "discussionId": "67b33c478904ba09caa988dd"
        },
        "publishedAt": "2025-02-17T08:41:41.933Z",
        "title": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "627c1360f19c5eb46d55ba05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652712871747-627c1360f19c5eb46d55ba05.jpeg",
            "fullname": "Rinon Gal",
            "name": "rinong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10235",
            "authors": [
                {
                    "_id": "67b30f528904ba09ca9d9ab4",
                    "user": {
                        "_id": "621d59ebd3df05d67132e8d9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d59ebd3df05d67132e8d9/0gPfPTRKKnz5kq0InTqm5.jpeg",
                        "isPro": false,
                        "fullname": "Abdelhakim Benechehab",
                        "user": "abenechehab",
                        "type": "user"
                    },
                    "name": "Abdelhakim Benechehab",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-17T10:50:21.800Z",
                    "hidden": false
                },
                {
                    "_id": "67b30f528904ba09ca9d9ab5",
                    "name": "Vasilii Feofanov",
                    "hidden": false
                },
                {
                    "_id": "67b30f528904ba09ca9d9ab6",
                    "name": "Giuseppe Paolo",
                    "hidden": false
                },
                {
                    "_id": "67b30f528904ba09ca9d9ab7",
                    "name": "Albert Thomas",
                    "hidden": false
                },
                {
                    "_id": "67b30f528904ba09ca9d9ab8",
                    "name": "Maurizio Filippone",
                    "hidden": false
                },
                {
                    "_id": "67b30f528904ba09ca9d9ab9",
                    "name": "Balázs Kégl",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T15:46:19.000Z",
            "title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting",
            "summary": "Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https://github.com/abenechehab/AdaPTS.",
            "upvotes": 7,
            "discussionId": "67b30f548904ba09ca9d9b1e"
        },
        "publishedAt": "2025-02-17T05:36:23.051Z",
        "title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/J5xioTqwZTvZTFQ1cnhbr.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/i8pNDJflC9XZ1IqO_Y3iX.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/_J0c16BuklV33a43j_Pxk.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/c_kr2HOzAyrzC9SsQ4kgk.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/dlZ9r1oZh3ogB6-p1sewz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/621d59ebd3df05d67132e8d9/yEX6Nt-xHKP3pbCQ9LmE8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10235.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "621d59ebd3df05d67132e8d9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d59ebd3df05d67132e8d9/0gPfPTRKKnz5kq0InTqm5.jpeg",
            "fullname": "Abdelhakim Benechehab",
            "name": "abenechehab",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.07586",
            "authors": [
                {
                    "_id": "67b30146b02f929c82ce075e",
                    "name": "John Hewitt",
                    "hidden": false
                },
                {
                    "_id": "67b30146b02f929c82ce075f",
                    "name": "Robert Geirhos",
                    "hidden": false
                },
                {
                    "_id": "67b30146b02f929c82ce0760",
                    "name": "Been Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T14:34:05.000Z",
            "title": "We Can't Understand AI Using our Existing Vocabulary",
            "summary": "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better.",
            "upvotes": 7,
            "discussionId": "67b30147b02f929c82ce079c"
        },
        "publishedAt": "2025-02-17T04:28:55.526Z",
        "title": "We Can't Understand AI Using our Existing Vocabulary",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07586.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "fullname": "Gabriele Sarti",
            "name": "gsarti",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 212
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09741",
            "authors": [
                {
                    "_id": "67b2b58f9edebc815a2a938c",
                    "name": "Tianyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b2b58f9edebc815a2a938d",
                    "name": "Deqing Fu",
                    "hidden": false
                },
                {
                    "_id": "67b2b58f9edebc815a2a938e",
                    "name": "Mahdi Soltanolkotabi",
                    "hidden": false
                },
                {
                    "_id": "67b2b58f9edebc815a2a938f",
                    "name": "Robin Jia",
                    "hidden": false
                },
                {
                    "_id": "67b2b58f9edebc815a2a9390",
                    "name": "Vatsal Sharan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T19:54:59.000Z",
            "title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
            "summary": "Large Language Models (LLMs) typically represent numbers using multiple\ntokens, which requires the model to aggregate these tokens to interpret\nnumerical values. This fragmentation makes both training and inference less\nefficient and adversely affects the model's performance on number-related\ntasks. Inspired by the observation that pre-trained LLMs internally learn\nFourier-like features for number tokens, we propose Fourier Number Embedding\n(FoNE), a novel method that directly maps numbers into the embedding space with\ntheir Fourier features. FoNE encodes each number as a single token with only\ntwo embedding dimensions per digit, effectively capturing numerical values\nwithout fragmentation. This compact representation accelerates both training\nand inference. Compared to traditional subword and digit-wise embeddings, FoNE\nnot only reduces computational overhead but also achieves higher accuracy\nacross various numerical tasks including addition, subtraction and\nmultiplication. On 6-digit decimal addition, FoNE requires 64times less data\nto achieve 99% accuracy than subword and digit-wise embeddings while using\n3times and 6times fewer tokens per number, respectively. Furthermore,\nFoNE is the only method that yields 100% accuracy on over 100,000 test examples\nfor addition, subtraction, and multiplication. The codes and visualization are\navailable at https://fouriernumber.github.io/.",
            "upvotes": 7,
            "discussionId": "67b2b5919edebc815a2a93fc"
        },
        "publishedAt": "2025-02-16T23:07:53.170Z",
        "title": "FoNE: Precise Single-Token Number Embeddings via Fourier Features",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09741.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "fullname": "Deqing Fu",
            "name": "deqing",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10392",
            "authors": [
                {
                    "_id": "67b346bab6c58a3e0a26190a",
                    "name": "Wenxuan Guo",
                    "hidden": false
                },
                {
                    "_id": "67b346bab6c58a3e0a26190b",
                    "name": "Xiuwei Xu",
                    "hidden": false
                },
                {
                    "_id": "67b346bab6c58a3e0a26190c",
                    "name": "Ziwei Wang",
                    "hidden": false
                },
                {
                    "_id": "67b346bab6c58a3e0a26190d",
                    "name": "Jianjiang Feng",
                    "hidden": false
                },
                {
                    "_id": "67b346bab6c58a3e0a26190e",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b346bab6c58a3e0a26190f",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T18:59:59.000Z",
            "title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
            "summary": "In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with +1.13 lead of Acc@0.5 on\nScanRefer, and +2.6 and +3.2 leads on NR3D and SR3D respectively. The code\nis available at\nhttps://github.com/GWxuan/TSP3D{https://github.com/GWxuan/TSP3D}.",
            "upvotes": 4,
            "discussionId": "67b346bcb6c58a3e0a26195c"
        },
        "publishedAt": "2025-02-17T09:25:39.949Z",
        "title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10392.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648ac65fd044b25978015634",
            "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
            "fullname": "Xiuwei Xu",
            "name": "xuxw98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10140",
            "authors": [
                {
                    "_id": "67b32e9dff65b4ec02cb6d81",
                    "name": "Daniil Gurgurov",
                    "hidden": false
                },
                {
                    "_id": "67b32e9dff65b4ec02cb6d82",
                    "user": {
                        "_id": "64b7e5a89e7deb6a7824301b",
                        "avatarUrl": "/avatars/34d8843fe6203c4d09bf1b442405a4ab.svg",
                        "isPro": false,
                        "fullname": "Ivan Vykopal",
                        "user": "ivykopal",
                        "type": "user"
                    },
                    "name": "Ivan Vykopal",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-17T12:42:07.190Z",
                    "hidden": false
                },
                {
                    "_id": "67b32e9dff65b4ec02cb6d83",
                    "name": "Josef van Genabith",
                    "hidden": false
                },
                {
                    "_id": "67b32e9dff65b4ec02cb6d84",
                    "name": "Simon Ostermann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T13:10:39.000Z",
            "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages",
            "summary": "Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage.",
            "upvotes": 4,
            "discussionId": "67b32e9fff65b4ec02cb6dcd"
        },
        "publishedAt": "2025-02-17T08:29:25.102Z",
        "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10140.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6427f45beb320ead3d287acf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427f45beb320ead3d287acf/J7DLdQM6ehiJBke112s0q.jpeg",
            "fullname": "Daniil Gurgurov",
            "name": "DGurgurov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.08130",
            "authors": [
                {
                    "_id": "67b3716bab1b992c7f4599da",
                    "name": "Sonam Gupta",
                    "hidden": false
                },
                {
                    "_id": "67b3716bab1b992c7f4599db",
                    "name": "Yatin Nandwani",
                    "hidden": false
                },
                {
                    "_id": "67b3716bab1b992c7f4599dc",
                    "name": "Asaf Yehudai",
                    "hidden": false
                },
                {
                    "_id": "67b3716bab1b992c7f4599dd",
                    "name": "Dinesh Khandelwal",
                    "hidden": false
                },
                {
                    "_id": "67b3716bab1b992c7f4599de",
                    "name": "Dinesh Raghu",
                    "hidden": false
                },
                {
                    "_id": "67b3716bab1b992c7f4599df",
                    "name": "Sachindra Joshi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T05:24:21.000Z",
            "title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large\n  Language Models",
            "summary": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-to-Supervised Fine-Tuning\n(S3FT), a fine-tuning approach that achieves better performance than the\nstandard supervised fine-tuning (SFT) while improving generalization. S3FT\nleverages the existence of multiple valid responses to a query. By utilizing\nthe model's correct responses, S3FT reduces model specialization during the\nfine-tuning stage. S3FT first identifies the correct model responses from the\ntraining set by deploying an appropriate judge. Then, it fine-tunes the model\nusing the correct model responses and the gold response (or its paraphrase) for\nthe remaining samples. The effectiveness of S3FT is demonstrated through\nexperiments on mathematical reasoning, Python programming and reading\ncomprehension tasks. The results show that standard SFT can lead to an average\nperformance drop of up to 4.4 on multiple benchmarks, such as MMLU and\nTruthfulQA. In contrast, S3FT reduces this drop by half, i.e. 2.5, indicating\nbetter generalization capabilities than SFT while performing significantly\nbetter on the fine-tuning tasks.",
            "upvotes": 3,
            "discussionId": "67b3716bab1b992c7f459a15"
        },
        "publishedAt": "2025-02-17T12:27:43.231Z",
        "title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08130.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638324f862badff43269e588",
            "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
            "fullname": "Asaf Yehudai",
            "name": "Asaf-Yehudai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09638",
            "authors": [
                {
                    "_id": "67b2c3386ccf462ccaa45860",
                    "name": "Jeremy Kritz",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45861",
                    "name": "Vaughn Robinson",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45862",
                    "name": "Robert Vacareanu",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45863",
                    "name": "Bijan Varjavand",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45864",
                    "name": "Michael Choi",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45865",
                    "name": "Bobby Gogov",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45866",
                    "name": "Scale Red Team",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45867",
                    "name": "Summer Yue",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45868",
                    "name": "Willow E. Primack",
                    "hidden": false
                },
                {
                    "_id": "67b2c3386ccf462ccaa45869",
                    "user": {
                        "_id": "66976d1007b36ccd01586ce5",
                        "avatarUrl": "/avatars/5811e350907a29b71f6e4d57ffd53e66.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "ZifanScale",
                        "type": "user"
                    },
                    "name": "Zifan Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-17T05:03:53.788Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-09T20:49:16.000Z",
            "title": "Jailbreaking to Jailbreak",
            "summary": "Refusal training on Large Language Models (LLMs) prevents harmful outputs,\nyet this defense remains vulnerable to both automated and human-crafted\njailbreaks. We present a novel LLM-as-red-teamer approach in which a human\njailbreaks a refusal-trained LLM to make it willing to jailbreak itself or\nother LLMs. We refer to the jailbroken LLMs as J_2 attackers, which can\nsystematically evaluate target models using various red teaming strategies and\nimprove its performance via in-context learning from the previous failures. Our\nexperiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other\nLLMs as J_2, achieving 93.0% and 91.0% attack success rates (ASRs)\nrespectively against GPT-4o (and similar results across other capable LLMs) on\nHarmbench. Our work not only introduces a scalable approach to strategic red\nteaming, drawing inspiration from human red teamers, but also highlights\njailbreaking-to-jailbreak as an overlooked failure mode of the safeguard.\nSpecifically, an LLM can bypass its own safeguards by employing a jailbroken\nversion of itself that is willing to assist in further jailbreaking. To prevent\nany direct misuse with J_2, while advancing research in AI safety, we\npublicly share our methodology while keeping specific prompting details\nprivate.",
            "upvotes": 3,
            "discussionId": "67b2c3396ccf462ccaa458b3"
        },
        "publishedAt": "2025-02-17T00:04:19.389Z",
        "title": "Jailbreaking to Jailbreak",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09638.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6120
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10177",
            "authors": [
                {
                    "_id": "67b29f472ea5fd965beb91ed",
                    "name": "Mingcong Lei",
                    "hidden": false
                },
                {
                    "_id": "67b29f472ea5fd965beb91ee",
                    "name": "Yiming Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b29f472ea5fd965beb91ef",
                    "name": "Ge Wang",
                    "hidden": false
                },
                {
                    "_id": "67b29f472ea5fd965beb91f0",
                    "name": "Zhixin Mai",
                    "hidden": false
                },
                {
                    "_id": "67b29f472ea5fd965beb91f1",
                    "name": "Shuguang Cui",
                    "hidden": false
                },
                {
                    "_id": "67b29f472ea5fd965beb91f2",
                    "name": "Yatong Han",
                    "hidden": false
                },
                {
                    "_id": "67b29f472ea5fd965beb91f3",
                    "name": "Jinke Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T14:12:09.000Z",
            "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
            "summary": "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
            "upvotes": 3,
            "discussionId": "67b29f4a2ea5fd965beb9286"
        },
        "publishedAt": "2025-02-16T21:31:11.459Z",
        "title": "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6628c6107751d297d7025a71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
            "fullname": "Lei Mingcong",
            "name": "SP4595",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.07856",
            "authors": [
                {
                    "_id": "67b2dedc8a276e7b485a9bcd",
                    "name": "Ao Li",
                    "hidden": false
                },
                {
                    "_id": "67b2dedc8a276e7b485a9bce",
                    "name": "Wei Fang",
                    "hidden": false
                },
                {
                    "_id": "67b2dedc8a276e7b485a9bcf",
                    "name": "Hongbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b2dedc8a276e7b485a9bd0",
                    "name": "Le Lu",
                    "hidden": false
                },
                {
                    "_id": "67b2dedc8a276e7b485a9bd1",
                    "name": "Ge Yang",
                    "hidden": false
                },
                {
                    "_id": "67b2dedc8a276e7b485a9bd2",
                    "name": "Minfeng Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-11T14:57:33.000Z",
            "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE\n  Solvers",
            "summary": "In applications of diffusion models, controllable generation is of practical\nsignificance, but is also challenging. Current methods for controllable\ngeneration primarily focus on modifying the score function of diffusion models,\nwhile Mean Reverting (MR) Diffusion directly modifies the structure of the\nstochastic differential equation (SDE), making the incorporation of image\nconditions simpler and more natural. However, current training-free fast\nsamplers are not directly applicable to MR Diffusion. And thus MR Diffusion\nrequires hundreds of NFEs (number of function evaluations) to obtain\nhigh-quality samples. In this paper, we propose a new algorithm named MRS (MR\nSampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time\nSDE and the probability flow ordinary differential equation (PF-ODE) associated\nwith MR Diffusion, and derive semi-analytical solutions. The solutions consist\nof an analytical function and an integral parameterized by a neural network.\nBased on this solution, we can generate high-quality samples in fewer steps.\nOur approach does not require training and supports all mainstream\nparameterizations, including noise prediction, data prediction and velocity\nprediction. Extensive experiments demonstrate that MR Sampler maintains high\nsampling quality with a speedup of 10 to 20 times across ten different image\nrestoration tasks. Our algorithm accelerates the sampling procedure of MR\nDiffusion, making it more practical in controllable generation.",
            "upvotes": 2,
            "discussionId": "67b2dedd8a276e7b485a9c0b"
        },
        "publishedAt": "2025-02-17T02:03:05.624Z",
        "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07856.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64100834c025ddf6189c415e",
            "avatarUrl": "/avatars/9b9bbecef5d5815540abf92d74012f55.svg",
            "fullname": "Hongbo Zhao",
            "name": "z-hb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10362",
            "authors": [
                {
                    "_id": "67b2e11dd2ee8e627dec1bc2",
                    "name": "Shangda Wu",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc3",
                    "name": "Zhancheng Guo",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc4",
                    "name": "Ruibin Yuan",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc5",
                    "name": "Junyan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc6",
                    "name": "Seungheon Doh",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc7",
                    "name": "Gus Xia",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc8",
                    "name": "Juhan Nam",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bc9",
                    "name": "Xiaobing Li",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bca",
                    "name": "Feng Yu",
                    "hidden": false
                },
                {
                    "_id": "67b2e11dd2ee8e627dec1bcb",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T18:42:25.000Z",
            "title": "CLaMP 3: Universal Music Information Retrieval Across Unaligned\n  Modalities and Unseen Languages",
            "summary": "CLaMP 3 is a unified framework developed to address challenges of cross-modal\nand cross-lingual generalization in music information retrieval. Using\ncontrastive learning, it aligns all major music modalities--including sheet\nmusic, performance signals, and audio recordings--with multilingual text in a\nshared representation space, enabling retrieval across unaligned modalities\nwith text as a bridge. It features a multilingual text encoder adaptable to\nunseen languages, exhibiting strong cross-lingual generalization. Leveraging\nretrieval-augmented generation, we curated M4-RAG, a web-scale dataset\nconsisting of 2.31 million music-text pairs. This dataset is enriched with\ndetailed metadata that represents a wide array of global musical traditions. To\nadvance future research, we release WikiMT-X, a benchmark comprising 1,000\ntriplets of sheet music, audio, and richly varied text descriptions.\nExperiments show that CLaMP 3 achieves state-of-the-art performance on multiple\nMIR tasks, significantly surpassing previous strong baselines and demonstrating\nexcellent generalization in multimodal and multilingual music contexts.",
            "upvotes": 1,
            "discussionId": "67b2e11ed2ee8e627dec1c25"
        },
        "publishedAt": "2025-02-17T10:18:04.718Z",
        "title": "CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10362.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6120
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.08769",
            "authors": [
                {
                    "_id": "67b32a554d60b7d162dffd89",
                    "name": "Timothée Darcet",
                    "hidden": false
                },
                {
                    "_id": "67b32a554d60b7d162dffd8a",
                    "name": "Federico Baldassarre",
                    "hidden": false
                },
                {
                    "_id": "67b32a554d60b7d162dffd8b",
                    "name": "Maxime Oquab",
                    "hidden": false
                },
                {
                    "_id": "67b32a554d60b7d162dffd8c",
                    "name": "Julien Mairal",
                    "hidden": false
                },
                {
                    "_id": "67b32a554d60b7d162dffd8d",
                    "name": "Piotr Bojanowski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T20:17:10.000Z",
            "title": "Cluster and Predict Latents Patches for Improved Masked Image Modeling",
            "summary": "Masked Image Modeling (MIM) offers a promising approach to self-supervised\nrepresentation learning, however existing MIM models still lag behind the\nstate-of-the-art. In this paper, we systematically analyze target\nrepresentations, loss functions, and architectures, to introduce CAPI - a novel\npure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and\nexhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%\naccuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,\nsubstantially outperforming previous MIM methods and approaching the\nperformance of the current state-of-the-art, DINOv2. We release all our code\nand models.",
            "upvotes": 1,
            "discussionId": "67b32a574d60b7d162dffdd4"
        },
        "publishedAt": "2025-02-17T07:24:28.545Z",
        "title": "Cluster and Predict Latents Patches for Improved Masked Image Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08769.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 762
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10173",
            "authors": [
                {
                    "_id": "67b306ba817e86482ef224d5",
                    "name": "Bo Ni",
                    "hidden": false
                },
                {
                    "_id": "67b306ba817e86482ef224d6",
                    "name": "Markus J. Buehler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T14:07:54.000Z",
            "title": "Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a\n  Language Diffusion Model",
            "summary": "Proteins are dynamic molecular machines whose biological functions, spanning\nenzymatic catalysis, signal transduction, and structural adaptation, are\nintrinsically linked to their motions. Designing proteins with targeted dynamic\nproperties, however, remains a challenge due to the complex, degenerate\nrelationships between sequence, structure, and molecular motion. Here, we\nintroduce VibeGen, a generative AI framework that enables end-to-end de novo\nprotein design conditioned on normal mode vibrations. VibeGen employs an\nagentic dual-model architecture, comprising a protein designer that generates\nsequence candidates based on specified vibrational modes and a protein\npredictor that evaluates their dynamic accuracy. This approach synergizes\ndiversity, accuracy, and novelty during the design process. Via full-atom\nmolecular simulations as direct validation, we demonstrate that the designed\nproteins accurately reproduce the prescribed normal mode amplitudes across the\nbackbone while adopting various stable, functionally relevant structures.\nNotably, generated sequences are de novo, exhibiting no significant similarity\nto natural proteins, thereby expanding the accessible protein space beyond\nevolutionary constraints. Our work integrates protein dynamics into generative\nprotein design, and establishes a direct, bidirectional link between sequence\nand vibrational behavior, unlocking new pathways for engineering biomolecules\nwith tailored dynamical and functional properties. This framework holds broad\nimplications for the rational design of flexible enzymes, dynamic scaffolds,\nand biomaterials, paving the way toward dynamics-informed AI-driven protein\nengineering.",
            "upvotes": 1,
            "discussionId": "67b306ba817e86482ef224fa"
        },
        "publishedAt": "2025-02-17T05:09:33.663Z",
        "title": "Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/623ce1c6b66fedf374859fe7/rcgnOK5A9wV0qO9I3Mxny.png",
            "https://cdn-uploads.huggingface.co/production/uploads/623ce1c6b66fedf374859fe7/xD8WOPTgKHpIIPwHh9KHf.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10173.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "623ce1c6b66fedf374859fe7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg",
            "fullname": "Markus Buehler",
            "name": "mjbuehler",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09980",
            "authors": [
                {
                    "_id": "67b2d7e86a002d59a415fc99",
                    "name": "Hsu-kuang Chiu",
                    "hidden": false
                },
                {
                    "_id": "67b2d7e86a002d59a415fc9a",
                    "name": "Ryo Hachiuma",
                    "hidden": false
                },
                {
                    "_id": "67b2d7e86a002d59a415fc9b",
                    "name": "Chien-Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2d7e86a002d59a415fc9c",
                    "name": "Stephen F. Smith",
                    "hidden": false
                },
                {
                    "_id": "67b2d7e86a002d59a415fc9d",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "67b2d7e86a002d59a415fc9e",
                    "name": "Min-Hung Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T08:05:41.000Z",
            "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multi-Modal Large Language Models",
            "summary": "Current autonomous driving vehicles rely mainly on their individual sensors\nto understand surrounding scenes and plan for future trajectories, which can be\nunreliable when the sensors are malfunctioning or occluded. To address this\nproblem, cooperative perception methods via vehicle-to-vehicle (V2V)\ncommunication have been proposed, but they have tended to focus on detection\nand tracking. How those approaches contribute to overall cooperative planning\nperformance is still under-explored. Inspired by recent progress using Large\nLanguage Models (LLMs) to build autonomous driving systems, we propose a novel\nproblem setting that integrates an LLM into cooperative autonomous driving,\nwith the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and\nbenchmark. We also propose our baseline method Vehicle-to-Vehicle Large\nLanguage Model (V2V-LLM), which uses an LLM to fuse perception information from\nmultiple connected autonomous vehicles (CAVs) and answer driving-related\nquestions: grounding, notable object identification, and planning. Experimental\nresults show that our proposed V2V-LLM can be a promising unified model\narchitecture for performing various tasks in cooperative autonomous driving,\nand outperforms other baseline methods that use different fusion approaches.\nOur work also creates a new research direction that can improve the safety of\nfuture autonomous driving systems. Our project website:\nhttps://eddyhkchiu.github.io/v2vllm.github.io/ .",
            "upvotes": 1,
            "discussionId": "67b2d7ee6a002d59a415fe34"
        },
        "publishedAt": "2025-02-17T01:33:15.971Z",
        "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09980.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "fullname": "Min-Hung Chen",
            "name": "cmhungsteve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    }
]