[
    {
        "paper": {
            "id": "2502.11089",
            "authors": [
                {
                    "_id": "67b43211d3c5f50aa9c03a2d",
                    "name": "Jingyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a2e",
                    "user": {
                        "_id": "64e370be59aa5366642ac329",
                        "avatarUrl": "/avatars/0fa1eb6ac6c1aeff3e65bc86a6617f64.svg",
                        "isPro": false,
                        "fullname": "Huazuo Gao",
                        "user": "gaohuazuo",
                        "type": "user"
                    },
                    "name": "Huazuo Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:43:19.672Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a2f",
                    "user": {
                        "_id": "659389f8de82e1ef7b9a8b13",
                        "avatarUrl": "/avatars/896ed9f4cdbd317493b303d070b7e12a.svg",
                        "isPro": false,
                        "fullname": "Damai Dai",
                        "user": "DeepSeekDDM",
                        "type": "user"
                    },
                    "name": "Damai Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:43:30.267Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a30",
                    "user": {
                        "_id": "66e6c6372c78909baf44cdf8",
                        "avatarUrl": "/avatars/458ea1d545d7c022b0463e7fbbd91db1.svg",
                        "isPro": false,
                        "fullname": "Junyu Luo",
                        "user": "junyuluo",
                        "type": "user"
                    },
                    "name": "Junyu Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:43:36.295Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a31",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a32",
                    "user": {
                        "_id": "65654ed2219af7f841640f27",
                        "avatarUrl": "/avatars/e6904b3479fc5e65ea1f752919ca8290.svg",
                        "isPro": false,
                        "fullname": "Zhengyan Zhang",
                        "user": "ZhengyanZhang",
                        "type": "user"
                    },
                    "name": "Zhengyan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:44:02.477Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a33",
                    "user": {
                        "_id": "6797ca96e9e2793006a15110",
                        "avatarUrl": "/avatars/2d393d6e5fc2e1a867f7fdd44e055a2f.svg",
                        "isPro": false,
                        "fullname": "zhenda xie",
                        "user": "Zhendaxie",
                        "type": "user"
                    },
                    "name": "Zhenda Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:44:16.691Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a34",
                    "name": "Y. X. Wei",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a35",
                    "user": {
                        "_id": "650c509472afb1e60e6151ae",
                        "avatarUrl": "/avatars/c16ab5053a586819dc2b965303215ff7.svg",
                        "isPro": false,
                        "fullname": "Lean Wang",
                        "user": "AdaHousman",
                        "type": "user"
                    },
                    "name": "Lean Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:44:26.979Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a36",
                    "user": {
                        "_id": "66ab566e30c55e83b02aa050",
                        "avatarUrl": "/avatars/62692be88b9ad34ad3f474fb0359ae20.svg",
                        "isPro": false,
                        "fullname": "Zhiping Xiao",
                        "user": "Shockzipper",
                        "type": "user"
                    },
                    "name": "Zhiping Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:44:34.873Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a37",
                    "name": "Yuqing Wang",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a38",
                    "user": {
                        "_id": "6398203609f12714ed1935c2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6398203609f12714ed1935c2/uXgl0LgKnFYjq1Wz39-a6.jpeg",
                        "isPro": false,
                        "fullname": "Chong Ruan",
                        "user": "Chester111",
                        "type": "user"
                    },
                    "name": "Chong Ruan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-18T16:45:33.988Z",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a39",
                    "name": "Ming Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a3a",
                    "name": "Wenfeng Liang",
                    "hidden": false
                },
                {
                    "_id": "67b43211d3c5f50aa9c03a3b",
                    "name": "Wangding Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T11:53:44.000Z",
            "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse\n  Attention",
            "summary": "Long-context modeling is crucial for next-generation language models, yet the\nhigh computational cost of standard attention mechanisms poses significant\ncomputational challenges. Sparse attention offers a promising direction for\nimproving efficiency while maintaining model capabilities. We present NSA, a\nNatively trainable Sparse Attention mechanism that integrates algorithmic\ninnovations with hardware-aligned optimizations to achieve efficient\nlong-context modeling. NSA employs a dynamic hierarchical sparse strategy,\ncombining coarse-grained token compression with fine-grained token selection to\npreserve both global context awareness and local precision. Our approach\nadvances sparse attention design with two key innovations: (1) We achieve\nsubstantial speedups through arithmetic intensity-balanced algorithm design,\nwith implementation optimizations for modern hardware. (2) We enable end-to-end\ntraining, reducing pretraining computation without sacrificing model\nperformance. As shown in Figure 1, experiments show the model pretrained with\nNSA maintains or exceeds Full Attention models across general benchmarks,\nlong-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves\nsubstantial speedups over Full Attention on 64k-length sequences across\ndecoding, forward propagation, and backward propagation, validating its\nefficiency throughout the model lifecycle.",
            "upvotes": 62,
            "discussionId": "67b43212d3c5f50aa9c03a5c"
        },
        "publishedAt": "2025-02-18T06:07:36.212Z",
        "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11089.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645e054ff7a55f0d780a8ff7",
            "avatarUrl": "/avatars/9614510443bee3bd5d6266efd1c39fc1.svg",
            "fullname": "Chunjiang Ge",
            "name": "HelloJiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.12152",
            "authors": [
                {
                    "_id": "67b41ed52867282b4eb37ce4",
                    "name": "Xialin He",
                    "hidden": false
                },
                {
                    "_id": "67b41ed52867282b4eb37ce5",
                    "user": {
                        "_id": "6201fc5d91d53938a6432fbf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
                        "isPro": false,
                        "fullname": "Runpei Dong",
                        "user": "RunpeiDong",
                        "type": "user"
                    },
                    "name": "Runpei Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:13.178Z",
                    "hidden": false
                },
                {
                    "_id": "67b41ed52867282b4eb37ce6",
                    "name": "Zixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "67b41ed52867282b4eb37ce7",
                    "name": "Saurabh Gupta",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T18:59:06.000Z",
            "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
            "summary": "Automatic fall recovery is a crucial prerequisite before humanoid robots can\nbe reliably deployed. Hand-designing controllers for getting up is difficult\nbecause of the varied configurations a humanoid can end up in after a fall and\nthe challenging terrains humanoid robots are expected to operate on. This paper\ndevelops a learning framework to produce controllers that enable humanoid\nrobots to get up from varying configurations on varying terrains. Unlike\nprevious successful applications of humanoid locomotion learning, the\ngetting-up task involves complex contact patterns, which necessitates\naccurately modeling the collision geometry and sparser rewards. We address\nthese challenges through a two-phase approach that follows a curriculum. The\nfirst stage focuses on discovering a good getting-up trajectory under minimal\nconstraints on smoothness or speed / torque limits. The second stage then\nrefines the discovered motions into deployable (i.e. smooth and slow) motions\nthat are robust to variations in initial configuration and terrains. We find\nthese innovations enable a real-world G1 humanoid robot to get up from two main\nsituations that we considered: a) lying face up and b) lying face down, both\ntested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass\nand snowfield). To the best of our knowledge, this is the first successful\ndemonstration of learned getting-up policies for human-sized humanoid robots in\nthe real world. Project page: https://humanoid-getup.github.io/",
            "upvotes": 32,
            "discussionId": "67b41edb2867282b4eb37ddf"
        },
        "publishedAt": "2025-02-18T00:49:53.124Z",
        "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/x35BuXOhc6ubukxLfiVzt.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12152.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "fullname": "Runpei Dong",
            "name": "RunpeiDong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.12115",
            "authors": [
                {
                    "_id": "67b41a72a38d04cc6148d80e",
                    "name": "Samuel Miserendino",
                    "hidden": false
                },
                {
                    "_id": "67b41a72a38d04cc6148d80f",
                    "name": "Michele Wang",
                    "hidden": false
                },
                {
                    "_id": "67b41a72a38d04cc6148d810",
                    "name": "Tejal Patwardhan",
                    "hidden": false
                },
                {
                    "_id": "67b41a72a38d04cc6148d811",
                    "name": "Johannes Heidecke",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T18:41:16.000Z",
            "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance\n  Software Engineering?",
            "summary": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software\nengineering tasks from Upwork, valued at \\1 million USD total in real-world\npayouts. SWE-Lancer encompasses both independent engineering tasks--ranging\nfrom 50 bug fixes to \\$32,000 feature implementations--and managerial tasks,\nwhere models choose between technical implementation proposals. Independent\ntasks are graded with end-to-end tests triple-verified by experienced software\nengineers, while managerial decisions are assessed against the choices of the\noriginal hired engineering managers. We evaluate model performance and find\nthat frontier models are still unable to solve the majority of tasks. To\nfacilitate future research, we open-source a unified Docker image and a public\nevaluation split, SWE-Lancer Diamond\n(https://github.com/openai/SWELancer-Benchmark). By mapping model performance\nto monetary value, we hope SWE-Lancer enables greater research into the\neconomic impact of AI model development.",
            "upvotes": 27,
            "discussionId": "67b41a74a38d04cc6148d84b"
        },
        "publishedAt": "2025-02-18T00:28:31.293Z",
        "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12115.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6137
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11190",
            "authors": [
                {
                    "_id": "67b420dfb2528c023491f455",
                    "name": "Haoming Xu",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f456",
                    "name": "Ningyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f457",
                    "name": "Liming Yang",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f458",
                    "name": "Sendong Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f459",
                    "name": "Shumin Deng",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f45a",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f45b",
                    "name": "Bryan Hooi",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f45c",
                    "name": "Nay Oo",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f45d",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "67b420dfb2528c023491f45e",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:11.243Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T16:31:00.000Z",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "summary": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.",
            "upvotes": 18,
            "discussionId": "67b420e2b2528c023491f506"
        },
        "publishedAt": "2025-02-18T00:58:24.094Z",
        "title": "ReLearn: Unlearning via Learning for Large Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/A4YB7t6hDVty6QrvLN0a7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11190.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09061",
            "authors": [
                {
                    "_id": "67b401de3995f28d45c212d6",
                    "name": "Debangshu Banerjee",
                    "hidden": false
                },
                {
                    "_id": "67b401de3995f28d45c212d7",
                    "name": "Tarun Suresh",
                    "hidden": false
                },
                {
                    "_id": "67b401de3995f28d45c212d8",
                    "name": "Shubham Ugare",
                    "hidden": false
                },
                {
                    "_id": "67b401de3995f28d45c212d9",
                    "name": "Sasa Misailovic",
                    "hidden": false
                },
                {
                    "_id": "67b401de3995f28d45c212da",
                    "name": "Gagandeep Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T08:23:42.000Z",
            "title": "CRANE: Reasoning with constrained LLM generation",
            "summary": "Code generation, symbolic math reasoning, and other tasks require LLMs to\nproduce outputs that are both syntactically and semantically correct.\nConstrained LLM generation is a promising direction to enforce adherence to\nformal grammar, but prior works have empirically observed that strict\nenforcement of formal constraints often diminishes the reasoning capabilities\nof LLMs. In this work, we first provide a theoretical explanation for why\nconstraining LLM outputs to very restrictive grammars that only allow\nsyntactically valid final answers reduces the reasoning capabilities of the\nmodel. Second, we demonstrate that by augmenting the output grammar with\ncarefully designed additional rules, it is always possible to preserve the\nreasoning capabilities of the LLM while ensuring syntactic and semantic\ncorrectness in its outputs. Building on these theoretical insights, we propose\na reasoning-augmented constrained decoding algorithm, CRANE, which effectively\nbalances the correctness of constrained generation with the flexibility of\nunconstrained generation. Experiments on multiple open-source LLMs and\nbenchmarks show that CRANE significantly outperforms both state-of-the-art\nconstrained decoding strategies and standard unconstrained decoding, showing up\nto 10% points accuracy improvement over baselines on challenging symbolic\nreasoning benchmarks GSM-symbolic and FOLIO.",
            "upvotes": 16,
            "discussionId": "67b401e03995f28d45c21354"
        },
        "publishedAt": "2025-02-17T22:43:51.555Z",
        "title": "CRANE: Reasoning with constrained LLM generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09061.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e7bb35e5e78134ab049942",
            "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
            "fullname": "Tarun Suresh",
            "name": "tarsur909",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.08745",
            "authors": [
                {
                    "_id": "67b4cf1994ec5e365fb7995d",
                    "user": {
                        "_id": "63bf9695da08ed054400205e",
                        "avatarUrl": "/avatars/b6fca49559a61cf66628088c60d26c10.svg",
                        "isPro": false,
                        "fullname": "Zhihan Zhang",
                        "user": "zhihz0535",
                        "type": "user"
                    },
                    "name": "Zhihan Zhang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-18T18:19:31.455Z",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb7995e",
                    "name": "Shiyang Li",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb7995f",
                    "name": "Zixuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79960",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79961",
                    "name": "Haoming Jiang",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79962",
                    "name": "Xianfeng Tang",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79963",
                    "name": "Yifan Gao",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79964",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79965",
                    "name": "Haodong Wang",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79966",
                    "name": "Zhaoxuan Tan",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79967",
                    "name": "Yichuan Li",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79968",
                    "name": "Qingyu Yin",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb79969",
                    "name": "Bing Yin",
                    "hidden": false
                },
                {
                    "_id": "67b4cf1994ec5e365fb7996a",
                    "name": "Meng Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T19:35:28.000Z",
            "title": "IHEval: Evaluating Language Models on Following the Instruction\n  Hierarchy",
            "summary": "The instruction hierarchy, which establishes a priority order from system\nmessages to user messages, conversation history, and tool outputs, is essential\nfor ensuring consistent and safe behavior in language models (LMs). Despite its\nimportance, this topic receives limited attention, and there is a lack of\ncomprehensive benchmarks for evaluating models' ability to follow the\ninstruction hierarchy. We bridge this gap by introducing IHEval, a novel\nbenchmark comprising 3,538 examples across nine tasks, covering cases where\ninstructions in different priorities either align or conflict. Our evaluation\nof popular LMs highlights their struggle to recognize instruction priorities.\nAll evaluated models experience a sharp performance decline when facing\nconflicting instructions, compared to their original instruction-following\nperformance. Moreover, the most competitive open-source model only achieves 48%\naccuracy in resolving such conflicts. Our results underscore the need for\ntargeted optimization in the future development of LMs.",
            "upvotes": 15,
            "discussionId": "67b4cf1a94ec5e365fb799c1"
        },
        "publishedAt": "2025-02-18T13:21:05.722Z",
        "title": "IHEval: Evaluating Language Models on Following the Instruction Hierarchy",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08745.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bf9695da08ed054400205e",
            "avatarUrl": "/avatars/b6fca49559a61cf66628088c60d26c10.svg",
            "fullname": "Zhihan Zhang",
            "name": "zhihz0535",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.12148",
            "authors": [
                {
                    "_id": "67b40c8cdb88dfd19ab917f3",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67b40c8cdb88dfd19ab917f4",
                    "user": {
                        "_id": "653e5d31ffd60206c8b64bb5",
                        "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
                        "isPro": false,
                        "fullname": "Xinchen Zhang",
                        "user": "comin",
                        "type": "user"
                    },
                    "name": "Xinchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:31.841Z",
                    "hidden": false
                },
                {
                    "_id": "67b40c8cdb88dfd19ab917f5",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "67b40c8cdb88dfd19ab917f6",
                    "name": "Chenming Shang",
                    "hidden": false
                },
                {
                    "_id": "67b40c8cdb88dfd19ab917f7",
                    "name": "Minghao Xu",
                    "hidden": false
                },
                {
                    "_id": "67b40c8cdb88dfd19ab917f8",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b40c8cdb88dfd19ab917f9",
                    "name": "Bin Cui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T18:57:51.000Z",
            "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation",
            "summary": "The remarkable success of the autoregressive paradigm has made significant\nadvancement in Multimodal Large Language Models (MLLMs), with powerful models\nlike Show-o, Transfusion and Emu3 achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we propose HermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take the homologous data as input to curate homologous\npreference data of both understanding and generation. Through Pair-DPO and\nself-play iterative optimization, HermesFlow effectively aligns multimodal\nunderstanding and generation using homologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential of HermesFlow as a general\nalignment framework for next-generation multimodal foundation models. Code:\nhttps://github.com/Gen-Verse/HermesFlow",
            "upvotes": 14,
            "discussionId": "67b40c8edb88dfd19ab9183f"
        },
        "publishedAt": "2025-02-17T23:29:29.396Z",
        "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12148.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "fullname": "Xinchen Zhang",
            "name": "comin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11196",
            "authors": [
                {
                    "_id": "67b42223c2fe54b8d43efed6",
                    "name": "Yixin Ou",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efed7",
                    "name": "Yunzhi Yao",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efed8",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:04.227Z",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efed9",
                    "name": "Hui Jin",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efeda",
                    "name": "Jiacheng Sun",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efedb",
                    "name": "Shumin Deng",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efedc",
                    "name": "Zhenguo Li",
                    "hidden": false
                },
                {
                    "_id": "67b42223c2fe54b8d43efedd",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T16:55:43.000Z",
            "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on\n  Continual Pre-Training",
            "summary": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language\nModels (LLMs) face a critical gap in understanding how they internalize new\nknowledge, particularly how to structurally embed acquired knowledge in their\nneural computations. We address this issue through the lens of knowledge\ncircuit evolution, identifying computational subgraphs that facilitate\nknowledge storage and processing. Our systematic analysis of circuit evolution\nthroughout continual pre-training reveals several key findings: (1) the\nacquisition of new knowledge is influenced by its relevance to pre-existing\nknowledge; (2) the evolution of knowledge circuits exhibits a distinct phase\nshift from formation to optimization; (3) the evolution of knowledge circuits\nfollows a deep-to-shallow pattern. These insights not only advance our\ntheoretical understanding of the mechanisms of new knowledge acquisition in\nLLMs, but also provide potential implications for improving continual\npre-training strategies to enhance model performance. Code and data will be\navailable at https://github.com/zjunlp/DynamicKnowledgeCircuits.",
            "upvotes": 13,
            "discussionId": "67b42225c2fe54b8d43eff9b"
        },
        "publishedAt": "2025-02-18T01:02:25.236Z",
        "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/_LGnwvwslWc3YDIirfOKS.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11196.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.10458",
            "authors": [
                {
                    "_id": "67b3ea0f4dd7ea0538ce589d",
                    "user": {
                        "_id": "6354bda206d707b33249c4c2",
                        "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
                        "isPro": false,
                        "fullname": "Zhenxing Mi",
                        "user": "Mifucius",
                        "type": "user"
                    },
                    "name": "Zhenxing Mi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:52.837Z",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce589e",
                    "name": "Kuan-Chieh Wang",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce589f",
                    "name": "Guocheng Qian",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce58a0",
                    "name": "Hanrong Ye",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce58a1",
                    "name": "Runtao Liu",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce58a2",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce58a3",
                    "name": "Kfir Aberman",
                    "hidden": false
                },
                {
                    "_id": "67b3ea0f4dd7ea0538ce58a4",
                    "name": "Dan Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T05:30:08.000Z",
            "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning\n  in Diffusion Models",
            "summary": "This paper presents ThinkDiff, a novel alignment paradigm that empowers\ntext-to-image diffusion models with multimodal in-context understanding and\nreasoning capabilities by integrating the strengths of vision-language models\n(VLMs). Existing multimodal diffusion finetuning methods largely focus on\npixel-level reconstruction rather than in-context reasoning, and are\nconstrained by the complexity and limited availability of reasoning-based\ndatasets. ThinkDiff addresses these challenges by leveraging vision-language\ntraining as a proxy task, aligning VLMs with the decoder of an encoder-decoder\nlarge language model (LLM) instead of a diffusion decoder. This proxy task\nbuilds on the observation that the LLM decoder shares the same input\nfeature space with diffusion decoders that use the corresponding\nLLM encoder for prompt embedding. As a result, aligning VLMs with\ndiffusion decoders can be simplified through alignment with the LLM decoder.\nWithout complex training and datasets, ThinkDiff effectively unleashes\nunderstanding, reasoning, and composing capabilities in diffusion models.\nExperiments demonstrate that ThinkDiff significantly improves accuracy from\n19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context\nreasoning generation, with only 5 hours of training on 4 A100 GPUs.\nAdditionally, ThinkDiff demonstrates exceptional performance in composing\nmultiple images and texts into logically coherent images. Project page:\nhttps://mizhenxing.github.io/ThinkDiff.",
            "upvotes": 11,
            "discussionId": "67b3ea124dd7ea0538ce592d"
        },
        "publishedAt": "2025-02-18T04:33:41.120Z",
        "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10458.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6354bda206d707b33249c4c2",
            "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
            "fullname": "Zhenxing Mi",
            "name": "Mifucius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11167",
            "authors": [
                {
                    "_id": "67b4221bbc387d2eda6f8637",
                    "user": {
                        "_id": "650267e7e751d03da933a24a",
                        "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
                        "isPro": false,
                        "fullname": "Bohan22",
                        "user": "Bohan22",
                        "type": "user"
                    },
                    "name": "Bohan Lyu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:06.388Z",
                    "hidden": false
                },
                {
                    "_id": "67b4221bbc387d2eda6f8638",
                    "name": "Siqiao Huang",
                    "hidden": false
                },
                {
                    "_id": "67b4221bbc387d2eda6f8639",
                    "user": {
                        "_id": "67286718746a95c09d04cb1d",
                        "avatarUrl": "/avatars/317efa8459cca08c2ff56c3ab116e15c.svg",
                        "isPro": false,
                        "fullname": "Zichen Liang",
                        "user": "zcliang22",
                        "type": "user"
                    },
                    "name": "Zichen Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:08.469Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T15:38:19.000Z",
            "title": "SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors",
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, such as code understanding and code generation. However, an\nequally important yet underexplored question is whether LLMs can serve as\ngeneral-purpose surrogate code executors, to predict the output and behavior of\na program without actually running it. To systematically investigate this\ncapability, we introduce SURGE, a comprehensive benchmark covering eight key\naspects: multi-language programming tasks, competition-level programming\nproblems, repository-level code analysis, high-cost scientific computing,\ntime-complexity-intensive algorithms, buggy code analysis, programs dependent\non specific compilers or execution environments, and formal mathematical proof\nverification. We evaluate multiple open-source and proprietary LLMs on SURGE\nand conduct a scaling study to analyze the impact of model size and training\ndata scale on surrogate execution accuracy. Additionally, we categorize model\nprediction errors and explore potential areas for improvement. Our findings\nindicate that while LLMs can predict code execution results in certain cases,\nthey exhibit limitations in general-purpose surrogate execution. This study\nprovides empirical insights into the feasibility of using LLMs as surrogate\ncode executors. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.",
            "upvotes": 11,
            "discussionId": "67b4221ebc387d2eda6f8717"
        },
        "publishedAt": "2025-02-18T01:01:24.331Z",
        "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11167.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650267e7e751d03da933a24a",
            "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
            "fullname": "Bohan22",
            "name": "Bohan22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.12146",
            "authors": [
                {
                    "_id": "67b40ce4d3c5f50aa9b71df5",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "67b40ce4d3c5f50aa9b71df6",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67b40ce4d3c5f50aa9b71df7",
                    "user": {
                        "_id": "653e5d31ffd60206c8b64bb5",
                        "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
                        "isPro": false,
                        "fullname": "Xinchen Zhang",
                        "user": "comin",
                        "type": "user"
                    },
                    "name": "Xinchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:29.697Z",
                    "hidden": false
                },
                {
                    "_id": "67b40ce4d3c5f50aa9b71df8",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "67b40ce4d3c5f50aa9b71df9",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "67b40ce4d3c5f50aa9b71dfa",
                    "name": "Bin Cui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T18:57:26.000Z",
            "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising\n  Trajectory Sharpening",
            "summary": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances\ndownstream alignment by optimizing sampling trajectories. Existing RL-based\nfine-tuning methods focus on single training timesteps and neglect\ntrajectory-level alignment, while recent sampling trajectory optimization\nmethods incur significant inference NFE costs. Diffusion-Sharpening overcomes\nthis by using a path integral framework to select optimal trajectories during\ntraining, leveraging reward feedback, and amortizing inference costs. Our\nmethod demonstrates superior training efficiency with faster convergence, and\nbest inference efficiency without requiring additional NFEs. Extensive\nexperiments show that Diffusion-Sharpening outperforms RL-based fine-tuning\nmethods (e.g., Diffusion-DPO) and sampling trajectory optimization methods\n(e.g., Inference Scaling) across diverse metrics including text alignment,\ncompositional capabilities, and human preferences, offering a scalable and\nefficient solution for future diffusion model fine-tuning. Code:\nhttps://github.com/Gen-Verse/Diffusion-Sharpening",
            "upvotes": 11,
            "discussionId": "67b40ce8d3c5f50aa9b71f9a"
        },
        "publishedAt": "2025-02-17T23:30:53.097Z",
        "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12146.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "fullname": "Xinchen Zhang",
            "name": "comin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11831",
            "authors": [
                {
                    "_id": "67b450cf315f7b69956df3d6",
                    "name": "Quentin Garrido",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3d7",
                    "name": "Nicolas Ballas",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3d8",
                    "name": "Mahmoud Assran",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3d9",
                    "name": "Adrien Bardes",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3da",
                    "name": "Laurent Najman",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3db",
                    "name": "Michael Rabbat",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3dc",
                    "name": "Emmanuel Dupoux",
                    "hidden": false
                },
                {
                    "_id": "67b450cf315f7b69956df3dd",
                    "name": "Yann LeCun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T14:27:14.000Z",
            "title": "Intuitive physics understanding emerges from self-supervised pretraining\n  on natural videos",
            "summary": "We investigate the emergence of intuitive physics understanding in\ngeneral-purpose deep neural network models trained to predict masked regions in\nnatural videos. Leveraging the violation-of-expectation framework, we find that\nvideo prediction models trained to predict outcomes in a learned representation\nspace demonstrate an understanding of various intuitive physics properties,\nsuch as object permanence and shape consistency. In contrast, video prediction\nin pixel space and multimodal large language models, which reason through text,\nachieve performance closer to chance. Our comparisons of these architectures\nreveal that jointly learning an abstract representation space while predicting\nmissing parts of sensory input, akin to predictive coding, is sufficient to\nacquire an understanding of intuitive physics, and that even models trained on\none week of unique video achieve above chance performance. This challenges the\nidea that core knowledge -- a set of innate systems to help understand the\nworld -- needs to be hardwired to develop an understanding of intuitive\nphysics.",
            "upvotes": 7,
            "discussionId": "67b450d0315f7b69956df3f9"
        },
        "publishedAt": "2025-02-18T04:20:25.916Z",
        "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11831.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 765
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11357",
            "authors": [
                {
                    "_id": "67b3f1f1f5bd60d66133e1f3",
                    "user": {
                        "_id": "6556717676fe5cfa6a115405",
                        "avatarUrl": "/avatars/570dd8f4eb6baaff12d7ebe11dde6348.svg",
                        "isPro": false,
                        "fullname": "Vardaan Pahuja",
                        "user": "vardaan123",
                        "type": "user"
                    },
                    "name": "Vardaan Pahuja",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:47.969Z",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1f4",
                    "user": {
                        "_id": "664bbd75a6bd1b3d2ac7fc34",
                        "avatarUrl": "/avatars/127bf5d611b46ef95a1859a8cf21a160.svg",
                        "isPro": false,
                        "fullname": "Yadong Lu",
                        "user": "adamlu1",
                        "type": "user"
                    },
                    "name": "Yadong Lu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-18T02:35:29.988Z",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1f5",
                    "name": "Corby Rosset",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1f6",
                    "name": "Boyu Gou",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1f7",
                    "name": "Arindam Mitra",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1f8",
                    "name": "Spencer Whitehead",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1f9",
                    "name": "Yu Su",
                    "hidden": false
                },
                {
                    "_id": "67b3f1f1f5bd60d66133e1fa",
                    "name": "Ahmed Awadallah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T02:13:48.000Z",
            "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for\n  Multimodal Web Agents",
            "summary": "Recent success in large multimodal models (LMMs) has sparked promising\napplications of agents capable of autonomously completing complex web tasks.\nWhile open-source LMM agents have made significant advances in offline\nevaluation benchmarks, their performance still falls substantially short of\nhuman-level capabilities in more realistic online settings. A key bottleneck is\nthe lack of diverse and large-scale trajectory-level datasets across various\ndomains, which are expensive to collect. In this paper, we address this\nchallenge by developing a scalable recipe to synthesize the largest and most\ndiverse trajectory-level dataset to date, containing over 94K successful\nmultimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and\n33M web elements. In particular, we leverage extensive web exploration and\nrefinement to obtain diverse task intents. The average cost is 28 cents per\nsuccessful trajectory, making it affordable to a wide range of users in the\ncommunity. Leveraging this dataset, we train Explorer, a multimodal web agent,\nand demonstrate strong performance on both offline and online web agent\nbenchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.\nAdditionally, our experiments highlight data scaling as a key driver for\nimproving web agent capabilities. We hope this study makes state-of-the-art\nLMM-based agent research at a larger scale more accessible.",
            "upvotes": 6,
            "discussionId": "67b3f1f1f5bd60d66133e24b"
        },
        "publishedAt": "2025-02-18T11:57:43.538Z",
        "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11357.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6556717676fe5cfa6a115405",
            "avatarUrl": "/avatars/570dd8f4eb6baaff12d7ebe11dde6348.svg",
            "fullname": "Vardaan Pahuja",
            "name": "vardaan123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11438",
            "authors": [
                {
                    "_id": "67b406993d0f54ab381594f5",
                    "name": "Jimin Lee",
                    "hidden": false
                },
                {
                    "_id": "67b406993d0f54ab381594f6",
                    "name": "Ingeol Baek",
                    "hidden": false
                },
                {
                    "_id": "67b406993d0f54ab381594f7",
                    "name": "Byeongjeong Kim",
                    "hidden": false
                },
                {
                    "_id": "67b406993d0f54ab381594f8",
                    "name": "Hwanhee Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T04:52:24.000Z",
            "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example\n  Selection for Text-to-SQL",
            "summary": "Text-to-SQL aims to convert natural language questions into executable SQL\nqueries. While previous approaches, such as skeleton-masked selection, have\ndemonstrated strong performance by retrieving similar training examples to\nguide large language models (LLMs), they struggle in real-world scenarios where\nsuch examples are unavailable. To overcome this limitation, we propose\nSelf-Augmentation in-context learning with Fine-grained Example selection for\nText-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by\ngenerating and filtering self-augmented examples. SAFE-SQL first prompts an LLM\nto generate multiple Text-to-SQL examples relevant to the test input. Then\nSAFE-SQL filters these examples through three relevance assessments,\nconstructing high-quality in-context learning examples. Using self-generated\nexamples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL\nframeworks, achieving higher execution accuracy. Notably, our approach provides\nadditional performance gains in extra hard and unseen scenarios, where\nconventional methods often fail.",
            "upvotes": 6,
            "discussionId": "67b4069a3d0f54ab38159520"
        },
        "publishedAt": "2025-02-17T23:06:03.562Z",
        "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11438.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f6f245e94ed998c46316df",
            "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
            "fullname": "ingeolbaek",
            "name": "ingeol",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.12135",
            "authors": [
                {
                    "_id": "67b4028237db78705fb256e1",
                    "user": {
                        "_id": "64fb31a34c8924c4fe7498bc",
                        "avatarUrl": "/avatars/6c8e4a66e1b8b3c786a4000210089392.svg",
                        "isPro": false,
                        "fullname": "Chaoyue Song",
                        "user": "chaoyue7",
                        "type": "user"
                    },
                    "name": "Chaoyue Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:40.771Z",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e2",
                    "name": "Jianfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e3",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e4",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e5",
                    "name": "Yiwen Chen",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e6",
                    "name": "Zhongcong Xu",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e7",
                    "name": "Jun Hao Liew",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e8",
                    "name": "Xiaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256e9",
                    "name": "Fayao Liu",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256ea",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "67b4028237db78705fb256eb",
                    "name": "Guosheng Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T18:53:27.000Z",
            "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
            "summary": "With the explosive growth of 3D content creation, there is an increasing\ndemand for automatically converting static 3D models into articulation-ready\nversions that support realistic animation. Traditional approaches rely heavily\non manual annotation, which is both time-consuming and labor-intensive.\nMoreover, the lack of large-scale benchmarks has hindered the development of\nlearning-based solutions. In this work, we present MagicArticulate, an\neffective framework that automatically transforms static 3D models into\narticulation-ready assets. Our key contributions are threefold. First, we\nintroduce Articulation-XL, a large-scale benchmark containing over 33k 3D\nmodels with high-quality articulation annotations, carefully curated from\nObjaverse-XL. Second, we propose a novel skeleton generation method that\nformulates the task as a sequence modeling problem, leveraging an\nauto-regressive transformer to naturally handle varying numbers of bones or\njoints within skeletons and their inherent dependencies across different 3D\nmodels. Third, we predict skinning weights using a functional diffusion process\nthat incorporates volumetric geodesic distance priors between vertices and\njoints. Extensive experiments demonstrate that MagicArticulate significantly\noutperforms existing methods across diverse object categories, achieving\nhigh-quality articulation that enables realistic animation. Project page:\nhttps://chaoyuesong.github.io/MagicArticulate.",
            "upvotes": 5,
            "discussionId": "67b4028437db78705fb25726"
        },
        "publishedAt": "2025-02-18T04:34:15.786Z",
        "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12135.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64fb31a34c8924c4fe7498bc",
            "avatarUrl": "/avatars/6c8e4a66e1b8b3c786a4000210089392.svg",
            "fullname": "Chaoyue Song",
            "name": "chaoyue7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11775",
            "authors": [
                {
                    "_id": "67b4147f7721b4fe4d2bd466",
                    "name": "Guangzhi Sun",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd467",
                    "name": "Yudong Yang",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd468",
                    "name": "Jimin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd469",
                    "name": "Changli Tang",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd46a",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd46b",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd46c",
                    "name": "Zejun MA",
                    "hidden": false
                },
                {
                    "_id": "67b4147f7721b4fe4d2bd46d",
                    "name": "Chao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T13:07:40.000Z",
            "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
            "summary": "While recent advancements in reasoning optimization have significantly\nenhanced the capabilities of large language models (LLMs), existing efforts to\nimprove reasoning have been limited to solving mathematical problems and\nfocusing on visual graphical inputs, neglecting broader applications in general\nvideo understanding.This paper proposes video-SALMONN-o1, the first open-source\nreasoning-enhanced audio-visual LLM designed for general video understanding\ntasks. To enhance its reasoning abilities, we develop a reasoning-intensive\ndataset featuring challenging audio-visual questions with step-by-step\nsolutions. We also propose process direct preference optimization (pDPO), which\nleverages contrastive step selection to achieve efficient step-level reward\nmodelling tailored for multimodal inputs. Additionally, we introduce RivaBench,\nthe first reasoning-intensive video understanding benchmark, featuring over\n4,000 high-quality, expert-curated question-answer pairs across scenarios such\nas standup comedy, academic presentations, and synthetic video detection.\nvideo-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision\nbaseline across different video reasoning benchmarks. Besides, pDPO achieves\n6-8% improvements compared to the supervised fine-tuning model on RivaBench.\nEnhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection\ncapabilities.",
            "upvotes": 5,
            "discussionId": "67b414827721b4fe4d2bd534"
        },
        "publishedAt": "2025-02-18T00:06:55.671Z",
        "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11775.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6137
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11275",
            "authors": [
                {
                    "_id": "67b3fa2862838a378b21860d",
                    "name": "Letian Peng",
                    "hidden": false
                },
                {
                    "_id": "67b3fa2862838a378b21860e",
                    "name": "Zilong Wang",
                    "hidden": false
                },
                {
                    "_id": "67b3fa2862838a378b21860f",
                    "name": "Feng Yao",
                    "hidden": false
                },
                {
                    "_id": "67b3fa2862838a378b218610",
                    "name": "Jingbo Shang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T21:32:20.000Z",
            "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
            "summary": "Massive high-quality data, both pre-training raw texts and post-training\nannotations, have been carefully prepared to incubate advanced large language\nmodels (LLMs). In contrast, for information extraction (IE), pre-training data,\nsuch as BIO-tagged sequences, are hard to scale up. We show that IE models can\nact as free riders on LLM resources by reframing next-token prediction\ninto extraction for tokens already present in the context. Specifically,\nour proposed next tokens extraction (NTE) paradigm learns a versatile IE model,\nCuckoo, with 102.6M extractive data converted from LLM's pre-training\nand post-training data. Under the few-shot setting, Cuckoo adapts effectively\nto traditional and complex instruction-following IE with better performance\nthan existing pre-trained IE models. As a free rider, Cuckoo can naturally\nevolve with the ongoing advancements in LLM data preparation, benefiting from\nimprovements in LLM training pipelines without additional manual effort.",
            "upvotes": 5,
            "discussionId": "67b3fa2962838a378b21867b"
        },
        "publishedAt": "2025-02-17T22:10:49.900Z",
        "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11275.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64323dd503d81fa4d26deaf9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
            "fullname": "Letian Peng",
            "name": "KomeijiForce",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.10550",
            "authors": [
                {
                    "_id": "67b478517fa6ecaa21d1498d",
                    "user": {
                        "_id": "6668687caee0993c95b0eb81",
                        "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
                        "isPro": false,
                        "fullname": "Egor Cherepanov",
                        "user": "avanturist",
                        "type": "user"
                    },
                    "name": "Egor Cherepanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T16:39:34.993Z",
                    "hidden": false
                },
                {
                    "_id": "67b478517fa6ecaa21d1498e",
                    "name": "Nikita Kachaev",
                    "hidden": false
                },
                {
                    "_id": "67b478517fa6ecaa21d1498f",
                    "name": "Alexey K. Kovalev",
                    "hidden": false
                },
                {
                    "_id": "67b478517fa6ecaa21d14990",
                    "name": "Aleksandr I. Panov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T20:46:19.000Z",
            "title": "Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with\n  Reinforcement Learning",
            "summary": "Memory is crucial for enabling agents to tackle complex tasks with temporal\nand spatial dependencies. While many reinforcement learning (RL) algorithms\nincorporate memory, the field lacks a universal benchmark to assess an agent's\nmemory capabilities across diverse scenarios. This gap is particularly evident\nin tabletop robotic manipulation, where memory is essential for solving tasks\nwith partial observability and ensuring robust performance, yet no standardized\nbenchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills\nAssessment Suite for Agents), a comprehensive benchmark for memory RL, with\nthree key contributions: (1) we propose a comprehensive classification\nframework for memory-intensive RL tasks, (2) we collect MIKASA-Base - a unified\nbenchmark that enables systematic evaluation of memory-enhanced agents across\ndiverse scenarios, and (3) we develop MIKASA-Robo - a novel benchmark of 32\ncarefully designed memory-intensive tasks that assess memory capabilities in\ntabletop robotic manipulation. Our contributions establish a unified framework\nfor advancing memory RL research, driving the development of more reliable\nsystems for real-world applications. The code is available at\nhttps://sites.google.com/view/memorybenchrobots/.",
            "upvotes": 4,
            "discussionId": "67b478557fa6ecaa21d14a24"
        },
        "publishedAt": "2025-02-18T07:16:07.632Z",
        "title": "Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6668687caee0993c95b0eb81/zl6FgeOWq-7PC7PRLEyzW.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10550.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6668687caee0993c95b0eb81",
            "avatarUrl": "/avatars/301fe1f395e0a129b1c9785868fa9858.svg",
            "fullname": "Egor Cherepanov",
            "name": "avanturist",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11157",
            "authors": [
                {
                    "_id": "67b44baa5fd91177ed7760a2",
                    "user": {
                        "_id": "6608fa4f5baec84322ec85ea",
                        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
                        "isPro": false,
                        "fullname": "Zhong",
                        "user": "Jianyuan1",
                        "type": "user"
                    },
                    "name": "Jianyuan Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:30:45.385Z",
                    "hidden": false
                },
                {
                    "_id": "67b44baa5fd91177ed7760a3",
                    "user": {
                        "_id": "664ac4f7fe822b08e6f06814",
                        "avatarUrl": "/avatars/23193494fcc8e58faf1eee5f1223aca6.svg",
                        "isPro": false,
                        "fullname": "Zeju Li",
                        "user": "zeju-0727",
                        "type": "user"
                    },
                    "name": "Zeju Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T16:39:43.674Z",
                    "hidden": false
                },
                {
                    "_id": "67b44baa5fd91177ed7760a4",
                    "name": "Zhijian Xu",
                    "hidden": false
                },
                {
                    "_id": "67b44baa5fd91177ed7760a5",
                    "user": {
                        "_id": "641b1b36a5f876fe30c49542",
                        "avatarUrl": "/avatars/ac9267925f45d325c2adb2eb0e38077b.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Wen",
                        "user": "XiangyuWen",
                        "type": "user"
                    },
                    "name": "Xiangyu Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T16:39:45.646Z",
                    "hidden": false
                },
                {
                    "_id": "67b44baa5fd91177ed7760a6",
                    "name": "Qiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T15:11:19.000Z",
            "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
            "summary": "We present Dyve, a dynamic process verifier that enhances reasoning error\ndetection in large language models by integrating fast and slow thinking,\ninspired by Kahneman's Systems Theory. Dyve adaptively applies immediate\ntoken-level confirmation System 1 for straightforward steps and comprehensive\nanalysis System 2 for complex ones. Leveraging a novel step-wise\nconsensus-filtered process supervision technique, combining Monte Carlo\nestimation with LLM based evaluation, Dyve curates high-quality supervision\nsignals from noisy data. Experimental results on ProcessBench and the MATH\ndataset confirm that Dyve significantly outperforms existing process-based\nverifiers and boosts performance in Best-of-N settings.",
            "upvotes": 4,
            "discussionId": "67b44bab5fd91177ed7760ca"
        },
        "publishedAt": "2025-02-18T06:33:31.888Z",
        "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6608fa4f5baec84322ec85ea/iiYwe_FlXRwT1RjPvzF-b.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11157.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "fullname": "Zhong",
            "name": "Jianyuan1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.12054",
            "authors": [
                {
                    "_id": "67b44a6888813676da9f8239",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f823a",
                    "name": "Yuxuan Dong",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f823b",
                    "name": "Yanrui Wu",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f823c",
                    "name": "Jiaxing Huang",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f823d",
                    "user": {
                        "_id": "6602548a68d519ed324b47c5",
                        "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
                        "isPro": false,
                        "fullname": "ChengyouJia",
                        "user": "ChengyouJia",
                        "type": "user"
                    },
                    "name": "Chengyou Jia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:30:47.313Z",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f823e",
                    "name": "Basura Fernando",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f823f",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f8240",
                    "name": "Lingling Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b44a6888813676da9f8241",
                    "name": "Jun Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T17:24:14.000Z",
            "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
            "summary": "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:/dxzxy12138.github.io/PhysReason.",
            "upvotes": 4,
            "discussionId": "67b44a6988813676da9f82d0"
        },
        "publishedAt": "2025-02-18T03:53:47.570Z",
        "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12054.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6602548a68d519ed324b47c5",
            "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
            "fullname": "ChengyouJia",
            "name": "ChengyouJia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11330",
            "authors": [
                {
                    "_id": "67b42c5632929e97a92dee90",
                    "name": "Minbyul Jeong",
                    "hidden": false
                },
                {
                    "_id": "67b42c5632929e97a92dee91",
                    "name": "Jungho Cho",
                    "hidden": false
                },
                {
                    "_id": "67b42c5632929e97a92dee92",
                    "name": "Minsoo Khang",
                    "hidden": false
                },
                {
                    "_id": "67b42c5632929e97a92dee93",
                    "name": "Dawoon Jung",
                    "hidden": false
                },
                {
                    "_id": "67b42c5632929e97a92dee94",
                    "name": "Teakgyu Hong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T01:05:31.000Z",
            "title": "System Message Generation for User Preferences using Open-Source Models",
            "summary": "System messages play a crucial role in interactions with large language\nmodels (LLMs), often serving as prompts to initiate conversations. Through\nsystem messages, users can assign specific roles, perform intended tasks,\nincorporate background information, specify various output formats and\ncommunication styles. Despite such versatility, publicly available data are\noften lack system messages and subject to strict license constraints in the\nindustry field. Manual labeling of publicly available data with system messages\nthat align with user instructions demands significant resources. In view of\nsuch challenges, our work introduces SysGen, a pipeline for generating system\nmessages with better aligned assistant responses from the supervised\nfine-tuning dataset without system messages. Training on SysGen data has\ndemonstrated substantial improvements in the alignment of model responses with\nsystem messages and user instructions, as demonstrated across various\nopen-source models on the Multifacet benchmark, while maintaining minimal\nimpact on other unseen benchmarks such as Open LLM Leaderboard 2. Our\nqualitative analysis highlights the importance of diverse system messages to\nensure better adaptability across different contexts.",
            "upvotes": 4,
            "discussionId": "67b42c5732929e97a92deed7"
        },
        "publishedAt": "2025-02-18T01:45:36.359Z",
        "title": "System Message Generation for User Preferences using Open-Source Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11330.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64587be872b60ae7a3817858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
            "fullname": "Minbyul Jeong",
            "name": "Minbyul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11098",
            "authors": [
                {
                    "_id": "67b411e45e634139c0d86a1e",
                    "name": "Zhao Wang",
                    "hidden": false
                },
                {
                    "_id": "67b411e45e634139c0d86a1f",
                    "name": "Sota Moriyama",
                    "hidden": false
                },
                {
                    "_id": "67b411e45e634139c0d86a20",
                    "name": "Wei-Yao Wang",
                    "hidden": false
                },
                {
                    "_id": "67b411e45e634139c0d86a21",
                    "name": "Briti Gangopadhyay",
                    "hidden": false
                },
                {
                    "_id": "67b411e45e634139c0d86a22",
                    "name": "Shingo Takamatsu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T12:26:58.000Z",
            "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM\n  Multi-Agent Systems",
            "summary": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown\npromise, yet significant challenges remain in managing communication and\nrefinement when agents collaborate on complex tasks. In this paper, we propose\nTalk Structurally, Act Hierarchically (TalkHier), a novel framework\nthat introduces a structured communication protocol for context-rich exchanges\nand a hierarchical refinement system to address issues such as incorrect\noutputs, falsehoods, and biases. TalkHier surpasses various types of\nSoTA, including inference scaling model (OpenAI-o1), open-source multi-agent\nmodels (e.g., AgentVerse), and majority voting strategies on current LLM and\nsingle-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including\nopen-domain question answering, domain-specific selective questioning, and\npractical advertisement text generation. These results highlight its potential\nto set a new standard for LLM-MA systems, paving the way for more effective,\nadaptable, and collaborative multi-agent frameworks. The code is available\nhttps://github.com/sony/talkhier.",
            "upvotes": 4,
            "discussionId": "67b411e55e634139c0d86a4c"
        },
        "publishedAt": "2025-02-17T23:51:50.821Z",
        "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11098.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6137
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11901",
            "authors": [
                {
                    "_id": "67b3f8cc1bfe04e82830b752",
                    "name": "Dylan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b3f8cc1bfe04e82830b753",
                    "name": "Justin Wang",
                    "hidden": false
                },
                {
                    "_id": "67b3f8cc1bfe04e82830b754",
                    "name": "Tianran Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T15:24:11.000Z",
            "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o\n  Under Data Scarsity",
            "summary": "Existing LMs struggle with proof-oriented programming due to data scarcity,\nwhich manifest in two key ways: (1) a lack of sufficient corpora for\nproof-oriented programming languages such as F*, and (2) the absence of\nlarge-scale, project-level proof-oriented implementations that can teach the\nmodel the intricate reasoning process when performing proof-oriented\nprogramming. We present the first on synthetic data augmentation for project\nlevel proof oriented programming for both generation and repair. Our method\naddresses data scarcity by synthesizing basic proof-oriented programming\nproblems for proficiency in that language; incorporating diverse coding data\nfor reasoning capability elicitation and creating new proofs and repair data\nwithin existing repositories. This approach enables language models to both\nsynthesize and repair proofs for function- and repository-level code. We show\nthat our fine-tuned 14B parameter model, PoPilot, can exceed the performance of\nthe models that outperforms GPT-4o in project-level proof-oriented programming\nby 64% relative margin, and can improve GPT-4o's performance by 54% by\nrepairing its outputs over GPT-4o's self-repair.",
            "upvotes": 4,
            "discussionId": "67b3f8cd1bfe04e82830b77f"
        },
        "publishedAt": "2025-02-17T22:05:54.047Z",
        "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11901.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "642b8add48f67b6f21d4eb20",
            "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
            "fullname": "Dylan",
            "name": "shizhuo2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.09509",
            "authors": [
                {
                    "_id": "67b4e4259beded220ad14729",
                    "name": "Theodoros Kouzelis",
                    "hidden": false
                },
                {
                    "_id": "67b4e4259beded220ad1472a",
                    "name": "Ioannis Kakogeorgiou",
                    "hidden": false
                },
                {
                    "_id": "67b4e4259beded220ad1472b",
                    "name": "Spyros Gidaris",
                    "hidden": false
                },
                {
                    "_id": "67b4e4259beded220ad1472c",
                    "name": "Nikos Komodakis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T17:21:51.000Z",
            "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling",
            "summary": "Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.",
            "upvotes": 3,
            "discussionId": "67b4e4289beded220ad147c7"
        },
        "publishedAt": "2025-02-18T14:56:45.613Z",
        "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/661ba524bd9243bf7e598355/9XkVow22TY84dDgXm-Duc.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09509.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661ba524bd9243bf7e598355",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661ba524bd9243bf7e598355/i77yD4XgJn2vUbn_mIsT8.jpeg",
            "fullname": "Ioannis Kakogeorgiou",
            "name": "gkakogeorgiou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11748",
            "authors": [
                {
                    "_id": "67b465600e5142133055d7c1",
                    "name": "Giorgos Kordopatis-Zilos",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c2",
                    "user": {
                        "_id": "66a3ae59f33ff23e1c027ccd",
                        "avatarUrl": "/avatars/216717d547bf785a2b1696171e5f4b11.svg",
                        "isPro": false,
                        "fullname": "Vladan Stojnic",
                        "user": "stojnvla",
                        "type": "user"
                    },
                    "name": "Vladan Stojni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T16:39:41.653Z",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c3",
                    "name": "Anna Manko",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c4",
                    "name": "Pavel uma",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c5",
                    "name": "Nikolaos-Antonios Ypsilantis",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c6",
                    "name": "Nikos Efthymiadis",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c7",
                    "name": "Zakaria Laskar",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c8",
                    "name": "Ji Matas",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7c9",
                    "name": "Ondej Chum",
                    "hidden": false
                },
                {
                    "_id": "67b465600e5142133055d7ca",
                    "name": "Giorgos Tolias",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T12:42:38.000Z",
            "title": "ILIAS: Instance-Level Image retrieval At Scale",
            "summary": "This work introduces ILIAS, a new test dataset for Instance-Level Image\nretrieval At Scale. It is designed to evaluate the ability of current and\nfuture foundation models and retrieval techniques to recognize particular\nobjects. The key benefits over existing datasets include large scale, domain\ndiversity, accurate ground truth, and a performance that is far from saturated.\nILIAS includes query and positive images for 1,000 object instances, manually\ncollected to capture challenging conditions and diverse domains. Large-scale\nretrieval is conducted against 100 million distractor images from YFCC100M. To\navoid false negatives without extra annotation effort, we include only query\nobjects confirmed to have emerged after 2014, i.e. the compilation date of\nYFCC100M. An extensive benchmarking is performed with the following\nobservations: i) models fine-tuned on specific domains, such as landmarks or\nproducts, excel in that domain but fail on ILIAS ii) learning a linear\nadaptation layer using multi-domain class supervision results in performance\nimprovements, especially for vision-language models iii) local descriptors in\nretrieval re-ranking are still a key ingredient, especially in the presence of\nsevere background clutter iv) the text-to-image performance of the\nvision-language foundation models is surprisingly close to the corresponding\nimage-to-image case. website: https://vrg.fel.cvut.cz/ilias/",
            "upvotes": 3,
            "discussionId": "67b465680e5142133055d97d"
        },
        "publishedAt": "2025-02-18T11:42:58.976Z",
        "title": "ILIAS: Instance-Level Image retrieval At Scale",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11748.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a3ae59f33ff23e1c027ccd",
            "avatarUrl": "/avatars/216717d547bf785a2b1696171e5f4b11.svg",
            "fullname": "Vladan Stojnic",
            "name": "stojnvla",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.10454",
            "authors": [
                {
                    "_id": "67b40e56bffd44cc85976ecd",
                    "name": "Yinghui Li",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ece",
                    "name": "Jiayi Kuang",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ecf",
                    "name": "Haojing Huang",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed0",
                    "name": "Zhikun Xu",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed1",
                    "name": "Xinnian Liang",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed2",
                    "name": "Yi Yu",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed3",
                    "name": "Wenlian Lu",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed4",
                    "name": "Yangning Li",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed5",
                    "name": "Xiaoyu Tan",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed6",
                    "name": "Chao Qu",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed7",
                    "name": "Ying Shen",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed8",
                    "name": "Hai-Tao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67b40e56bffd44cc85976ed9",
                    "name": "Philip S. Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T02:01:10.000Z",
            "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual\n  Reasoning in Mathematical LLMs",
            "summary": "Leveraging mathematical Large Language Models (LLMs) for proof generation is\na fundamental topic in LLMs research. We argue that the ability of current LLMs\nto prove statements largely depends on whether they have encountered the\nrelevant proof process during training. This reliance limits their deeper\nunderstanding of mathematical theorems and related concepts. Inspired by the\npedagogical method of \"proof by counterexamples\" commonly used in human\nmathematics education, our work aims to enhance LLMs' ability to conduct\nmathematical reasoning and proof through counterexamples. Specifically, we\nmanually create a high-quality, university-level mathematical benchmark,\nCounterMATH, which requires LLMs to prove mathematical statements by providing\ncounterexamples, thereby assessing their grasp of mathematical concepts.\nAdditionally, we develop a data engineering framework to automatically obtain\ntraining data for further model improvement. Extensive experiments and detailed\nanalyses demonstrate that CounterMATH is challenging, indicating that LLMs,\nsuch as OpenAI o1, have insufficient counterexample-driven proof capabilities.\nMoreover, our exploration into model training reveals that strengthening LLMs'\ncounterexample-driven conceptual reasoning abilities is crucial for improving\ntheir overall mathematical capabilities. We believe that our work offers new\nperspectives on the community of mathematical LLMs.",
            "upvotes": 3,
            "discussionId": "67b40e57bffd44cc85976f0e"
        },
        "publishedAt": "2025-02-17T23:37:16.770Z",
        "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10454.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6137
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.08826",
            "authors": [
                {
                    "_id": "67b303f18bd6e9a5cad8bc4d",
                    "user": {
                        "_id": "64ba58d377dd483716aba098",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba58d377dd483716aba098/6VASAUkFpDC-PR01yUJWj.png",
                        "isPro": false,
                        "fullname": "Mahdi Abootorabi",
                        "user": "aboots",
                        "type": "user"
                    },
                    "name": "Mohammad Mahdi Abootorabi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-17T09:40:27.588Z",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc4e",
                    "name": "Amirhosein Zobeiri",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc4f",
                    "name": "Mahdi Dehghani",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc50",
                    "name": "Mohammadali Mohammadkhani",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc51",
                    "name": "Bardia Mohammadi",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc52",
                    "name": "Omid Ghahroodi",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc53",
                    "user": {
                        "_id": "661a88f0bcd78151e521bc60",
                        "avatarUrl": "/avatars/bedab01ce7909ecde7a60f891770c18c.svg",
                        "isPro": false,
                        "fullname": "Mahdieh Soleymani Baghshah",
                        "user": "Soleymani",
                        "type": "user"
                    },
                    "name": "Mahdieh Soleymani Baghshah",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-17T09:40:02.049Z",
                    "hidden": false
                },
                {
                    "_id": "67b303f18bd6e9a5cad8bc54",
                    "name": "Ehsaneddin Asgari",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T22:33:41.000Z",
            "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation",
            "summary": "Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
            "upvotes": 2,
            "discussionId": "67b303f28bd6e9a5cad8bc85"
        },
        "publishedAt": "2025-02-18T13:59:31.380Z",
        "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ba58d377dd483716aba098/N0fZ0I60EfZjITEnf6gPc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ba58d377dd483716aba098/CtLxMqUEhWr6d9ztU1YZq.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64ba58d377dd483716aba098/HczPPOjzArOwgdwb5yv5Z.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08826.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ba58d377dd483716aba098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba58d377dd483716aba098/6VASAUkFpDC-PR01yUJWj.png",
            "fullname": "Mahdi Abootorabi",
            "name": "aboots",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.08820",
            "authors": [
                {
                    "_id": "67aece59f2e8a2ee35b5affd",
                    "user": {
                        "_id": "63888d3fd68e37abd599f428",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                        "isPro": true,
                        "fullname": "emre can",
                        "user": "emrecanacikgoz",
                        "type": "user"
                    },
                    "name": "Emre Can Acikgoz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:34:01.421Z",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5affe",
                    "name": "Jeremiah Greer",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5afff",
                    "name": "Akul Datta",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5b000",
                    "name": "Ze Yang",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5b001",
                    "name": "William Zeng",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5b002",
                    "name": "Oussama Elachqar",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5b003",
                    "name": "Emmanouil Koukoumidis",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5b004",
                    "name": "Dilek Hakkani-Tr",
                    "hidden": false
                },
                {
                    "_id": "67aece59f2e8a2ee35b5b005",
                    "name": "Gokhan Tur",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T22:18:34.000Z",
            "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CALM: A Unified Conversational Agentic Language Model",
            "summary": "Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and\nCALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks.",
            "upvotes": 2,
            "discussionId": "67aece5af2e8a2ee35b5b03e"
        },
        "publishedAt": "2025-02-18T08:59:34.204Z",
        "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08820.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63888d3fd68e37abd599f428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
            "fullname": "emre can",
            "name": "emrecanacikgoz",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11085",
            "authors": [
                {
                    "_id": "67b44f44620ae0bad17d6699",
                    "name": "Yasir Ghunaim",
                    "hidden": false
                },
                {
                    "_id": "67b44f44620ae0bad17d669a",
                    "user": {
                        "_id": "642b51385bf2355d02a23d15",
                        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
                        "isPro": true,
                        "fullname": "Hasan Abed Al Kader Hammoud",
                        "user": "hammh0a",
                        "type": "user"
                    },
                    "name": "Hasan Abed Al Kader Hammoud",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:30:43.057Z",
                    "hidden": false
                },
                {
                    "_id": "67b44f44620ae0bad17d669b",
                    "name": "Bernard Ghanem",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T11:46:23.000Z",
            "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
            "summary": "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
            "upvotes": 2,
            "discussionId": "67b44f45620ae0bad17d66b0"
        },
        "publishedAt": "2025-02-18T04:16:28.219Z",
        "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/bLvTbh56AkUmcmRst8mT3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11085.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "642b51385bf2355d02a23d15",
            "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
            "fullname": "Hasan Abed Al Kader Hammoud",
            "name": "hammh0a",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11574",
            "authors": [
                {
                    "_id": "67b435c29e5685b308a8edac",
                    "user": {
                        "_id": "65bcbc01d6d0ffbceb8b2e6e",
                        "avatarUrl": "/avatars/73edb2d6b7b11208439ac88b365079e8.svg",
                        "isPro": false,
                        "fullname": "Johan Boye",
                        "user": "jboye",
                        "type": "user"
                    },
                    "name": "Johan Boye",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-18T07:24:50.956Z",
                    "hidden": false
                },
                {
                    "_id": "67b435c29e5685b308a8edad",
                    "user": {
                        "_id": "6033e34a9aa44495c80dd043",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
                        "isPro": false,
                        "fullname": "Birger Moell",
                        "user": "birgermoell",
                        "type": "user"
                    },
                    "name": "Birger Moell",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:30:49.328Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T09:07:32.000Z",
            "title": "Large Language Models and Mathematical Reasoning Failures",
            "summary": "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.",
            "upvotes": 2,
            "discussionId": "67b435c29e5685b308a8edf1"
        },
        "publishedAt": "2025-02-18T02:26:18.856Z",
        "title": "Large Language Models and Mathematical Reasoning Failures",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11574.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6033e34a9aa44495c80dd043",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
            "fullname": "Birger Moell",
            "name": "birgermoell",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 36
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09969",
            "authors": [
                {
                    "_id": "67b4cb6c777b7676c8b3c43d",
                    "user": {
                        "_id": "6391e4e984afa726d66180b9",
                        "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
                        "isPro": false,
                        "fullname": "Ishika Agarwal",
                        "user": "ishikaa",
                        "type": "user"
                    },
                    "name": "Ishika Agarwal",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-18T18:06:42.786Z",
                    "hidden": false
                },
                {
                    "_id": "67b4cb6c777b7676c8b3c43e",
                    "name": "Dilek Hakkani-Tr",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-14T07:55:47.000Z",
            "title": "Data Valuation using Neural Networks for Efficient Instruction\n  Fine-Tuning",
            "summary": "Influence functions provide crucial insights into model training, but\nexisting methods suffer from large computational costs and limited\ngeneralization. Particularly, recent works have proposed various metrics and\nalgorithms to calculate the influence of data using language models, which do\nnot scale well with large models and datasets. This is because of the expensive\nforward and backward passes required for computation, substantial memory\nrequirements to store large models, and poor generalization of influence\nestimates to new data. In this paper, we explore the use of small neural\nnetworks -- which we refer to as the InfluenceNetwork -- to estimate influence\nvalues, achieving up to 99% cost reduction. Our evaluation demonstrates that\ninfluence values can be estimated with models just 0.0027% the size of full\nlanguage models (we use 7B and 8B versions). We apply our algorithm of\nestimating influence values (called NN-CIFT: Neural Networks for effiCient\nInstruction Fine-Tuning) to the downstream task of subset selection for general\ninstruction fine-tuning. In our study, we include four state-of-the-art\ninfluence functions and show no compromise in performance, despite large\nspeedups, between NN-CIFT and the original influence functions. We provide an\nin-depth hyperparameter analyses of NN-CIFT. The code for our method can be\nfound here: https://github.com/agarwalishika/NN-CIFT.",
            "upvotes": 1,
            "discussionId": "67b4cb6d777b7676c8b3c45c"
        },
        "publishedAt": "2025-02-18T13:04:04.423Z",
        "title": "Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09969.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6391e4e984afa726d66180b9",
            "avatarUrl": "/avatars/e437e2820745b522a868b8da27d9a11f.svg",
            "fullname": "Ishika Agarwal",
            "name": "ishikaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.08441",
            "authors": [
                {
                    "_id": "67b30311a2b3622dd42a51ff",
                    "user": {
                        "_id": "66867e1675f10ce7ef96180e",
                        "avatarUrl": "/avatars/ac85c00ba9d4dc48887b8864a0626743.svg",
                        "isPro": false,
                        "fullname": "Felix Stollenwerk",
                        "user": "flxst",
                        "type": "user"
                    },
                    "name": "Felix Stollenwerk",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:32:36.770Z",
                    "hidden": false
                },
                {
                    "_id": "67b30311a2b3622dd42a5200",
                    "name": "Tobias Stollenwerk",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-12T14:32:17.000Z",
            "title": "Better Embeddings with Coupled Adam",
            "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
            "upvotes": 1,
            "discussionId": "67b30312a2b3622dd42a522d"
        },
        "publishedAt": "2025-02-18T05:28:54.029Z",
        "title": "Better Embeddings with Coupled Adam",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08441.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66867e1675f10ce7ef96180e",
            "avatarUrl": "/avatars/ac85c00ba9d4dc48887b8864a0626743.svg",
            "fullname": "Felix Stollenwerk",
            "name": "flxst",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.09083",
            "authors": [
                {
                    "_id": "67b30726d4665a0448e6436d",
                    "user": {
                        "_id": "6698cffdb2ebada9f4a7e7d7",
                        "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
                        "isPro": false,
                        "fullname": "Greta Warren",
                        "user": "gretawarren",
                        "type": "user"
                    },
                    "name": "Greta Warren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:32:34.585Z",
                    "hidden": false
                },
                {
                    "_id": "67b30726d4665a0448e6436e",
                    "name": "Irina Shklovski",
                    "hidden": false
                },
                {
                    "_id": "67b30726d4665a0448e6436f",
                    "name": "Isabelle Augenstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-13T08:56:25.000Z",
            "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated\n  Fact-Checking",
            "summary": "The pervasiveness of large language models and generative AI in online media\nhas amplified the need for effective automated fact-checking to assist\nfact-checkers in tackling the increasing volume and sophistication of\nmisinformation. The complex nature of fact-checking demands that automated\nfact-checking systems provide explanations that enable fact-checkers to\nscrutinise their outputs. However, it is unclear how these explanations should\nalign with the decision-making and reasoning processes of fact-checkers to be\neffectively integrated into their workflows. Through semi-structured interviews\nwith fact-checking professionals, we bridge this gap by: (i) providing an\naccount of how fact-checkers assess evidence, make decisions, and explain their\nprocesses; (ii) examining how fact-checkers use automated tools in practice;\nand (iii) identifying fact-checker explanation requirements for automated\nfact-checking tools. The findings show unmet explanation needs and identify\nimportant criteria for replicable fact-checking explanations that trace the\nmodel's reasoning path, reference specific evidence, and highlight uncertainty\nand information gaps.",
            "upvotes": 1,
            "discussionId": "67b30727d4665a0448e6438d"
        },
        "publishedAt": "2025-02-18T04:37:21.573Z",
        "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6698cffdb2ebada9f4a7e7d7/55xAEeg9Xsk87DXHTH9gM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09083.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "fullname": "Greta Warren",
            "name": "gretawarren",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11336",
            "authors": [
                {
                    "_id": "67b52de36007d463b988b202",
                    "name": "Ryuto Koike",
                    "hidden": false
                },
                {
                    "_id": "67b52de36007d463b988b203",
                    "name": "Masahiro Kaneko",
                    "hidden": false
                },
                {
                    "_id": "67b52de36007d463b988b204",
                    "name": "Ayana Niwa",
                    "hidden": false
                },
                {
                    "_id": "67b52de36007d463b988b205",
                    "name": "Preslav Nakov",
                    "hidden": false
                },
                {
                    "_id": "67b52de36007d463b988b206",
                    "name": "Naoaki Okazaki",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T01:15:07.000Z",
            "title": "ExaGPT: Example-Based Machine-Generated Text Detection for Human\n  Interpretability",
            "summary": "Detecting texts generated by Large Language Models (LLMs) could cause grave\nmistakes due to incorrect decisions, such as undermining student's academic\ndignity. LLM text detection thus needs to ensure the interpretability of the\ndecision, which can help users judge how reliably correct its prediction is.\nWhen humans verify whether a text is human-written or LLM-generated, they\nintuitively investigate with which of them it shares more similar spans.\nHowever, existing interpretable detectors are not aligned with the human\ndecision-making process and fail to offer evidence that users easily\nunderstand. To bridge this gap, we introduce ExaGPT, an interpretable detection\napproach grounded in the human decision-making process for verifying the origin\nof a text. ExaGPT identifies a text by checking whether it shares more similar\nspans with human-written vs. with LLM-generated texts from a datastore. This\napproach can provide similar span examples that contribute to the decision for\neach span in the text as evidence. Our human evaluation demonstrates that\nproviding similar span examples contributes more effectively to judging the\ncorrectness of the decision than existing interpretable methods. Moreover,\nextensive experiments in four domains and three generators show that ExaGPT\nmassively outperforms prior powerful detectors by up to +40.9 points of\naccuracy at a false positive rate of 1%.",
            "upvotes": 0,
            "discussionId": "67b52de46007d463b988b279"
        },
        "publishedAt": "2025-02-18T20:05:09.186Z",
        "title": "ExaGPT: Example-Based Machine-Generated Text Detection for Human Interpretability",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6538e649f940c8a0358aa8b8/LTS6uI3uy5AxEeoD9-oMX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11336.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6538e649f940c8a0358aa8b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538e649f940c8a0358aa8b8/veNw6QJuZu8anWCXtOXxu.jpeg",
            "fullname": "Ryuto Koike",
            "name": "ryuryukke",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.12154",
            "authors": [
                {
                    "_id": "67b400719ff3ff79dae14701",
                    "user": {
                        "_id": "6372f265112fb535baf254c4",
                        "avatarUrl": "/avatars/9b821bc533175c7dded48cdb3a3e1a12.svg",
                        "isPro": false,
                        "fullname": "tzco",
                        "user": "tzco",
                        "type": "user"
                    },
                    "name": "Zhicong Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:31:45.361Z",
                    "hidden": false
                },
                {
                    "_id": "67b400719ff3ff79dae14702",
                    "name": "Jianmin Bao",
                    "hidden": false
                },
                {
                    "_id": "67b400719ff3ff79dae14703",
                    "name": "Dong Chen",
                    "hidden": false
                },
                {
                    "_id": "67b400719ff3ff79dae14704",
                    "name": "Baining Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T18:59:50.000Z",
            "title": "Diffusion Models without Classifier-free Guidance",
            "summary": "This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.",
            "upvotes": 0,
            "discussionId": "67b400789ff3ff79dae147ee"
        },
        "publishedAt": "2025-02-18T18:58:34.838Z",
        "title": "Diffusion Models without Classifier-free Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12154.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6372f265112fb535baf254c4",
            "avatarUrl": "/avatars/9b821bc533175c7dded48cdb3a3e1a12.svg",
            "fullname": "tzco",
            "name": "tzco",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.11177",
            "authors": [
                {
                    "_id": "67b47dd2e638b35196b8e014",
                    "name": "Wanli Yang",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e015",
                    "name": "Fei Sun",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e016",
                    "name": "Jiajun Tan",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e017",
                    "name": "Xinyu Ma",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e018",
                    "name": "Qi Cao",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e019",
                    "name": "Dawei Yin",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e01a",
                    "name": "Huawei Shen",
                    "hidden": false
                },
                {
                    "_id": "67b47dd2e638b35196b8e01b",
                    "name": "Xueqi Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-16T15:57:55.000Z",
            "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild",
            "summary": "Despite near-perfect results in artificial evaluations, the effectiveness of\nmodel editing in real-world applications remains unexplored. To bridge this\ngap, we propose to study model editing in question answering (QA) by\nestablishing a rigorous evaluation practice to assess the effectiveness of\nediting methods in correcting LLMs' errors. It consists of QAEdit, a new\nbenchmark derived from popular QA datasets, and a standardized evaluation\nframework. Our single editing experiments indicate that current editing methods\nperform substantially worse than previously reported (38.5% vs. ~96%). Through\nmodule analysis and controlled experiments, we demonstrate that this\nperformance decline stems from issues in evaluation practices of prior editing\nresearch. One key issue is the inappropriate use of teacher forcing in testing\nprevents error propagation by feeding ground truth tokens (inaccessible in\nreal-world scenarios) as input. Furthermore, we simulate real-world deployment\nby sequential editing, revealing that current approaches fail drastically with\nonly 1000 edits. Our analysis provides a fundamental reexamination of both the\nreal-world applicability of existing model editing methods and their evaluation\npractices, and establishes a rigorous evaluation framework with key insights to\nadvance reliable and practical model editing research.",
            "upvotes": 0,
            "discussionId": "67b47dd2e638b35196b8e03a"
        },
        "publishedAt": "2025-02-18T07:33:17.294Z",
        "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e4090f222b232f03fe5f63",
            "avatarUrl": "/avatars/1e97328de374d726f64bf16528d36ca4.svg",
            "fullname": "Wanli Yang",
            "name": "WenDingY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.11578",
            "authors": [
                {
                    "_id": "67b435475bff5f34c1ebee1b",
                    "user": {
                        "_id": "6033e34a9aa44495c80dd043",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
                        "isPro": false,
                        "fullname": "Birger Moell",
                        "user": "birgermoell",
                        "type": "user"
                    },
                    "name": "Birger Moell",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-18T09:30:52.639Z",
                    "hidden": false
                },
                {
                    "_id": "67b435475bff5f34c1ebee1c",
                    "user": {
                        "_id": "65bcbc01d6d0ffbceb8b2e6e",
                        "avatarUrl": "/avatars/73edb2d6b7b11208439ac88b365079e8.svg",
                        "isPro": false,
                        "fullname": "Johan Boye",
                        "user": "jboye",
                        "type": "user"
                    },
                    "name": "Johan Boye",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-18T07:22:48.554Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-17T09:09:58.000Z",
            "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for\n  Evaluating LLM Performance",
            "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage generation but often face challenges in tasks requiring precise\ncalculations and structural analysis. This paper investigates the performance\nof state-of-the-art LLMs on language complexity measurement tasks, through the\ncomputation of the LIX readability metric and Average Dependency Distance\n(ADD). Using Swedish high school and university-level essays, we evaluate the\nmodels' abilities to compute LIX scores and perform dependency parsing,\ncomparing their results to established ground truths. Our findings reveal that\nwhile all models demonstrate some capacity for these tasks, ChatGPT-o1-mini\nperforms most consistently, achieving the highest accuracy in both LIX\ncomputation and dependency parsing. Additionally, we observe a strong\nsignificant correlation -0.875 p 0.026 (N=6) between the models' accuracy in\ncomputing LIX and their overall performance on the Massive Multitask Language\nUnderstanding (MMLU) benchmark. These results suggest that language complexity\nmeasurement abilities can serve as a noisy zero-shot proxies for assessing the\ngeneral capabilities of LLMs, providing a practical method for model evaluation\nwithout the need for extensive benchmarking datasets.",
            "upvotes": 0,
            "discussionId": "67b435485bff5f34c1ebee52"
        },
        "publishedAt": "2025-02-18T02:23:29.869Z",
        "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11578.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6033e34a9aa44495c80dd043",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
            "fullname": "Birger Moell",
            "name": "birgermoell",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 36
        },
        "isAuthorParticipating": true
    }
]