[
    {
        "paper": {
            "id": "2502.14499",
            "authors": [
                {
                    "_id": "67b7ee1dfedfe971271dcca0",
                    "user": {
                        "_id": "6114c9fae7a2566ae7d1a1a7",
                        "avatarUrl": "/avatars/c71ab1850322fcf5ef239cb8d31cb137.svg",
                        "isPro": false,
                        "fullname": "Deepak Nathani",
                        "user": "dnathani",
                        "type": "user"
                    },
                    "name": "Deepak Nathani",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-21T07:20:46.836Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca1",
                    "name": "Lovish Madaan",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca2",
                    "name": "Nicholas Roberts",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca3",
                    "user": {
                        "_id": "633476edc3cb9eda9328e556",
                        "avatarUrl": "/avatars/a127e270a606c18623fe00cd723313f6.svg",
                        "isPro": false,
                        "fullname": "Nikolay B",
                        "user": "bashnick",
                        "type": "user"
                    },
                    "name": "Nikolay Bashlykov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:58.738Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca4",
                    "name": "Ajay Menon",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca5",
                    "name": "Vincent Moens",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca6",
                    "name": "Amar Budhiraja",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca7",
                    "name": "Despoina Magka",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca8",
                    "name": "Vladislav Vorotilov",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dcca9",
                    "name": "Gaurav Chaurasia",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccaa",
                    "name": "Dieuwke Hupkes",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccab",
                    "user": {
                        "_id": "67b8749ffa8442592bce008e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ctbzupAxRhcNXDka75ANi.png",
                        "isPro": false,
                        "fullname": "Ricardo Silveira Cabral",
                        "user": "rscabral",
                        "type": "user"
                    },
                    "name": "Ricardo Silveira Cabral",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:55.449Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccac",
                    "user": {
                        "_id": "60dc2eb60037b630c5df57aa",
                        "avatarUrl": "/avatars/fbe707b1231a3d9dc6e87ec011e0e738.svg",
                        "isPro": false,
                        "fullname": "Tatiana",
                        "user": "Shavrina",
                        "type": "user"
                    },
                    "name": "Tatiana Shavrina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:43:03.777Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccad",
                    "name": "Jakob Foerster",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccae",
                    "user": {
                        "_id": "671b9e3ba54e59639d597fcb",
                        "avatarUrl": "/avatars/95201b98d1cf2ffa68d23f3b74e387fb.svg",
                        "isPro": false,
                        "fullname": "Yoram Bachrach",
                        "user": "yorambac",
                        "type": "user"
                    },
                    "name": "Yoram Bachrach",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:43:01.304Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccaf",
                    "name": "William Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7ee1dfedfe971271dccb0",
                    "user": {
                        "_id": "633e94793a17ab61de8e2b9c",
                        "avatarUrl": "/avatars/5f2f58ddeed211393660ada6b135f0d5.svg",
                        "isPro": false,
                        "fullname": "Roberta Raileanu",
                        "user": "rraileanu",
                        "type": "user"
                    },
                    "name": "Roberta Raileanu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-21T03:08:15.471Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T12:28:23.000Z",
            "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
            "summary": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for\nevaluating and developing LLM agents on AI research tasks. This is the first\nGym environment for machine learning (ML) tasks, enabling research on\nreinforcement learning (RL) algorithms for training such agents. MLGym-bench\nconsists of 13 diverse and open-ended AI research tasks from diverse domains\nsuch as computer vision, natural language processing, reinforcement learning,\nand game theory. Solving these tasks requires real-world AI research skills\nsuch as generating new ideas and hypotheses, creating and processing data,\nimplementing ML methods, training models, running experiments, analyzing the\nresults, and iterating through this process to improve on a given task. We\nevaluate a number of frontier large language models (LLMs) on our benchmarks\nsuch as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5\nPro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate\nmodels or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that\ncurrent frontier models can improve on the given baselines, usually by finding\nbetter hyperparameters, but do not generate novel hypotheses, algorithms,\narchitectures, or substantial improvements. We open-source our framework and\nbenchmark to facilitate future research in advancing the AI research\ncapabilities of LLM agents.",
            "upvotes": 122,
            "discussionId": "67b7ee1ffedfe971271dcd3a"
        },
        "publishedAt": "2025-02-20T22:08:38.225Z",
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14786",
            "authors": [
                {
                    "_id": "67b7ed0d58f6b70b18dda7b4",
                    "user": {
                        "_id": "6489893e1ec8356ba5bb9777",
                        "avatarUrl": "/avatars/54354c1e5774cadd1d83d42054e9d96b.svg",
                        "isPro": false,
                        "fullname": "Michael Tschannen",
                        "user": "mitsch",
                        "type": "user"
                    },
                    "name": "Michael Tschannen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:23:44.125Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7b5",
                    "user": {
                        "_id": "62d8f9887b8dc0ba17271415",
                        "avatarUrl": "/avatars/12ec78d34fd849bad44217b212f31e98.svg",
                        "isPro": false,
                        "fullname": "Alexey Gritsenko",
                        "user": "AlexeyG",
                        "type": "user"
                    },
                    "name": "Alexey Gritsenko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:23:50.440Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7b6",
                    "name": "Xiao Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7b7",
                    "name": "Muhammad Ferjad Naeem",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7b8",
                    "user": {
                        "_id": "630545da20668afe24860235",
                        "avatarUrl": "/avatars/5d82be2e7412bff1af15cc5eafa60b7d.svg",
                        "isPro": false,
                        "fullname": "Ibrahim Alabdulmohsin",
                        "user": "ibomohsin",
                        "type": "user"
                    },
                    "name": "Ibrahim Alabdulmohsin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:24:03.702Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7b9",
                    "name": "Nikhil Parthasarathy",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7ba",
                    "user": {
                        "_id": "66be3074dac7d71326f613cf",
                        "avatarUrl": "/avatars/012a531aad3eb1a2751bb3c31a619bf5.svg",
                        "isPro": false,
                        "fullname": "Talfan Evans",
                        "user": "talfanevans",
                        "type": "user"
                    },
                    "name": "Talfan Evans",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:24:14.273Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7bb",
                    "user": {
                        "_id": "642d334ff65714b4585f2de4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642d334ff65714b4585f2de4/gxBynq5KyoUP0VlAQD3-w.jpeg",
                        "isPro": false,
                        "fullname": "Lucas Beyer",
                        "user": "giffmana",
                        "type": "user"
                    },
                    "name": "Lucas Beyer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:44:51.778Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7bc",
                    "name": "Ye Xia",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7bd",
                    "user": {
                        "_id": "63cfcad6e23b90128c66685c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674562206311-noauth.png",
                        "isPro": false,
                        "fullname": "Basil Mustafa",
                        "user": "BasilMustafa",
                        "type": "user"
                    },
                    "name": "Basil Mustafa",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:44:44.933Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7be",
                    "user": {
                        "_id": "64f30d8ceb5f2982081db604",
                        "avatarUrl": "/avatars/eedf65a104d099d8a60bbffe69bc2571.svg",
                        "isPro": false,
                        "fullname": "Olivier Henaff",
                        "user": "olivierhenaff",
                        "type": "user"
                    },
                    "name": "Olivier HÃ©naff",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:44:38.156Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7bf",
                    "user": {
                        "_id": "65d77c0e1e7686c460255fda",
                        "avatarUrl": "/avatars/1d26a7a7ffdc5ca9e67b97030f21b098.svg",
                        "isPro": false,
                        "fullname": "Jeremiah Harmsen",
                        "user": "jharmsen",
                        "type": "user"
                    },
                    "name": "Jeremiah Harmsen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:44:32.302Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7c0",
                    "name": "Andreas Steiner",
                    "hidden": false
                },
                {
                    "_id": "67b7ed0d58f6b70b18dda7c1",
                    "user": {
                        "_id": "65dcd90082bddd501f68174b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/M2bc9PyKeFs1cCXjTfGGq.jpeg",
                        "isPro": false,
                        "fullname": "Xiaohua Zhai",
                        "user": "xiaohuazhai",
                        "type": "user"
                    },
                    "name": "Xiaohua Zhai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:44:07.595Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:08:29.000Z",
            "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features",
            "summary": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).",
            "upvotes": 79,
            "discussionId": "67b7ed0e58f6b70b18dda7f4"
        },
        "publishedAt": "2025-02-20T22:33:22.039Z",
        "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14739",
            "authors": [
                {
                    "_id": "67b7efc26348a1df80a8ae53",
                    "name": "M-A-P Team",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae54",
                    "user": {
                        "_id": "654907a4a1faff97850c4eff",
                        "avatarUrl": "/avatars/458c90151614bc7f116943b6e67d6b8a.svg",
                        "isPro": false,
                        "fullname": "du",
                        "user": "dododododo",
                        "type": "user"
                    },
                    "name": "Xinrun Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:53.525Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae55",
                    "name": "Yifan Yao",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae56",
                    "user": {
                        "_id": "65eb65722fbf6807134a636c",
                        "avatarUrl": "/avatars/282920ff99c8d83cdac5fd6ee507096a.svg",
                        "isPro": false,
                        "fullname": "Kaijing Ma",
                        "user": "mkj69",
                        "type": "user"
                    },
                    "name": "Kaijing Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:20:18.121Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae57",
                    "user": {
                        "_id": "658d0a228cff48d3a4612689",
                        "avatarUrl": "/avatars/70e297c6cb12d1bdde6d91c23f590b63.svg",
                        "isPro": false,
                        "fullname": "Bingli Wang",
                        "user": "BingliW",
                        "type": "user"
                    },
                    "name": "Bingli Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:20:24.792Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae58",
                    "user": {
                        "_id": "64ab99dcb76bfd863eba64c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
                        "isPro": false,
                        "fullname": "TY.Zheng",
                        "user": "aaabiao",
                        "type": "user"
                    },
                    "name": "Tianyu Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:24.002Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae59",
                    "user": {
                        "_id": "6578265ddea7e2122d02f6ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
                        "isPro": false,
                        "fullname": "kang zhu",
                        "user": "kangz",
                        "type": "user"
                    },
                    "name": "Kang Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:20:45.485Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae5a",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:25.894Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae5b",
                    "user": {
                        "_id": "6555e8d8a0c34cd61a6b9ce3",
                        "avatarUrl": "/avatars/71dc562cef4bd42f6b762f036357c800.svg",
                        "isPro": false,
                        "fullname": "yimingliang",
                        "user": "yimingliang",
                        "type": "user"
                    },
                    "name": "Yiming Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:21:08.553Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae5c",
                    "name": "Xiaolong Jin",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae5d",
                    "user": {
                        "_id": "67375a6ae6b1d15ff5359a54",
                        "avatarUrl": "/avatars/9d32d9e3bfb43b8d001c6ddeae720ec5.svg",
                        "isPro": false,
                        "fullname": "weizhenlin",
                        "user": "vzl123",
                        "type": "user"
                    },
                    "name": "Zhenlin Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:21:31.565Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae5e",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:34.124Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae5f",
                    "name": "Kaixing Deng",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae60",
                    "name": "Shuyue Guo",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae61",
                    "name": "Shian Jia",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae62",
                    "user": {
                        "_id": "675085408119fa5fac3cd7cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0Mrrkzhv0wBggP5kGKtSt.png",
                        "isPro": false,
                        "fullname": "jiangsichao",
                        "user": "jsc137",
                        "type": "user"
                    },
                    "name": "Sichao Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:22:09.140Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae63",
                    "user": {
                        "_id": "67a9d186b22571659007a43d",
                        "avatarUrl": "/avatars/79b06cf0983083b6161374e66a8c51b2.svg",
                        "isPro": false,
                        "fullname": "Yiyan Liao",
                        "user": "yiyanliao",
                        "type": "user"
                    },
                    "name": "Yiyan Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:22:15.619Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae64",
                    "name": "Rui Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae65",
                    "name": "Qinrui Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae66",
                    "user": {
                        "_id": "67ab7826ab5ebf181a7f78d7",
                        "avatarUrl": "/avatars/d6baf414011d6df659da4eb58e9d8958.svg",
                        "isPro": false,
                        "fullname": "Sirun Li",
                        "user": "inorganicwriter",
                        "type": "user"
                    },
                    "name": "Sirun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:22:30.478Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae67",
                    "user": {
                        "_id": "6382252f54421460665ec501",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6382252f54421460665ec501/gW9fev3T5QPcNq4f9hqB1.jpeg",
                        "isPro": false,
                        "fullname": "Yizhi Li",
                        "user": "yizhilll",
                        "type": "user"
                    },
                    "name": "Yizhi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:51.449Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae68",
                    "name": "Yunwen Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae69",
                    "name": "Dehua Ma",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae6a",
                    "user": {
                        "_id": "64de37ee5e192985054be575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
                        "isPro": false,
                        "fullname": "Yuansheng Ni",
                        "user": "yuanshengni",
                        "type": "user"
                    },
                    "name": "Yuansheng Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:30.371Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae6b",
                    "name": "Haoran Que",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae6c",
                    "user": {
                        "_id": "64560618bfdf9c63ce2d658a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
                        "isPro": false,
                        "fullname": "Mathsion Wong",
                        "user": "QiYao-Wang",
                        "type": "user"
                    },
                    "name": "Qiyao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:28.639Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae6d",
                    "name": "Zhoufutu Wen",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae6e",
                    "name": "Siwei Wu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae6f",
                    "name": "Tianshun Xing",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae70",
                    "name": "Ming Xu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae71",
                    "name": "Zhenzhu Yang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae72",
                    "name": "Zekun Moore Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae73",
                    "name": "Junting Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae74",
                    "name": "Yuelin Bai",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae75",
                    "name": "Xingyuan Bu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae76",
                    "user": {
                        "_id": "64f9c21b681224dbe49a2280",
                        "avatarUrl": "/avatars/df26cc4b4c6105af2c77392db61e3a27.svg",
                        "isPro": false,
                        "fullname": "caichenglin",
                        "user": "easy4mego",
                        "type": "user"
                    },
                    "name": "Chenglin Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:23:17.731Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae77",
                    "name": "Liang Chen",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae78",
                    "name": "Yifan Chen",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae79",
                    "name": "Chengtuo Cheng",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae7a",
                    "name": "Tianhao Cheng",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae7b",
                    "name": "Keyi Ding",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae7c",
                    "name": "Siming Huang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae7d",
                    "name": "Yun Huang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae7e",
                    "name": "Yaoru Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae7f",
                    "name": "Yizhe Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae80",
                    "name": "Zhaoqun Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae81",
                    "name": "Tianhao Liang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae82",
                    "user": {
                        "_id": "66751c722b487c2e015a1f60",
                        "avatarUrl": "/avatars/d66a98b625451ccea1b4dfcdaf623304.svg",
                        "isPro": false,
                        "fullname": "lin",
                        "user": "adams6435",
                        "type": "user"
                    },
                    "name": "Chengdong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T15:15:33.213Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae83",
                    "name": "Hongquan Lin",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae84",
                    "name": "Yinghao Ma",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae85",
                    "name": "Zhongyuan Peng",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae86",
                    "user": {
                        "_id": "65adda5299c3bd19c74d6a8d",
                        "avatarUrl": "/avatars/1ce504b64ab60f375b235ebaf81cafd6.svg",
                        "isPro": false,
                        "fullname": "PENG ZIFAN",
                        "user": "Ziffer",
                        "type": "user"
                    },
                    "name": "Zifan Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:20.429Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae87",
                    "name": "Qige Qi",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae88",
                    "name": "Shi Qiu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae89",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae8a",
                    "name": "Yizhou Tan",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae8b",
                    "name": "Zili Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae8c",
                    "name": "Chenqing Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae8d",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae8e",
                    "name": "Yiya Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae8f",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae90",
                    "name": "Jiajun Xu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae91",
                    "name": "Kexin Yang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae92",
                    "name": "Ruibin Yuan",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae93",
                    "name": "Yuanhao Yue",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae94",
                    "name": "Tianyang Zhan",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae95",
                    "name": "Chun Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae96",
                    "name": "Jingyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae97",
                    "name": "Xiyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae98",
                    "name": "Xingjian Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae99",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae9a",
                    "name": "Yongchi Zhao",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae9b",
                    "name": "Xiangyu Zheng",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae9c",
                    "name": "Chenghua Zhong",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae9d",
                    "name": "Yang Gao",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae9e",
                    "name": "Zhoujun Li",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8ae9f",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea0",
                    "user": {
                        "_id": "612ee6a7b960e78c6d2319d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:32.399Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea1",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea2",
                    "name": "Shiwen Ni",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea3",
                    "name": "Junran Peng",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea4",
                    "name": "Yujia Qin",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea5",
                    "name": "Wenbo Su",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea6",
                    "user": {
                        "_id": "6490d4ba1afdee3acd1147f6",
                        "avatarUrl": "/avatars/ae13c7b21fe9ced7541dcd664d1b94ed.svg",
                        "isPro": false,
                        "fullname": "Guoyin Wang",
                        "user": "guoyinwang",
                        "type": "user"
                    },
                    "name": "Guoyin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T10:23:25.946Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea7",
                    "name": "Shi Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea8",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aea9",
                    "name": "Min Yang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeaa",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeab",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeac",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aead",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeae",
                    "user": {
                        "_id": "65377c30e48353201e6fdda0",
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:22.185Z",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeaf",
                    "name": "Qunshu Lin",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeb0",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "67b7efc26348a1df80a8aeb1",
                    "name": "Ge Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T17:05:58.000Z",
            "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
            "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.",
            "upvotes": 78,
            "discussionId": "67b7efc66348a1df80a8afc8"
        },
        "publishedAt": "2025-02-20T22:15:33.133Z",
        "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14502",
            "authors": [
                {
                    "_id": "67b83fd69fb3eedaf6d0aef3",
                    "user": {
                        "_id": "5dfa8e07da6d0311fd3d5430",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
                        "isPro": false,
                        "fullname": "Sergey Pletenev",
                        "user": "memyprokotow",
                        "type": "user"
                    },
                    "name": "Sergey Pletenev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:49:33.682Z",
                    "hidden": false
                },
                {
                    "_id": "67b83fd69fb3eedaf6d0aef4",
                    "user": {
                        "_id": "660ee18e2dcd816ad14b3739",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
                        "isPro": false,
                        "fullname": "Maria Marina",
                        "user": "zlatamaria",
                        "type": "user"
                    },
                    "name": "Maria Marina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:47.555Z",
                    "hidden": false
                },
                {
                    "_id": "67b83fd69fb3eedaf6d0aef5",
                    "user": {
                        "_id": "61ade264f602880813dbe10b",
                        "avatarUrl": "/avatars/a92dea7d853bbabbf60b351c207b6875.svg",
                        "isPro": false,
                        "fullname": "Daniil Moskovskiy",
                        "user": "etomoscow",
                        "type": "user"
                    },
                    "name": "Daniil Moskovskiy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:49.828Z",
                    "hidden": false
                },
                {
                    "_id": "67b83fd69fb3eedaf6d0aef6",
                    "name": "Vasily Konovalov",
                    "hidden": false
                },
                {
                    "_id": "67b83fd69fb3eedaf6d0aef7",
                    "name": "Pavel Braslavski",
                    "hidden": false
                },
                {
                    "_id": "67b83fd69fb3eedaf6d0aef8",
                    "name": "Alexander Panchenko",
                    "hidden": false
                },
                {
                    "_id": "67b83fd69fb3eedaf6d0aef9",
                    "user": {
                        "_id": "62bd6c6baaf1480f1aa2222e",
                        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
                        "isPro": false,
                        "fullname": "Mikhail Salnikov",
                        "user": "msalnikov",
                        "type": "user"
                    },
                    "name": "Mikhail Salnikov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-21T09:01:04.770Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T12:31:03.000Z",
            "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
            "summary": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.",
            "upvotes": 50,
            "discussionId": "67b83fd79fb3eedaf6d0af16"
        },
        "publishedAt": "2025-02-21T05:29:18.157Z",
        "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62bd6c6baaf1480f1aa2222e/_N4zn03NcZY7lGoHmp9-j.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62bd6c6baaf1480f1aa2222e/nh3VgACDbU_BXjhHG8nsF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bd6c6baaf1480f1aa2222e",
            "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
            "fullname": "Mikhail Salnikov",
            "name": "msalnikov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14382",
            "authors": [
                {
                    "_id": "67b7ed3e58f6b70b18ddb4bc",
                    "user": {
                        "_id": "63715b25ffc0489ed7d1f415",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63715b25ffc0489ed7d1f415/xZJepbs0LRqFbW1knnBKR.jpeg",
                        "isPro": false,
                        "fullname": "Dacheng Li",
                        "user": "DachengLi",
                        "type": "user"
                    },
                    "name": "Dacheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:45:13.558Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4bd",
                    "user": {
                        "_id": "64ebbae6895a36ab28de811a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ebbae6895a36ab28de811a/gBiaQP4paS4L13eu-yRm7.jpeg",
                        "isPro": false,
                        "fullname": "Shiyi Cao",
                        "user": "eva98",
                        "type": "user"
                    },
                    "name": "Shiyi Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:43.358Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4be",
                    "name": "Chengkun Cao",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4bf",
                    "user": {
                        "_id": "644570ba2d91b15b4c7f6311",
                        "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
                        "isPro": false,
                        "fullname": "Xiuyu Li",
                        "user": "xiuyul",
                        "type": "user"
                    },
                    "name": "Xiuyu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:45:28.150Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4c0",
                    "user": {
                        "_id": "663cfbd9b0f659de3db65c1a",
                        "avatarUrl": "/avatars/b7d82d281026ee04a9932b44a770b840.svg",
                        "isPro": false,
                        "fullname": "Shangyin Tan",
                        "user": "shangyint",
                        "type": "user"
                    },
                    "name": "Shangyin Tan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:45:33.730Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4c1",
                    "user": {
                        "_id": "6251bf4b183aa4266924ad91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678041834400-6251bf4b183aa4266924ad91.jpeg",
                        "isPro": true,
                        "fullname": "Kurt Keutzer",
                        "user": "kurtkeutzer",
                        "type": "user"
                    },
                    "name": "Kurt Keutzer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:45:39.902Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4c2",
                    "user": {
                        "_id": "66ff8aa43a31c499dc48fdd6",
                        "avatarUrl": "/avatars/060dc90fb13991bd013ce8173f12ae3e.svg",
                        "isPro": false,
                        "fullname": "Jiarong Xing",
                        "user": "JerryPotter",
                        "type": "user"
                    },
                    "name": "Jiarong Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:45:45.774Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4c3",
                    "user": {
                        "_id": "645d2e8401f4eaab2a0878ce",
                        "avatarUrl": "/avatars/1273c5fb607b4b622a746a42692fa632.svg",
                        "isPro": false,
                        "fullname": "Joseph E. Gonzalez",
                        "user": "ProfJoeyG",
                        "type": "user"
                    },
                    "name": "Joseph E. Gonzalez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:45:52.578Z",
                    "hidden": false
                },
                {
                    "_id": "67b7ed3e58f6b70b18ddb4c4",
                    "name": "Ion Stoica",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T09:18:53.000Z",
            "title": "S*: Test Time Scaling for Code Generation",
            "summary": "Increasing test-time compute for LLMs shows promise across domains but\nremains underexplored in code generation, despite extensive study in math. In\nthis paper, we propose S*, the first hybrid test-time scaling framework that\nsubstantially improves the coverage and selection accuracy of generated code.\nS* extends the existing parallel scaling paradigm with sequential scaling to\npush performance boundaries. It further leverages a novel selection mechanism\nthat adaptively generates distinguishing inputs for pairwise comparison,\ncombined with execution-grounded information to robustly identify correct\nsolutions. We evaluate across 12 Large Language Models and Large Reasoning\nModel and show: (1) S* consistently improves performance across model families\nand sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables\nnon-reasoning models to surpass reasoning models - GPT-4o-mini with S*\noutperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts\nstate-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S*\nachieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be\navailable under https://github.com/NovaSky-AI/SkyThought.",
            "upvotes": 38,
            "discussionId": "67b7ed3f58f6b70b18ddb510"
        },
        "publishedAt": "2025-02-20T22:04:42.635Z",
        "title": "S*: Test Time Scaling for Code Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14768",
            "authors": [
                {
                    "_id": "67b7f08c357c2729ac20a81b",
                    "name": "Tian Xie",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a81c",
                    "user": {
                        "_id": "641ddac5be3bd3a5a06ed4a4",
                        "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
                        "isPro": false,
                        "fullname": "zitian gao",
                        "user": "zgao3186",
                        "type": "user"
                    },
                    "name": "Zitian Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:50:06.783Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a81d",
                    "name": "Qingnan Ren",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a81e",
                    "user": {
                        "_id": "65f2e0dbb64166d55502b932",
                        "avatarUrl": "/avatars/e49579af37b884cfbe6df57d7983f512.svg",
                        "isPro": false,
                        "fullname": "luohaoming",
                        "user": "aluo83",
                        "type": "user"
                    },
                    "name": "Haoming Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:49:51.664Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a81f",
                    "name": "Yuqian Hong",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a820",
                    "user": {
                        "_id": "67090a395f4ed2ff1f0b3658",
                        "avatarUrl": "/avatars/03744a2fcbcb0e8074c04bad83b3e34c.svg",
                        "isPro": false,
                        "fullname": "Zhenbang Dai",
                        "user": "BryanDai",
                        "type": "user"
                    },
                    "name": "Bryan Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:49:20.858Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a821",
                    "name": "Joey Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a822",
                    "name": "Kai Qiu",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a823",
                    "user": {
                        "_id": "67034414f2b11c7dd251e232",
                        "avatarUrl": "/avatars/6b741ac2eab48c6f72185342f9af7d1f.svg",
                        "isPro": false,
                        "fullname": "wzr",
                        "user": "wuzhirong",
                        "type": "user"
                    },
                    "name": "Zhirong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:48:55.865Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f08c357c2729ac20a824",
                    "user": {
                        "_id": "676a328148d749b7086782d0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Tt7u8l8f_1oVBWmBp7tkm.png",
                        "isPro": false,
                        "fullname": "Chong Luo",
                        "user": "cluo-ms",
                        "type": "user"
                    },
                    "name": "Chong Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:48:44.370Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T17:49:26.000Z",
            "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning",
            "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.",
            "upvotes": 24,
            "discussionId": "67b7f08e357c2729ac20a88f"
        },
        "publishedAt": "2025-02-20T22:19:05.902Z",
        "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14372",
            "authors": [
                {
                    "_id": "67b81870cc6b0136b3d84254",
                    "user": {
                        "_id": "6530a78069751712276d60ed",
                        "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
                        "isPro": false,
                        "fullname": "Austin He",
                        "user": "basil2115",
                        "type": "user"
                    },
                    "name": "Austin Yubo He",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-21T06:30:16.645Z",
                    "hidden": false
                },
                {
                    "_id": "67b81870cc6b0136b3d84255",
                    "name": "Zi-Wen Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T09:05:34.000Z",
            "title": "Discovering highly efficient low-weight quantum error-correcting codes\n  with reinforcement learning",
            "summary": "The realization of scalable fault-tolerant quantum computing is expected to\nhinge on quantum error-correcting codes. In the quest for more efficient\nquantum fault tolerance, a critical code parameter is the weight of\nmeasurements that extract information about errors to enable error correction:\nas higher measurement weights require higher implementation costs and introduce\nmore errors, it is important in code design to optimize measurement weight.\nThis underlies the surging interest in quantum low-density parity-check (qLDPC)\ncodes, the study of which has primarily focused on the asymptotic\n(large-code-limit) properties. In this work, we introduce a versatile and\ncomputationally efficient approach to stabilizer code weight reduction based on\nreinforcement learning (RL), which produces new low-weight codes that\nsubstantially outperform the state of the art in practically relevant parameter\nregimes, extending significantly beyond previously accessible small distances.\nFor example, our approach demonstrates savings in physical qubit overhead\ncompared to existing results by 1 to 2 orders of magnitude for weight 6 codes\nand brings the overhead into a feasible range for near-future experiments. We\nalso investigate the interplay between code parameters using our RL framework,\noffering new insights into the potential efficiency and power of practically\nviable coding strategies. Overall, our results demonstrate how RL can\neffectively advance the crucial yet challenging problem of quantum code\ndiscovery and thereby facilitate a faster path to the practical implementation\nof fault-tolerant quantum technologies.",
            "upvotes": 20,
            "discussionId": "67b81873cc6b0136b3d8430a"
        },
        "publishedAt": "2025-02-21T01:11:34.971Z",
        "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6530a78069751712276d60ed",
            "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
            "fullname": "Austin He",
            "name": "basil2115",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14834",
            "authors": [
                {
                    "_id": "67b7f3c4d00e69f10cff219e",
                    "user": {
                        "_id": "648c48d8c0ddeee6df5b6d22",
                        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                        "isPro": false,
                        "fullname": "Shangqing Tu",
                        "user": "tsq2000",
                        "type": "user"
                    },
                    "name": "Shangqing Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:47:11.953Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff219f",
                    "name": "Yucheng Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a0",
                    "name": "Daniel Zhang-Li",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a1",
                    "user": {
                        "_id": "64ed568ccf6118a9379a61b8",
                        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                        "isPro": false,
                        "fullname": "Yushi Bai",
                        "user": "bys0318",
                        "type": "user"
                    },
                    "name": "Yushi Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:47:42.383Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a2",
                    "name": "Jifan Yu",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a3",
                    "name": "Yuhao Wu",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a4",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a5",
                    "name": "Huiqin Liu",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a6",
                    "user": {
                        "_id": "6310a3cd531cc21f9e06de6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
                        "isPro": false,
                        "fullname": "Zhiyuan Liu",
                        "user": "acharkq",
                        "type": "user"
                    },
                    "name": "Zhiyuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:48:24.334Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a7",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "67b7f3c4d00e69f10cff21a8",
                    "user": {
                        "_id": "65df8cbc2705d9672f55d1aa",
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:48:30.590Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:47:36.000Z",
            "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in\n  Vision-Language Models",
            "summary": "Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V",
            "upvotes": 19,
            "discussionId": "67b7f3c7d00e69f10cff2258"
        },
        "publishedAt": "2025-02-20T22:39:21.551Z",
        "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/8AYx7CcK4CT6flX3nRDlB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648c48d8c0ddeee6df5b6d22",
            "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
            "fullname": "Shangqing Tu",
            "name": "tsq2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14258",
            "authors": [
                {
                    "_id": "67b7fa96c3f48f8b3fc632fe",
                    "user": {
                        "_id": "64e5c8e594aa0690321f6b29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png",
                        "isPro": false,
                        "fullname": "Yein Park",
                        "user": "P-YI",
                        "type": "user"
                    },
                    "name": "Yein Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:46:28.888Z",
                    "hidden": false
                },
                {
                    "_id": "67b7fa96c3f48f8b3fc632ff",
                    "user": {
                        "_id": "66569dbaed45f790fbbebb83",
                        "avatarUrl": "/avatars/a4915e88d2bdff48cb30dd9972640d1e.svg",
                        "isPro": false,
                        "fullname": "Chanwoong Yoon",
                        "user": "cwyoon99",
                        "type": "user"
                    },
                    "name": "Chanwoong Yoon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:46:35.045Z",
                    "hidden": false
                },
                {
                    "_id": "67b7fa96c3f48f8b3fc63300",
                    "user": {
                        "_id": "60f8435644e75317cc02ed51",
                        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
                        "isPro": false,
                        "fullname": "Jungwoo Park",
                        "user": "affjljoo3581",
                        "type": "user"
                    },
                    "name": "Jungwoo Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:46:49.118Z",
                    "hidden": false
                },
                {
                    "_id": "67b7fa96c3f48f8b3fc63301",
                    "user": {
                        "_id": "64587be872b60ae7a3817858",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
                        "isPro": false,
                        "fullname": "Minbyul Jeong",
                        "user": "Minbyul",
                        "type": "user"
                    },
                    "name": "Minbyul Jeong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:46:55.040Z",
                    "hidden": false
                },
                {
                    "_id": "67b7fa96c3f48f8b3fc63302",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T04:52:05.000Z",
            "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall\n  Time-specific Information",
            "summary": "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads primarily responsible for\nprocessing temporal knowledge through circuit analysis. We confirm that these\nheads are present across multiple models, though their specific locations may\nvary, and their responses differ depending on the type of knowledge and its\ncorresponding years. Disabling these heads degrades the model's ability to\nrecall time-specific knowledge while maintaining its general capabilities\nwithout compromising time-invariant and question-answering performances.\nMoreover, the heads are activated not only numeric conditions (\"In 2004\") but\nalso textual aliases (\"In the year ...\"), indicating that they encode a\ntemporal dimension beyond simple numerical representation. Furthermore, we\nexpand the potential of our findings by demonstrating how temporal knowledge\ncan be edited by adjusting the values of these heads.",
            "upvotes": 18,
            "discussionId": "67b7fa9ac3f48f8b3fc63452"
        },
        "publishedAt": "2025-02-20T23:02:42.672Z",
        "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64587be872b60ae7a3817858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
            "fullname": "Minbyul Jeong",
            "name": "Minbyul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.12853",
            "authors": [
                {
                    "_id": "67b69b6717ccb022c6a95b38",
                    "user": {
                        "_id": "648294b2eb4befee378951c1",
                        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
                        "isPro": false,
                        "fullname": "Ruotian Ma",
                        "user": "vvibt",
                        "type": "user"
                    },
                    "name": "Ruotian Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:58:55.028Z",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b39",
                    "user": {
                        "_id": "626f98528a894872cfbf620c",
                        "avatarUrl": "/avatars/fe31d20313e6ca85e96bc249424c5383.svg",
                        "isPro": false,
                        "fullname": "Peisong Wang",
                        "user": "duke1852022",
                        "type": "user"
                    },
                    "name": "Peisong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:59:00.273Z",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b3a",
                    "name": "Cheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b3b",
                    "name": "Xingyan Liu",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b3c",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b3d",
                    "name": "Bang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b3e",
                    "name": "Xin Zhou",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b3f",
                    "name": "Nan Du",
                    "hidden": false
                },
                {
                    "_id": "67b69b6717ccb022c6a95b40",
                    "name": "Jia Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-18T13:40:22.000Z",
            "title": "S^2R: Teaching LLMs to Self-verify and Self-correct via Reinforcement\n  Learning",
            "summary": "Recent studies have demonstrated the effectiveness of LLM test-time scaling.\nHowever, existing approaches to incentivize LLMs' deep thinking abilities\ngenerally require large-scale data or significant training efforts. Meanwhile,\nit remains unclear how to improve the thinking abilities of less powerful base\nmodels. In this work, we introduce S^2R, an efficient framework that enhances\nLLM reasoning by teaching models to self-verify and self-correct during\ninference. Specifically, we first initialize LLMs with iterative\nself-verification and self-correction behaviors through supervised fine-tuning\non carefully curated data. The self-verification and self-correction skills are\nthen further strengthened by both outcome-level and process-level reinforcement\nlearning, with minimized resource requirements, enabling the model to\nadaptively refine its reasoning process during inference. Our results\ndemonstrate that, with only 3.1k self-verifying and self-correcting behavior\ninitialization samples, Qwen2.5-math-7B achieves an accuracy improvement from\n51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of\nlong-CoT distilled data. Extensive experiments and analysis based on three base\nmodels across both in-domain and out-of-domain benchmarks validate the\neffectiveness of S^2R. Our code and data are available at\nhttps://github.com/NineAbyss/S2R.",
            "upvotes": 15,
            "discussionId": "67b69b6817ccb022c6a95b6e"
        },
        "publishedAt": "2025-02-21T05:00:18.645Z",
        "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12853.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "fullname": "Ruotian Ma",
            "name": "vvibt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14282",
            "authors": [
                {
                    "_id": "67b7f5587f4d732dc469270e",
                    "name": "Haowei Liu",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc469270f",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692710",
                    "user": {
                        "_id": "66e8d7d2483df532fd364913",
                        "avatarUrl": "/avatars/300aea4b8c571b2aeac629de58281444.svg",
                        "isPro": false,
                        "fullname": "Haiyang Xu",
                        "user": "msxxx",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:58:29.601Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692711",
                    "name": "Yuyang Wanyan",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692712",
                    "user": {
                        "_id": "6438f6415aa69077ffb16942",
                        "avatarUrl": "/avatars/c83dbd3e10e88db97c2a86092bad5917.svg",
                        "isPro": false,
                        "fullname": "Junyang Wang",
                        "user": "junyangwang0410",
                        "type": "user"
                    },
                    "name": "Junyang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:58:16.343Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692713",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692714",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692715",
                    "name": "Chunfeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692716",
                    "user": {
                        "_id": "6222e721271a284f976f43d8",
                        "avatarUrl": "/avatars/5e5c135e25408ebb5c6f932037cb2ce6.svg",
                        "isPro": false,
                        "fullname": "ChangshengXu",
                        "user": "ChangshengXu",
                        "type": "user"
                    },
                    "name": "Changsheng Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T14:57:51.040Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692717",
                    "name": "Weiming Hu",
                    "hidden": false
                },
                {
                    "_id": "67b7f5587f4d732dc4692718",
                    "name": "Fei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T05:41:55.000Z",
            "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex\n  Task Automation on PC",
            "summary": "In the field of MLLM-based GUI agents, compared to smartphones, the PC\nscenario not only features a more complex interactive environment, but also\ninvolves more intricate intra- and inter-app workflows. To address these\nissues, we propose a hierarchical agent framework named PC-Agent. Specifically,\nfrom the perception perspective, we devise an Active Perception Module (APM) to\novercome the inadequate abilities of current MLLMs in perceiving screenshot\ncontent. From the decision-making perspective, to handle complex user\ninstructions and interdependent subtasks more effectively, we propose a\nhierarchical multi-agent collaboration architecture that decomposes\ndecision-making processes into Instruction-Subtask-Action levels. Within this\narchitecture, three agents (i.e., Manager, Progress and Decision) are set up\nfor instruction decomposition, progress tracking and step-by-step\ndecision-making respectively. Additionally, a Reflection agent is adopted to\nenable timely bottom-up error feedback and adjustment. We also introduce a new\nbenchmark PC-Eval with 25 real-world complex instructions. Empirical results on\nPC-Eval show that our PC-Agent achieves a 32% absolute improvement of task\nsuccess rate over previous state-of-the-art methods. The code will be publicly\navailable.",
            "upvotes": 12,
            "discussionId": "67b7f55b7f4d732dc46927c1"
        },
        "publishedAt": "2025-02-20T22:39:48.180Z",
        "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/feg9OYb4onJJermpjc6nh.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "fullname": "xuhaiyang",
            "name": "xhyandwyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14844",
            "authors": [
                {
                    "_id": "67b7f5ee8b3dff28b749be78",
                    "user": {
                        "_id": "630428fa7b50dd9d0a38cde0",
                        "avatarUrl": "/avatars/d1baf7fd17daf4be16ba5bd6cd4f2277.svg",
                        "isPro": false,
                        "fullname": "Rameen Abdal",
                        "user": "RameenAbdal",
                        "type": "user"
                    },
                    "name": "Rameen Abdal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:00:23.357Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be79",
                    "user": {
                        "_id": "62853516e483e0d37b354ce1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62853516e483e0d37b354ce1/t5Tyd3E07w26B9Z3XpZWI.jpeg",
                        "isPro": false,
                        "fullname": "Or Patashnik",
                        "user": "orpatashnik",
                        "type": "user"
                    },
                    "name": "Or Patashnik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:00:32.474Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be7a",
                    "user": {
                        "_id": "63610db13c7147fae7de88e3",
                        "avatarUrl": "/avatars/d7e97a16cfee39e1e50d7a5b747876f1.svg",
                        "isPro": false,
                        "fullname": "Ivan Skorokhodov",
                        "user": "universome",
                        "type": "user"
                    },
                    "name": "Ivan Skorokhodov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:00:39.038Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be7b",
                    "user": {
                        "_id": "6315358a362e3e95ea538081",
                        "avatarUrl": "/avatars/3b089a25a87c2e83c6b23ccb5d2dc73e.svg",
                        "isPro": false,
                        "fullname": "Willi Menapace",
                        "user": "willi-menapace",
                        "type": "user"
                    },
                    "name": "Willi Menapace",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:00:45.032Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be7c",
                    "user": {
                        "_id": "64276311eb9a0ed86180715b",
                        "avatarUrl": "/avatars/76f933cd549f10e5e2db379de235d304.svg",
                        "isPro": false,
                        "fullname": "Aliaksandr Siarohin",
                        "user": "aliaksandr-siarohin",
                        "type": "user"
                    },
                    "name": "Aliaksandr Siarohin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:00:51.221Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be7d",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be7e",
                    "user": {
                        "_id": "628507161949ebcae8e24ec3",
                        "avatarUrl": "/avatars/008ecb3daa4c8187b5f339f1176b3c39.svg",
                        "isPro": false,
                        "fullname": "Daniel Cohen-Or",
                        "user": "cohenor",
                        "type": "user"
                    },
                    "name": "Daniel Cohen-Or",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:01:00.737Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f5ee8b3dff28b749be7f",
                    "user": {
                        "_id": "64db29097266618e853dd6ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db29097266618e853dd6ec/r0MaPQCfAxeKv3ycdKYLK.jpeg",
                        "isPro": false,
                        "fullname": "Kfir Aberman",
                        "user": "kaberman",
                        "type": "user"
                    },
                    "name": "Kfir Aberman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:01:06.434Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:53:39.000Z",
            "title": "Dynamic Concepts Personalization from Single Videos",
            "summary": "Personalizing generative text-to-image models has seen remarkable progress,\nbut extending this personalization to text-to-video models presents unique\nchallenges. Unlike static concepts, personalizing text-to-video models has the\npotential to capture dynamic concepts, i.e., entities defined not only by their\nappearance but also by their motion. In this paper, we introduce\nSet-and-Sequence, a novel framework for personalizing Diffusion Transformers\n(DiTs)-based generative video models with dynamic concepts. Our approach\nimposes a spatio-temporal weight space within an architecture that does not\nexplicitly separate spatial and temporal features. This is achieved in two key\nstages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an\nunordered set of frames from the video to learn an identity LoRA basis that\nrepresents the appearance, free from temporal interference. In the second\nstage, with the identity LoRAs frozen, we augment their coefficients with\nMotion Residuals and fine-tune them on the full video sequence, capturing\nmotion dynamics. Our Set-and-Sequence framework results in a spatio-temporal\nweight space that effectively embeds dynamic concepts into the video model's\noutput domain, enabling unprecedented editability and compositionality while\nsetting a new benchmark for personalizing dynamic concepts.",
            "upvotes": 10,
            "discussionId": "67b7f5f18b3dff28b749bf45"
        },
        "publishedAt": "2025-02-20T22:41:47.210Z",
        "title": "Dynamic Concepts Personalization from Single Videos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14678",
            "authors": [
                {
                    "_id": "67b886298512a3eca0668ba6",
                    "name": "Arkil Patel",
                    "hidden": false
                },
                {
                    "_id": "67b886298512a3eca0668ba7",
                    "name": "Siva Reddy",
                    "hidden": false
                },
                {
                    "_id": "67b886298512a3eca0668ba8",
                    "name": "Dzmitry Bahdanau",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T16:09:55.000Z",
            "title": "How to Get Your LLM to Generate Challenging Problems for Evaluation",
            "summary": "The pace of evolution of Large Language Models (LLMs) necessitates new\napproaches for rigorous and comprehensive evaluation. Traditional human\nannotation is increasingly impracticable due to the complexities and costs\ninvolved in generating high-quality, challenging problems. In this work, we\nintroduce CHASE, a unified framework to synthetically generate challenging\nproblems using LLMs without human involvement. For a given task, our approach\nbuilds a hard problem in a bottom-up manner from simpler components. Moreover,\nour framework decomposes the generation process into independently verifiable\nsub-tasks, thereby ensuring a high level of quality and correctness. We\nimplement CHASE to create evaluation benchmarks across three diverse domains:\n(1) document-based question answering, (2) repository-level code completion,\nand (3) math reasoning. The performance of state-of-the-art LLMs on these\nsynthetic benchmarks lies in the range of 40-60% accuracy, thereby\ndemonstrating the effectiveness of our framework at generating challenging\nproblems. We publicly release our benchmarks and code.",
            "upvotes": 9,
            "discussionId": "67b8862a8512a3eca0668c00"
        },
        "publishedAt": "2025-02-21T11:36:30.717Z",
        "title": "How to Get Your LLM to Generate Challenging Problems for Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14678.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631a523c04f8ed65eff16fb4",
            "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
            "fullname": "Arkil Patel",
            "name": "arkilpatel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14846",
            "authors": [
                {
                    "_id": "67b7f4f1b15c19d57189fc5e",
                    "user": {
                        "_id": "62f6c68904e5e02f82b04690",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f6c68904e5e02f82b04690/kK2-PkeAGzAOLhkfajswf.jpeg",
                        "isPro": true,
                        "fullname": "Yue Yang",
                        "user": "yyupenn",
                        "type": "user"
                    },
                    "name": "Yue Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T15:06:20.097Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc5f",
                    "name": "Ajay Patel",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc60",
                    "user": {
                        "_id": "61c388aa727d1257bf3cf58b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670871898141-61c388aa727d1257bf3cf58b.jpeg",
                        "isPro": true,
                        "fullname": "Matt Deitke",
                        "user": "mattdeitke",
                        "type": "user"
                    },
                    "name": "Matt Deitke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:03:20.545Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc61",
                    "name": "Tanmay Gupta",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc62",
                    "user": {
                        "_id": "620acbdb3c0931626a7c9297",
                        "avatarUrl": "/avatars/f63b1d225ed81e223d3e8876a5c708c4.svg",
                        "isPro": false,
                        "fullname": "Luca Weihs",
                        "user": "lucaweihs",
                        "type": "user"
                    },
                    "name": "Luca Weihs",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:02:58.922Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc63",
                    "user": {
                        "_id": "6360289588e41d249ecd3e26",
                        "avatarUrl": "/avatars/0e84dbd72f07e99967a8d25cda938efe.svg",
                        "isPro": false,
                        "fullname": "Andrew Head",
                        "user": "ChittyChins",
                        "type": "user"
                    },
                    "name": "Andrew Head",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:02:53.395Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc64",
                    "user": {
                        "_id": "631f42e1b6628770f6efd87a",
                        "avatarUrl": "/avatars/0fa93b3513ebca737cce26dfa5611cf1.svg",
                        "isPro": false,
                        "fullname": "Mark Yatskar",
                        "user": "myatskar",
                        "type": "user"
                    },
                    "name": "Mark Yatskar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:02:47.575Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc65",
                    "user": {
                        "_id": "6303ce25fc783bfc744216af",
                        "avatarUrl": "/avatars/09f5e87c1f56a1b7f6ef9c5037682285.svg",
                        "isPro": false,
                        "fullname": "Chris Callison-Burch",
                        "user": "CCB",
                        "type": "user"
                    },
                    "name": "Chris Callison-Burch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:02:41.757Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc66",
                    "user": {
                        "_id": "66429868ab89e3a3a85668b0",
                        "avatarUrl": "/avatars/170e0daa454838deee2bf946f7118651.svg",
                        "isPro": false,
                        "fullname": "Ranjay Krishna",
                        "user": "ranjaykrishna",
                        "type": "user"
                    },
                    "name": "Ranjay Krishna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:02:35.267Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc67",
                    "name": "Aniruddha Kembhavi",
                    "hidden": false
                },
                {
                    "_id": "67b7f4f1b15c19d57189fc68",
                    "name": "Christopher Clark",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:55:30.000Z",
            "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
            "summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.",
            "upvotes": 9,
            "discussionId": "67b7f4f2b15c19d57189fc95"
        },
        "publishedAt": "2025-02-20T22:38:36.406Z",
        "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14638",
            "authors": [
                {
                    "_id": "67b8760bf8311235c642d7a4",
                    "user": {
                        "_id": "648c4af819bb04c06467189c",
                        "avatarUrl": "/avatars/8b9372a233d4c00b555625fa7b5203e2.svg",
                        "isPro": false,
                        "fullname": "Zheyuan Zhang",
                        "user": "Zheyuan22",
                        "type": "user"
                    },
                    "name": "Zheyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T14:42:43.700Z",
                    "hidden": false
                },
                {
                    "_id": "67b8760bf8311235c642d7a5",
                    "name": "Runze Li",
                    "hidden": false
                },
                {
                    "_id": "67b8760bf8311235c642d7a6",
                    "user": {
                        "_id": "6676067b8a4064c02b4ef6b7",
                        "avatarUrl": "/avatars/319ff99740183793cab3045ae3bf1395.svg",
                        "isPro": false,
                        "fullname": "Tasnim Kabir",
                        "user": "TasnimKabir12",
                        "type": "user"
                    },
                    "name": "Tasnim Kabir",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:01:51.663Z",
                    "hidden": false
                },
                {
                    "_id": "67b8760bf8311235c642d7a7",
                    "name": "Jordan Boyd-Graber",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T15:21:35.000Z",
            "title": "NAVIG: Natural Language-guided Analysis with Vision Language Models for\n  Image Geo-localization",
            "summary": "Image geo-localization is the task of predicting the specific location of an\nimage and requires complex reasoning across visual, geographical, and cultural\ncontexts. While prior Vision Language Models (VLMs) have the best accuracy at\nthis task, there is a dearth of high-quality datasets and models for analytical\nreasoning. We first create NaviClues, a high-quality dataset derived from\nGeoGuessr, a popular geography game, to supply examples of expert reasoning\nfrom language. Using this dataset, we present Navig, a comprehensive image\ngeo-localization framework integrating global and fine-grained image\ninformation. By reasoning with language, Navig reduces the average distance\nerror by 14% compared to previous state-of-the-art models while requiring fewer\nthan 1000 training samples. Our dataset and code are available at\nhttps://github.com/SparrowZheyuan18/Navig/.",
            "upvotes": 8,
            "discussionId": "67b8760ef8311235c642d89d"
        },
        "publishedAt": "2025-02-21T08:18:34.557Z",
        "title": "NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14638.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648c4af819bb04c06467189c",
            "avatarUrl": "/avatars/8b9372a233d4c00b555625fa7b5203e2.svg",
            "fullname": "Zheyuan Zhang",
            "name": "Zheyuan22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14802",
            "authors": [
                {
                    "_id": "67b878b8f17ca6989fd21e92",
                    "name": "Bernal JimÃ©nez GutiÃ©rrez",
                    "hidden": false
                },
                {
                    "_id": "67b878b8f17ca6989fd21e93",
                    "user": {
                        "_id": "60a4ebfbaa9320dbbe69e37c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a4ebfbaa9320dbbe69e37c/qQVuQYKWLklzogvpNzdc3.jpeg",
                        "isPro": false,
                        "fullname": "Yiheng Shu",
                        "user": "yhshu",
                        "type": "user"
                    },
                    "name": "Yiheng Shu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:15:54.523Z",
                    "hidden": false
                },
                {
                    "_id": "67b878b8f17ca6989fd21e94",
                    "user": {
                        "_id": "64d0e3cf484264a3b30e5696",
                        "avatarUrl": "/avatars/76c41fd195d7de179c746645dbbd1853.svg",
                        "isPro": false,
                        "fullname": "Qi",
                        "user": "WeijianQi",
                        "type": "user"
                    },
                    "name": "Weijian Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:16:01.233Z",
                    "hidden": false
                },
                {
                    "_id": "67b878b8f17ca6989fd21e95",
                    "user": {
                        "_id": "65030fc90a57e8f2b26bcaa3",
                        "avatarUrl": "/avatars/8c25a4a9735f268ee8541f3e2017d92c.svg",
                        "isPro": false,
                        "fullname": "Sizhe Zhou",
                        "user": "KevinSRR",
                        "type": "user"
                    },
                    "name": "Sizhe Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:16:08.161Z",
                    "hidden": false
                },
                {
                    "_id": "67b878b8f17ca6989fd21e96",
                    "name": "Yu Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:26:02.000Z",
            "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language\n  Models",
            "summary": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.",
            "upvotes": 6,
            "discussionId": "67b878bcf17ca6989fd21f7a"
        },
        "publishedAt": "2025-02-21T08:00:41.165Z",
        "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14802.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60a4ebfbaa9320dbbe69e37c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a4ebfbaa9320dbbe69e37c/qQVuQYKWLklzogvpNzdc3.jpeg",
            "fullname": "Yiheng Shu",
            "name": "yhshu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14866",
            "authors": [
                {
                    "_id": "67b7f46218d8b6a80a14220b",
                    "user": {
                        "_id": "641d8bacd526196afc12766d",
                        "avatarUrl": "/avatars/73f7b2d86a7bf27940bec2b1f199d71b.svg",
                        "isPro": false,
                        "fullname": "Shang Yang",
                        "user": "Shangy",
                        "type": "user"
                    },
                    "name": "Shang Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:16.384Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a14220c",
                    "user": {
                        "_id": "64e702982eba1760dfb0166c",
                        "avatarUrl": "/avatars/3291112513914d823cd524dafec66c87.svg",
                        "isPro": false,
                        "fullname": "Junxian Guo",
                        "user": "JerryGJX",
                        "type": "user"
                    },
                    "name": "Junxian Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:17:47.441Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a14220d",
                    "user": {
                        "_id": "646791c5374fe5728d403369",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646791c5374fe5728d403369/2kyCP48za0T3PTj-IR-J0.jpeg",
                        "isPro": false,
                        "fullname": "Haotian Tang",
                        "user": "kentang1998",
                        "type": "user"
                    },
                    "name": "Haotian Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:17:54.758Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a14220e",
                    "user": {
                        "_id": "67a6d5f424a2ace09c62640d",
                        "avatarUrl": "/avatars/0360e2543d791c4d046dd516eb70ced1.svg",
                        "isPro": false,
                        "fullname": "Qinghao Hu",
                        "user": "huqinghao",
                        "type": "user"
                    },
                    "name": "Qinghao Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:18:00.291Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a14220f",
                    "user": {
                        "_id": "6362fefe19cf373a5fc5b39e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362fefe19cf373a5fc5b39e/v4uJ5bzjpZJOxqGUHYPM2.jpeg",
                        "isPro": false,
                        "fullname": "Guangxuan Xiao",
                        "user": "Guangxuan-Xiao",
                        "type": "user"
                    },
                    "name": "Guangxuan Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:18:07.219Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a142210",
                    "user": {
                        "_id": "65ac13385860f06ff21c9a8a",
                        "avatarUrl": "/avatars/5c4c151e90c0dfc0e321623013594bbe.svg",
                        "isPro": false,
                        "fullname": "Jiaming Tang",
                        "user": "Dudep",
                        "type": "user"
                    },
                    "name": "Jiaming Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:18:13.068Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a142211",
                    "user": {
                        "_id": "66a156136609d2b2b0f6353a",
                        "avatarUrl": "/avatars/fc6850b5fc437269bf0870f6a6cdcf40.svg",
                        "isPro": false,
                        "fullname": "Yujun Lin",
                        "user": "synxlin",
                        "type": "user"
                    },
                    "name": "Yujun Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:18:19.276Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a142212",
                    "user": {
                        "_id": "650dac79b959b0e1d41d7378",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
                        "isPro": false,
                        "fullname": "Zhijian Liu",
                        "user": "zhijianliu",
                        "type": "user"
                    },
                    "name": "Zhijian Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:18:26.473Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a142213",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "67b7f46218d8b6a80a142214",
                    "user": {
                        "_id": "63797f727df2fefdcaf3ff7e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668906853549-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "songhan",
                        "type": "user"
                    },
                    "name": "Song Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:18:34.872Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:59:52.000Z",
            "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
            "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
            "upvotes": 4,
            "discussionId": "67b7f46318d8b6a80a142267"
        },
        "publishedAt": "2025-02-21T09:39:36.504Z",
        "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14866.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "640d3eaa3623f6a56dde856d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
            "fullname": "vansin",
            "name": "vansin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14044",
            "authors": [
                {
                    "_id": "67b87e3a346553e4006bf37c",
                    "user": {
                        "_id": "64beb6b6140491ca9f803ebf",
                        "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
                        "isPro": false,
                        "fullname": "Yucheng SHi",
                        "user": "YuchengShi",
                        "type": "user"
                    },
                    "name": "Yucheng Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:14:19.237Z",
                    "hidden": false
                },
                {
                    "_id": "67b87e3a346553e4006bf37d",
                    "name": "Quanzheng Li",
                    "hidden": false
                },
                {
                    "_id": "67b87e3a346553e4006bf37e",
                    "user": {
                        "_id": "6409cde91ee054d66a66c817",
                        "avatarUrl": "/avatars/ed18aa9902760a7ad6e9c5789b26dbe3.svg",
                        "isPro": false,
                        "fullname": "jin sun",
                        "user": "jinsun",
                        "type": "user"
                    },
                    "name": "Jin Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:15:13.205Z",
                    "hidden": false
                },
                {
                    "_id": "67b87e3a346553e4006bf37f",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "67b87e3a346553e4006bf380",
                    "name": "Ninghao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-19T19:05:45.000Z",
            "title": "Enhancing Cognition and Explainability of Multimodal Foundation Models\n  with Self-Synthesized Data",
            "summary": "Large multimodal models (LMMs) have shown impressive capabilities in a wide\nrange of visual tasks. However, they often struggle with fine-grained visual\nreasoning, failing to identify domain-specific objectives and provide\njustifiable explanations for their predictions. To address this, we propose a\nnovel visual rejection sampling framework to improve the cognition and\nexplainability of LMMs using self-synthesized data. Specifically, visual\nfine-tuning requires images, queries, and target answers. Our approach begins\nby synthesizing interpretable answers that include human-verifiable visual\nfeatures. These features are based on expert-defined concepts, carefully\nselected based on their alignment with the image content. After each round of\nfine-tuning, we apply a reward model-free filtering mechanism to select the\nhighest-quality interpretable answers for the next round of tuning. This\niterative process of data synthesis and fine-tuning progressively improves the\nmodel's ability to generate accurate and reasonable explanations. Experimental\nresults demonstrate the effectiveness of our method in improving both the\naccuracy and explainability of specialized visual classification tasks.",
            "upvotes": 4,
            "discussionId": "67b87e3d346553e4006bf416"
        },
        "publishedAt": "2025-02-21T08:26:31.100Z",
        "title": "Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14044.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64beb6b6140491ca9f803ebf",
            "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
            "fullname": "Yucheng SHi",
            "name": "YuchengShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14541",
            "authors": [
                {
                    "_id": "67b8439499159e6fc939970b",
                    "user": {
                        "_id": "67b841b821956199df64926b",
                        "avatarUrl": "/avatars/e714df967ca1b78425fa188b6843f057.svg",
                        "isPro": false,
                        "fullname": "Seunghwan Bang",
                        "user": "Breadbang",
                        "type": "user"
                    },
                    "name": "Seunghwan Bang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:49:31.964Z",
                    "hidden": false
                },
                {
                    "_id": "67b8439499159e6fc939970c",
                    "name": "Hwanjun Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T13:20:19.000Z",
            "title": "LLM-based User Profile Management for Recommender System",
            "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.",
            "upvotes": 4,
            "discussionId": "67b8439599159e6fc9399735"
        },
        "publishedAt": "2025-02-21T07:16:00.307Z",
        "title": "LLM-based User Profile Management for Recommender System",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14541.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67b841b821956199df64926b",
            "avatarUrl": "/avatars/e714df967ca1b78425fa188b6843f057.svg",
            "fullname": "Seunghwan Bang",
            "name": "Breadbang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14377",
            "authors": [
                {
                    "_id": "67b7f350357c2729ac216494",
                    "user": {
                        "_id": "66e4077369d1083dd97c7cd8",
                        "avatarUrl": "/avatars/0dad41e3e2f38f89b7b21c12d673f432.svg",
                        "isPro": false,
                        "fullname": "Ke Cao",
                        "user": "kecao",
                        "type": "user"
                    },
                    "name": "Ke Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:08:00.737Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac216495",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac216496",
                    "user": {
                        "_id": "65292034825f4bba46c78581",
                        "avatarUrl": "/avatars/7f212daaa20ab0d7405e9d6351ec308c.svg",
                        "isPro": false,
                        "fullname": "Ao Ma",
                        "user": "AoMa",
                        "type": "user"
                    },
                    "name": "Ao Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:07:53.347Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac216497",
                    "name": "Jiasong Feng",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac216498",
                    "user": {
                        "_id": "640496240ab5e22719f31719",
                        "avatarUrl": "/avatars/be957a1b0e1c037f03f1439d6142e9ce.svg",
                        "isPro": false,
                        "fullname": "Zhanjie Zhang",
                        "user": "zhangzhanjay",
                        "type": "user"
                    },
                    "name": "Zhanjie Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:07:37.282Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac216499",
                    "user": {
                        "_id": "64375035946fb080c6fc4551",
                        "avatarUrl": "/avatars/8dba2c8911726656d4088862a2b8fe7c.svg",
                        "isPro": false,
                        "fullname": "Xuanhua He",
                        "user": "Alexhe101",
                        "type": "user"
                    },
                    "name": "Xuanhua He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:07:32.005Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac21649a",
                    "user": {
                        "_id": "666197e302a5e5f4377a545c",
                        "avatarUrl": "/avatars/6d6ef51cc403ffccc81265ad8adf43bc.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "shanyuanLiu",
                        "type": "user"
                    },
                    "name": "Shanyuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:07:26.104Z",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac21649b",
                    "name": "Bo Cheng",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac21649c",
                    "name": "Dawei Leng",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac21649d",
                    "name": "Yuhui Yin",
                    "hidden": false
                },
                {
                    "_id": "67b7f350357c2729ac21649e",
                    "name": "Jie Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T09:10:05.000Z",
            "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
            "summary": "The Diffusion Transformer plays a pivotal role in advancing text-to-image and\ntext-to-video generation, owing primarily to its inherent scalability. However,\nexisting controlled diffusion transformer methods incur significant parameter\nand computational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance of control information\nacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\nthe Diffusion Transformer. First, we evaluate the relevance of each layer in\nthe Diffusion Transformer to the control information by assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace the self-attention and FFN in the commonly used\ncopy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM),\nenabling efficient implementation of both the token mixer and channel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achieves superior performance with only 15% of the parameters and\ncomputational complexity compared to PixArt-delta. More examples are available\nat https://relactrl.github.io/RelaCtrl/.",
            "upvotes": 4,
            "discussionId": "67b7f354357c2729ac216582"
        },
        "publishedAt": "2025-02-20T22:30:51.542Z",
        "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14669",
            "authors": [
                {
                    "_id": "67b7eeddaf9f1b1bd95b878b",
                    "user": {
                        "_id": "62d7b2339b629105a5d6888a",
                        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                        "isPro": false,
                        "fullname": "Alan Dao",
                        "user": "alandao",
                        "type": "user"
                    },
                    "name": "Alan Dao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:03:59.165Z",
                    "hidden": false
                },
                {
                    "_id": "67b7eeddaf9f1b1bd95b878c",
                    "name": "Dinh Bach Vu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T16:05:18.000Z",
            "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO",
            "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.",
            "upvotes": 4,
            "discussionId": "67b7eeddaf9f1b1bd95b87c8"
        },
        "publishedAt": "2025-02-20T22:11:45.130Z",
        "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6165
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.14854",
            "authors": [
                {
                    "_id": "67b7edf6a1d1394d1682c085",
                    "user": {
                        "_id": "65b976fdf69f4d0377aef3fe",
                        "avatarUrl": "/avatars/1201194e2956c56b50098cc465a04c11.svg",
                        "isPro": false,
                        "fullname": "Chau Minh Pham",
                        "user": "chtmp223",
                        "type": "user"
                    },
                    "name": "Chau Minh Pham",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:36.388Z",
                    "hidden": false
                },
                {
                    "_id": "67b7edf6a1d1394d1682c086",
                    "user": {
                        "_id": "5f1dcc06cb8f993fa01f4775",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668660782327-5f1dcc06cb8f993fa01f4775.png",
                        "isPro": false,
                        "fullname": "Yapei Chang",
                        "user": "yapeichang",
                        "type": "user"
                    },
                    "name": "Yapei Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:17:25.220Z",
                    "hidden": false
                },
                {
                    "_id": "67b7edf6a1d1394d1682c087",
                    "user": {
                        "_id": "6669df71d1652853e4b66ee5",
                        "avatarUrl": "/avatars/8a40bca9423ef76f39ae24d7a9e63478.svg",
                        "isPro": false,
                        "fullname": "Mohit Iyyer",
                        "user": "mohitiyyer",
                        "type": "user"
                    },
                    "name": "Mohit Iyyer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:17:30.925Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:58:03.000Z",
            "title": "CLIPPER: Compression enables long-context synthetic data generation",
            "summary": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA).",
            "upvotes": 3,
            "discussionId": "67b7edf8a1d1394d1682c0d4"
        },
        "publishedAt": "2025-02-21T07:52:55.537Z",
        "title": "CLIPPER: Compression enables long-context synthetic data generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14854.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65b976fdf69f4d0377aef3fe",
            "avatarUrl": "/avatars/1201194e2956c56b50098cc465a04c11.svg",
            "fullname": "Chau Minh Pham",
            "name": "chtmp223",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.12769",
            "authors": [
                {
                    "_id": "67b597146e53744c2a39e335",
                    "user": {
                        "_id": "6182910a68444be3259d8b67",
                        "avatarUrl": "/avatars/b47b0609f61b1192a1337fd7c9f8a75b.svg",
                        "isPro": false,
                        "fullname": "Saad Obaid ul Islam",
                        "user": "saadob12",
                        "type": "user"
                    },
                    "name": "Saad Obaid ul Islam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-20T15:53:18.182Z",
                    "hidden": false
                },
                {
                    "_id": "67b597146e53744c2a39e336",
                    "user": {
                        "_id": "626c02e7703f3b27dd590896",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654503075060-626c02e7703f3b27dd590896.jpeg",
                        "isPro": false,
                        "fullname": "Anne Lauscher",
                        "user": "anlausch",
                        "type": "user"
                    },
                    "name": "Anne Lauscher",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:05:49.990Z",
                    "hidden": false
                },
                {
                    "_id": "67b597146e53744c2a39e337",
                    "user": {
                        "_id": "6335af67a09fc16c7e7b4879",
                        "avatarUrl": "/avatars/047fad65ceb33203f97064d6a92ecdc1.svg",
                        "isPro": false,
                        "fullname": "Goran GlavaÅ¡",
                        "user": "gg42554",
                        "type": "user"
                    },
                    "name": "Goran GlavaÅ¡",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:05:56.804Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-18T11:32:43.000Z",
            "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual\n  Estimation of LLM Hallucination in the Wild",
            "summary": "In the age of misinformation, hallucination -- the tendency of Large Language\nModels (LLMs) to generate non-factual or unfaithful responses -- represents the\nmain risk for their global utility. Despite LLMs becoming increasingly\nmultilingual, the vast majority of research on detecting and quantifying LLM\nhallucination are (a) English-centric and (b) focus on machine translation (MT)\nand summarization, tasks that are less common ``in the wild'' than open\ninformation seeking. In contrast, we aim to quantify the extent of LLM\nhallucination across languages in knowledge-intensive long-form question\nanswering. To this end, we train a multilingual hallucination detection model\nand conduct a large-scale study across 30 languages and 6 open-source LLM\nfamilies. We start from an English hallucination detection dataset and rely on\nMT to generate (noisy) training data in other languages. We also manually\nannotate gold data for five high-resource languages; we then demonstrate, for\nthese languages, that the estimates of hallucination rates are similar between\nsilver (LLM-generated) and gold test sets, validating the use of silver data\nfor estimating hallucination rates for other languages. For the final rates\nestimation, we build a knowledge-intensive QA dataset for 30 languages with\nLLM-generated prompts and Wikipedia articles as references. We find that, while\nLLMs generate longer responses with more hallucinated tokens for\nhigher-resource languages, there is no correlation between length-normalized\nhallucination rates of languages and their digital representation. Further, we\nfind that smaller LLMs exhibit larger hallucination rates than larger models.",
            "upvotes": 3,
            "discussionId": "67b597156e53744c2a39e36f"
        },
        "publishedAt": "2025-02-21T05:28:42.882Z",
        "title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12769.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6182910a68444be3259d8b67",
            "avatarUrl": "/avatars/b47b0609f61b1192a1337fd7c9f8a75b.svg",
            "fullname": "Saad Obaid ul Islam",
            "name": "saadob12",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14409",
            "authors": [
                {
                    "_id": "67b83a20a9fa331061e84ecd",
                    "user": {
                        "_id": "60a643b9213fe60589b8fdf9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
                        "isPro": false,
                        "fullname": "Dustin Wright",
                        "user": "dwright37",
                        "type": "user"
                    },
                    "name": "Dustin Wright",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:02.288Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a20a9fa331061e84ece",
                    "user": {
                        "_id": "637e8b1b66ee00bcb2468ed0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669240174964-637e8b1b66ee00bcb2468ed0.jpeg",
                        "isPro": false,
                        "fullname": "Zain",
                        "user": "zainmujahid",
                        "type": "user"
                    },
                    "name": "Zain Muhammad Mujahid",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:16:52.600Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a20a9fa331061e84ecf",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "67b83a20a9fa331061e84ed0",
                    "user": {
                        "_id": "608918b7df398c3b285ce960",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621507769190-608918b7df398c3b285ce960.jpeg",
                        "isPro": false,
                        "fullname": "Isabelle Augenstein",
                        "user": "IAugenstein",
                        "type": "user"
                    },
                    "name": "Isabelle Augenstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:17:02.420Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a20a9fa331061e84ed1",
                    "user": {
                        "_id": "63516acdce7cf1fe8a854cdc",
                        "avatarUrl": "/avatars/980124c58796fbbf43008bacc3dc2261.svg",
                        "isPro": false,
                        "fullname": "David Jurgens",
                        "user": "davidjurgens",
                        "type": "user"
                    },
                    "name": "David Jurgens",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:17:08.686Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T09:57:42.000Z",
            "title": "Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization",
            "summary": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query. Extracting and properly citing\nevidence spans could help improve the transparency and reliability of these\nsummaries. At the same time, LLMs suffer from positional biases in terms of\nwhich information they understand and attend to, which could affect evidence\ncitation. Whereas previous work has focused on evidence citation with\npredefined levels of granularity (e.g. sentence, paragraph, document, etc.), we\npropose the task of long-context query focused summarization with unstructured\nevidence citation. We show how existing systems struggle to generate and\nproperly cite unstructured evidence from their context, and that evidence tends\nto be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with\nUnstructured Evidence Text dataset (SUnsET), a synthetic dataset generated\nusing a novel domain-agnostic pipeline which can be used as supervision to\nadapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4\ndatasets with varying document types and lengths that LLMs adapted with SUnsET\ndata generate more relevant and factually consistent evidence than their base\nmodels, extract evidence from more diverse locations in their context, and can\ngenerate more relevant and consistent summaries.",
            "upvotes": 3,
            "discussionId": "67b83a21a9fa331061e84f36"
        },
        "publishedAt": "2025-02-21T03:33:40.641Z",
        "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14409.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60a643b9213fe60589b8fdf9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
            "fullname": "Dustin Wright",
            "name": "dwright37",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.13759",
            "authors": [
                {
                    "_id": "67b83a1f26e7d5f7cb0b7c9d",
                    "user": {
                        "_id": "65407ba7a38390065750233f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
                        "isPro": false,
                        "fullname": "Zirui Song",
                        "user": "Ziruibest",
                        "type": "user"
                    },
                    "name": "Zirui Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:58:04.247Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7c9e",
                    "user": {
                        "_id": "67551c3578f56eff362039ab",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/87GrxXfW2VCgRHweCJzWz.png",
                        "isPro": false,
                        "fullname": "Jingpu Yang",
                        "user": "yyds404",
                        "type": "user"
                    },
                    "name": "Jingpu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:06:10.877Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7c9f",
                    "name": "Yuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7ca0",
                    "user": {
                        "_id": "641ed835f8c8b04c0ba7ac2a",
                        "avatarUrl": "/avatars/73af2b882e10938b230d4a2073e64098.svg",
                        "isPro": false,
                        "fullname": "Jonathan Tonglet",
                        "user": "saiga1420",
                        "type": "user"
                    },
                    "name": "Jonathan Tonglet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:06:50.179Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7ca1",
                    "user": {
                        "_id": "65b00730403a23a2fd765110",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "ZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-21T15:06:43.114Z",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7ca2",
                    "name": "Tao Cheng",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7ca3",
                    "name": "Meng Fang",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7ca4",
                    "name": "Iryna Gurevych",
                    "hidden": false
                },
                {
                    "_id": "67b83a1f26e7d5f7cb0b7ca5",
                    "name": "Xiuying Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-19T14:21:25.000Z",
            "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and\n  Human-Like Reasoning Framework",
            "summary": "Geolocation, the task of identifying an image's location, requires complex\nreasoning and is crucial for navigation, monitoring, and cultural preservation.\nHowever, current methods often produce coarse, imprecise, and non-interpretable\nlocalization. A major challenge lies in the quality and scale of existing\ngeolocation datasets. These datasets are typically small-scale and\nautomatically constructed, leading to noisy data and inconsistent task\ndifficulty, with images that either reveal answers too easily or lack\nsufficient clues for reliable inference. To address these challenges, we\nintroduce a comprehensive geolocation framework with three key components:\nGeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,\nan evaluation metric, collectively designed to address critical challenges and\ndrive advancements in geolocation research. At the core of this framework is\nGeoComp (Geolocation Competition Dataset), a large-scale dataset collected from\na geolocation game platform involving 740K users over two years. It comprises\n25 million entries of metadata and 3 million geo-tagged locations spanning much\nof the globe, with each location annotated thousands to tens of thousands of\ntimes by human users. The dataset offers diverse difficulty levels for detailed\nanalysis and highlights key gaps in current models. Building on this dataset,\nwe propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning\nframework designed to enhance the reasoning capabilities of Large Vision Models\n(LVMs) in geolocation tasks. GeoCoT improves performance by integrating\ncontextual and spatial cues through a multi-step process that mimics human\ngeolocation reasoning. Finally, using the GeoEval metric, we demonstrate that\nGeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing\ninterpretability.",
            "upvotes": 3,
            "discussionId": "67b83a2226e7d5f7cb0b7d66"
        },
        "publishedAt": "2025-02-21T03:33:28.852Z",
        "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13759.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "fullname": "Zirui Song",
            "name": "Ziruibest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.13928",
            "authors": [
                {
                    "_id": "67b7cdac904136d47c3966d8",
                    "user": {
                        "_id": "65222f97ef06bb99753cb829",
                        "avatarUrl": "/avatars/f1a743d74e6d38b916acaec91b4e7e4f.svg",
                        "isPro": false,
                        "fullname": "Shengguang Wu",
                        "user": "danielwusg",
                        "type": "user"
                    },
                    "name": "Shengguang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T09:59:21.001Z",
                    "hidden": false
                },
                {
                    "_id": "67b7cdac904136d47c3966d9",
                    "name": "Fan-Yun Sun",
                    "hidden": false
                },
                {
                    "_id": "67b7cdac904136d47c3966da",
                    "name": "Kaiyue Wen",
                    "hidden": false
                },
                {
                    "_id": "67b7cdac904136d47c3966db",
                    "name": "Nick Haber",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-19T18:05:42.000Z",
            "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language\n  Models with Minimal Contrastive Images",
            "summary": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to\nneglect image content and over-rely on language-model priors, resulting in\nerrors in visually grounded tasks and hallucinations. We hypothesize that this\nissue arises because existing VLMs are not explicitly trained to generate texts\nthat are accurately grounded in fine-grained image details. To enhance visual\nfeedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive\nOptimization), a novel finetuning objective that steers the model toward\ncapturing important visual details and aligning them with corresponding text\ntokens. To further facilitate this detailed alignment, we introduce MVC, a\npaired image-text dataset built by automatically filtering and augmenting\nvisual counterfactual data to challenge the model with hard contrastive cases\ninvolving Minimal Visual Contrasts. Experiments show that our method\nconsistently improves VLM performance across diverse benchmarks covering\nvarious abilities and domains, achieving up to a 22% reduction in\nhallucinations, and significant gains in vision-centric and general tasks.\nNotably, these improvements become increasingly pronounced in benchmarks with\nhigher visual dependency. In short, S-VCO offers a significant enhancement of\nVLM's visually-dependent task performance while retaining or even improving the\nmodel's general abilities. We opensource our code at https://s-vco.github.io/",
            "upvotes": 1,
            "discussionId": "67b7cdb8904136d47c396910"
        },
        "publishedAt": "2025-02-21T13:42:50.546Z",
        "title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13928.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65222f97ef06bb99753cb829",
            "avatarUrl": "/avatars/f1a743d74e6d38b916acaec91b4e7e4f.svg",
            "fullname": "Shengguang Wu",
            "name": "danielwusg",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14191",
            "authors": [
                {
                    "_id": "67b8aaa5ef55d96f2cbd7eaa",
                    "user": {
                        "_id": "621e9388345a1d9ab65391c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e9388345a1d9ab65391c3/RxurNzyAWJOUdgeSHQi1R.jpeg",
                        "isPro": false,
                        "fullname": "Michihiro Yasunaga",
                        "user": "michiyasunaga",
                        "type": "user"
                    },
                    "name": "Michihiro Yasunaga",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-21T16:39:03.035Z",
                    "hidden": false
                },
                {
                    "_id": "67b8aaa5ef55d96f2cbd7eab",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "67b8aaa5ef55d96f2cbd7eac",
                    "name": "Marjan Ghazvininejad",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T01:48:13.000Z",
            "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision\n  Language Models",
            "summary": "Reward models play an essential role in training vision-language models\n(VLMs) by assessing output quality to enable aligning with human preferences.\nDespite their importance, the research community lacks comprehensive open\nbenchmarks for evaluating multimodal reward models in VLMs. To address this\ngap, we introduce Multimodal RewardBench, an expert-annotated benchmark\ncovering six domains: general correctness, preference, knowledge, reasoning,\nsafety, and visual question-answering. Our dataset comprises 5,211 annotated\n(prompt, chosen response, rejected response) triplets collected from various\nVLMs. In evaluating a range of VLM judges, we find that even the top-performing\nmodels, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall\naccuracy. Notably, most models struggle in the reasoning and safety domains.\nThese findings suggest that Multimodal RewardBench offers a challenging testbed\nfor advancing reward model development across multiple domains. We release the\nbenchmark at https://github.com/facebookresearch/multimodal_rewardbench.",
            "upvotes": 1,
            "discussionId": "67b8aaa6ef55d96f2cbd7edf"
        },
        "publishedAt": "2025-02-21T11:34:53.838Z",
        "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/621e9388345a1d9ab65391c3/FaHBHPH3KH5KQ-kQ4bBek.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14191.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "621e9388345a1d9ab65391c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e9388345a1d9ab65391c3/RxurNzyAWJOUdgeSHQi1R.jpeg",
            "fullname": "Michihiro Yasunaga",
            "name": "michiyasunaga",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14842",
            "authors": [
                {
                    "_id": "67b8c05e109d4be55d85d1f0",
                    "name": "Alexia Jolicoeur-Martineau",
                    "hidden": false
                },
                {
                    "_id": "67b8c05e109d4be55d85d1f1",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67b8c05e109d4be55d85d1f2",
                    "user": {
                        "_id": "63e16165f0039731dfdd442a",
                        "avatarUrl": "/avatars/37cf99dc016c291c800f60d260173482.svg",
                        "isPro": false,
                        "fullname": "Boris Knyazev",
                        "user": "bknyaz",
                        "type": "user"
                    },
                    "name": "Boris Knyazev",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-21T18:14:23.859Z",
                    "hidden": false
                },
                {
                    "_id": "67b8c05e109d4be55d85d1f3",
                    "name": "Aristide Baratin",
                    "hidden": false
                },
                {
                    "_id": "67b8c05e109d4be55d85d1f4",
                    "name": "Cheng-Hao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:52:42.000Z",
            "title": "Generating Ï-Functional Molecules Using STGG+ with Active Learning",
            "summary": "Generating novel molecules with out-of-distribution properties is a major\nchallenge in molecular discovery. While supervised learning methods generate\nhigh-quality molecules similar to those in a dataset, they struggle to\ngeneralize to out-of-distribution properties. Reinforcement learning can\nexplore new chemical spaces but often conducts 'reward-hacking' and generates\nnon-synthesizable molecules. In this work, we address this problem by\nintegrating a state-of-the-art supervised learning method, STGG+, in an active\nlearning loop. Our approach iteratively generates, evaluates, and fine-tunes\nSTGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We\napply STGG+AL to the design of organic pi-functional materials, specifically\ntwo challenging tasks: 1) generating highly absorptive molecules characterized\nby high oscillator strength and 2) designing absorptive molecules with\nreasonable oscillator strength in the near-infrared (NIR) range. The generated\nmolecules are validated and rationalized in-silico with time-dependent density\nfunctional theory. Our results demonstrate that our method is highly effective\nin generating novel molecules with high oscillator strength, contrary to\nexisting methods such as reinforcement learning (RL) methods. We open-source\nour active-learning code along with our Conjugated-xTB dataset containing 2.9\nmillion pi-conjugated molecules and the function for approximating the\noscillator strength and absorption wavelength (based on sTDA-xTB).",
            "upvotes": 0,
            "discussionId": "67b8c05f109d4be55d85d249"
        },
        "publishedAt": "2025-02-21T13:05:36.173Z",
        "title": "Generating $Ï$-Functional Molecules Using STGG+ with Active Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14842.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 768
        },
        "isAuthorParticipating": false
    }
]