[
    {
        "paper": {
            "id": "2502.18411",
            "authors": [
                {
                    "_id": "67be834ae7b05f9e43b172b2",
                    "user": {
                        "_id": "6530e62f536dbca918e71c3e",
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:26:02.247Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b3",
                    "user": {
                        "_id": "646cd947da8e99940b6e55cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
                        "isPro": false,
                        "fullname": "Shengyuan Ding",
                        "user": "ChrisDing1105",
                        "type": "user"
                    },
                    "name": "Shengyuan Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:59.887Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b4",
                    "user": {
                        "_id": "675aa937ab6aa7ecd09341ce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/d_CNUsNOw92pg7MVhf9Vm.png",
                        "isPro": false,
                        "fullname": "Zicheng Zhang",
                        "user": "UniverseCA",
                        "type": "user"
                    },
                    "name": "Zicheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:10.028Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b5",
                    "name": "Haian Huang",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b6",
                    "name": "Maosong Cao",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b7",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:45.520Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b8",
                    "user": {
                        "_id": "64638c4d51fa6e63060521b5",
                        "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
                        "isPro": false,
                        "fullname": "JIaqi",
                        "user": "Jiaqiwang",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:38.876Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172b9",
                    "user": {
                        "_id": "64f5f8dd9b17cd59c453c57f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                        "isPro": false,
                        "fullname": "Xinyu Fang",
                        "user": "nebulae09",
                        "type": "user"
                    },
                    "name": "Xinyu Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:26:04.433Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172ba",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:28.151Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172bb",
                    "name": "Guangtao Zhai",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172bc",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:48:20.155Z",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172bd",
                    "name": "Hua Yang",
                    "hidden": false
                },
                {
                    "_id": "67be834ae7b05f9e43b172be",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T18:05:14.000Z",
            "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
            "summary": "Recent advancements in open-source multi-modal large language models (MLLMs)\nhave primarily focused on enhancing foundational capabilities, leaving a\nsignificant gap in human preference alignment. This paper introduces\nOmniAlign-V, a comprehensive dataset of 200K high-quality training samples\nfeaturing diverse images, complex questions, and varied response formats to\nimprove MLLMs' alignment with human preferences. We also present MM-AlignBench,\na human-annotated benchmark specifically designed to evaluate MLLMs' alignment\nwith human values. Experimental results show that finetuning MLLMs with\nOmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference\nOptimization (DPO), significantly enhances human preference alignment while\nmaintaining or enhancing performance on standard VQA benchmarks, preserving\ntheir fundamental capabilities. Our datasets, benchmark, code and checkpoints\nhave been released at https://github.com/PhoenixZ810/OmniAlign-V.",
            "upvotes": 54,
            "discussionId": "67be834ce7b05f9e43b1730a"
        },
        "publishedAt": "2025-02-25T22:01:56.532Z",
        "title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6530e62f536dbca918e71c3e",
            "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
            "fullname": "Xiangyu Z",
            "name": "PhoenixZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18137",
            "authors": [
                {
                    "_id": "67be8443ed8e258c0f70063a",
                    "user": {
                        "_id": "66c0a08bac74db25de8427ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                        "isPro": false,
                        "fullname": "Jintao Zhang",
                        "user": "jt-zhang",
                        "type": "user"
                    },
                    "name": "Jintao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:57.704Z",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063b",
                    "user": {
                        "_id": "6329bdbbde087eac2921e6a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663679904323-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiangchendong",
                        "user": "Xiang-cd",
                        "type": "user"
                    },
                    "name": "Chendong Xiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:29.341Z",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063c",
                    "name": "Haofeng Huang",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063d",
                    "name": "Jia Wei",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063e",
                    "user": {
                        "_id": "65d5a000ec7e31555e4db57e",
                        "avatarUrl": "/avatars/aab8319fbaffdd53faff59a40ca5a5ea.svg",
                        "isPro": false,
                        "fullname": "Haocheng Xi",
                        "user": "hxi0408",
                        "type": "user"
                    },
                    "name": "Haocheng Xi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:45.446Z",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f70063f",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "67be8443ed8e258c0f700640",
                    "user": {
                        "_id": "65fcad0ba0d7adc40b54fac2",
                        "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
                        "isPro": false,
                        "fullname": "Jianfei Chen",
                        "user": "surfingtomchen",
                        "type": "user"
                    },
                    "name": "Jianfei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:49:52.550Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T12:02:17.000Z",
            "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
            "summary": "An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn.",
            "upvotes": 41,
            "discussionId": "67be8447ed8e258c0f70075f"
        },
        "publishedAt": "2025-02-25T22:04:57.351Z",
        "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18137.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "fullname": "Jintao Zhang",
            "name": "jt-zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18449",
            "authors": [
                {
                    "_id": "67be845a8a5a80542314579f",
                    "user": {
                        "_id": "632a176259950c1d279d5ea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a176259950c1d279d5ea7/xsSGhBXalt9RaKzSKY8uk.jpeg",
                        "isPro": false,
                        "fullname": "Yuxiang Wei",
                        "user": "yuxiang630",
                        "type": "user"
                    },
                    "name": "Yuxiang Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:50:44.837Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a0",
                    "name": "Olivier Duchenne",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a1",
                    "user": {
                        "_id": "6481e0ac50b759c75d5fdad0",
                        "avatarUrl": "/avatars/49f08d989ca505ae01bce5578a94f6fe.svg",
                        "isPro": false,
                        "fullname": "Jade Copet",
                        "user": "JadeCopet",
                        "type": "user"
                    },
                    "name": "Jade Copet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:50:58.290Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a2",
                    "name": "Quentin Carbonneaux",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a3",
                    "user": {
                        "_id": "656f473c14fa8cfccd14559e",
                        "avatarUrl": "/avatars/8f4fef3d835a7a11c2ab66dbf04f3424.svg",
                        "isPro": false,
                        "fullname": "Lingming Zhang",
                        "user": "lingming",
                        "type": "user"
                    },
                    "name": "Lingming Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:10.640Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a4",
                    "name": "Daniel Fried",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a5",
                    "user": {
                        "_id": "630eac7931970d1cd4fbacf2",
                        "avatarUrl": "/avatars/b7ccbddfa745db854dc342be1327cd53.svg",
                        "isPro": false,
                        "fullname": "Gabriel Synnaeve",
                        "user": "gsynnaeve",
                        "type": "user"
                    },
                    "name": "Gabriel Synnaeve",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:21.641Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a6",
                    "user": {
                        "_id": "6597e5a6420dcc68501a69e9",
                        "avatarUrl": "/avatars/da48b13e07c367ecd5c891abfd6c3ded.svg",
                        "isPro": false,
                        "fullname": "Rishabh Singh",
                        "user": "RishabhSingh021",
                        "type": "user"
                    },
                    "name": "Rishabh Singh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:28.321Z",
                    "hidden": false
                },
                {
                    "_id": "67be845a8a5a8054231457a7",
                    "name": "Sida I. Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T18:45:04.000Z",
            "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution",
            "summary": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.",
            "upvotes": 37,
            "discussionId": "67be845b8a5a8054231457d6"
        },
        "publishedAt": "2025-02-25T22:03:08.515Z",
        "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18449.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6226
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17363",
            "authors": [
                {
                    "_id": "67bd6d2bbf6d46017e619f31",
                    "user": {
                        "_id": "66078994c50f8393c56ed837",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/aYYde45zaFACRllyEhJyU.jpeg",
                        "isPro": true,
                        "fullname": "Tianrui Zhu",
                        "user": "xilluill",
                        "type": "user"
                    },
                    "name": "Tianrui Zhu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-25T07:24:35.845Z",
                    "hidden": false
                },
                {
                    "_id": "67bd6d2bbf6d46017e619f32",
                    "user": {
                        "_id": "6315d306a9456afe2b9bf34a",
                        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
                        "isPro": false,
                        "fullname": "ElevenZ",
                        "user": "shiyi0408",
                        "type": "user"
                    },
                    "name": "Shiyi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:30:48.262Z",
                    "hidden": false
                },
                {
                    "_id": "67bd6d2bbf6d46017e619f33",
                    "user": {
                        "_id": "646c6985d072747f7ebf352a",
                        "avatarUrl": "/avatars/8aaf92045687b21b56c257db62bf4fa5.svg",
                        "isPro": false,
                        "fullname": "Jiawei Shao",
                        "user": "jewelshaw",
                        "type": "user"
                    },
                    "name": "Jiawei Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:50:09.030Z",
                    "hidden": false
                },
                {
                    "_id": "67bd6d2bbf6d46017e619f34",
                    "name": "Yansong Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T17:40:09.000Z",
            "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
            "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to O(1) using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
            "upvotes": 26,
            "discussionId": "67bd6d2dbf6d46017e619f99"
        },
        "publishedAt": "2025-02-25T21:36:19.851Z",
        "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17363.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66078994c50f8393c56ed837",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/aYYde45zaFACRllyEhJyU.jpeg",
            "fullname": "Tianrui Zhu",
            "name": "xilluill",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18364",
            "authors": [
                {
                    "_id": "67be81414084d82ee69ad4a2",
                    "user": {
                        "_id": "647e83257f9ad5e44babe82a",
                        "avatarUrl": "/avatars/2d9593775c49856fe5dfa5bd23dfcda7.svg",
                        "isPro": false,
                        "fullname": "yifan pu",
                        "user": "yifanpu001",
                        "type": "user"
                    },
                    "name": "Yifan Pu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:24.942Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a3",
                    "user": {
                        "_id": "65e78ebf24a38e0fc5e149e6",
                        "avatarUrl": "/avatars/d05e267f29de7de226c4fc0ae37c95ff.svg",
                        "isPro": false,
                        "fullname": "Yiming Zhao",
                        "user": "2JZ",
                        "type": "user"
                    },
                    "name": "Yiming Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:30.860Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a4",
                    "name": "Zhicong Tang",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a5",
                    "name": "Ruihong Yin",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a6",
                    "user": {
                        "_id": "65229f2f6b01183a67e86370",
                        "avatarUrl": "/avatars/b218207fce28497b30e22c807d44b2d2.svg",
                        "isPro": false,
                        "fullname": "Haoxing Ye",
                        "user": "131131yhx",
                        "type": "user"
                    },
                    "name": "Haoxing Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:52.821Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a7",
                    "name": "Yuhui Yuan",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a8",
                    "user": {
                        "_id": "666470a28f5513b0cf11e850",
                        "avatarUrl": "/avatars/7beea758882677ad32a12ce56d4d084a.svg",
                        "isPro": false,
                        "fullname": "Dong Chen",
                        "user": "DongChen06",
                        "type": "user"
                    },
                    "name": "Dong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:16.526Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4a9",
                    "user": {
                        "_id": "646b2f4bb1202bc77c0fb396",
                        "avatarUrl": "/avatars/6b09dec5d5affe817ad6acda60f61740.svg",
                        "isPro": false,
                        "fullname": "Jianmin_bao",
                        "user": "JianminBao",
                        "type": "user"
                    },
                    "name": "Jianmin Bao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:22.654Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4aa",
                    "user": {
                        "_id": "64f7f119a92703ef65d9a717",
                        "avatarUrl": "/avatars/118524faab66cecba6d4da622034b44b.svg",
                        "isPro": false,
                        "fullname": "Sirui Zhang",
                        "user": "zsr200901",
                        "type": "user"
                    },
                    "name": "Sirui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:30.766Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ab",
                    "user": {
                        "_id": "67965a5a9f57883759a6efc3",
                        "avatarUrl": "/avatars/9138a879fbe1f60c2f4720810bfdfda6.svg",
                        "isPro": false,
                        "fullname": "Yanbin Wang",
                        "user": "yanbinwang",
                        "type": "user"
                    },
                    "name": "Yanbin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:38.138Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ac",
                    "name": "Lin Liang",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ad",
                    "user": {
                        "_id": "6672e20d1dbdf7da8310dd92",
                        "avatarUrl": "/avatars/5d2fb23f92a7f9ff025a5be17a26de4d.svg",
                        "isPro": false,
                        "fullname": "lijuanwang",
                        "user": "lijuanwang228",
                        "type": "user"
                    },
                    "name": "Lijuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:00:05.520Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4ae",
                    "name": "Ji Li",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4af",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4b0",
                    "user": {
                        "_id": "64c882f7527d7636555bbb2c",
                        "avatarUrl": "/avatars/578a118a945dd6fa62fd3be9d6e4e986.svg",
                        "isPro": false,
                        "fullname": "Zhouhui Lian",
                        "user": "lianzhouhui",
                        "type": "user"
                    },
                    "name": "Zhouhui Lian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:59:57.943Z",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4b1",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "67be81414084d82ee69ad4b2",
                    "name": "Baining Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T16:57:04.000Z",
            "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent\n  Image Generation",
            "summary": "Multi-layer image generation is a fundamental task that enables users to\nisolate, select, and edit specific image layers, thereby revolutionizing\ninteractions with generative models. In this paper, we introduce the Anonymous\nRegion Transformer (ART), which facilitates the direct generation of variable\nmulti-layer transparent images based on a global text prompt and an anonymous\nregion layout. Inspired by Schema theory suggests that knowledge is organized\nin frameworks (schemas) that enable people to interpret and learn from new\ninformation by linking it to prior knowledge.}, this anonymous region layout\nallows the generative model to autonomously determine which set of visual\ntokens should align with which text tokens, which is in contrast to the\npreviously dominant semantic layout for the image generation task. In addition,\nthe layer-wise region crop mechanism, which only selects the visual tokens\nbelonging to each anonymous region, significantly reduces attention computation\ncosts and enables the efficient generation of images with numerous distinct\nlayers (e.g., 50+). When compared to the full attention approach, our method is\nover 12 times faster and exhibits fewer layer conflicts. Furthermore, we\npropose a high-quality multi-layer transparent image autoencoder that supports\nthe direct encoding and decoding of the transparency of variable multi-layer\nimages in a joint manner. By enabling precise control and scalable layer\ngeneration, ART establishes a new paradigm for interactive content creation.",
            "upvotes": 21,
            "discussionId": "67be81464084d82ee69ad576"
        },
        "publishedAt": "2025-02-25T21:50:19.941Z",
        "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18364.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646f69a6041e48e1c4728de3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f69a6041e48e1c4728de3/U5OaW6PgsXTXnfG03xs9Q.png",
            "fullname": "GlyphByT5",
            "name": "GlyphByT5",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 34
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17262",
            "authors": [
                {
                    "_id": "67bd3870a917fc506d9f3d15",
                    "user": {
                        "_id": "66ab06956b8847339d449128",
                        "avatarUrl": "/avatars/d71490acb91981459121005b84e556d8.svg",
                        "isPro": false,
                        "fullname": "Xu Chengyin",
                        "user": "JerryXu98",
                        "type": "user"
                    },
                    "name": "Chengyin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-25T09:39:44.252Z",
                    "hidden": false
                },
                {
                    "_id": "67bd3870a917fc506d9f3d16",
                    "user": {
                        "_id": "636b4d796e6981ebad73f398",
                        "avatarUrl": "/avatars/bcd405b98c12afaf1e32d85ad8ce7f23.svg",
                        "isPro": false,
                        "fullname": "Kaiyuan Chen",
                        "user": "Lucky2022",
                        "type": "user"
                    },
                    "name": "Kaiyuan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-25T09:40:01.532Z",
                    "hidden": false
                },
                {
                    "_id": "67bd3870a917fc506d9f3d17",
                    "name": "Xiao Li",
                    "hidden": false
                },
                {
                    "_id": "67bd3870a917fc506d9f3d18",
                    "user": {
                        "_id": "645604eebabbbbd3486dc615",
                        "avatarUrl": "/avatars/17a5ca8274e2bfc8f183a4af9878a930.svg",
                        "isPro": false,
                        "fullname": "shenke",
                        "user": "shenke18",
                        "type": "user"
                    },
                    "name": "Ke Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-25T09:39:49.578Z",
                    "hidden": false
                },
                {
                    "_id": "67bd3870a917fc506d9f3d19",
                    "name": "Chenggang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T15:44:57.000Z",
            "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective",
            "summary": "The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks.",
            "upvotes": 14,
            "discussionId": "67bd3872a917fc506d9f3d8f"
        },
        "publishedAt": "2025-02-25T22:18:24.064Z",
        "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17262.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636b4d796e6981ebad73f398",
            "avatarUrl": "/avatars/bcd405b98c12afaf1e32d85ad8ce7f23.svg",
            "fullname": "Kaiyuan Chen",
            "name": "Lucky2022",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.15499",
            "authors": [
                {
                    "_id": "67be86743ea16c7e9491ff16",
                    "name": "Ya Wang",
                    "hidden": false
                },
                {
                    "_id": "67be86743ea16c7e9491ff17",
                    "user": {
                        "_id": "66335b9c95c5b79ebf306f30",
                        "avatarUrl": "/avatars/d57784ee65cbef014360c9bac1ad4119.svg",
                        "isPro": false,
                        "fullname": "Zhijian Zhuo",
                        "user": "BryceZhuo",
                        "type": "user"
                    },
                    "name": "Zhijian Zhuo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:58:10.556Z",
                    "hidden": false
                },
                {
                    "_id": "67be86743ea16c7e9491ff18",
                    "user": {
                        "_id": "6371128eafbe42caa5a5222b",
                        "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
                        "isPro": false,
                        "fullname": "Yutao Zeng",
                        "user": "Taoer",
                        "type": "user"
                    },
                    "name": "Yutao Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:55.016Z",
                    "hidden": false
                },
                {
                    "_id": "67be86743ea16c7e9491ff19",
                    "user": {
                        "_id": "62533db4a06ec75172eeabe7",
                        "avatarUrl": "/avatars/b1a4dad90afae5c00df97233a97777db.svg",
                        "isPro": false,
                        "fullname": "xunzhou",
                        "user": "xunzhou",
                        "type": "user"
                    },
                    "name": "Xun Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:52:26.974Z",
                    "hidden": false
                },
                {
                    "_id": "67be86743ea16c7e9491ff1a",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "67be86743ea16c7e9491ff1b",
                    "user": {
                        "_id": "64648638351adef1a847a7ad",
                        "avatarUrl": "/avatars/7518e058fcf81ee81a06c96e996531e9.svg",
                        "isPro": false,
                        "fullname": "Xiaoqing Li",
                        "user": "LLIXQ",
                        "type": "user"
                    },
                    "name": "Xiaoqing Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T08:51:57.314Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-21T14:49:34.000Z",
            "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models",
            "summary": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing gradient explosion\nand dissipation. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.",
            "upvotes": 12,
            "discussionId": "67be86753ea16c7e9491ff49"
        },
        "publishedAt": "2025-02-25T22:26:11.421Z",
        "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eu6jpeTjTn34I1SJ4_K1a.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/P6mXXagZPsH6fwQ6myMlr.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "fullname": "Yutao Zeng",
            "name": "Taoer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.16069",
            "authors": [
                {
                    "_id": "67bf51f8653c05485b571e71",
                    "name": "Patrick Tser Jern Kon",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e72",
                    "name": "Jiachen Liu",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e73",
                    "name": "Qiuyi Ding",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e74",
                    "name": "Yiming Qiu",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e75",
                    "name": "Zhenning Yang",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e76",
                    "name": "Yibo Huang",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e77",
                    "name": "Jayanth Srinivasa",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e78",
                    "name": "Myungjin Lee",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e79",
                    "name": "Mosharaf Chowdhury",
                    "hidden": false
                },
                {
                    "_id": "67bf51f8653c05485b571e7a",
                    "name": "Ang Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-22T03:58:19.000Z",
            "title": "Curie: Toward Rigorous and Automated Scientific Experimentation with AI\n  Agents",
            "summary": "Scientific experimentation, a cornerstone of human progress, demands rigor in\nreliability, methodical control, and interpretability to yield meaningful\nresults. Despite the growing capabilities of large language models (LLMs) in\nautomating different aspects of the scientific process, automating rigorous\nexperimentation remains a significant challenge. To address this gap, we\npropose Curie, an AI agent framework designed to embed rigor into the\nexperimentation process through three key components: an intra-agent rigor\nmodule to enhance reliability, an inter-agent rigor module to maintain\nmethodical control, and an experiment knowledge module to enhance\ninterpretability. To evaluate Curie, we design a novel experimental benchmark\ncomposed of 46 questions across four computer science domains, derived from\ninfluential research papers, and widely adopted open-source projects. Compared\nto the strongest baseline tested, we achieve a 3.4times improvement in\ncorrectly answering experimental questions.Curie is open-sourced at\nhttps://github.com/Just-Curieous/Curie.",
            "upvotes": 9,
            "discussionId": "67bf51fa653c05485b571f00"
        },
        "publishedAt": "2025-02-26T12:51:05.089Z",
        "title": "Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16069.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648fc22019e7511674b31f12",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648fc22019e7511674b31f12/9kRR00GMFYcuj6zR0BVfx.jpeg",
            "fullname": "Amber",
            "name": "AmberLJC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.18461",
            "authors": [
                {
                    "_id": "67bea0cc2d6011a72335f704",
                    "user": {
                        "_id": "67be9daa65ae638b17e461e9",
                        "avatarUrl": "/avatars/30ab04b8a6a4d3e1d211943c0344b95e.svg",
                        "isPro": false,
                        "fullname": "Ziheng Ouyang",
                        "user": "oyzh2005",
                        "type": "user"
                    },
                    "name": "Ziheng Ouyang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:46.941Z",
                    "hidden": false
                },
                {
                    "_id": "67bea0cc2d6011a72335f705",
                    "user": {
                        "_id": "6285a9133ab6642179158944",
                        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
                        "isPro": false,
                        "fullname": "Zhen Li",
                        "user": "Paper99",
                        "type": "user"
                    },
                    "name": "Zhen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T15:37:36.061Z",
                    "hidden": false
                },
                {
                    "_id": "67bea0cc2d6011a72335f706",
                    "name": "Qibin Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T18:59:12.000Z",
            "title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs",
            "summary": "Recent studies have explored combining different LoRAs to jointly generate\nlearned style and content. However, existing methods either fail to effectively\npreserve both the original subject and style simultaneously or require\nadditional training. In this paper, we argue that the intrinsic properties of\nLoRA can effectively guide diffusion models in merging learned subject and\nstyle. Building on this insight, we propose K-LoRA, a simple yet effective\ntraining-free LoRA fusion approach. In each attention layer, K-LoRA compares\nthe Top-K elements in each LoRA to be fused, determining which LoRA to select\nfor optimal fusion. This selection mechanism ensures that the most\nrepresentative features of both subject and style are retained during the\nfusion process, effectively balancing their contributions. Experimental results\ndemonstrate that the proposed method effectively integrates the subject and\nstyle information learned by the original LoRAs, outperforming state-of-the-art\ntraining-based approaches in both qualitative and quantitative results.",
            "upvotes": 9,
            "discussionId": "67bea0cf2d6011a72335f7aa"
        },
        "publishedAt": "2025-02-26T00:56:27.275Z",
        "title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18461.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6285a9133ab6642179158944",
            "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
            "fullname": "Zhen Li",
            "name": "Paper99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18356",
            "authors": [
                {
                    "_id": "67be8866823e790d21a2bb90",
                    "user": {
                        "_id": "6529aa1460e706730575baa9",
                        "avatarUrl": "/avatars/550fac58a6ebf937a65d19a48e71eb45.svg",
                        "isPro": false,
                        "fullname": "George Thomas",
                        "user": "georgethomas",
                        "type": "user"
                    },
                    "name": "George Thomas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:06:53.500Z",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb91",
                    "user": {
                        "_id": "636c1e4415cd58e915bc45df",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c1e4415cd58e915bc45df/KnPgdPe0G5ngvXaCBua6R.jpeg",
                        "isPro": false,
                        "fullname": "Alex J. Chan",
                        "user": "XanderJC",
                        "type": "user"
                    },
                    "name": "Alex J. Chan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-26T03:20:08.029Z",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb92",
                    "user": {
                        "_id": "6489e10ca13f65198dc6e122",
                        "avatarUrl": "/avatars/4aa9eab488157711b2f0298ddadee2f4.svg",
                        "isPro": false,
                        "fullname": "Kang",
                        "user": "JaxonK",
                        "type": "user"
                    },
                    "name": "Jikun Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T15:37:37.981Z",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb93",
                    "user": {
                        "_id": "63a2f1dfc8a2aa5d9e85f8f6",
                        "avatarUrl": "/avatars/f2191e3a0ce92563f9bfe83283d8d966.svg",
                        "isPro": false,
                        "fullname": "Wenqi Wu",
                        "user": "BiggieW",
                        "type": "user"
                    },
                    "name": "Wenqi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:08:43.843Z",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb94",
                    "user": {
                        "_id": "64f46b681d337935d0495d4d",
                        "avatarUrl": "/avatars/cce5a4910617931fb13062b832e14ef8.svg",
                        "isPro": false,
                        "fullname": "Filippos Christianos",
                        "user": "semitable",
                        "type": "user"
                    },
                    "name": "Filippos Christianos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:08:23.534Z",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb95",
                    "user": {
                        "_id": "5f195784925b9863e28ad610",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1595496291585-noauth.png",
                        "isPro": false,
                        "fullname": "Fraser Greenlee",
                        "user": "Fraser",
                        "type": "user"
                    },
                    "name": "Fraser Greenlee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:08:16.380Z",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb96",
                    "name": "Andy Toulis",
                    "hidden": false
                },
                {
                    "_id": "67be8866823e790d21a2bb97",
                    "user": {
                        "_id": "6787c4a970c0f5272f456968",
                        "avatarUrl": "/avatars/bdfa53add57b0f0a9e4e94e24115b354.svg",
                        "isPro": false,
                        "fullname": "Marvin Purtorab",
                        "user": "comvergent-marvin",
                        "type": "user"
                    },
                    "name": "Marvin Purtorab",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:07:42.610Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T16:45:08.000Z",
            "title": "WebGames: Challenging General-Purpose Web-Browsing AI Agents",
            "summary": "We introduce WebGames, a comprehensive benchmark suite designed to evaluate\ngeneral-purpose web-browsing AI agents through a collection of 50+ interactive\nchallenges. These challenges are specifically crafted to be straightforward for\nhumans while systematically testing the limitations of current AI systems\nacross fundamental browser interactions, advanced input processing, cognitive\ntasks, workflow automation, and interactive entertainment. Our framework\neliminates external dependencies through a hermetic testing environment,\nensuring reproducible evaluation with verifiable ground-truth solutions. We\nevaluate leading vision-language models including GPT-4o, Claude Computer-Use,\nGemini-1.5-Pro, and Qwen2-VL against human performance. Results reveal a\nsubstantial capability gap, with the best AI system achieving only 43.1%\nsuccess rate compared to human performance of 95.7%, highlighting fundamental\nlimitations in current AI systems' ability to handle common web interaction\npatterns that humans find intuitive. The benchmark is publicly available at\nwebgames.convergence.ai, offering a lightweight, client-side implementation\nthat facilitates rapid evaluation cycles. Through its modular architecture and\nstandardized challenge specifications, WebGames provides a robust foundation\nfor measuring progress in development of more capable web-browsing agents.",
            "upvotes": 6,
            "discussionId": "67be8868823e790d21a2bbea"
        },
        "publishedAt": "2025-02-25T22:20:16.916Z",
        "title": "WebGames: Challenging General-Purpose Web-Browsing AI Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18356.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6226
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17425",
            "authors": [
                {
                    "_id": "67bddd63c7d8b835b82ced9a",
                    "user": {
                        "_id": "635364b3c41f548fe39db945",
                        "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
                        "isPro": false,
                        "fullname": "Runpeng Yu",
                        "user": "rp-yu",
                        "type": "user"
                    },
                    "name": "Runpeng Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:14:07.580Z",
                    "hidden": false
                },
                {
                    "_id": "67bddd63c7d8b835b82ced9b",
                    "user": {
                        "_id": "64396ebc21221ac7411852b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg",
                        "isPro": false,
                        "fullname": "Xinyin Ma",
                        "user": "horseee",
                        "type": "user"
                    },
                    "name": "Xinyin Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:14:13.670Z",
                    "hidden": false
                },
                {
                    "_id": "67bddd63c7d8b835b82ced9c",
                    "user": {
                        "_id": "63fc03a50aab060792ffef39",
                        "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
                        "isPro": false,
                        "fullname": "Wangxinchao",
                        "user": "wxcTest",
                        "type": "user"
                    },
                    "name": "Xinchao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:14:53.838Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T18:56:12.000Z",
            "title": "Introducing Visual Perception Token into Multimodal Large Language Model",
            "summary": "To utilize visual information, Multimodal Large Language Model (MLLM) relies\non the perception process of its vision encoder. The completeness and accuracy\nof visual perception significantly influence the precision of spatial\nreasoning, fine-grained understanding, and other tasks. However, MLLM still\nlacks the autonomous capability to control its own visual perception processes,\nfor example, selectively reviewing specific regions of an image or focusing on\ninformation related to specific object categories. In this work, we propose the\nconcept of Visual Perception Token, aiming to empower MLLM with a mechanism to\ncontrol its visual perception processes. We design two types of Visual\nPerception Tokens, termed the Region Selection Token and the Vision Re-Encoding\nToken. MLLMs autonomously generate these tokens, just as they generate text,\nand use them to trigger additional visual perception actions. The Region\nSelection Token explicitly identifies specific regions in an image that require\nfurther perception, while the Vision Re-Encoding Token uses its hidden states\nas control signals to guide additional visual perception processes. Extensive\nexperiments demonstrate the advantages of these tokens in handling spatial\nreasoning, improving fine-grained understanding, and other tasks. On average,\nthe introduction of Visual Perception Tokens improves the performance of a 2B\nmodel by 23.6\\%, increasing its score from 0.572 to 0.708, and even outperforms\na 7B parameter model by 13.4\\% (from 0.624). Please check out our repo\nhttps://github.com/yu-rp/VisualPerceptionToken",
            "upvotes": 5,
            "discussionId": "67bddd64c7d8b835b82cee5a"
        },
        "publishedAt": "2025-02-26T02:37:36.287Z",
        "title": "Introducing Visual Perception Token into Multimodal Large Language Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17425.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635364b3c41f548fe39db945",
            "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
            "fullname": "Runpeng Yu",
            "name": "rp-yu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.16825",
            "authors": [
                {
                    "_id": "67bf243823f222a2cc2858d0",
                    "name": "Yao Xiao",
                    "hidden": false
                },
                {
                    "_id": "67bf243823f222a2cc2858d1",
                    "name": "Hai Ye",
                    "hidden": false
                },
                {
                    "_id": "67bf243823f222a2cc2858d2",
                    "name": "Linyao Chen",
                    "hidden": false
                },
                {
                    "_id": "67bf243823f222a2cc2858d3",
                    "name": "Hwee Tou Ng",
                    "hidden": false
                },
                {
                    "_id": "67bf243823f222a2cc2858d4",
                    "name": "Lidong Bing",
                    "hidden": false
                },
                {
                    "_id": "67bf243823f222a2cc2858d5",
                    "name": "Xiaoli Li",
                    "hidden": false
                },
                {
                    "_id": "67bf243823f222a2cc2858d6",
                    "name": "Roy Ka-wei Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T04:22:57.000Z",
            "title": "Finding the Sweet Spot: Preference Data Construction for Scaling\n  Preference Optimization",
            "summary": "Iterative data generation and model retraining are widely used to align large\nlanguage models (LLMs). It typically involves a policy model to generate\non-policy responses and a reward model to guide training data selection. Direct\nPreference Optimization (DPO) further enhances this process by constructing\npreference pairs of chosen and rejected responses. In this work, we aim to\nscale up the number of on-policy samples via repeated random sampling to\nimprove alignment performance. Conventional practice selects the sample with\nthe highest reward as chosen and the lowest as rejected for DPO. However, our\nexperiments reveal that this strategy leads to a decline in performance\nas the sample size increases. To address this, we investigate preference data\nconstruction through the lens of underlying normal distribution of sample\nrewards. We categorize the reward space into seven representative points and\nsystematically explore all 21 (C_7^2) pairwise combinations. Through\nevaluations on four models using AlpacaEval 2, we find that selecting the\nrejected response at reward position mu - 2sigma rather than the minimum\nreward, is crucial for optimal performance. We finally introduce a scalable\npreference data construction strategy that consistently enhances model\nperformance as the sample scale increases.",
            "upvotes": 4,
            "discussionId": "67bf243923f222a2cc285919"
        },
        "publishedAt": "2025-02-26T09:40:23.169Z",
        "title": "Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16825.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6239888e7fef05b7bdd5fcff",
            "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg",
            "fullname": "Hai Ye",
            "name": "oceanpty",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17535",
            "authors": [
                {
                    "_id": "67beaec94a1d9d7e368a7840",
                    "user": {
                        "_id": "66a4a319a1711696948b045c",
                        "avatarUrl": "/avatars/1d92d57a949332cb8227697b9a0c2f39.svg",
                        "isPro": false,
                        "fullname": "Zhenheng Tang",
                        "user": "coolzhtang",
                        "type": "user"
                    },
                    "name": "Zhenheng Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:15:34.971Z",
                    "hidden": false
                },
                {
                    "_id": "67beaec94a1d9d7e368a7841",
                    "name": "Xiang Liu",
                    "hidden": false
                },
                {
                    "_id": "67beaec94a1d9d7e368a7842",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "67beaec94a1d9d7e368a7843",
                    "name": "Peijie Dong",
                    "hidden": false
                },
                {
                    "_id": "67beaec94a1d9d7e368a7844",
                    "name": "Bingsheng He",
                    "hidden": false
                },
                {
                    "_id": "67beaec94a1d9d7e368a7845",
                    "user": {
                        "_id": "6676935fcd0b89a0115174b0",
                        "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
                        "isPro": false,
                        "fullname": "Xiaowen Chu",
                        "user": "wenxinsiju",
                        "type": "user"
                    },
                    "name": "Xiaowen Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:15:41.884Z",
                    "hidden": false
                },
                {
                    "_id": "67beaec94a1d9d7e368a7846",
                    "name": "Bo Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T15:39:35.000Z",
            "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
            "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
            "upvotes": 4,
            "discussionId": "67beaeca4a1d9d7e368a7875"
        },
        "publishedAt": "2025-02-26T01:04:23.776Z",
        "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17535.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "fullname": "Xiang Liu",
            "name": "Dominic789654",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.16794",
            "authors": [
                {
                    "_id": "67be86a78a5a80542314f0e6",
                    "user": {
                        "_id": "6531a65daed617662c7f1007",
                        "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
                        "isPro": false,
                        "fullname": "Xilin Jiang",
                        "user": "xi-j",
                        "type": "user"
                    },
                    "name": "Xilin Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:25:52.841Z",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0e7",
                    "user": {
                        "_id": "661361993bb67cb4f356c3de",
                        "avatarUrl": "/avatars/b707c07f9c70d2ed1e8cd8cff2551c69.svg",
                        "isPro": false,
                        "fullname": "Sukru Samet Dindar",
                        "user": "susameddin",
                        "type": "user"
                    },
                    "name": "Sukru Samet Dindar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:11:37.706Z",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0e8",
                    "user": {
                        "_id": "670e8671ba29b3fca221b8c9",
                        "avatarUrl": "/avatars/20f6479bd5218d6d3e304539df5003f9.svg",
                        "isPro": false,
                        "fullname": "Vishal Choudhari",
                        "user": "vchoudhari",
                        "type": "user"
                    },
                    "name": "Vishal Choudhari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-26T09:11:45.258Z",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0e9",
                    "name": "Stephan Bickel",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0ea",
                    "name": "Ashesh Mehta",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0eb",
                    "name": "Guy M McKhann",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0ec",
                    "name": "Adeen Flinker",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0ed",
                    "name": "Daniel Friedman",
                    "hidden": false
                },
                {
                    "_id": "67be86a78a5a80542314f0ee",
                    "name": "Nima Mesgarani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T03:06:45.000Z",
            "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
            "summary": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io.",
            "upvotes": 4,
            "discussionId": "67be86a98a5a80542314f16e"
        },
        "publishedAt": "2025-02-25T22:20:08.416Z",
        "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16794.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6531a65daed617662c7f1007",
            "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
            "fullname": "Xilin Jiang",
            "name": "xi-j",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14855",
            "authors": [
                {
                    "_id": "67b8e77477a3ed169f302415",
                    "name": "Evan Frick",
                    "hidden": false
                },
                {
                    "_id": "67b8e77477a3ed169f302416",
                    "name": "Connor Chen",
                    "hidden": false
                },
                {
                    "_id": "67b8e77477a3ed169f302417",
                    "name": "Joseph Tennyson",
                    "hidden": false
                },
                {
                    "_id": "67b8e77477a3ed169f302418",
                    "name": "Tianle Li",
                    "hidden": false
                },
                {
                    "_id": "67b8e77477a3ed169f302419",
                    "name": "Wei-Lin Chiang",
                    "hidden": false
                },
                {
                    "_id": "67b8e77477a3ed169f30241a",
                    "name": "Anastasios N. Angelopoulos",
                    "hidden": false
                },
                {
                    "_id": "67b8e77477a3ed169f30241b",
                    "name": "Ion Stoica",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:58:07.000Z",
            "title": "Prompt-to-Leaderboard",
            "summary": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the \\#1 spot in the\nChatbot Arena leaderboard. Our code is available at this GitHub link:\nhttps://github.com/lmarena/p2l.",
            "upvotes": 3,
            "discussionId": "67b8e77577a3ed169f302470"
        },
        "publishedAt": "2025-02-26T10:43:07.864Z",
        "title": "Prompt-to-Leaderboard",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14855.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6226
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17422",
            "authors": [
                {
                    "_id": "67bf6ea633d6740f711cc995",
                    "name": "Jiarui Zhang",
                    "hidden": false
                },
                {
                    "_id": "67bf6ea633d6740f711cc996",
                    "name": "Mahyar Khayatkhoei",
                    "hidden": false
                },
                {
                    "_id": "67bf6ea633d6740f711cc997",
                    "name": "Prateek Chhikara",
                    "hidden": false
                },
                {
                    "_id": "67bf6ea633d6740f711cc998",
                    "name": "Filip Ilievski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T18:54:40.000Z",
            "title": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs",
            "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk.",
            "upvotes": 2,
            "discussionId": "67bf6eaa33d6740f711ccac2"
        },
        "publishedAt": "2025-02-26T14:46:51.721Z",
        "title": "MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17422.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635b99d47a1656011516bff9",
            "avatarUrl": "/avatars/7243c4171ff127ba90631f105881d9d7.svg",
            "fullname": "jiarui zhang",
            "name": "jrzhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17814",
            "authors": [
                {
                    "_id": "67bf50aa9a1df81dba235650",
                    "name": "Wenlong Ji",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235651",
                    "name": "Weizhe Yuan",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235652",
                    "name": "Emily Getzen",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235653",
                    "name": "Kyunghyun Cho",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235654",
                    "name": "Michael I. Jordan",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235655",
                    "name": "Song Mei",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235656",
                    "name": "Jason E Weston",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235657",
                    "name": "Weijie J. Su",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235658",
                    "name": "Jing Xu",
                    "hidden": false
                },
                {
                    "_id": "67bf50aa9a1df81dba235659",
                    "name": "Linjun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T03:40:36.000Z",
            "title": "An Overview of Large Language Models for Statisticians",
            "summary": "Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence (AI), exhibiting remarkable capabilities across diverse\ntasks such as text generation, reasoning, and decision-making. While their\nsuccess has primarily been driven by advances in computational power and deep\nlearning architectures, emerging problems -- in areas such as uncertainty\nquantification, decision-making, causal inference, and distribution shift --\nrequire a deeper engagement with the field of statistics. This paper explores\npotential areas where statisticians can make important contributions to the\ndevelopment of LLMs, particularly those that aim to engender trustworthiness\nand transparency for human users. Thus, we focus on issues such as uncertainty\nquantification, interpretability, fairness, privacy, watermarking and model\nadaptation. We also consider possible roles for LLMs in statistical analysis.\nBy bridging AI and statistics, we aim to foster a deeper collaboration that\nadvances both the theoretical foundations and practical applications of LLMs,\nultimately shaping their role in addressing complex societal challenges.",
            "upvotes": 2,
            "discussionId": "67bf50ab9a1df81dba2356ba"
        },
        "publishedAt": "2025-02-26T12:34:59.916Z",
        "title": "An Overview of Large Language Models for Statisticians",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17814.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6226
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.15612",
            "authors": [
                {
                    "_id": "67bc8aeb70194f240328e1cf",
                    "user": {
                        "_id": "62d19a4b1e36881a57f31c6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d19a4b1e36881a57f31c6a/C-tAc0uXvpIggh0nWB2Dy.jpeg",
                        "isPro": false,
                        "fullname": "Hugo Pitorro",
                        "user": "twigs",
                        "type": "user"
                    },
                    "name": "Hugo Pitorro",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-24T15:46:13.861Z",
                    "hidden": false
                },
                {
                    "_id": "67bc8aeb70194f240328e1d0",
                    "name": "Marcos Treviso",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-21T17:33:59.000Z",
            "title": "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models",
            "summary": "State space models (SSMs), such as Mamba, have emerged as an efficient\nalternative to transformers for long-context sequence modeling. However,\ndespite their growing adoption, SSMs lack the interpretability tools that have\nbeen crucial for understanding and improving attention-based architectures.\nWhile recent efforts provide insights into Mamba's internal mechanisms, they do\nnot explicitly decompose token-wise contributions, leaving gaps in\nunderstanding how Mamba selectively processes sequences across layers. In this\nwork, we introduce LaTIM, a novel token-level decomposition method for both\nMamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively\nevaluate our method across diverse tasks, including machine translation,\ncopying, and retrieval-based generation, demonstrating its effectiveness in\nrevealing Mamba's token-to-token interaction patterns.",
            "upvotes": 2,
            "discussionId": "67bc8aed70194f240328e2cc"
        },
        "publishedAt": "2025-02-26T07:28:05.618Z",
        "title": "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62d19a4b1e36881a57f31c6a/GN78Zj956f5CoUuGof_RC.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15612.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d19a4b1e36881a57f31c6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d19a4b1e36881a57f31c6a/C-tAc0uXvpIggh0nWB2Dy.jpeg",
            "fullname": "Hugo Pitorro",
            "name": "twigs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.17092",
            "authors": [
                {
                    "_id": "67bea8cc7e54112af6c372aa",
                    "user": {
                        "_id": "63d9e09f1cae35c27bf80cb2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Syed Abdul Gaffar Shakhadri",
                        "user": "SyedAbdul",
                        "type": "user"
                    },
                    "name": "Syed Abdul Gaffar Shakhadri",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-26T05:52:19.355Z",
                    "hidden": false
                },
                {
                    "_id": "67bea8cc7e54112af6c372ab",
                    "user": {
                        "_id": "5fb7ae48e6ae537272bdeb3c",
                        "avatarUrl": "/avatars/e5d01cb428f4b22161e0d17895a5c678.svg",
                        "isPro": false,
                        "fullname": "Kruthika",
                        "user": "kruthika",
                        "type": "user"
                    },
                    "name": "Kruthika KR",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-26T05:38:21.529Z",
                    "hidden": false
                },
                {
                    "_id": "67bea8cc7e54112af6c372ac",
                    "user": {
                        "_id": "677cc34fe4cf361eedccd085",
                        "avatarUrl": "/avatars/e97a3f9a84ed258ab4b75c12865562d6.svg",
                        "isPro": false,
                        "fullname": "Kartik Basavaraj Angadi",
                        "user": "KartikAngadi",
                        "type": "user"
                    },
                    "name": "Kartik Basavaraj Angadi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-26T05:38:21.529Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T12:15:07.000Z",
            "title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI",
            "summary": "We introduce Shakti VLM, a family of vision-language models in the capacity\nof 1B and 4B parameters designed to address data efficiency challenges in\nmultimodal learning. While recent VLMs achieve strong performance through\nextensive training data, Shakti models leverage architectural innovations to\nattain competitive results with fewer tokens. Key advancements include\nQK-Normalization for attention stability, hybrid normalization techniques, and\nenhanced positional encoding. A three-stage training strategy further optimizes\nlearning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and\nShakti-VLM-4B excel in document understanding, Visual Reasoning, OCR\nextraction, and general multimodal reasoning. Our results highlight that high\nperformance can be achieved through model design and training strategy rather\nthan sheer data volume, making Shakti an efficient solution for\nenterprise-scale multimodal tasks.",
            "upvotes": 2,
            "discussionId": "67bea8cd7e54112af6c37305"
        },
        "publishedAt": "2025-02-26T00:38:42.527Z",
        "title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17092.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d9e09f1cae35c27bf80cb2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
            "fullname": "Syed Abdul Gaffar Shakhadri",
            "name": "SyedAbdul",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.17910",
            "authors": [
                {
                    "_id": "67be7f96b4ca41e2807a4fb0",
                    "user": {
                        "_id": "64d98ef7a4839890b25eb78b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
                        "isPro": true,
                        "fullname": "Fangyuan Yu",
                        "user": "Ksgk-fy",
                        "type": "user"
                    },
                    "name": "Fangyuan Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T08:26:07.778Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T07:18:29.000Z",
            "title": "Scaling LLM Pre-training with Vocabulary Curriculum",
            "summary": "Modern language models rely on static vocabularies, fixed before pretraining,\nin contrast to the adaptive vocabulary acquisition observed in human language\nlearning. To bridge this gap, we introduce vocabulary curriculum learning, an\napproach that improves pretraining efficiency with log-linear scaling gains\nrelative to vocabulary size. Our method alternates between entropy-guided\nvocabulary expansion and model optimization, enabling models to learn\ntransferable representations across diverse tokenization granularities. This\napproach naturally gives rise to an optimal computation allocation pattern:\nlonger tokens capture predictable content, while shorter tokens focus on more\ncomplex, harder-to-predict contexts. Experiments on small-scale GPT models\ndemonstrate improved scaling efficiency, reinforcing the effectiveness of\ndynamic tokenization. We release our code to support further research and plan\nto extend our experiments to larger models and diverse domains.",
            "upvotes": 1,
            "discussionId": "67be7f97b4ca41e2807a4fed"
        },
        "publishedAt": "2025-02-26T18:40:15.965Z",
        "title": "Scaling LLM Pre-training with Vocabulary Curriculum",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17910.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d98ef7a4839890b25eb78b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18302",
            "authors": [
                {
                    "_id": "67befd09afb202a5b7518572",
                    "user": {
                        "_id": "64edcb9b84cc47a8b50bfab7",
                        "avatarUrl": "/avatars/1b4defb79eef3753a540efa76c16462a.svg",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "Kinpz",
                        "type": "user"
                    },
                    "name": "Pengzhi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T15:37:26.179Z",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518573",
                    "name": "Pengfei Yu",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518574",
                    "name": "Zide Liu",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518575",
                    "name": "Wei He",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518576",
                    "name": "Xuhao Pan",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518577",
                    "name": "Xudong Rao",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518578",
                    "name": "Tao Wei",
                    "hidden": false
                },
                {
                    "_id": "67befd09afb202a5b7518579",
                    "name": "Wei Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T15:42:34.000Z",
            "title": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven\n  Language Representation",
            "summary": "In this paper, we introduce LDGen, a novel method for integrating large\nlanguage models (LLMs) into existing text-to-image diffusion models while\nminimizing computational demands. Traditional text encoders, such as CLIP and\nT5, exhibit limitations in multilingual processing, hindering image generation\nacross diverse languages. We address these challenges by leveraging the\nadvanced capabilities of LLMs. Our approach employs a language representation\nstrategy that applies hierarchical caption optimization and human instruction\ntechniques to derive precise semantic information,. Subsequently, we\nincorporate a lightweight adapter and a cross-modal refiner to facilitate\nefficient feature alignment and interaction between LLMs and image features.\nLDGen reduces training time and enables zero-shot multilingual image\ngeneration. Experimental results indicate that our method surpasses baseline\nmodels in both prompt adherence and image aesthetic quality, while seamlessly\nsupporting multiple languages. Project page: https://zrealli.github.io/LDGen.",
            "upvotes": 0,
            "discussionId": "67befd0cafb202a5b751865e"
        },
        "publishedAt": "2025-02-26T16:56:34.818Z",
        "title": "LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18302.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64edcb9b84cc47a8b50bfab7",
            "avatarUrl": "/avatars/1b4defb79eef3753a540efa76c16462a.svg",
            "fullname": "Li",
            "name": "Kinpz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18316",
            "authors": [
                {
                    "_id": "67bf0ccbb2f5c23eb0a69a7d",
                    "user": {
                        "_id": "6586f687ce38d143c4092ed7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6586f687ce38d143c4092ed7/uPYZgk-lGGEfxa0kASX0y.jpeg",
                        "isPro": false,
                        "fullname": "Ahmed Mohamed Elhady",
                        "user": "ahmedselhady",
                        "type": "user"
                    },
                    "name": "Ahmed Elhady",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T15:37:14.872Z",
                    "hidden": false
                },
                {
                    "_id": "67bf0ccbb2f5c23eb0a69a7e",
                    "name": "Eneko Agirre",
                    "hidden": false
                },
                {
                    "_id": "67bf0ccbb2f5c23eb0a69a7f",
                    "name": "Mikel Artetxe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T16:09:38.000Z",
            "title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging",
            "summary": "We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks.",
            "upvotes": 0,
            "discussionId": "67bf0ccdb2f5c23eb0a69b25"
        },
        "publishedAt": "2025-02-26T10:53:44.153Z",
        "title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18316.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6586f687ce38d143c4092ed7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6586f687ce38d143c4092ed7/uPYZgk-lGGEfxa0kASX0y.jpeg",
            "fullname": "Ahmed Mohamed Elhady",
            "name": "ahmedselhady",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]