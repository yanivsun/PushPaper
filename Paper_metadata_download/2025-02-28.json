[
    {
        "paper": {
            "id": "2502.19613",
            "authors": [
                {
                    "_id": "67c12987505a88e4a185e0d7",
                    "name": "Wei Xiong",
                    "hidden": false
                },
                {
                    "_id": "67c12987505a88e4a185e0d8",
                    "user": {
                        "_id": "6470e0f1cfd57849519033a5",
                        "avatarUrl": "/avatars/7ffefee3e36a4e37b9f4510bc6b689d1.svg",
                        "isPro": false,
                        "fullname": "Hanning Zhang",
                        "user": "HanningZhang",
                        "type": "user"
                    },
                    "name": "Hanning Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:22:33.128Z",
                    "hidden": false
                },
                {
                    "_id": "67c12987505a88e4a185e0d9",
                    "user": {
                        "_id": "65eec5c1d7d63c2ed0615421",
                        "avatarUrl": "/avatars/8c32f5e7d4b1940088bdec73c0b86fab.svg",
                        "isPro": false,
                        "fullname": "Chenlu Ye",
                        "user": "Chenlu123",
                        "type": "user"
                    },
                    "name": "Chenlu Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:22:38.981Z",
                    "hidden": false
                },
                {
                    "_id": "67c12987505a88e4a185e0da",
                    "user": {
                        "_id": "62323bb408bcea92917e42ee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62323bb408bcea92917e42ee/2vHxkv-oSROtLteOnqa8P.jpeg",
                        "isPro": false,
                        "fullname": "Lichang Chen",
                        "user": "Lichang-Chen",
                        "type": "user"
                    },
                    "name": "Lichang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:14:29.479Z",
                    "hidden": false
                },
                {
                    "_id": "67c12987505a88e4a185e0db",
                    "user": {
                        "_id": "64b8922ca1827cc8d04ae919",
                        "avatarUrl": "/avatars/0aaa83e3d09a82434e1d6af724aaa485.svg",
                        "isPro": false,
                        "fullname": "Nan Jiang",
                        "user": "nanjiang",
                        "type": "user"
                    },
                    "name": "Nan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:23:02.992Z",
                    "hidden": false
                },
                {
                    "_id": "67c12987505a88e4a185e0dc",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T23:01:16.000Z",
            "title": "Self-rewarding correction for mathematical reasoning",
            "summary": "We study self-rewarding reasoning large language models (LLMs), which can\nsimultaneously generate step-by-step reasoning and evaluate the correctness of\ntheir outputs during the inference time-without external feedback. This\nintegrated approach allows a single model to independently guide its reasoning\nprocess, offering computational advantages for model deployment. We\nparticularly focus on the representative task of self-correction, where models\nautonomously detect errors in their responses, revise outputs, and decide when\nto terminate iterative refinement loops. To enable this, we propose a\ntwo-staged algorithmic framework for constructing self-rewarding reasoning\nmodels using only self-generated data. In the first stage, we employ sequential\nrejection sampling to synthesize long chain-of-thought trajectories that\nincorporate both self-rewarding and self-correction mechanisms. Fine-tuning\nmodels on these curated data allows them to learn the patterns of\nself-rewarding and self-correction. In the second stage, we further enhance the\nmodels' ability to assess response accuracy and refine outputs through\nreinforcement learning with rule-based signals. Experiments with Llama-3 and\nQwen-2.5 demonstrate that our approach surpasses intrinsic self-correction\ncapabilities and achieves performance comparable to systems that rely on\nexternal reward models.",
            "upvotes": 49,
            "discussionId": "67c12989505a88e4a185e115"
        },
        "publishedAt": "2025-02-27T22:15:54.222Z",
        "title": "Self-rewarding correction for mathematical reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19613.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "643e59806db6ba8c5ee123f3",
            "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
            "fullname": "Wei Xiong",
            "name": "weqweasdas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.19634",
            "authors": [
                {
                    "_id": "67c12bf3505a88e4a1866a01",
                    "name": "Jiazhen Pan",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a02",
                    "user": {
                        "_id": "631b9ff5824f2502e3557c7e",
                        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "che111",
                        "type": "user"
                    },
                    "name": "Che Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T09:28:38.598Z",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a03",
                    "user": {
                        "_id": "6317257fc92fd6fee317ff7c",
                        "avatarUrl": "/avatars/2f460a2f28562c987becb2acad8d93e7.svg",
                        "isPro": false,
                        "fullname": "Junde Wu",
                        "user": "morson",
                        "type": "user"
                    },
                    "name": "Junde Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:14:18.528Z",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a04",
                    "user": {
                        "_id": "647c7c311f878439e2fe50e7",
                        "avatarUrl": "/avatars/be0ddfc98c98f66b88c939c0451907a5.svg",
                        "isPro": false,
                        "fullname": "Fenglin Liu",
                        "user": "fenglinliu",
                        "type": "user"
                    },
                    "name": "Fenglin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:23:44.550Z",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a05",
                    "user": {
                        "_id": "66aff1d8ccc0fb3883dd19a8",
                        "avatarUrl": "/avatars/dc9a0c622f0509c5bc9bf82d8f6ad7e3.svg",
                        "isPro": false,
                        "fullname": "Jiayuan Zhu",
                        "user": "jiayuanz3",
                        "type": "user"
                    },
                    "name": "Jiayuan Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T15:14:46.124Z",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a06",
                    "name": "Hongwei Bran Li",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a07",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a08",
                    "user": {
                        "_id": "67c1e8cd73ae02d78044324b",
                        "avatarUrl": "/avatars/45cdcc832ee46da139e8163969186d26.svg",
                        "isPro": false,
                        "fullname": "C O",
                        "user": "ellivreksaB",
                        "type": "user"
                    },
                    "name": "Cheng Ouyang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T17:49:25.580Z",
                    "hidden": false
                },
                {
                    "_id": "67c12bf3505a88e4a1866a09",
                    "name": "Daniel Rueckert",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T23:57:34.000Z",
            "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language\n  Models (VLMs) via Reinforcement Learning",
            "summary": "Reasoning is a critical frontier for advancing medical image analysis, where\ntransparency and trustworthiness play a central role in both clinician trust\nand regulatory approval. Although Medical Visual Language Models (VLMs) show\npromise for radiological tasks, most existing VLMs merely produce final answers\nwithout revealing the underlying reasoning. To address this gap, we introduce\nMedVLM-R1, a medical VLM that explicitly generates natural language reasoning\nto enhance transparency and trustworthiness. Instead of relying on supervised\nfine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs a\nreinforcement learning framework that incentivizes the model to discover\nhuman-interpretable reasoning paths without using any reasoning references.\nDespite limited training data (600 visual question answering samples) and model\nparameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,\nCT, and X-ray benchmarks, outperforming larger models trained on over a million\nsamples. It also demonstrates robust domain generalization under\nout-of-distribution tasks. By unifying medical image analysis with explicit\nreasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable\nAI in clinical practice.",
            "upvotes": 43,
            "discussionId": "67c12bf4505a88e4a1866a35"
        },
        "publishedAt": "2025-02-28T04:36:05.045Z",
        "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19634.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20395",
            "authors": [
                {
                    "_id": "67c12b5def9af74902537b98",
                    "user": {
                        "_id": "671002fd13203512e7b8f9e3",
                        "avatarUrl": "/avatars/313d8ea313ed300750cfdaaca44fdb6e.svg",
                        "isPro": false,
                        "fullname": "Zhongyang Li",
                        "user": "Lzy01241010",
                        "type": "user"
                    },
                    "name": "Zhongyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:14:22.809Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b5def9af74902537b99",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "67c12b5def9af74902537b9a",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:14:16.482Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T18:59:32.000Z",
            "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
            "summary": "In large multimodal models (LMMs), the perception of non-language modalities\n(e.g., visual representations) is usually not on par with the large language\nmodels (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on\nchallenging downstream tasks. This weakness has been recently mitigated by\nreplacing the vision encoder with a mixture-of-experts (MoE), which provides\nrich, multi-granularity, and diverse representations required by diverse\ndownstream tasks. The performance of multimodal MoE largely depends on its\nrouter, which reweights and mixes the representations of different experts for\neach input. However, we find that the end-to-end trained router does not always\nproduce the optimal routing weights for every test sample. To bridge the gap,\nwe propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that\nlocally optimizes the vector of routing weights in test-time by moving it\ntoward those vectors of the correctly predicted samples in a neighborhood of\nthe test sample. We propose three R2-T2 strategies with different optimization\nobjectives and neighbor-search spaces. R2-T2 consistently and greatly improves\nstate-of-the-art LMMs' performance on challenging benchmarks of diverse tasks,\nwithout training any base-model parameters.",
            "upvotes": 32,
            "discussionId": "67c12b5eef9af74902537c00"
        },
        "publishedAt": "2025-02-27T22:27:24.486Z",
        "title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/PaZkWIhqZBRCSfBA-k4OX.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FASlyPDiSb9VHZaeWMj9H.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/kGeIJVMDDAbIassiuYIb2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/Tw2Bf_RsFTPARKLJWIlKM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20395.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20082",
            "authors": [
                {
                    "_id": "67c12b6d25c74ee5b6e2ce8e",
                    "user": {
                        "_id": "632bc663eafe8eca5e9bfdbc",
                        "avatarUrl": "/avatars/787553c73e9a96adc5219e67acd29c00.svg",
                        "isPro": false,
                        "fullname": "Ning Shang",
                        "user": "J-shang",
                        "type": "user"
                    },
                    "name": "Ning Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:28:26.117Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce8f",
                    "user": {
                        "_id": "62b0009c72043b05d29492b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
                        "isPro": false,
                        "fullname": "Li Lyna Zhang",
                        "user": "lynazhang",
                        "type": "user"
                    },
                    "name": "Li Lyna Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:26:58.131Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce90",
                    "user": {
                        "_id": "6495b0b844bc2e9ce6cc849b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/j6aucl_tefMHwtD-bdUAw.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Wang",
                        "user": "OldKingMeister",
                        "type": "user"
                    },
                    "name": "Siyuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:14:20.687Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce91",
                    "user": {
                        "_id": "65efe691ccef3501d586bb62",
                        "avatarUrl": "/avatars/c4716c532754b487359e77e43afe09bc.svg",
                        "isPro": false,
                        "fullname": "Gaokai Zhang",
                        "user": "gaokaiz2",
                        "type": "user"
                    },
                    "name": "Gaokai Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:28:05.040Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce92",
                    "user": {
                        "_id": "60c790f1accf7da31ed8240d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c790f1accf7da31ed8240d/YDohCmgf9OUeWqZIs3Thh.jpeg",
                        "isPro": false,
                        "fullname": "Gilsinia Lopez",
                        "user": "lgg",
                        "type": "user"
                    },
                    "name": "Gilsinia Lopez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:27:52.164Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce93",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce94",
                    "user": {
                        "_id": "64da876370446182be5b608d",
                        "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
                        "isPro": false,
                        "fullname": "Weizhu Chen",
                        "user": "chenweizhu",
                        "type": "user"
                    },
                    "name": "Weizhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:27:17.122Z",
                    "hidden": false
                },
                {
                    "_id": "67c12b6d25c74ee5b6e2ce95",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T13:41:07.000Z",
            "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
            "summary": "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.",
            "upvotes": 21,
            "discussionId": "67c12b6e25c74ee5b6e2ceb5"
        },
        "publishedAt": "2025-02-27T22:22:53.713Z",
        "title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20082.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b0009c72043b05d29492b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png",
            "fullname": "Li Lyna Zhang",
            "name": "lynazhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 27
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20238",
            "authors": [
                {
                    "_id": "67c15306333e2f71f01c8e35",
                    "user": {
                        "_id": "64e85b3edb3767299865e0e3",
                        "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Guizhen",
                        "type": "user"
                    },
                    "name": "Guizhen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:28:41.974Z",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e36",
                    "user": {
                        "_id": "67627b97fc88502751bfd2b8",
                        "avatarUrl": "/avatars/4b1f5c333f9255181d7b9078c5d4eb32.svg",
                        "isPro": false,
                        "fullname": "Wei",
                        "user": "weiwenxu",
                        "type": "user"
                    },
                    "name": "Weiwen Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:28:47.949Z",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e37",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e38",
                    "name": "Hou Pong Chan",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e39",
                    "user": {
                        "_id": "61657b0b20606e5e73f611cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
                        "isPro": false,
                        "fullname": "CHAOQUN LIU",
                        "user": "lukecq",
                        "type": "user"
                    },
                    "name": "Chaoqun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:28:58.424Z",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e3a",
                    "user": {
                        "_id": "6454685a548f22be598414c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
                        "isPro": false,
                        "fullname": "Lidong Bing",
                        "user": "LidongBing",
                        "type": "user"
                    },
                    "name": "Lidong Bing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:29:04.845Z",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e3b",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e3c",
                    "user": {
                        "_id": "655722e80438e0854fae7554",
                        "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg",
                        "isPro": false,
                        "fullname": "Luu Anh Tuan",
                        "user": "anhtuanluu36",
                        "type": "user"
                    },
                    "name": "Anh Tuan Luu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:29:21.748Z",
                    "hidden": false
                },
                {
                    "_id": "67c15306333e2f71f01c8e3d",
                    "name": "Yu Rong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T16:23:25.000Z",
            "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving",
            "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.",
            "upvotes": 19,
            "discussionId": "67c15307333e2f71f01c8ebc"
        },
        "publishedAt": "2025-02-28T01:14:11.268Z",
        "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20238.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e85b3edb3767299865e0e3",
            "avatarUrl": "/avatars/fdbe121535dea940edd2766161393485.svg",
            "fullname": "Chen",
            "name": "Guizhen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.16645",
            "authors": [
                {
                    "_id": "67c12e60d8247a49b805694f",
                    "user": {
                        "_id": "6441270ead24e9b2cfbc45e0",
                        "avatarUrl": "/avatars/92eab1ae50efaaee070674ae20244fc0.svg",
                        "isPro": false,
                        "fullname": "Wang Chenlong",
                        "user": "Wildxxxxx75",
                        "type": "user"
                    },
                    "name": "Chenlong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:29:50.564Z",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056950",
                    "user": {
                        "_id": "64fb128552e82dd432682b06",
                        "avatarUrl": "/avatars/c141326a5d8c17d35be40e12579810bb.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Chu",
                        "user": "chuzy",
                        "type": "user"
                    },
                    "name": "Zhaoyang Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:29:56.482Z",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056951",
                    "user": {
                        "_id": "669096da35cddb688a352ca8",
                        "avatarUrl": "/avatars/d01f34d99d89447d27c0fd43734ae6d9.svg",
                        "isPro": false,
                        "fullname": "zxiang",
                        "user": "zx10086",
                        "type": "user"
                    },
                    "name": "Zhengxiang Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T09:28:33.569Z",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056952",
                    "user": {
                        "_id": "6743e9d4303e7ce5b9d13e9b",
                        "avatarUrl": "/avatars/cdaf150380e9c8916547185b968a2670.svg",
                        "isPro": false,
                        "fullname": "xy",
                        "user": "yxy0807",
                        "type": "user"
                    },
                    "name": "Xuyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T09:28:31.564Z",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056953",
                    "name": "Kaiyue Qiu",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056954",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056955",
                    "name": "Zhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056956",
                    "name": "Xuanhua Shi",
                    "hidden": false
                },
                {
                    "_id": "67c12e60d8247a49b8056957",
                    "user": {
                        "_id": "65e2be1e630e2db23829ee8d",
                        "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
                        "isPro": false,
                        "fullname": "Dongping Chen",
                        "user": "fjchendp",
                        "type": "user"
                    },
                    "name": "Dongping Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:30:19.705Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-23T16:46:18.000Z",
            "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code\n  Evolution at Scale",
            "summary": "Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.",
            "upvotes": 15,
            "discussionId": "67c12e61d8247a49b805698f"
        },
        "publishedAt": "2025-02-27T23:04:14.619Z",
        "title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16645.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20321",
            "authors": [
                {
                    "_id": "67c13c68d8247a49b808fdac",
                    "user": {
                        "_id": "62c585eb09baf76938a70de8",
                        "avatarUrl": "/avatars/ae8cca53710b3325bf0dd0f08c2b1bbf.svg",
                        "isPro": false,
                        "fullname": "Chuofan Ma",
                        "user": "cfma",
                        "type": "user"
                    },
                    "name": "Chuofan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:31:29.232Z",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdad",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdae",
                    "user": {
                        "_id": "6572ac949a5c2d6df9fab3c5",
                        "avatarUrl": "/avatars/b5d70a86a452198381eee1c8f513ceec.svg",
                        "isPro": false,
                        "fullname": "Junfeng Wu",
                        "user": "JunfengWu",
                        "type": "user"
                    },
                    "name": "Junfeng Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:31:35.467Z",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdaf",
                    "user": {
                        "_id": "6304baf041387c7f1177a5d2",
                        "avatarUrl": "/avatars/795c63f2394080eec78ca7981d4a1f78.svg",
                        "isPro": false,
                        "fullname": "Jihan Yang",
                        "user": "jihanyang",
                        "type": "user"
                    },
                    "name": "Jihan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:31:41.522Z",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdb0",
                    "name": "Xin Yu",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdb1",
                    "user": {
                        "_id": "661a80af3557013b638061d5",
                        "avatarUrl": "/avatars/4c551aeb223e257a5fc45b5b6c7ded49.svg",
                        "isPro": false,
                        "fullname": "Zehuan Yuan",
                        "user": "sweetrabor",
                        "type": "user"
                    },
                    "name": "Zehuan Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:32:06.178Z",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdb2",
                    "name": "Bingyue Peng",
                    "hidden": false
                },
                {
                    "_id": "67c13c68d8247a49b808fdb3",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T17:47:01.000Z",
            "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
            "summary": "The representation disparity between visual generation and understanding\nimposes a critical gap in integrating these capabilities into a single\nframework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer\nthat encodes fine-grained details for generation while also capturing\nhigh-level semantics for understanding. Despite recent studies have shown that\nthese objectives could induce loss conflicts in training, we reveal that the\nunderlying bottleneck stems from limited representational capacity of discrete\ntokens. We address this by introducing multi-codebook quantization, which\ndivides vector quantization with several independent sub-codebooks to expand\nthe latent feature space, while avoiding training instability caused by\noverlarge codebooks. Our method significantly raises the upper limit of unified\ndiscrete tokenizers to match or even surpass domain-specific continuous\ntokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus\n0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on\nImageNet. Our code is available at https://github.com/FoundationVision/UniTok.",
            "upvotes": 13,
            "discussionId": "67c13c6ad8247a49b8090003"
        },
        "publishedAt": "2025-02-27T23:34:45.416Z",
        "title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20321.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6344dcb1cd37e44d9ed46508",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg",
            "fullname": "Yi Jiang",
            "name": "JiangYi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.19587",
            "authors": [
                {
                    "_id": "67c13aa6a43d7939d60eb02e",
                    "user": {
                        "_id": "6512e961332b85e7cf8c1431",
                        "avatarUrl": "/avatars/d4bdb9670166112dcb36753bc1823b28.svg",
                        "isPro": false,
                        "fullname": "Lola Le Breton",
                        "user": "Lolalb",
                        "type": "user"
                    },
                    "name": "Lola Le Breton",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:35:52.732Z",
                    "hidden": false
                },
                {
                    "_id": "67c13aa6a43d7939d60eb02f",
                    "name": "Quentin Fournier",
                    "hidden": false
                },
                {
                    "_id": "67c13aa6a43d7939d60eb030",
                    "user": {
                        "_id": "6504c139eac45ee2e4d36893",
                        "avatarUrl": "/avatars/cc3d65f558988ab885aa0357f6e2d29d.svg",
                        "isPro": false,
                        "fullname": "Mariam El Mezouar",
                        "user": "mariamelm",
                        "type": "user"
                    },
                    "name": "Mariam El Mezouar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:36:28.502Z",
                    "hidden": false
                },
                {
                    "_id": "67c13aa6a43d7939d60eb031",
                    "user": {
                        "_id": "66fabb66c2bf89d75e8cdd4d",
                        "avatarUrl": "/avatars/c40f55a77e2fb34ba38a79f04df82893.svg",
                        "isPro": false,
                        "fullname": "Sarath Chandar",
                        "user": "apsarath",
                        "type": "user"
                    },
                    "name": "Sarath Chandar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:36:22.663Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T22:00:22.000Z",
            "title": "NeoBERT: A Next-Generation BERT",
            "summary": "Recent innovations in architecture, pre-training, and fine-tuning have led to\nthe remarkable in-context learning and reasoning abilities of large\nauto-regressive language models such as LLaMA and DeepSeek. In contrast,\nencoders like BERT and RoBERTa have not seen the same level of progress despite\nbeing foundational for many downstream NLP applications. To bridge this gap, we\nintroduce NeoBERT, a next-generation encoder that redefines the capabilities of\nbidirectional models by integrating state-of-the-art advancements in\narchitecture, modern data, and optimized pre-training methodologies. NeoBERT is\ndesigned for seamless adoption: it serves as a plug-and-play replacement for\nexisting base models, relies on an optimal depth-to-width ratio, and leverages\nan extended context length of 4,096 tokens. Despite its compact 250M parameter\nfootprint, it achieves state-of-the-art results on the massive MTEB benchmark,\noutperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under\nidentical fine-tuning conditions. In addition, we rigorously evaluate the\nimpact of each modification on GLUE and design a uniform fine-tuning and\nevaluation framework for MTEB. We release all code, data, checkpoints, and\ntraining scripts to accelerate research and real-world adoption.",
            "upvotes": 10,
            "discussionId": "67c13aa7a43d7939d60eb065"
        },
        "publishedAt": "2025-02-28T03:27:32.294Z",
        "title": "NeoBERT: A Next-Generation BERT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19587.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6317233cc92fd6fee317e030",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6317233cc92fd6fee317e030/cJHSvvimr1kqgQfHOjO5n.png",
            "fullname": "Tom Aarsen",
            "name": "tomaarsen",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 1596
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20172",
            "authors": [
                {
                    "_id": "67c17b8f60206395233b7e46",
                    "user": {
                        "_id": "658c481dd1c8b106727a8b73",
                        "avatarUrl": "/avatars/d34a7a62c3a524e5fdd2d5994348db58.svg",
                        "isPro": false,
                        "fullname": "Liang Chen",
                        "user": "liangchen-ms",
                        "type": "user"
                    },
                    "name": "Liang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:34:40.397Z",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e47",
                    "user": {
                        "_id": "63451cf0a05b51f7ded25505",
                        "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
                        "isPro": false,
                        "fullname": "shuai bai",
                        "user": "bluelike",
                        "type": "user"
                    },
                    "name": "Shuai Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:33:50.796Z",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e48",
                    "user": {
                        "_id": "637c7503fe115289cfecbe6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
                        "isPro": false,
                        "fullname": "Wenhao Chai",
                        "user": "wchai",
                        "type": "user"
                    },
                    "name": "Wenhao Chai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:33:57.358Z",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e49",
                    "user": {
                        "_id": "678609789a285d232ee14157",
                        "avatarUrl": "/avatars/a6cb2c571d9ef6deb0b1659f754afe7f.svg",
                        "isPro": false,
                        "fullname": "Weichu Xie",
                        "user": "akarinmoe",
                        "type": "user"
                    },
                    "name": "Weichu Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:34:48.933Z",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e4a",
                    "name": "Haozhe Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e4b",
                    "name": "Leon Vinci",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e4c",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:34:24.275Z",
                    "hidden": false
                },
                {
                    "_id": "67c17b8f60206395233b7e4d",
                    "name": "Baobao Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T15:08:39.000Z",
            "title": "Multimodal Representation Alignment for Image Generation: Text-Image\n  Interleaved Control Is Easier Than You Think",
            "summary": "The field of advanced text-to-image generation is witnessing the emergence of\nunified frameworks that integrate powerful text encoders, such as CLIP and T5,\nwith Diffusion Transformer backbones. Although there have been efforts to\ncontrol output images with additional conditions, like canny and depth map, a\ncomprehensive framework for arbitrary text-image interleaved control is still\nlacking. This gap is especially evident when attempting to merge concepts or\nvisual elements from multiple images in the generation process. To mitigate the\ngap, we conducted preliminary experiments showing that large multimodal models\n(LMMs) offer an effective shared representation space, where image and text can\nbe well-aligned to serve as a condition for external diffusion models. Based on\nthis discovery, we propose Dream Engine, an efficient and unified framework\ndesigned for arbitrary text-image interleaved control in image generation\nmodels. Building on powerful text-to-image models like SD3.5, we replace the\noriginal text-only encoders by incorporating versatile multimodal information\nencoders such as QwenVL. Our approach utilizes a two-stage training paradigm,\nconsisting of joint text-image alignment and multimodal interleaved instruction\ntuning. Our experiments demonstrate that this training method is effective,\nachieving a 0.69 overall score on the GenEval benchmark, and matching the\nperformance of state-of-the-art text-to-image models like SD3.5 and FLUX.",
            "upvotes": 9,
            "discussionId": "67c17b9160206395233b7e9c"
        },
        "publishedAt": "2025-02-28T04:02:19.534Z",
        "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20172.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 31
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.16944",
            "authors": [
                {
                    "_id": "67be807e8a5a805423137ca2",
                    "user": {
                        "_id": "664af07a691370727c281031",
                        "avatarUrl": "/avatars/e5ed17342e0ea953bacc7d57e9f3b686.svg",
                        "isPro": false,
                        "fullname": "Cheng Hua Huang",
                        "user": "LanceZomax",
                        "type": "user"
                    },
                    "name": "Chenghua Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:30:31.972Z",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca3",
                    "user": {
                        "_id": "6406afa8a577649430c64363",
                        "avatarUrl": "/avatars/9bd1768c91d509c8c49970e9fd7775a5.svg",
                        "isPro": false,
                        "fullname": "LuWang",
                        "user": "LuWang",
                        "type": "user"
                    },
                    "name": "Lu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:30:39.129Z",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca4",
                    "user": {
                        "_id": "669dcf6200970c3b27aafa5d",
                        "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
                        "isPro": false,
                        "fullname": "kaikai yang",
                        "user": "keanudicap",
                        "type": "user"
                    },
                    "name": "Fangkai Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T09:17:46.382Z",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca5",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca6",
                    "user": {
                        "_id": "661dd71d2ae9013218415e6f",
                        "avatarUrl": "/avatars/389883e5886628c07cb0b08fc8c93c3b.svg",
                        "isPro": false,
                        "fullname": "Zhixu Li",
                        "user": "ZhixuLi",
                        "type": "user"
                    },
                    "name": "Zhixu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:30:46.084Z",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca7",
                    "user": {
                        "_id": "652fc9f39bc50a6c0e435224",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc9f39bc50a6c0e435224/70OBVDHHBsxG2giJ-E3_1.jpeg",
                        "isPro": false,
                        "fullname": "Lin Qingwei",
                        "user": "Eliblo1969",
                        "type": "user"
                    },
                    "name": "Qingwei Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:31:00.637Z",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca8",
                    "user": {
                        "_id": "66473d2c7abe6ad66e81a3dd",
                        "avatarUrl": "/avatars/82f40244806c06ffeaa1c4265e9725ea.svg",
                        "isPro": false,
                        "fullname": "ZHANGDONGMEI",
                        "user": "ZDM6426",
                        "type": "user"
                    },
                    "name": "Dongmei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:31:14.045Z",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137ca9",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "67be807e8a5a805423137caa",
                    "name": "Qi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T08:11:33.000Z",
            "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value\n  Guidance",
            "summary": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human\nFeedback (RLHF) is essential for aligning large language models (LLMs) with\nhuman preferences. It requires joint training of an actor and critic with a\npretrained, fixed reward model for guidance. This approach increases\ncomputational complexity and instability due to actor-critic interdependence.\nAdditionally, PPO lacks access to true environment rewards in LLM tasks,\nlimiting its adaptability. Under such conditions, pretraining a value model or\na reward model becomes equivalent, as both provide fixed supervisory signals\nwithout new ground-truth feedback. To address these issues, we propose\nDecoupled Value Policy Optimization (DVPO), a lean framework that\nreplaces traditional reward modeling with a pretrained global value model\n(GVM). The GVM is conditioned on policy trajectories and predicts token-level\nreturn-to-go estimates. By decoupling value model from policy training (via\nfrozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence,\nreducing GPU memory usage by 40\\% and training time by 35\\% compared to\nconventional RLHF. Experiments across benchmarks show DVPO outperforms\nefficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in\nperformance.",
            "upvotes": 9,
            "discussionId": "67be807e8a5a805423137cc2"
        },
        "publishedAt": "2025-02-28T01:55:41.427Z",
        "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16944.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669dcf6200970c3b27aafa5d",
            "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
            "fullname": "kaikai yang",
            "name": "keanudicap",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20126",
            "authors": [
                {
                    "_id": "67c14524af5eaa8dd062a216",
                    "user": {
                        "_id": "62f8f4ff92e64c61bc6938da",
                        "avatarUrl": "/avatars/d386eb35d2c3d52186b2a8ec957f51bc.svg",
                        "isPro": false,
                        "fullname": "Sotiris Anagnostidis",
                        "user": "sanagnos",
                        "type": "user"
                    },
                    "name": "Sotiris Anagnostidis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:32:18.844Z",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a217",
                    "user": {
                        "_id": "64996389a3f227b05cbd956f",
                        "avatarUrl": "/avatars/a586e7f99efdc7b61e05d62945575096.svg",
                        "isPro": false,
                        "fullname": "Gregor Bachmann",
                        "user": "gregorbachmann",
                        "type": "user"
                    },
                    "name": "Gregor Bachmann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:32:24.781Z",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a218",
                    "user": {
                        "_id": "65d6a010c57d1c140e395e31",
                        "avatarUrl": "/avatars/ddc9cfc98da36b639bd9205ee65b6967.svg",
                        "isPro": false,
                        "fullname": "Yeongmin Kim",
                        "user": "YeongminKim",
                        "type": "user"
                    },
                    "name": "Yeongmin Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:32:30.708Z",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a219",
                    "user": {
                        "_id": "6408a3a19e9f790c905281c2",
                        "avatarUrl": "/avatars/3517962e54bda141018e13f7e21fb1ae.svg",
                        "isPro": false,
                        "fullname": "jonas khler",
                        "user": "Jonaskohler",
                        "type": "user"
                    },
                    "name": "Jonas Kohler",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:32:36.925Z",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a21a",
                    "name": "Markos Georgopoulos",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a21b",
                    "name": "Artsiom Sanakoyeu",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a21c",
                    "name": "Yuming Du",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a21d",
                    "name": "Albert Pumarola",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a21e",
                    "name": "Ali Thabet",
                    "hidden": false
                },
                {
                    "_id": "67c14524af5eaa8dd062a21f",
                    "name": "Edgar Schnfeld",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T14:16:56.000Z",
            "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality\n  Samples with Less Compute",
            "summary": "Despite their remarkable performance, modern Diffusion Transformers are\nhindered by substantial resource requirements during inference, stemming from\nthe fixed and large amount of compute needed for each denoising step. In this\nwork, we revisit the conventional static paradigm that allocates a fixed\ncompute budget per denoising iteration and propose a dynamic strategy instead.\nOur simple and sample-efficient framework enables pre-trained DiT models to be\nconverted into flexible ones -- dubbed FlexiDiT -- allowing them to\nprocess inputs at varying compute budgets. We demonstrate how a single\nflexible model can generate images without any drop in quality, while\nreducing the required FLOPs by more than 40\\% compared to their static\ncounterparts, for both class-conditioned and text-conditioned image generation.\nOur method is general and agnostic to input and conditioning modalities. We\nshow how our approach can be readily extended for video generation, where\nFlexiDiT models generate samples with up to 75\\% less compute without\ncompromising performance.",
            "upvotes": 9,
            "discussionId": "67c14529af5eaa8dd062a38c"
        },
        "publishedAt": "2025-02-28T00:10:30.864Z",
        "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20126.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6248
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.16750",
            "authors": [
                {
                    "_id": "67c1b63744d780e60d7c5274",
                    "user": {
                        "_id": "653425f4ed74ace63395826c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QJlB0DOEel6U9b-95wasK.png",
                        "isPro": false,
                        "fullname": "Saikat Barua",
                        "user": "AlignAI",
                        "type": "user"
                    },
                    "name": "Saikat Barua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T13:24:57.086Z",
                    "hidden": false
                },
                {
                    "_id": "67c1b63744d780e60d7c5275",
                    "name": "Mostafizur Rahman",
                    "hidden": false
                },
                {
                    "_id": "67c1b63744d780e60d7c5276",
                    "user": {
                        "_id": "63c99ab3dfac8071d01b61d4",
                        "avatarUrl": "/avatars/9151241b8af4d64d7771740587d1b7a5.svg",
                        "isPro": false,
                        "fullname": "MD Jafor Sadek Khan",
                        "user": "Jafor",
                        "type": "user"
                    },
                    "name": "Md Jafor Sadek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:21:48.563Z",
                    "hidden": false
                },
                {
                    "_id": "67c1b63744d780e60d7c5277",
                    "name": "Rafiul Islam",
                    "hidden": false
                },
                {
                    "_id": "67c1b63744d780e60d7c5278",
                    "name": "Shehnaz Khaled",
                    "hidden": false
                },
                {
                    "_id": "67c1b63744d780e60d7c5279",
                    "name": "Ahmedul Kabir",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-23T23:35:15.000Z",
            "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with\n  Agentic System",
            "summary": "The autonomous AI agents using large language models can create undeniable\nvalues in all span of the society but they face security threats from\nadversaries that warrants immediate protective solutions because trust and\nsafety issues arise. Considering the many-shot jailbreaking and deceptive\nalignment as some of the main advanced attacks, that cannot be mitigated by the\nstatic guardrails used during the supervised training, points out a crucial\nresearch priority for real world robustness. The combination of static\nguardrails in dynamic multi-agent system fails to defend against those attacks.\nWe intend to enhance security for LLM-based agents through the development of\nnew evaluation frameworks which identify and counter threats for safe\noperational deployment. Our work uses three examination methods to detect rogue\nagents through a Reverse Turing Test and analyze deceptive alignment through\nmulti-agent simulations and develops an anti-jailbreaking system by testing it\nwith GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated\nadversarial scenarios. The detection capabilities are strong such as 94\\%\naccuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities\nwhen under long attacks as prompt length increases attack success rates (ASR)\nand diversity metrics become ineffective in prediction while revealing multiple\ncomplex system faults. The findings demonstrate the necessity of adopting\nflexible security systems based on active monitoring that can be performed by\nthe agents themselves together with adaptable interventions by system admin as\nthe current models can create vulnerabilities that can lead to the unreliable\nand vulnerable system. So, in our work, we try to address such situations and\npropose a comprehensive framework to counteract the security issues.",
            "upvotes": 7,
            "discussionId": "67c1b63a44d780e60d7c5317"
        },
        "publishedAt": "2025-02-28T08:46:19.110Z",
        "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/653425f4ed74ace63395826c/czZ9fF4yF6yz3E89YtU6e.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16750.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653425f4ed74ace63395826c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QJlB0DOEel6U9b-95wasK.png",
            "fullname": "Saikat Barua",
            "name": "AlignAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20307",
            "authors": [
                {
                    "_id": "67c1460201cef6d4b9b9ac73",
                    "name": "Xiuli Bi",
                    "hidden": false
                },
                {
                    "_id": "67c1460201cef6d4b9b9ac74",
                    "name": "Jianfei Yuan",
                    "hidden": false
                },
                {
                    "_id": "67c1460201cef6d4b9b9ac75",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "67c1460201cef6d4b9b9ac76",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c1460201cef6d4b9b9ac77",
                    "user": {
                        "_id": "63184c517ca1b876d99b7e0e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
                        "isPro": false,
                        "fullname": "Xiaodong Cun",
                        "user": "vinthony",
                        "type": "user"
                    },
                    "name": "Xiaodong Cun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:38:07.038Z",
                    "hidden": false
                },
                {
                    "_id": "67c1460201cef6d4b9b9ac78",
                    "name": "Chi-Man Pun",
                    "hidden": false
                },
                {
                    "_id": "67c1460201cef6d4b9b9ac79",
                    "name": "Bin Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T17:33:51.000Z",
            "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
            "summary": "We present Mobius, a novel method to generate seamlessly looping videos from\ntext descriptions directly without any user annotations, thereby creating new\nvisual materials for the multi-media presentation. Our method repurposes the\npre-trained video latent diffusion model for generating looping videos from\ntext prompts without any training. During inference, we first construct a\nlatent cycle by connecting the starting and ending noise of the videos. Given\nthat the temporal consistency can be maintained by the context of the video\ndiffusion model, we perform multi-frame latent denoising by gradually shifting\nthe first-frame latent to the end in each step. As a result, the denoising\ncontext varies in each step while maintaining consistency throughout the\ninference process. Moreover, the latent cycle in our method can be of any\nlength. This extends our latent-shifting approach to generate seamless looping\nvideos beyond the scope of the video diffusion model's context. Unlike previous\ncinemagraphs, the proposed method does not require an image as appearance,\nwhich will restrict the motions of the generated results. Instead, our method\ncan produce more dynamic motion and better visual quality. We conduct multiple\nexperiments and comparisons to verify the effectiveness of the proposed method,\ndemonstrating its efficacy in different scenarios. All the code will be made\navailable.",
            "upvotes": 7,
            "discussionId": "67c1460501cef6d4b9b9addf"
        },
        "publishedAt": "2025-02-28T00:14:01.841Z",
        "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20307.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6248
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20127",
            "authors": [
                {
                    "_id": "67c12de08cd49ca63e230b99",
                    "user": {
                        "_id": "654da66fb36f85a025bc24b6",
                        "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
                        "isPro": false,
                        "fullname": "Zexiong Ma",
                        "user": "mizersy",
                        "type": "user"
                    },
                    "name": "Zexiong Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T09:28:35.503Z",
                    "hidden": false
                },
                {
                    "_id": "67c12de08cd49ca63e230b9a",
                    "user": {
                        "_id": "64425a502f4abae43fc0446c",
                        "avatarUrl": "/avatars/7448a8d024813d8a20e09c162a189304.svg",
                        "isPro": false,
                        "fullname": "Chao Peng",
                        "user": "pengchao",
                        "type": "user"
                    },
                    "name": "Chao Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T15:13:55.041Z",
                    "hidden": false
                },
                {
                    "_id": "67c12de08cd49ca63e230b9b",
                    "user": {
                        "_id": "663f2dde4aeb9c177297fbd8",
                        "avatarUrl": "/avatars/ddc143481d2af893c9cdff1a33ccda28.svg",
                        "isPro": false,
                        "fullname": "PengFei",
                        "user": "PengFeiGao",
                        "type": "user"
                    },
                    "name": "Pengfei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:40:45.303Z",
                    "hidden": false
                },
                {
                    "_id": "67c12de08cd49ca63e230b9c",
                    "name": "Xiangxin Meng",
                    "hidden": false
                },
                {
                    "_id": "67c12de08cd49ca63e230b9d",
                    "name": "Yanzhen Zou",
                    "hidden": false
                },
                {
                    "_id": "67c12de08cd49ca63e230b9e",
                    "name": "Bing Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T14:19:45.000Z",
            "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
            "summary": "Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.",
            "upvotes": 7,
            "discussionId": "67c12de08cd49ca63e230bd1"
        },
        "publishedAt": "2025-02-27T22:38:04.562Z",
        "title": "SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20127.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654da66fb36f85a025bc24b6",
            "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
            "fullname": "Zexiong Ma",
            "name": "mizersy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.19459",
            "authors": [
                {
                    "_id": "67c185f46a31b8fe77434551",
                    "user": {
                        "_id": "636de85cc4a7a729c164d2b5",
                        "avatarUrl": "/avatars/3e281e547e1697e1c06805e7e63f3918.svg",
                        "isPro": false,
                        "fullname": "Yu Liu",
                        "user": "YuLiu",
                        "type": "user"
                    },
                    "name": "Yu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:35:40.098Z",
                    "hidden": false
                },
                {
                    "_id": "67c185f46a31b8fe77434552",
                    "user": {
                        "_id": "6304b389bad6ce7fc02691d5",
                        "avatarUrl": "/avatars/a762ca59624ce409650165f36b973488.svg",
                        "isPro": false,
                        "fullname": "Baoxiong Jia",
                        "user": "BuzzBeater",
                        "type": "user"
                    },
                    "name": "Baoxiong Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:35:33.475Z",
                    "hidden": false
                },
                {
                    "_id": "67c185f46a31b8fe77434553",
                    "user": {
                        "_id": "64ab8cb76324705e6a65f7c4",
                        "avatarUrl": "/avatars/15dcae6c345d31ea6e17c11108a7deb7.svg",
                        "isPro": false,
                        "fullname": "Ruijie Lu",
                        "user": "JasonAplp",
                        "type": "user"
                    },
                    "name": "Ruijie Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:35:27.394Z",
                    "hidden": false
                },
                {
                    "_id": "67c185f46a31b8fe77434554",
                    "user": {
                        "_id": "65ae5edddacd99fd58277620",
                        "avatarUrl": "/avatars/5ba35c984d54eef4eacf11ebebafa3a0.svg",
                        "isPro": false,
                        "fullname": "Junfeng Ni",
                        "user": "JunfengNi",
                        "type": "user"
                    },
                    "name": "Junfeng Ni",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:35:20.550Z",
                    "hidden": false
                },
                {
                    "_id": "67c185f46a31b8fe77434555",
                    "name": "Song-Chun Zhu",
                    "hidden": false
                },
                {
                    "_id": "67c185f46a31b8fe77434556",
                    "user": {
                        "_id": "63c7a33121bd95f80ed74652",
                        "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
                        "isPro": false,
                        "fullname": "Siyuan Huang",
                        "user": "thuhsy",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:35:02.710Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T10:25:32.000Z",
            "title": "Building Interactable Replicas of Complex Articulated Objects via\n  Gaussian Splatting",
            "summary": "Building articulated objects is a key challenge in computer vision. Existing\nmethods often fail to effectively integrate information across different object\nstates, limiting the accuracy of part-mesh reconstruction and part dynamics\nmodeling, particularly for complex multi-part articulated objects. We introduce\nArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient\nrepresentation to address these issues. Our method incorporates canonical\nGaussians with coarse-to-fine initialization and updates for aligning\narticulated part information across different object states, and employs a\nskinning-inspired part dynamics modeling module to improve both part-mesh\nreconstruction and articulation learning. Extensive experiments on both\nsynthetic and real-world datasets, including a new benchmark for complex\nmulti-part objects, demonstrate that ArtGS achieves state-of-the-art\nperformance in joint parameter estimation and part mesh reconstruction. Our\napproach significantly improves reconstruction quality and efficiency,\nespecially for multi-part articulated objects. Additionally, we provide\ncomprehensive analyses of our design choices, validating the effectiveness of\neach component to highlight potential areas for future improvement.",
            "upvotes": 6,
            "discussionId": "67c185f66a31b8fe774345d2"
        },
        "publishedAt": "2025-02-28T04:47:08.197Z",
        "title": "Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19459.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c7a33121bd95f80ed74652",
            "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
            "fullname": "Siyuan Huang",
            "name": "thuhsy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.19735",
            "authors": [
                {
                    "_id": "67c1438fd7ffcd1cab1fc412",
                    "user": {
                        "_id": "6727998d4fc2e4f7cc0c85d3",
                        "avatarUrl": "/avatars/ac18eaadd606f7fae64996502f393cf2.svg",
                        "isPro": false,
                        "fullname": "he",
                        "user": "boommmmm",
                        "type": "user"
                    },
                    "name": "Minggui He",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-02-28T05:03:12.675Z",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc413",
                    "user": {
                        "_id": "6380991c5c62156ce7dfae8c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380991c5c62156ce7dfae8c/wtEDILD7gC_WM3gBKi1tk.jpeg",
                        "isPro": false,
                        "fullname": "Yilun Liu",
                        "user": "liuyilun2000",
                        "type": "user"
                    },
                    "name": "Yilun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:43:28.441Z",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc414",
                    "name": "Shimin Tao",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc415",
                    "name": "Yuanchang Luo",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc416",
                    "name": "Hongyong Zeng",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc417",
                    "name": "Chang Su",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc418",
                    "name": "Li Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc419",
                    "name": "Hongxia Ma",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc41a",
                    "name": "Daimeng Wei",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc41b",
                    "user": {
                        "_id": "67285a87e347d62d66473b9a",
                        "avatarUrl": "/avatars/6aeb022c2728ace62bf6884fdb3c9f9c.svg",
                        "isPro": false,
                        "fullname": "WeibinMeng",
                        "user": "weibinmeng",
                        "type": "user"
                    },
                    "name": "Weibin Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:44:36.712Z",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc41c",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc41d",
                    "user": {
                        "_id": "66f98351ee969ff116986327",
                        "avatarUrl": "/avatars/e038ef1462f77f5e87e868339993f92d.svg",
                        "isPro": false,
                        "fullname": "Boxing Chen",
                        "user": "BoxingChen",
                        "type": "user"
                    },
                    "name": "Boxing Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:44:51.085Z",
                    "hidden": false
                },
                {
                    "_id": "67c1438fd7ffcd1cab1fc41e",
                    "name": "Osamu Yoshie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T03:57:00.000Z",
            "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning",
            "summary": "Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans and supervised fine-tuning (SFT) prone to catastrophic forgetting,\nlimiting their adaptability to diverse translation scenarios. This paper\nintroduces R1-Translator (R1-T1), a novel framework to achieve inference-time\nreasoning for general MT via reinforcement learning (RL) with human-aligned\nCoTs comprising six common patterns. Our approach pioneers three innovations:\n(1) extending reasoning-based translation beyond MT sub-tasks to six languages\nand diverse tasks (e.g., legal/medical domain adaptation, idiom resolution);\n(2) formalizing six expert-curated CoT templates that mirror hybrid human\nstrategies like context-aware paraphrasing and back translation; and (3)\nenabling self-evolving CoT discovery and anti-forgetting adaptation through RL\nwith KL-constrained rewards. Experimental results indicate a steady translation\nperformance improvement in 21 languages and 80 translation directions on\nFlores-101 test set, especially on the 15 languages unseen from training, with\nits general multilingual abilities preserved compared with plain SFT.",
            "upvotes": 6,
            "discussionId": "67c14390d7ffcd1cab1fc479"
        },
        "publishedAt": "2025-02-28T00:03:34.893Z",
        "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19735.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6248
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20388",
            "authors": [
                {
                    "_id": "67c1643aa4ccbde471532ba6",
                    "name": "Sucheng Ren",
                    "hidden": false
                },
                {
                    "_id": "67c1643aa4ccbde471532ba7",
                    "name": "Qihang Yu",
                    "hidden": false
                },
                {
                    "_id": "67c1643aa4ccbde471532ba8",
                    "name": "Ju He",
                    "hidden": false
                },
                {
                    "_id": "67c1643aa4ccbde471532ba9",
                    "name": "Xiaohui Shen",
                    "hidden": false
                },
                {
                    "_id": "67c1643aa4ccbde471532baa",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "67c1643aa4ccbde471532bab",
                    "name": "Liang-Chieh Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T18:59:08.000Z",
            "title": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual\n  Generation",
            "summary": "Autoregressive (AR) modeling, known for its next-token prediction paradigm,\nunderpins state-of-the-art language and visual generative models.\nTraditionally, a ``token'' is treated as the smallest prediction unit, often a\ndiscrete symbol in language or a quantized patch in vision. However, the\noptimal token definition for 2D image structures remains an open question.\nMoreover, AR models suffer from exposure bias, where teacher forcing during\ntraining leads to error accumulation at inference. In this paper, we propose\nxAR, a generalized AR framework that extends the notion of a token to an entity\nX, which can represent an individual patch token, a cell (a ktimes k\ngrouping of neighboring patches), a subsample (a non-local grouping of distant\npatches), a scale (coarse-to-fine resolution), or even a whole image.\nAdditionally, we reformulate discrete token classification as\ncontinuous entity regression, leveraging flow-matching methods at each\nAR step. This approach conditions training on noisy entities instead of ground\ntruth tokens, leading to Noisy Context Learning, which effectively alleviates\nexposure bias. As a result, xAR offers two key advantages: (1) it enables\nflexible prediction units that capture different contextual granularity and\nspatial structures, and (2) it mitigates exposure bias by avoiding reliance on\nteacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B\n(172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20times faster\ninference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24,\nrunning 2.2times faster than the previous best-performing model without\nrelying on vision foundation modules (\\eg, DINOv2) or advanced guidance\ninterval sampling.",
            "upvotes": 5,
            "discussionId": "67c1643ba4ccbde471532c03"
        },
        "publishedAt": "2025-02-28T13:21:13.227Z",
        "title": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20388.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65317ea1501804124f011950",
            "avatarUrl": "/avatars/b055c3aba0c65d5377c69472e4576480.svg",
            "fullname": "Ren",
            "name": "OliverRen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17355",
            "authors": [
                {
                    "_id": "67bf1808b91e7e6477d92c1e",
                    "user": {
                        "_id": "653f7e569e84d1e8b6a66e70",
                        "avatarUrl": "/avatars/24eaa6434508a162c349aebfc51990ff.svg",
                        "isPro": false,
                        "fullname": "Yihong Liu",
                        "user": "yihongLiu",
                        "type": "user"
                    },
                    "name": "Yihong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T15:14:48.351Z",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c1f",
                    "user": {
                        "_id": "63629b9f2a84d82a8c8feb32",
                        "avatarUrl": "/avatars/8484b5bf8311b28249757729b1ce80f8.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Runsheng",
                        "type": "user"
                    },
                    "name": "Runsheng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:16:28.041Z",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c20",
                    "user": {
                        "_id": "658559148615630cb3ec5b6b",
                        "avatarUrl": "/avatars/dd804ca277e6b19903bb550cc167ba4a.svg",
                        "isPro": false,
                        "fullname": "Lea Hirlimann",
                        "user": "hirlimann",
                        "type": "user"
                    },
                    "name": "Lea Hirlimann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:16:18.398Z",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c21",
                    "user": {
                        "_id": "62502669d2d191ac43320ade",
                        "avatarUrl": "/avatars/7997e9b2012059edb22b745c3b737481.svg",
                        "isPro": false,
                        "fullname": "Ahmad Dawar Hakimi",
                        "user": "adhakimi",
                        "type": "user"
                    },
                    "name": "Ahmad Dawar Hakimi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:16:11.693Z",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c22",
                    "name": "Mingyang Wang",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c23",
                    "user": {
                        "_id": "61bf84c8ca59d6d196a1b4e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
                        "isPro": false,
                        "fullname": "Amir Hossein Kargaran",
                        "user": "kargaranamir",
                        "type": "user"
                    },
                    "name": "Amir Hossein Kargaran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-26T15:37:07.932Z",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c24",
                    "name": "Sascha Rothe",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c25",
                    "user": {
                        "_id": "62ab10f04bd2ebf5dbad205c",
                        "avatarUrl": "/avatars/65356b3b057159cc67a86efb26b53486.svg",
                        "isPro": false,
                        "fullname": "Franois Yvon",
                        "user": "fyvo",
                        "type": "user"
                    },
                    "name": "Franois Yvon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:16:57.343Z",
                    "hidden": false
                },
                {
                    "_id": "67bf1808b91e7e6477d92c26",
                    "name": "Hinrich Schtze",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T17:33:18.000Z",
            "title": "On Relation-Specific Neurons in Large Language Models",
            "summary": "In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation r on the LLM's ability to\nhandle (1) facts whose relation is r and (2) facts whose relation is a\ndifferent relation r' neq r. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. (i) Neuron cumulativity. The neurons for\nr present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in r. (ii) Neuron\nversatility. Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n(iii) Neuron interference. Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.",
            "upvotes": 4,
            "discussionId": "67bf1808b91e7e6477d92c55"
        },
        "publishedAt": "2025-02-28T08:54:03.125Z",
        "title": "On Relation-Specific Neurons in Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17355.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bf84c8ca59d6d196a1b4e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
            "fullname": "Amir Hossein Kargaran",
            "name": "kargaranamir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18197",
            "authors": [
                {
                    "_id": "67c07fa2a43d7939d6d90d54",
                    "user": {
                        "_id": "67c07f498589d8ecb7912686",
                        "avatarUrl": "/avatars/84e77389c211a7c4237f73208658c23a.svg",
                        "isPro": false,
                        "fullname": "Gianluigi Silvestri",
                        "user": "gisilvs",
                        "type": "user"
                    },
                    "name": "Gianluigi Silvestri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-27T22:09:04.844Z",
                    "hidden": false
                },
                {
                    "_id": "67c07fa2a43d7939d6d90d55",
                    "name": "Luca Ambrogioni",
                    "hidden": false
                },
                {
                    "_id": "67c07fa2a43d7939d6d90d56",
                    "name": "Chieh-Hsin Lai",
                    "hidden": false
                },
                {
                    "_id": "67c07fa2a43d7939d6d90d57",
                    "user": {
                        "_id": "66138c4074f830bc7d9d6622",
                        "avatarUrl": "/avatars/d50da6d7597d3bcf63f9f0c74e910155.svg",
                        "isPro": false,
                        "fullname": "Yuhta Takida",
                        "user": "ytakida",
                        "type": "user"
                    },
                    "name": "Yuhta Takida",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:15:50.340Z",
                    "hidden": false
                },
                {
                    "_id": "67c07fa2a43d7939d6d90d58",
                    "user": {
                        "_id": "665e32384ecc8a7181634f6d",
                        "avatarUrl": "/avatars/8752f952010540d14f45eac849e91371.svg",
                        "isPro": false,
                        "fullname": "Yuki Mitsufuji",
                        "user": "mittu1204",
                        "type": "user"
                    },
                    "name": "Yuki Mitsufuji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T15:15:56.987Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T13:38:04.000Z",
            "title": "Training Consistency Models with Variational Noise Coupling",
            "summary": "Consistency Training (CT) has recently emerged as a promising alternative to\ndiffusion models, achieving competitive performance in image generation tasks.\nHowever, non-distillation consistency training often suffers from high variance\nand instability, and analyzing and improving its training dynamics is an active\narea of research. In this work, we propose a novel CT training approach based\non the Flow Matching framework. Our main contribution is a trained\nnoise-coupling scheme inspired by the architecture of Variational Autoencoders\n(VAE). By training a data-dependent noise emission model implemented as an\nencoder architecture, our method can indirectly learn the geometry of the\nnoise-to-data mapping, which is instead fixed by the choice of the forward\nprocess in classical CT. Empirical results across diverse image datasets show\nsignificant generative improvements, with our model outperforming baselines and\nachieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and\nattaining FID on par with SoTA on ImageNet at 64 times 64 resolution in\n2-step generation. Our code is available at https://github.com/sony/vct .",
            "upvotes": 3,
            "discussionId": "67c07fa6a43d7939d6d90e1f"
        },
        "publishedAt": "2025-02-28T07:55:48.923Z",
        "title": "Training Consistency Models with Variational Noise Coupling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18197.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67c07f498589d8ecb7912686",
            "avatarUrl": "/avatars/84e77389c211a7c4237f73208658c23a.svg",
            "fullname": "Gianluigi Silvestri",
            "name": "gisilvs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.16111",
            "authors": [
                {
                    "_id": "67be18d2bb66802239ec8095",
                    "name": "Mihir Parmar",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec8096",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec8097",
                    "name": "Palash Goyal",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec8098",
                    "name": "Yanfei Chen",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec8099",
                    "name": "Long Le",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec809a",
                    "name": "Swaroop Mishra",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec809b",
                    "name": "Hossein Mobahi",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec809c",
                    "name": "Jindong Gu",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec809d",
                    "name": "Zifeng Wang",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec809e",
                    "name": "Hootan Nakhost",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec809f",
                    "name": "Chitta Baral",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec80a0",
                    "name": "Chen-Yu Lee",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec80a1",
                    "name": "Tomas Pfister",
                    "hidden": false
                },
                {
                    "_id": "67be18d2bb66802239ec80a2",
                    "name": "Hamid Palangi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-22T06:21:56.000Z",
            "title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning\n  Trajectories for Complex Problem Solving",
            "summary": "Recent agent frameworks and inference-time algorithms often struggle with\ncomplex planning problems due to limitations in verifying generated plans or\nreasoning and varying complexity of instances within a single task. Many\nexisting methods for these tasks either perform task-level verification without\nconsidering constraints or apply inference-time algorithms without adapting to\ninstance-level complexity. To address these limitations, we propose PlanGEN, a\nmodel-agnostic and easily scalable agent framework with three key components:\nconstraint, verification, and selection agents. Specifically, our approach\nproposes constraint-guided iterative verification to enhance performance of\ninference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN\nframework, the selection agent optimizes algorithm choice based on instance\ncomplexity, ensuring better adaptability to complex planning problems.\nExperimental results demonstrate significant improvements over the strongest\nbaseline across multiple benchmarks, achieving state-of-the-art results on\nNATURAL PLAN (sim8%uparrow), OlympiadBench (sim4%uparrow), DocFinQA\n(sim7%uparrow), and GPQA (sim1%uparrow). Our key finding highlights\nthat constraint-guided iterative verification improves inference-time\nalgorithms, and adaptive selection further boosts performance on complex\nplanning and reasoning problems.",
            "upvotes": 1,
            "discussionId": "67be18d3bb66802239ec80d1"
        },
        "publishedAt": "2025-02-28T16:51:51.551Z",
        "title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61a00714f5119f1651f7e4be/dZJBpAQlVaJSFYXhuE1Rl.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16111.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61a00714f5119f1651f7e4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651013366729-61a00714f5119f1651f7e4be.jpeg",
            "fullname": "Mihir Parmar",
            "name": "Mihir3009",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20378",
            "authors": [
                {
                    "_id": "67c1aa781c3a8036977ed8b1",
                    "user": {
                        "_id": "6442882f8443bce4c98a88aa",
                        "avatarUrl": "/avatars/70d5aa651b07b43629554096d76efd4c.svg",
                        "isPro": false,
                        "fullname": "Kong",
                        "user": "imsuperkong",
                        "type": "user"
                    },
                    "name": "Hanyang Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T15:13:50.949Z",
                    "hidden": false
                },
                {
                    "_id": "67c1aa781c3a8036977ed8b2",
                    "user": {
                        "_id": "634cfebc350bcee9bed20a4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                        "isPro": false,
                        "fullname": "Xingyi Yang",
                        "user": "adamdad",
                        "type": "user"
                    },
                    "name": "Xingyi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:45:42.180Z",
                    "hidden": false
                },
                {
                    "_id": "67c1aa781c3a8036977ed8b3",
                    "user": {
                        "_id": "63fc03a50aab060792ffef39",
                        "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
                        "isPro": false,
                        "fullname": "Wangxinchao",
                        "user": "wxcTest",
                        "type": "user"
                    },
                    "name": "Xinchao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-02-28T12:45:56.197Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T18:53:06.000Z",
            "title": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via\n  Sparse Time-Variant Attribute Modeling",
            "summary": "Rendering dynamic scenes from monocular videos is a crucial yet challenging\ntask. The recent deformable Gaussian Splatting has emerged as a robust solution\nto represent real-world dynamic scenes. However, it often leads to heavily\nredundant Gaussians, attempting to fit every training view at various time\nsteps, leading to slower rendering speeds. Additionally, the attributes of\nGaussians in static areas are time-invariant, making it unnecessary to model\nevery Gaussian, which can cause jittering in static regions. In practice, the\nprimary bottleneck in rendering speed for dynamic scenes is the number of\nGaussians. In response, we introduce Efficient Dynamic Gaussian Splatting\n(EDGS), which represents dynamic scenes via sparse time-variant attribute\nmodeling. Our approach formulates dynamic scenes using a sparse anchor-grid\nrepresentation, with the motion flow of dense Gaussians calculated via a\nclassical kernel representation. Furthermore, we propose an unsupervised\nstrategy to efficiently filter out anchors corresponding to static areas. Only\nanchors associated with deformable objects are input into MLPs to query\ntime-variant attributes. Experiments on two real-world datasets demonstrate\nthat our EDGS significantly improves the rendering speed with superior\nrendering quality compared to previous state-of-the-art methods.",
            "upvotes": 1,
            "discussionId": "67c1aa7a1c3a8036977ed977"
        },
        "publishedAt": "2025-02-28T07:25:35.166Z",
        "title": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20378.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6442882f8443bce4c98a88aa",
            "avatarUrl": "/avatars/70d5aa651b07b43629554096d76efd4c.svg",
            "fullname": "Kong",
            "name": "imsuperkong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]