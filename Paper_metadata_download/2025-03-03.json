[
    {
        "paper": {
            "id": "2502.20730",
            "authors": [
                {
                    "_id": "67c514aba3d873e41624a082",
                    "user": {
                        "_id": "63664c8fa2abcdf2fd6425ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
                        "isPro": false,
                        "fullname": "Li Zhuoqun",
                        "user": "lzq2021",
                        "type": "user"
                    },
                    "name": "Zhuoqun Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T08:07:26.218Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a083",
                    "user": {
                        "_id": "64a4ceda9a90f701134189b7",
                        "avatarUrl": "/avatars/859a189c5d2ae2fcb9aa2d79104fbfe7.svg",
                        "isPro": false,
                        "fullname": "Haiyang Yu",
                        "user": "yhycai",
                        "type": "user"
                    },
                    "name": "Haiyang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T09:31:12.493Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a084",
                    "user": {
                        "_id": "63ef664304b0e373992a2633",
                        "avatarUrl": "/avatars/cba554ff88bd8b68ae51bea8ee991d13.svg",
                        "isPro": false,
                        "fullname": "Xuanang Chen",
                        "user": "xuanang",
                        "type": "user"
                    },
                    "name": "Xuanang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:31.384Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a085",
                    "user": {
                        "_id": "6711c702f858a456b4b9f3a4",
                        "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
                        "isPro": false,
                        "fullname": "Hongyu  Lin",
                        "user": "sanmusunrise",
                        "type": "user"
                    },
                    "name": "Hongyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:28:09.791Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a086",
                    "user": {
                        "_id": "6216496a9b34d2fb49144599",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
                        "isPro": false,
                        "fullname": "Yaojie Lu",
                        "user": "luyaojie",
                        "type": "user"
                    },
                    "name": "Yaojie Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:38.957Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a087",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a088",
                    "user": {
                        "_id": "65e99a77e71555ed193609cf",
                        "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
                        "isPro": false,
                        "fullname": "Xianpei Han",
                        "user": "xphan",
                        "type": "user"
                    },
                    "name": "Xianpei Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:51.007Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a089",
                    "user": {
                        "_id": "66641b2fd8e1e34bc621e688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
                        "isPro": false,
                        "fullname": "Yongbin Li",
                        "user": "Yongbin-Li",
                        "type": "user"
                    },
                    "name": "Yongbin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:29:57.561Z",
                    "hidden": false
                },
                {
                    "_id": "67c514aba3d873e41624a08a",
                    "name": "Le Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T05:23:10.000Z",
            "title": "DeepSolution: Boosting Complex Engineering Solution Design via\n  Tree-based Exploration and Bi-point Thinking",
            "summary": "Designing solutions for complex engineering challenges is crucial in human\nproduction activities. However, previous research in the retrieval-augmented\ngeneration (RAG) field has not sufficiently addressed tasks related to the\ndesign of complex engineering solutions. To fill this gap, we introduce a new\nbenchmark, SolutionBench, to evaluate a system's ability to generate complete\nand feasible solutions for engineering problems with multiple complex\nconstraints. To further advance the design of complex engineering solutions, we\npropose a novel system, SolutionRAG, that leverages the tree-based exploration\nand bi-point thinking mechanism to generate reliable solutions. Extensive\nexperimental results demonstrate that SolutionRAG achieves state-of-the-art\n(SOTA) performance on the SolutionBench, highlighting its potential to enhance\nthe automation and reliability of complex engineering solution design in\nreal-world applications.",
            "upvotes": 26,
            "discussionId": "67c514aca3d873e41624a10b",
            "githubRepo": "https://github.com/Li-Z-Q/DeepSolution"
        },
        "publishedAt": "2025-03-02T21:35:24.437Z",
        "title": "DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/y_kT4GP3xgm-5RdguMNV7.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/wDAS_USsxsVHbin1I5CEe.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63664c8fa2abcdf2fd6425ed/4lJgWp9V8pm4vDBUH4I5n.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20730.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63664c8fa2abcdf2fd6425ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63664c8fa2abcdf2fd6425ed/IywpB0DXZ_twkmZmVSCCD.jpeg",
            "fullname": "Li Zhuoqun",
            "name": "lzq2021",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18600",
            "authors": [
                {
                    "_id": "67c0a8058589d8ecb79d472b",
                    "user": {
                        "_id": "6594b1bb57a556fbe162915e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594b1bb57a556fbe162915e/WuYxqbbvaJaT-xsk5KhoT.jpeg",
                        "isPro": false,
                        "fullname": "Silei Xu",
                        "user": "sileixu",
                        "type": "user"
                    },
                    "name": "Silei Xu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-02-27T18:01:14.543Z",
                    "hidden": false
                },
                {
                    "_id": "67c0a8058589d8ecb79d472c",
                    "name": "Wenhao Xie",
                    "hidden": false
                },
                {
                    "_id": "67c0a8058589d8ecb79d472d",
                    "name": "Lingxiao Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c0a8058589d8ecb79d472e",
                    "user": {
                        "_id": "5efd09cf49ed724c8a135868",
                        "avatarUrl": "/avatars/af12bc94657979677a9f26183f0c9727.svg",
                        "isPro": false,
                        "fullname": "Pengcheng He",
                        "user": "DeBERTa",
                        "type": "user"
                    },
                    "name": "Pengcheng He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:30:43.479Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T19:36:06.000Z",
            "title": "Chain of Draft: Thinking Faster by Writing Less",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)\nprompting, which emphasizes verbose, step-by-step reasoning. However, humans\ntypically employ a more efficient strategy: drafting concise intermediate\nthoughts that capture only essential information. In this work, we propose\nChain of Draft (CoD), a novel paradigm inspired by human cognitive processes,\nwhere LLMs generate minimalistic yet informative intermediate reasoning outputs\nwhile solving tasks. By reducing verbosity and focusing on critical insights,\nCoD matches or surpasses CoT in accuracy while using as little as only 7.6% of\nthe tokens, significantly reducing cost and latency across various reasoning\ntasks.",
            "upvotes": 25,
            "discussionId": "67c0a8078589d8ecb79d47ed",
            "githubRepo": "https://github.com/sileix/chain-of-draft"
        },
        "publishedAt": "2025-03-03T02:35:09.967Z",
        "title": "Chain of Draft: Thinking Faster by Writing Less",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18600.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "63da3d7ae697e5898cb86854",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
            "fullname": "Talha Rüzgar Akkuş",
            "name": "Q-bert",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 85
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20380",
            "authors": [
                {
                    "_id": "67c34e3beae05d8f94f800b4",
                    "name": "Arnav Kumar Jain",
                    "hidden": false
                },
                {
                    "_id": "67c34e3beae05d8f94f800b5",
                    "user": {
                        "_id": "6421d2972143035270db37b9",
                        "avatarUrl": "/avatars/4fadeafc273d32cf72fe2f12d444c5e8.svg",
                        "isPro": false,
                        "fullname": "Gonzalo Gonzalez",
                        "user": "chalo2000",
                        "type": "user"
                    },
                    "name": "Gonzalo Gonzalez-Pumariega",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-02T20:15:14.593Z",
                    "hidden": false
                },
                {
                    "_id": "67c34e3beae05d8f94f800b6",
                    "name": "Wayne Chen",
                    "hidden": false
                },
                {
                    "_id": "67c34e3beae05d8f94f800b7",
                    "name": "Alexander M Rush",
                    "hidden": false
                },
                {
                    "_id": "67c34e3beae05d8f94f800b8",
                    "name": "Wenting Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c34e3beae05d8f94f800b9",
                    "name": "Sanjiban Choudhury",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T18:55:05.000Z",
            "title": "Multi-Turn Code Generation Through Single-Step Rewards",
            "summary": "We address the problem of code generation from multi-turn execution feedback.\nExisting methods either generate code without feedback or use complex,\nhierarchical reinforcement learning to optimize multi-turn rewards. We propose\na simple yet scalable approach, muCode, that solves multi-turn code\ngeneration using only single-step rewards. Our key insight is that code\ngeneration is a one-step recoverable MDP, where the correct code can be\nrecovered from any intermediate code state in a single turn. muCode\niteratively trains both a generator to provide code solutions conditioned on\nmulti-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant\nimprovements over the state-of-the-art baselines. We provide analysis of the\ndesign choices of the reward models and policy, and show the efficacy of\nmuCode at utilizing the execution feedback. Our code is available at\nhttps://github.com/portal-cornell/muCode.",
            "upvotes": 21,
            "discussionId": "67c34e3ceae05d8f94f8010e",
            "projectPage": "https://portal-cornell.github.io/muCode/",
            "githubRepo": "https://github.com/portal-cornell/muCode"
        },
        "publishedAt": "2025-03-03T11:25:57.425Z",
        "title": "Multi-Turn Code Generation Through Single-Step Rewards",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20380.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6421d2972143035270db37b9",
            "avatarUrl": "/avatars/4fadeafc273d32cf72fe2f12d444c5e8.svg",
            "fullname": "Gonzalo Gonzalez",
            "name": "chalo2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.21318",
            "authors": [
                {
                    "_id": "67c5c13ca10c7059c3d3d4c9",
                    "user": {
                        "_id": "63bb08b07fd5e883e13efd32",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63bb08b07fd5e883e13efd32/aKAR8alYsYteEQImBrWO7.jpeg",
                        "isPro": false,
                        "fullname": "Lucas Degeorge",
                        "user": "Lucasdegeorge",
                        "type": "user"
                    },
                    "name": "L. Degeorge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T16:07:10.195Z",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4ca",
                    "user": {
                        "_id": "66f971c83d94062a4aa808ef",
                        "avatarUrl": "/avatars/f1d6c4d85d20fd4a614278ecd784c772.svg",
                        "isPro": false,
                        "fullname": "Arijit Ghosh",
                        "user": "arijitghosh",
                        "type": "user"
                    },
                    "name": "A. Ghosh",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-03T18:07:11.151Z",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4cb",
                    "user": {
                        "_id": "630652803aed65d34e98eee3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630652803aed65d34e98eee3/XG_PuVFA6ziGQZd3UUZSF.jpeg",
                        "isPro": false,
                        "fullname": "Nicolas Dufour",
                        "user": "nicolas-dufour",
                        "type": "user"
                    },
                    "name": "N. Dufour",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T16:07:14.366Z",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4cc",
                    "name": "D. Picard",
                    "hidden": false
                },
                {
                    "_id": "67c5c13ca10c7059c3d3d4cd",
                    "name": "V. Kalogeiton",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T18:59:42.000Z",
            "title": "How far can we go with ImageNet for Text-to-Image generation?",
            "summary": "Recent text-to-image (T2I) generation models have achieved remarkable results\nby training on billion-scale datasets, following a `bigger is better' paradigm\nthat prioritizes data quantity over quality. We challenge this established\nparadigm by demonstrating that strategic data augmentation of small,\nwell-curated datasets can match or outperform models trained on massive\nweb-scraped collections. Using only ImageNet enhanced with well-designed text\nand image augmentations, we achieve a +2 overall score over SD-XL on GenEval\nand +5 on DPGBench while using just 1/10th the parameters and 1/1000th the\ntraining images. Our results suggest that strategic data augmentation, rather\nthan massive datasets, could offer a more sustainable path forward for T2I\ngeneration.",
            "upvotes": 15,
            "discussionId": "67c5c145a10c7059c3d3d693",
            "projectPage": "https://lucasdegeorge.github.io/projects/t2i_imagenet/",
            "githubRepo": "https://github.com/lucasdegeorge/T2I-ImageNet"
        },
        "publishedAt": "2025-03-03T09:49:10.381Z",
        "title": "How far can we go with ImageNet for Text-to-Image generation?",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630652803aed65d34e98eee3/8GIi2e6959v5dl4XUVqkc.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.21318.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630652803aed65d34e98eee3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630652803aed65d34e98eee3/XG_PuVFA6ziGQZd3UUZSF.jpeg",
            "fullname": "Nicolas Dufour",
            "name": "nicolas-dufour",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18017",
            "authors": [
                {
                    "_id": "67bef5a6070ec160042d99f4",
                    "user": {
                        "_id": "657429d833e5a4bf5b278615",
                        "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
                        "isPro": false,
                        "fullname": "QiuchenWang",
                        "user": "autumncc",
                        "type": "user"
                    },
                    "name": "Qiuchen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-02-28T12:15:57.850Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f5",
                    "name": "Ruixue Ding",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f6",
                    "user": {
                        "_id": "64892d31cbda0d1cdb956897",
                        "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
                        "isPro": false,
                        "fullname": "Zehui Chen",
                        "user": "lovesnowbest",
                        "type": "user"
                    },
                    "name": "Zehui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:32:18.129Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f7",
                    "user": {
                        "_id": "65351cbe6141b3927afaed17",
                        "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
                        "isPro": false,
                        "fullname": "weiqi wu",
                        "user": "vickywu",
                        "type": "user"
                    },
                    "name": "Weiqi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:32:12.075Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f8",
                    "user": {
                        "_id": "62e8efb14210d3fe69eacb42",
                        "avatarUrl": "/avatars/2feadd75274bf353b910f4679ef72b39.svg",
                        "isPro": false,
                        "fullname": "Shihang Wang",
                        "user": "shihang",
                        "type": "user"
                    },
                    "name": "Shihang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:32:05.679Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99f9",
                    "user": {
                        "_id": "63a091e42fabbbb89991f5ce",
                        "avatarUrl": "/avatars/d55485b06461764c36c9edf9d6e8892c.svg",
                        "isPro": false,
                        "fullname": "pengjun xie",
                        "user": "xpjandy",
                        "type": "user"
                    },
                    "name": "Pengjun Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:31:59.813Z",
                    "hidden": false
                },
                {
                    "_id": "67bef5a6070ec160042d99fa",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T09:26:12.000Z",
            "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\n  Iterative Reasoning Agents",
            "summary": "Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark.",
            "upvotes": 13,
            "discussionId": "67bef5a7070ec160042d9a65"
        },
        "publishedAt": "2025-03-02T22:22:01.895Z",
        "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18017.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "fullname": "QiuchenWang",
            "name": "autumncc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20545",
            "authors": [
                {
                    "_id": "67c51b459d5807d6674b3d3c",
                    "name": "Kechen Li",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d3d",
                    "name": "Wenqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d3e",
                    "name": "Coralia Cartis",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d3f",
                    "user": {
                        "_id": "64bb61e876a6e2efcc728e22",
                        "avatarUrl": "/avatars/b0ed1c9f13fd1f2c99d202155001e39b.svg",
                        "isPro": false,
                        "fullname": "Tianbo Ji",
                        "user": "jitianbo",
                        "type": "user"
                    },
                    "name": "Tianbo Ji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:35:49.782Z",
                    "hidden": false
                },
                {
                    "_id": "67c51b459d5807d6674b3d40",
                    "user": {
                        "_id": "65b04d2291e63920a7898c9e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
                        "isPro": false,
                        "fullname": "Liu",
                        "user": "Shiweiliuiiiiiii",
                        "type": "user"
                    },
                    "name": "Shiwei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T11:14:45.635Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T21:41:43.000Z",
            "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
            "summary": "Large Language Models (LLMs) have achieved human-level proficiency across\ndiverse tasks, but their ability to perform rigorous mathematical problem\nsolving remains an open challenge. In this work, we investigate a fundamental\nyet computationally intractable problem: determining whether a given\nmultivariate polynomial is nonnegative. This problem, closely related to\nHilbert's Seventeenth Problem, plays a crucial role in global polynomial\noptimization and has applications in various fields. First, we introduce\nSoS-1K, a meticulously curated dataset of approximately 1,000 polynomials,\nalong with expert-designed reasoning instructions based on five progressively\nchallenging criteria. Evaluating multiple state-of-the-art LLMs, we find that\nwithout structured guidance, all models perform only slightly above the random\nguess baseline 50%. However, high-quality reasoning instructions significantly\nimprove accuracy, boosting performance up to 81%. Furthermore, our 7B model,\nSoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3\nand GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation\ntime needed for letters, respectively. Our findings highlight the potential of\nLLMs to push the boundaries of mathematical reasoning and tackle NP-hard\nproblems.",
            "upvotes": 12,
            "discussionId": "67c51b469d5807d6674b3d88"
        },
        "publishedAt": "2025-03-02T22:00:31.796Z",
        "title": "SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20545.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6268
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20396",
            "authors": [
                {
                    "_id": "67c51d36c830dcb76bbb5994",
                    "user": {
                        "_id": "65e8b34632f166badb8d893a",
                        "avatarUrl": "/avatars/a55da1d08dc1104e6c539cd3f1ef1ebe.svg",
                        "isPro": false,
                        "fullname": "T",
                        "user": "toruowo",
                        "type": "user"
                    },
                    "name": "Toru Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T16:07:25.709Z",
                    "hidden": false
                },
                {
                    "_id": "67c51d36c830dcb76bbb5995",
                    "name": "Kartik Sachdev",
                    "hidden": false
                },
                {
                    "_id": "67c51d36c830dcb76bbb5996",
                    "name": "Linxi Fan",
                    "hidden": false
                },
                {
                    "_id": "67c51d36c830dcb76bbb5997",
                    "user": {
                        "_id": "65369a95605a07338de78ab0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
                        "isPro": false,
                        "fullname": "Jitendra Malik ",
                        "user": "jitendra1995",
                        "type": "user"
                    },
                    "name": "Jitendra Malik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:36:34.177Z",
                    "hidden": false
                },
                {
                    "_id": "67c51d36c830dcb76bbb5998",
                    "name": "Yuke Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T18:59:52.000Z",
            "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous\n  Manipulation on Humanoids",
            "summary": "Reinforcement learning has delivered promising results in achieving human- or\neven superhuman-level capabilities across diverse problem domains, but success\nin dexterous robot manipulation remains limited. This work investigates the key\nchallenges in applying reinforcement learning to solve a collection of\ncontact-rich manipulation tasks on a humanoid embodiment. We introduce novel\ntechniques to overcome the identified challenges with empirical validation. Our\nmain contributions include an automated real-to-sim tuning module that brings\nthe simulated environment closer to the real world, a generalized reward design\nscheme that simplifies reward engineering for long-horizon contact-rich\nmanipulation tasks, a divide-and-conquer distillation process that improves the\nsample efficiency of hard-exploration problems while maintaining sim-to-real\nperformance, and a mixture of sparse and dense object representations to bridge\nthe sim-to-real perception gap. We show promising results on three humanoid\ndexterous manipulation tasks, with ablation studies on each technique. Our work\npresents a successful approach to learning humanoid dexterous manipulation\nusing sim-to-real reinforcement learning, achieving robust generalization and\nhigh performance without the need for human demonstration.",
            "upvotes": 9,
            "discussionId": "67c51d39c830dcb76bbb5a1f"
        },
        "publishedAt": "2025-03-02T22:08:44.891Z",
        "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20396.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6268
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.19577",
            "authors": [
                {
                    "_id": "67c42356054ae6d1c760b643",
                    "user": {
                        "_id": "66588b6fd22637bfab498709",
                        "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
                        "isPro": false,
                        "fullname": "Hugues Turbé",
                        "user": "hturbe",
                        "type": "user"
                    },
                    "name": "Hugues Turbé",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-02T20:15:04.391Z",
                    "hidden": false
                },
                {
                    "_id": "67c42356054ae6d1c760b644",
                    "name": "Mina Bjelogrlic",
                    "hidden": false
                },
                {
                    "_id": "67c42356054ae6d1c760b645",
                    "name": "Gianmarco Mengaldo",
                    "hidden": false
                },
                {
                    "_id": "67c42356054ae6d1c760b646",
                    "name": "Christian Lovis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T21:40:30.000Z",
            "title": "Tell me why: Visual foundation models as self-explainable classifiers",
            "summary": "Visual foundation models (VFMs) have become increasingly popular due to their\nstate-of-the-art performance. However, interpretability remains crucial for\ncritical applications. In this sense, self-explainable models (SEM) aim to\nprovide interpretable classifiers that decompose predictions into a weighted\nsum of interpretable concepts. Despite their promise, recent studies have shown\nthat these explanations often lack faithfulness. In this work, we combine VFMs\nwith a novel prototypical architecture and specialized training objectives. By\ntraining only a lightweight head (approximately 1M parameters) on top of frozen\nVFMs, our approach (ProtoFM) offers an efficient and interpretable solution.\nEvaluations demonstrate that our approach achieves competitive classification\nperformance while outperforming existing models across a range of\ninterpretability metrics derived from the literature. Code is available at\nhttps://github.com/hturbe/proto-fm.",
            "upvotes": 7,
            "discussionId": "67c4235c054ae6d1c760b806"
        },
        "publishedAt": "2025-03-03T04:21:42.563Z",
        "title": "Tell me why: Visual foundation models as self-explainable classifiers",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66588b6fd22637bfab498709/4VG_eDtZKZ4kj1AdG_P14.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19577.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66588b6fd22637bfab498709",
            "avatarUrl": "/avatars/9007f0d3b078bd6193912a5359107f24.svg",
            "fullname": "Hugues Turbé",
            "name": "hturbe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20969",
            "authors": [
                {
                    "_id": "67c5bc8babe08983d98a4248",
                    "name": "Chien-Yu Lin",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4249",
                    "user": {
                        "_id": "6304ac1a412a1b9d381ca378",
                        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
                        "isPro": false,
                        "fullname": "Keisuke Kamahori",
                        "user": "kamahori",
                        "type": "user"
                    },
                    "name": "Keisuke Kamahori",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T16:07:17.078Z",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a424a",
                    "name": "Yiyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a424b",
                    "name": "Xiaoxiang Shi",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a424c",
                    "name": "Madhav Kashyap",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a424d",
                    "name": "Yile Gu",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a424e",
                    "name": "Rulin Shao",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a424f",
                    "name": "Zihao Ye",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4250",
                    "name": "Kan Zhu",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4251",
                    "name": "Stephanie Wang",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4252",
                    "name": "Arvind Krishnamurthy",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4253",
                    "name": "Rohan Kadekodi",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4254",
                    "name": "Luis Ceze",
                    "hidden": false
                },
                {
                    "_id": "67c5bc8babe08983d98a4255",
                    "name": "Baris Kasikci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T11:32:22.000Z",
            "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with\n  Lookahead Retrieval",
            "summary": "Retrieval-augmented generation (RAG) extends large language models (LLMs)\nwith external data sources to enhance factual correctness and domain coverage.\nModern RAG pipelines rely on large datastores, leading to system challenges in\nlatency-sensitive deployments, especially when limited GPU memory is available.\nTo address these challenges, we propose TeleRAG, an efficient inference system\nthat reduces RAG latency with minimal GPU memory requirements. The core\ninnovation of TeleRAG is lookahead retrieval, a prefetching mechanism that\nanticipates required data and transfers it from CPU to GPU in parallel with LLM\ngeneration. By leveraging the modularity of RAG pipelines, the inverted file\nindex (IVF) search algorithm and similarities between queries, TeleRAG\noptimally overlaps data movement and computation. Experimental results show\nthat TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average\ncompared to state-of-the-art systems, enabling faster, more memory-efficient\ndeployments of advanced RAG applications.",
            "upvotes": 6,
            "discussionId": "67c5bc8cabe08983d98a426c"
        },
        "publishedAt": "2025-03-03T09:33:49.658Z",
        "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6304ac1a412a1b9d381ca378/BYM8EdFZVDrDbfX8LKVC2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20969.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "fullname": "Keisuke Kamahori",
            "name": "kamahori",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20583",
            "authors": [
                {
                    "_id": "67c516998d02783fa3a52dc8",
                    "user": {
                        "_id": "6304ac1a412a1b9d381ca378",
                        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
                        "isPro": false,
                        "fullname": "Keisuke Kamahori",
                        "user": "kamahori",
                        "type": "user"
                    },
                    "name": "Keisuke Kamahori",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T08:07:02.986Z",
                    "hidden": false
                },
                {
                    "_id": "67c516998d02783fa3a52dc9",
                    "user": {
                        "_id": "62908273c740ebb981a6dba4",
                        "avatarUrl": "/avatars/465f50369c367b07670f5209c83d65f2.svg",
                        "isPro": false,
                        "fullname": "Jungo Kasai",
                        "user": "jungok",
                        "type": "user"
                    },
                    "name": "Jungo Kasai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:43:49.097Z",
                    "hidden": false
                },
                {
                    "_id": "67c516998d02783fa3a52dca",
                    "user": {
                        "_id": "628c26a8b80bb09700d6af86",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653352051245-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Noriyuki Kojima",
                        "user": "kojimano",
                        "type": "user"
                    },
                    "name": "Noriyuki Kojima",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:43:56.698Z",
                    "hidden": false
                },
                {
                    "_id": "67c516998d02783fa3a52dcb",
                    "user": {
                        "_id": "654132fe5a9a913c6c870e79",
                        "avatarUrl": "/avatars/2f6807eddef1929c571977e9af35f952.svg",
                        "isPro": false,
                        "fullname": "Baris Kasikci",
                        "user": "kasikci",
                        "type": "user"
                    },
                    "name": "Baris Kasikci",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:44:04.084Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T22:52:21.000Z",
            "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank\n  Approximation",
            "summary": "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper,\nrely on deep encoder-decoder architectures, and their encoders are a critical\nbottleneck for efficient deployment due to high computational intensity. We\nintroduce LiteASR, a low-rank compression scheme for ASR encoders that\nsignificantly reduces inference costs while maintaining transcription accuracy.\nOur approach leverages the strong low-rank properties observed in intermediate\nactivations: by applying principal component analysis (PCA) with a small\ncalibration dataset, we approximate linear transformations with a chain of\nlow-rank matrix multiplications, and further optimize self-attention to work in\nthe reduced dimension. Evaluation results show that our method can compress\nWhisper large-v3's encoder size by over 50%, matching Whisper medium's size\nwith better transcription accuracy, thereby establishing a new Pareto-optimal\nfrontier of efficiency and performance. The code of LiteASR is available at\nhttps://github.com/efeslab/LiteASR.",
            "upvotes": 6,
            "discussionId": "67c516998d02783fa3a52dfd",
            "githubRepo": "https://github.com/efeslab/LiteASR"
        },
        "publishedAt": "2025-03-02T21:48:46.577Z",
        "title": "LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20583.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "fullname": "Keisuke Kamahori",
            "name": "kamahori",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.17941",
            "authors": [
                {
                    "_id": "67c59a7e6eb050aa82406452",
                    "user": {
                        "_id": "668e62f6514c46e257387f6b",
                        "avatarUrl": "/avatars/601b111141141cb2ea710b3166e62cd0.svg",
                        "isPro": false,
                        "fullname": "Mingyuan Sun",
                        "user": "mingyuansun",
                        "type": "user"
                    },
                    "name": "Mingyuan Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T16:07:21.192Z",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406453",
                    "name": "Zheng Fang",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406454",
                    "name": "Jiaxu Wang",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406455",
                    "name": "Junjie Jiang",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406456",
                    "name": "Delei Kong",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406457",
                    "name": "Chenming Hu",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406458",
                    "name": "Yuetong Fang",
                    "hidden": false
                },
                {
                    "_id": "67c59a7e6eb050aa82406459",
                    "name": "Renjing Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-25T08:03:04.000Z",
            "title": "Optimal Brain Apoptosis",
            "summary": "The increasing complexity and parameter count of Convolutional Neural\nNetworks (CNNs) and Transformers pose challenges in terms of computational\nefficiency and resource demands. Pruning has been identified as an effective\nstrategy to address these challenges by removing redundant elements such as\nneurons, channels, or connections, thereby enhancing computational efficiency\nwithout heavily compromising performance. This paper builds on the foundational\nwork of Optimal Brain Damage (OBD) by advancing the methodology of parameter\nimportance estimation using the Hessian matrix. Unlike previous approaches that\nrely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel\npruning method that calculates the Hessian-vector product value directly for\neach parameter. By decomposing the Hessian matrix across network layers and\nidentifying conditions under which inter-layer Hessian submatrices are\nnon-zero, we propose a highly efficient technique for computing the\nsecond-order Taylor expansion of parameters. This approach allows for a more\nprecise pruning process, particularly in the context of CNNs and Transformers,\nas validated in our experiments including VGG19, ResNet32, ResNet50, and\nViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at\nhttps://github.com/NEU-REAL/OBA.",
            "upvotes": 5,
            "discussionId": "67c59a7f6eb050aa824064b9",
            "githubRepo": "https://github.com/NEU-REAL/OBA"
        },
        "publishedAt": "2025-03-03T07:04:47.515Z",
        "title": "Optimal Brain Apoptosis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668e62f6514c46e257387f6b",
            "avatarUrl": "/avatars/601b111141141cb2ea710b3166e62cd0.svg",
            "fullname": "Mingyuan Sun",
            "name": "mingyuansun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.19731",
            "authors": [
                {
                    "_id": "67c36b35e12b50f698e7db1d",
                    "name": "Mian Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c36b35e12b50f698e7db1e",
                    "name": "Shaun M. Eack",
                    "hidden": false
                },
                {
                    "_id": "67c36b35e12b50f698e7db1f",
                    "name": "Zhiyu Zoey Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T03:50:25.000Z",
            "title": "Preference Learning Unlocks LLMs' Psycho-Counseling Skills",
            "summary": "Applying large language models (LLMs) to assist in psycho-counseling is an\nemerging and meaningful approach, driven by the significant gap between patient\nneeds and the availability of mental health support. However, current LLMs\nstruggle to consistently provide effective responses to client speeches,\nlargely due to the lack of supervision from high-quality real psycho-counseling\ndata, whose content is typically inaccessible due to client privacy concerns.\nFurthermore, the quality of therapists' responses in available sessions can\nvary significantly based on their professional training and experience.\nAssessing the quality of therapists' responses remains an open challenge. In\nthis work, we address these challenges by first proposing a set of professional\nand comprehensive principles to evaluate therapists' responses to client\nspeeches. Using these principles, we create a preference dataset,\nPsychoCounsel-Preference, which contains 36k high-quality preference comparison\npairs. This dataset aligns with the preferences of professional\npsychotherapists, providing a robust foundation for evaluating and improving\nLLMs in psycho-counseling. Experiments on reward modeling and preference\nlearning demonstrate that PsychoCounsel-Preference is an excellent resource for\nLLMs to acquire essential skills for responding to clients in a counseling\nsession. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an\nimpressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference,\nPsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to\nfacilitate the research of psycho-counseling with LLMs at:\nhttps://hf.co/Psychotherapy-LLM.",
            "upvotes": 4,
            "discussionId": "67c36b36e12b50f698e7db51"
        },
        "publishedAt": "2025-03-03T10:56:33.810Z",
        "title": "Preference Learning Unlocks LLMs' Psycho-Counseling Skills",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19731.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650857fef3060ea840ffbbfe",
            "avatarUrl": "/avatars/3a339936021c040f19a21838ae1382c4.svg",
            "fullname": "Mian Zhang",
            "name": "billmianz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.20900",
            "authors": [
                {
                    "_id": "67c5beea1b2c18e03a3d5218",
                    "name": "Yifan Zhong",
                    "hidden": false
                },
                {
                    "_id": "67c5beea1b2c18e03a3d5219",
                    "name": "Xuchuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67c5beea1b2c18e03a3d521a",
                    "name": "Ruochong Li",
                    "hidden": false
                },
                {
                    "_id": "67c5beea1b2c18e03a3d521b",
                    "name": "Ceyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c5beea1b2c18e03a3d521c",
                    "name": "Yitao Liang",
                    "hidden": false
                },
                {
                    "_id": "67c5beea1b2c18e03a3d521d",
                    "name": "Yaodong Yang",
                    "hidden": false
                },
                {
                    "_id": "67c5beea1b2c18e03a3d521e",
                    "user": {
                        "_id": "6393a8af84c565d2c3419b7c",
                        "avatarUrl": "/avatars/f2a237a58dd0a25ef5c1a98e60acbb5c.svg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "yuanpei",
                        "type": "user"
                    },
                    "name": "Yuanpei Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-03T14:38:37.342Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T09:57:20.000Z",
            "title": "DexGraspVLA: A Vision-Language-Action Framework Towards General\n  Dexterous Grasping",
            "summary": "Dexterous grasping remains a fundamental yet challenging problem in robotics.\nA general-purpose robot must be capable of grasping diverse objects in\narbitrary scenarios. However, existing research typically relies on specific\nassumptions, such as single-object settings or limited environments, leading to\nconstrained generalization. Our solution is DexGraspVLA, a hierarchical\nframework that utilizes a pre-trained Vision-Language model as the high-level\ntask planner and learns a diffusion-based policy as the low-level Action\ncontroller. The key insight lies in iteratively transforming diverse language\nand visual inputs into domain-invariant representations, where imitation\nlearning can be effectively applied due to the alleviation of domain shift.\nThus, it enables robust generalization across a wide range of real-world\nscenarios. Notably, our method achieves a 90+% success rate under thousands of\nunseen object, lighting, and background combinations in a ``zero-shot''\nenvironment. Empirical analysis further confirms the consistency of internal\nmodel behavior across environmental variations, thereby validating our design\nand explaining its generalization performance. We hope our work can be a step\nforward in achieving general dexterous grasping. Our demo and code can be found\nat https://dexgraspvla.github.io/.",
            "upvotes": 4,
            "discussionId": "67c5beed1b2c18e03a3d52c0"
        },
        "publishedAt": "2025-03-03T09:44:46.734Z",
        "title": "DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20900.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655d9f43b5da99edaf3f2f81",
            "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
            "fullname": "Yifan Zhong",
            "name": "Yifan-Zhong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.21291",
            "authors": [
                {
                    "_id": "67c5aad632a7208c9ae1d020",
                    "name": "Xueyun Tian",
                    "hidden": false
                },
                {
                    "_id": "67c5aad632a7208c9ae1d021",
                    "user": {
                        "_id": "63044e025c70c21d0eaf08bc",
                        "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
                        "isPro": false,
                        "fullname": "Wei Li",
                        "user": "Wiley085",
                        "type": "user"
                    },
                    "name": "Wei Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-03T13:12:57.839Z",
                    "hidden": false
                },
                {
                    "_id": "67c5aad632a7208c9ae1d022",
                    "name": "Bingbing Xu",
                    "hidden": false
                },
                {
                    "_id": "67c5aad632a7208c9ae1d023",
                    "name": "Yige Yuan",
                    "hidden": false
                },
                {
                    "_id": "67c5aad632a7208c9ae1d024",
                    "name": "Yuanzhuo Wang",
                    "hidden": false
                },
                {
                    "_id": "67c5aad632a7208c9ae1d025",
                    "name": "Huawei Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T18:21:08.000Z",
            "title": "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
            "summary": "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion\nmechanism.This unification enables joint training of both tasks, providing two\nkey advantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https://github.com/Eureka-Maggie/MIGE.",
            "upvotes": 4,
            "discussionId": "67c5aad932a7208c9ae1d19a",
            "githubRepo": "https://github.com/Eureka-Maggie/MIGE"
        },
        "publishedAt": "2025-03-03T08:13:06.912Z",
        "title": "MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.21291.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 32
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.17125",
            "authors": [
                {
                    "_id": "67c0536530abbab5c723f2e0",
                    "user": {
                        "_id": "646264832538819c729e32ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646264832538819c729e32ba/syc-UpPQyR3Nbf-gYndc4.jpeg",
                        "isPro": true,
                        "fullname": "Adam Kovacs",
                        "user": "adaamko",
                        "type": "user"
                    },
                    "name": "Ádám Kovács",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-02T20:18:13.294Z",
                    "hidden": false
                },
                {
                    "_id": "67c0536530abbab5c723f2e1",
                    "name": "Gábor Recski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-24T13:11:47.000Z",
            "title": "LettuceDetect: A Hallucination Detection Framework for RAG Applications",
            "summary": "Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications.",
            "upvotes": 3,
            "discussionId": "67c0536630abbab5c723f31e",
            "githubRepo": "https://github.com/KRLabsOrg/LettuceDetect"
        },
        "publishedAt": "2025-03-03T07:33:14.717Z",
        "title": "LettuceDetect: A Hallucination Detection Framework for RAG Applications",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17125.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646264832538819c729e32ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646264832538819c729e32ba/syc-UpPQyR3Nbf-gYndc4.jpeg",
            "fullname": "Adam Kovacs",
            "name": "adaamko",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20490",
            "authors": [
                {
                    "_id": "67c5c853e7c5cfb1d2b52858",
                    "user": {
                        "_id": "63f6ba02a67b8acfa50407bb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f6ba02a67b8acfa50407bb/ueUb01p1mhuRNrkyfEHtc.jpeg",
                        "isPro": false,
                        "fullname": "MohammadHossein Rezaei",
                        "user": "mhr2004",
                        "type": "user"
                    },
                    "name": "MohammadHossein Rezaei",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-03T16:56:51.354Z",
                    "hidden": false
                },
                {
                    "_id": "67c5c853e7c5cfb1d2b52859",
                    "name": "Yicheng Fu",
                    "hidden": false
                },
                {
                    "_id": "67c5c853e7c5cfb1d2b5285a",
                    "name": "Phil Cuvin",
                    "hidden": false
                },
                {
                    "_id": "67c5c853e7c5cfb1d2b5285b",
                    "name": "Caleb Ziems",
                    "hidden": false
                },
                {
                    "_id": "67c5c853e7c5cfb1d2b5285c",
                    "name": "Yanzhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c5c853e7c5cfb1d2b5285d",
                    "user": {
                        "_id": "61aa376688c20eebf1e8deb3",
                        "avatarUrl": "/avatars/7c11dcb232c73547d7d87834be287822.svg",
                        "isPro": false,
                        "fullname": "Hao Zhu",
                        "user": "ProKil",
                        "type": "user"
                    },
                    "name": "Hao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-03T16:07:08.219Z",
                    "hidden": false
                },
                {
                    "_id": "67c5c853e7c5cfb1d2b5285e",
                    "name": "Diyi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T19:54:16.000Z",
            "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
            "summary": "Human activity is moderated by norms. When performing actions in the real\nworld, humans not only follow norms, but also consider the trade-off between\ndifferent norms However, machines are often trained without explicit\nsupervision on norm understanding and reasoning, especially when the norms are\ngrounded in a physical and social context. To improve and evaluate the\nnormative reasoning capability of vision-language models (VLMs), we present\nEgoNormia |epsilon|, consisting of 1,853 ego-centric videos of human\ninteractions, each of which has two related questions evaluating both the\nprediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench\nof 92%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation method, it is possible to use\nEgoNomia to enhance normative reasoning in VLMs.",
            "upvotes": 2,
            "discussionId": "67c5c857e7c5cfb1d2b52994",
            "projectPage": "https://egonormia.org",
            "githubRepo": "https://github.com/open-social-world/egonormia"
        },
        "publishedAt": "2025-03-03T10:26:31.746Z",
        "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20490.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61aa376688c20eebf1e8deb3",
            "avatarUrl": "/avatars/7c11dcb232c73547d7d87834be287822.svg",
            "fullname": "Hao Zhu",
            "name": "ProKil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20811",
            "authors": [
                {
                    "_id": "67c51c198d02783fa3a6249d",
                    "name": "Xiao Wang",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a6249e",
                    "name": "Jingyun Hua",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a6249f",
                    "user": {
                        "_id": "675a69699e086bd6250a36ef",
                        "avatarUrl": "/avatars/95c72e3975d1a37f8655a2fe629746ec.svg",
                        "isPro": false,
                        "fullname": "Weihong Lin",
                        "user": "lwher1996",
                        "type": "user"
                    },
                    "name": "Weihong Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-03T09:42:30.547Z",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a624a0",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a624a1",
                    "name": "Fuzheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a624a2",
                    "name": "Jianlong Wu",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a624a3",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c51c198d02783fa3a624a4",
                    "name": "Liqiang Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T07:53:40.000Z",
            "title": "HAIC: Improving Human Action Understanding and Generation with Better\n  Captions for Multi-modal Large Language Models",
            "summary": "Recent Multi-modal Large Language Models (MLLMs) have made great progress in\nvideo understanding. However, their performance on videos involving human\nactions is still limited by the lack of high-quality data. To address this, we\nintroduce a two-stage data annotation pipeline. First, we design strategies to\naccumulate videos featuring clear human actions from the Internet. Second,\nvideos are annotated in a standardized caption format that uses human\nattributes to distinguish individuals and chronologically details their actions\nand interactions. Through this pipeline, we curate two datasets, namely\nHAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs\ngenerated by Gemini-Pro and verified for training purposes. Meanwhile,\nHAICBench includes 500 manually annotated video-caption pairs and\n1,400 QA pairs, for a comprehensive evaluation of human action understanding.\nExperimental results demonstrate that training with HAICTrain not only\nsignificantly enhances human understanding abilities across 4 benchmarks, but\ncan also improve text-to-video generation results. Both the HAICTrain and\nHAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.",
            "upvotes": 1,
            "discussionId": "67c51c1b8d02783fa3a62543"
        },
        "publishedAt": "2025-03-02T22:04:15.087Z",
        "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20811.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6268
        },
        "isAuthorParticipating": false
    }
]