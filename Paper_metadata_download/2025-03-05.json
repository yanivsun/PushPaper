[
    {
        "paper": {
            "id": "2503.02682",
            "authors": [
                {
                    "_id": "67c7c3d073299239b63f5378",
                    "user": {
                        "_id": "6225a9983207dfc568407204",
                        "avatarUrl": "/avatars/c970db6232d84ae8c0fa5f11d561d67c.svg",
                        "isPro": false,
                        "fullname": "xwm",
                        "user": "xwm",
                        "type": "user"
                    },
                    "name": "Weimin Xiong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:35.055Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3d073299239b63f5379",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "67c7c3d073299239b63f537a",
                    "user": {
                        "_id": "670740744341dcee459fb990",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
                        "isPro": false,
                        "fullname": "Qingxiu Dong",
                        "user": "Rsy24",
                        "type": "user"
                    },
                    "name": "Qingxiu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:31:12.605Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3d073299239b63f537b",
                    "user": {
                        "_id": "6371af80789970f7bc65fef8",
                        "avatarUrl": "/avatars/0f91fed93d6714cd0534f3435ddcbcbd.svg",
                        "isPro": false,
                        "fullname": "Bingchan Zhao",
                        "user": "Adagio",
                        "type": "user"
                    },
                    "name": "Bingchan Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:31:05.438Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3d073299239b63f537c",
                    "user": {
                        "_id": "6447ca6ca478b20f1755b294",
                        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
                        "isPro": false,
                        "fullname": "Feifan Song",
                        "user": "songff",
                        "type": "user"
                    },
                    "name": "Feifan Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:30:59.446Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3d073299239b63f537d",
                    "name": "Xun Wang",
                    "hidden": false
                },
                {
                    "_id": "67c7c3d073299239b63f537e",
                    "user": {
                        "_id": "66021bd9d3d4aea81b12ee06",
                        "avatarUrl": "/avatars/13e7ffe1ea0d64ef447a87a874923531.svg",
                        "isPro": false,
                        "fullname": "Sujian Li",
                        "user": "sujianli",
                        "type": "user"
                    },
                    "name": "Sujian Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:30:37.203Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T14:54:45.000Z",
            "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
            "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
            "upvotes": 16,
            "discussionId": "67c7c3d173299239b63f53d6",
            "githubRepo": "https://github.com/WeiminXiong/MPO"
        },
        "publishedAt": "2025-03-04T22:30:53.253Z",
        "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02682.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6225a9983207dfc568407204",
            "avatarUrl": "/avatars/c970db6232d84ae8c0fa5f11d561d67c.svg",
            "fullname": "xwm",
            "name": "xwm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02846",
            "authors": [
                {
                    "_id": "67c7c3ce6f68759bf368533c",
                    "user": {
                        "_id": "6601196cc91ba4c08ad6e270",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
                        "isPro": false,
                        "fullname": "yuzhe gu",
                        "user": "vanilla1116",
                        "type": "user"
                    },
                    "name": "Yuzhe Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:38.988Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3ce6f68759bf368533d",
                    "user": {
                        "_id": "64e8505321540e1da3226b54",
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:32:26.106Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3ce6f68759bf368533e",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "67c7c3ce6f68759bf368533f",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:32:37.093Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c3ce6f68759bf3685340",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T18:20:24.000Z",
            "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
            "summary": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
            "upvotes": 14,
            "discussionId": "67c7c3d06f68759bf3685489",
            "githubRepo": "https://github.com/open-compass/ANAH"
        },
        "publishedAt": "2025-03-04T22:25:15.163Z",
        "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02846.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "fullname": "yuzhe gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02879",
            "authors": [
                {
                    "_id": "67c7c42269d99dd25c5ba0ce",
                    "user": {
                        "_id": "67c7c621a78dc9c14f0e2697",
                        "avatarUrl": "/avatars/94cdcaa2c9f3c2e672f4c988ea0c81e7.svg",
                        "isPro": false,
                        "fullname": "Siming Huang",
                        "user": "hsm316",
                        "type": "user"
                    },
                    "name": "Siming Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:32.237Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c42269d99dd25c5ba0cf",
                    "user": {
                        "_id": "66451b129a4de258b5a028b8",
                        "avatarUrl": "/avatars/bfa5903aa90d3071fac5c3c14cb1f115.svg",
                        "isPro": false,
                        "fullname": "Yuliang Xu",
                        "user": "sdzzxyl",
                        "type": "user"
                    },
                    "name": "Yuliang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:39:56.806Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c42269d99dd25c5ba0d0",
                    "user": {
                        "_id": "67890323f8796231c857231e",
                        "avatarUrl": "/avatars/f5ccd5186968d880fee9c36324a5f713.svg",
                        "isPro": false,
                        "fullname": "Mingmeng Geng",
                        "user": "mgeng",
                        "type": "user"
                    },
                    "name": "Mingmeng Geng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-05T13:45:31.195Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c42269d99dd25c5ba0d1",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "67c7c42269d99dd25c5ba0d2",
                    "user": {
                        "_id": "65e2be1e630e2db23829ee8d",
                        "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
                        "isPro": false,
                        "fullname": "Dongping Chen",
                        "user": "fjchendp",
                        "type": "user"
                    },
                    "name": "Dongping Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:40:02.647Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T18:58:13.000Z",
            "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
            "summary": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
            "upvotes": 13,
            "discussionId": "67c7c42369d99dd25c5ba103",
            "githubRepo": "https://github.com/HSM316/LLM_Wikipedia"
        },
        "publishedAt": "2025-03-04T22:25:53.653Z",
        "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02879.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.01935",
            "authors": [
                {
                    "_id": "67c7ba7f19b236e0564d1172",
                    "user": {
                        "_id": "66554507e6ea63012f35824c",
                        "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
                        "isPro": false,
                        "fullname": "Kunlun_Zhu",
                        "user": "Leozkl",
                        "type": "user"
                    },
                    "name": "Kunlun Zhu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-05T02:44:18.739Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1173",
                    "name": "Hongyi Du",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1174",
                    "user": {
                        "_id": "672516b24c0a2bf36e52ec9d",
                        "avatarUrl": "/avatars/ce20ec30586dd40d263fd0b88eaf002d.svg",
                        "isPro": false,
                        "fullname": "Zhaochen Hong",
                        "user": "zhaochenhong",
                        "type": "user"
                    },
                    "name": "Zhaochen Hong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:38:26.340Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1175",
                    "user": {
                        "_id": "64f98e9c31b3625c3844b73d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f98e9c31b3625c3844b73d/if9kvZUkXTuo9UuzJY236.jpeg",
                        "isPro": false,
                        "fullname": "Xiaocheng Yang",
                        "user": "Thomas-X-Yang",
                        "type": "user"
                    },
                    "name": "Xiaocheng Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:38:17.378Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1176",
                    "user": {
                        "_id": "65ca8d92d5822659ecc6c0d4",
                        "avatarUrl": "/avatars/6e9ab0470d68248a267eec892fce89c3.svg",
                        "isPro": false,
                        "fullname": "Shuyi",
                        "user": "ShuyiGuo",
                        "type": "user"
                    },
                    "name": "Shuyi Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:38:47.235Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1177",
                    "name": "Zhe Wang",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1178",
                    "user": {
                        "_id": "628d7265db4cd1d1717c884f",
                        "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
                        "isPro": false,
                        "fullname": "Zhenhailong Wang",
                        "user": "mikewang",
                        "type": "user"
                    },
                    "name": "Zhenhailong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:34:51.267Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d1179",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d117a",
                    "user": {
                        "_id": "63357c608adfa81faf2ac180",
                        "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
                        "isPro": false,
                        "fullname": "Xiangru Tang",
                        "user": "RTT1",
                        "type": "user"
                    },
                    "name": "Xiangru Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:35:40.308Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d117b",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "67c7ba7f19b236e0564d117c",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T05:18:50.000Z",
            "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM\n  agents",
            "summary": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous\nagents, yet existing benchmarks either focus on single-agent tasks or are\nconfined to narrow domains, failing to capture the dynamics of multi-agent\ncoordination and competition. In this paper, we introduce MultiAgentBench, a\ncomprehensive benchmark designed to evaluate LLM-based multi-agent systems\nacross diverse, interactive scenarios. Our framework measures not only task\ncompletion but also the quality of collaboration and competition using novel,\nmilestone-based key performance indicators. Moreover, we evaluate various\ncoordination protocols (including star, chain, tree, and graph topologies) and\ninnovative strategies such as group discussion and cognitive planning. Notably,\ngpt-4o-mini reaches the average highest task score, graph structure performs\nthe best among coordination protocols in the research scenario, and cognitive\nplanning improves milestone achievement rates by 3%. Code and datasets are\npublic available at https://github.com/MultiagentBench/MARBLE.",
            "upvotes": 12,
            "discussionId": "67c7ba8219b236e0564d124a",
            "githubRepo": "https://github.com/MultiagentBench/MARBLE"
        },
        "publishedAt": "2025-03-04T21:46:46.873Z",
        "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01935.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64c090a9f613170e7be93d2f",
            "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
            "fullname": "KunlunZhu",
            "name": "KunlunZhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.00735",
            "authors": [
                {
                    "_id": "67c7dc0c9ad085d8f8ffc0cd",
                    "user": {
                        "_id": "649e1f4d2ccae3ea1f2b6a7f",
                        "avatarUrl": "/avatars/04657fb53bde660dafb743da4273325f.svg",
                        "isPro": false,
                        "fullname": "Toby Simonds",
                        "user": "TamasSimonds",
                        "type": "user"
                    },
                    "name": "Toby Simonds",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:10.920Z",
                    "hidden": false
                },
                {
                    "_id": "67c7dc0c9ad085d8f8ffc0ce",
                    "user": {
                        "_id": "667b8f5f09faa9d48cf82d2c",
                        "avatarUrl": "/avatars/ca6c009ff73cedf4585ef8fea018443b.svg",
                        "isPro": false,
                        "fullname": "Akira Yoshiyama",
                        "user": "akiray1",
                        "type": "user"
                    },
                    "name": "Akira Yoshiyama",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:12.944Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-02T05:16:43.000Z",
            "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
            "summary": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.",
            "upvotes": 10,
            "discussionId": "67c7dc0e9ad085d8f8ffc113"
        },
        "publishedAt": "2025-03-05T05:24:20.464Z",
        "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00735.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667b8f5f09faa9d48cf82d2c",
            "avatarUrl": "/avatars/ca6c009ff73cedf4585ef8fea018443b.svg",
            "fullname": "Akira Yoshiyama",
            "name": "akiray1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02368",
            "authors": [
                {
                    "_id": "67c80f94ccc2e04adfa67079",
                    "user": {
                        "_id": "619b00dd4b0db5ca9d3ea35f",
                        "avatarUrl": "/avatars/fce5ac388b7f10cbbc63e9992a5a799f.svg",
                        "isPro": false,
                        "fullname": "Zhenhua Liu",
                        "user": "zhliu",
                        "type": "user"
                    },
                    "name": "Zhenhua Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:42:35.402Z",
                    "hidden": false
                },
                {
                    "_id": "67c80f94ccc2e04adfa6707a",
                    "user": {
                        "_id": "65ca1fbbecfd8608f5b89666",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YM6yIAjVQCRHjk6uJd1jr.jpeg",
                        "isPro": false,
                        "fullname": "lilijun",
                        "user": "lljhbxt",
                        "type": "user"
                    },
                    "name": "Lijun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:41:31.679Z",
                    "hidden": false
                },
                {
                    "_id": "67c80f94ccc2e04adfa6707b",
                    "name": "Ruizhe Chen",
                    "hidden": false
                },
                {
                    "_id": "67c80f94ccc2e04adfa6707c",
                    "user": {
                        "_id": "64a67bff9f3b568c203cb533",
                        "avatarUrl": "/avatars/630d117a6e4392b78b35d6b95066b52a.svg",
                        "isPro": false,
                        "fullname": "YU-XIAN JIANG",
                        "user": "ab3223323",
                        "type": "user"
                    },
                    "name": "Yuxian Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:40:51.943Z",
                    "hidden": false
                },
                {
                    "_id": "67c80f94ccc2e04adfa6707d",
                    "name": "Tong Zhu",
                    "hidden": false
                },
                {
                    "_id": "67c80f94ccc2e04adfa6707e",
                    "user": {
                        "_id": "63477237511cd17d2c7a1be9",
                        "avatarUrl": "/avatars/97cb3f640ded58344bca0ab279473134.svg",
                        "isPro": false,
                        "fullname": "Wenliang Chen",
                        "user": "jokephp",
                        "type": "user"
                    },
                    "name": "Wenliang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:40:45.604Z",
                    "hidden": false
                },
                {
                    "_id": "67c80f94ccc2e04adfa6707f",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T07:49:10.000Z",
            "title": "Iterative Value Function Optimization for Guided Decoding",
            "summary": "While Reinforcement Learning from Human Feedback (RLHF) has become the\npredominant method for controlling language model outputs, it suffers from high\ncomputational costs and training instability. Guided decoding, especially\nvalue-guided methods, offers a cost-effective alternative by controlling\noutputs without re-training models. However, the accuracy of the value function\nis crucial for value-guided decoding, as inaccuracies can lead to suboptimal\ndecision-making and degraded performance. Existing methods struggle with\naccurately estimating the optimal value function, leading to less effective\ncontrol. We propose Iterative Value Function Optimization, a novel framework\nthat addresses these limitations through two key components: Monte Carlo Value\nEstimation, which reduces estimation variance by exploring diverse\ntrajectories, and Iterative On-Policy Optimization, which progressively\nimproves value estimation through collecting trajectories from value-guided\npolicies. Extensive experiments on text summarization, multi-turn dialogue, and\ninstruction following demonstrate the effectiveness of value-guided decoding\napproaches in aligning language models. These approaches not only achieve\nalignment but also significantly reduce computational costs by leveraging\nprincipled value function optimization for efficient and effective control.",
            "upvotes": 10,
            "discussionId": "67c80f94ccc2e04adfa670b2"
        },
        "publishedAt": "2025-03-05T03:48:51.408Z",
        "title": "Iterative Value Function Optimization for Guided Decoding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02368.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "619b00dd4b0db5ca9d3ea35f",
            "avatarUrl": "/avatars/fce5ac388b7f10cbbc63e9992a5a799f.svg",
            "fullname": "Zhenhua Liu",
            "name": "zhliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.01328",
            "authors": [
                {
                    "_id": "67c7b5900b05ab9c7e805433",
                    "name": "Xinyi Wan",
                    "hidden": false
                },
                {
                    "_id": "67c7b5900b05ab9c7e805434",
                    "user": {
                        "_id": "63885f1d0bebb233d8ad6e5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Penghui Qi",
                        "user": "QPHutu",
                        "type": "user"
                    },
                    "name": "Penghui Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:12:19.621Z",
                    "hidden": false
                },
                {
                    "_id": "67c7b5900b05ab9c7e805435",
                    "user": {
                        "_id": "67c7b8c95d393f85737562e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KMmEVbXppNkfj1T9UOD7o.png",
                        "isPro": false,
                        "fullname": "Guangxing Huang",
                        "user": "huanggx-sea",
                        "type": "user"
                    },
                    "name": "Guangxing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:39:20.500Z",
                    "hidden": false
                },
                {
                    "_id": "67c7b5900b05ab9c7e805436",
                    "user": {
                        "_id": "658f147c50d39af7f4cd889d",
                        "avatarUrl": "/avatars/93bb33e975ab97c1f4179f20d384438f.svg",
                        "isPro": false,
                        "fullname": "Jialin Li",
                        "user": "JialinLi",
                        "type": "user"
                    },
                    "name": "Jialin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T10:39:29.789Z",
                    "hidden": false
                },
                {
                    "_id": "67c7b5900b05ab9c7e805437",
                    "name": "Min Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T09:11:06.000Z",
            "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory\n  Optimization",
            "summary": "Pipeline parallelism (PP) is widely used for training large language models\n(LLMs), yet its scalability is often constrained by high activation memory\nconsumption as the number of in-flight microbatches grows with the degree of\nPP. In this paper, we focus on addressing this challenge by leveraging the\nunder-explored memory offload strategy in PP. With empirical study, we discover\nthat in the majority of standard configurations, at least half, and potentially\nall, of the activations can be offloaded with negligible overhead. In the cases\nwhere full overload is not possible, we introduce a novel selective offload\nstrategy that decreases peak activation memory in a better-than-linear manner.\nFurthermore, we integrate memory offload with other techniques to jointly\nconsider overall throughput and memory limitation. Our experiments proves that\nthe per-device activation memory effectively reduces with the total number of\nstages, making PP a stronger alternative than TP, offering up to a 19\\%\nacceleration with even lower memory consumption. The implementation is\nopen-sourced at\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism{this url}.",
            "upvotes": 10,
            "discussionId": "67c7b5970b05ab9c7e8055a1",
            "githubRepo": "https://github.com/sail-sg/zero-bubble"
        },
        "publishedAt": "2025-03-04T21:30:49.808Z",
        "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01328.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63510eea0b94548566dad923",
            "avatarUrl": "/avatars/629eaaf810718259bf7588dc2e6cc0d5.svg",
            "fullname": "Xinyi Wan",
            "name": "ufotalent",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.00069",
            "authors": [
                {
                    "_id": "67c89f164e249cdfb8b7297d",
                    "user": {
                        "_id": "60a66731e1db8bc33b8d4112",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
                        "isPro": false,
                        "fullname": "Karolina Stanczak",
                        "user": "Karolina",
                        "type": "user"
                    },
                    "name": "Karolina Stańczak",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-05T22:06:13.315Z",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b7297e",
                    "name": "Nicholas Meade",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b7297f",
                    "name": "Mehar Bhatia",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72980",
                    "name": "Hattie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72981",
                    "name": "Konstantin Böttinger",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72982",
                    "name": "Jeremy Barnes",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72983",
                    "name": "Jason Stanley",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72984",
                    "name": "Jessica Montgomery",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72985",
                    "name": "Richard Zemel",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72986",
                    "name": "Nicolas Papernot",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72987",
                    "name": "Nicolas Chapados",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72988",
                    "name": "Denis Therien",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b72989",
                    "name": "Timothy P. Lillicrap",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b7298a",
                    "name": "Ana Marasović",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b7298b",
                    "name": "Sylvie Delacroix",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b7298c",
                    "name": "Gillian K. Hadfield",
                    "hidden": false
                },
                {
                    "_id": "67c89f164e249cdfb8b7298d",
                    "name": "Siva Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T13:26:07.000Z",
            "title": "Societal Alignment Frameworks Can Improve LLM Alignment",
            "summary": "Recent progress in large language models (LLMs) has focused on producing\nresponses that meet human expectations and align with shared values - a process\ncoined alignment. However, aligning LLMs remains challenging due to the\ninherent disconnect between the complexity of human values and the narrow\nnature of the technological approaches designed to address them. Current\nalignment methods often lead to misspecified objectives, reflecting the broader\nissue of incomplete contracts, the impracticality of specifying a contract\nbetween a model developer, and the model that accounts for every scenario in\nLLM alignment. In this paper, we argue that improving LLM alignment requires\nincorporating insights from societal alignment frameworks, including social,\neconomic, and contractual alignment, and discuss potential solutions drawn from\nthese domains. Given the role of uncertainty within societal alignment\nframeworks, we then investigate how it manifests in LLM alignment. We end our\ndiscussion by offering an alternative view on LLM alignment, framing the\nunderspecified nature of its objectives as an opportunity rather than perfect\ntheir specification. Beyond technical improvements in LLM alignment, we discuss\nthe need for participatory alignment interface designs.",
            "upvotes": 9,
            "discussionId": "67c89f174e249cdfb8b729c4"
        },
        "publishedAt": "2025-03-05T18:39:55.548Z",
        "title": "Societal Alignment Frameworks Can Improve LLM Alignment",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60a66731e1db8bc33b8d4112/77LG8iOHkraXoUS0rGcxE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00069.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60a66731e1db8bc33b8d4112",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
            "fullname": "Karolina Stanczak",
            "name": "Karolina",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.00955",
            "authors": [
                {
                    "_id": "67c7dbff25d0b3348ddace44",
                    "user": {
                        "_id": "64c2bea2ada7df214276913b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
                        "isPro": false,
                        "fullname": "Nguyen Van Nam",
                        "user": "DavidNguyen",
                        "type": "user"
                    },
                    "name": "Nam V. Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:25.689Z",
                    "hidden": false
                },
                {
                    "_id": "67c7dbff25d0b3348ddace45",
                    "user": {
                        "_id": "65217a88fd512a7371866d74",
                        "avatarUrl": "/avatars/d3ce90d34b1880eb362de58361de1413.svg",
                        "isPro": false,
                        "fullname": "Xuân Diện",
                        "user": "xuandin",
                        "type": "user"
                    },
                    "name": "Dien X. Tran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:17.563Z",
                    "hidden": false
                },
                {
                    "_id": "67c7dbff25d0b3348ddace46",
                    "name": "Thanh T. Tran",
                    "hidden": false
                },
                {
                    "_id": "67c7dbff25d0b3348ddace47",
                    "user": {
                        "_id": "650893f9d48ed98e638a0fd3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650893f9d48ed98e638a0fd3/-3UGrYcR-HxNhR2ElTNeV.jpeg",
                        "isPro": false,
                        "fullname": "Hoang Tien Anh",
                        "user": "TienAnh",
                        "type": "user"
                    },
                    "name": "Anh T. Hoang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:15.231Z",
                    "hidden": false
                },
                {
                    "_id": "67c7dbff25d0b3348ddace48",
                    "name": "Tai V. Duong",
                    "hidden": false
                },
                {
                    "_id": "67c7dbff25d0b3348ddace49",
                    "user": {
                        "_id": "6568b9ba3c0881893acc2443",
                        "avatarUrl": "/avatars/05de2c1e9aaf8ce6f43cf29374df5513.svg",
                        "isPro": false,
                        "fullname": "Le Thanh Di",
                        "user": "ThanhDi",
                        "type": "user"
                    },
                    "name": "Di T. Le",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:20.865Z",
                    "hidden": false
                },
                {
                    "_id": "67c7dbff25d0b3348ddace4a",
                    "name": "Phuc-Lu Le",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-02T16:22:46.000Z",
            "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information\n  Fact-Checking",
            "summary": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like\nGPT and Gemini, demands robust fact-checking solutions, especially for\nlow-resource languages like Vietnamese. Existing methods struggle with semantic\nambiguity, homonyms, and complex linguistic structures, often trading accuracy\nfor efficiency. We introduce SemViQA, a novel Vietnamese fact-checking\nframework integrating Semantic-based Evidence Retrieval (SER) and Two-step\nVerdict Classification (TVC). Our approach balances precision and speed,\nachieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01\nand 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.\nAdditionally, SemViQA Faster improves inference speed 7x while maintaining\ncompetitive accuracy. SemViQA sets a new benchmark for Vietnamese fact\nverification, advancing the fight against misinformation. The source code is\navailable at: https://github.com/DAVID-NGUYEN-S16/SemViQA.",
            "upvotes": 7,
            "discussionId": "67c7dc0025d0b3348ddace64",
            "githubRepo": "https://github.com/DAVID-NGUYEN-S16/SemViQA"
        },
        "publishedAt": "2025-03-05T00:08:53.214Z",
        "title": "SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00955.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c2bea2ada7df214276913b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
            "fullname": "Nguyen Van Nam",
            "name": "DavidNguyen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.14856",
            "authors": [
                {
                    "_id": "67bee83509a4524abf899511",
                    "user": {
                        "_id": "64b8ff3d95bd42c770878042",
                        "avatarUrl": "/avatars/564a4dccdf9e5b813a99979b0ef58183.svg",
                        "isPro": false,
                        "fullname": "Weilin Zhao",
                        "user": "Achazwl",
                        "type": "user"
                    },
                    "name": "Weilin Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:12:38.111Z",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899512",
                    "user": {
                        "_id": "6391a84891023bed86fc703e",
                        "avatarUrl": "/avatars/6e61d845aded1542e61f5220365e0d6f.svg",
                        "isPro": false,
                        "fullname": "ThonyPan",
                        "user": "ThonyPan",
                        "type": "user"
                    },
                    "name": "Tengyu Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:01:09.673Z",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899513",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899514",
                    "name": "Yudi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899515",
                    "name": "Ao Sun",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899516",
                    "user": {
                        "_id": "65394f942ddab2beff1cc967",
                        "avatarUrl": "/avatars/bdd5b47618758f6de080a51834a17655.svg",
                        "isPro": false,
                        "fullname": "Shawn Huang",
                        "user": "hyx21",
                        "type": "user"
                    },
                    "name": "Yuxiang Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:12:36.389Z",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899517",
                    "name": "Kaihuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899518",
                    "user": {
                        "_id": "6716194e503429f9f5fb5116",
                        "avatarUrl": "/avatars/4707e293df6bdc14dc97a6b6f3b8932f.svg",
                        "isPro": false,
                        "fullname": "zhao weilun",
                        "user": "zwl96",
                        "type": "user"
                    },
                    "name": "Weilun Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:05:23.448Z",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf899519",
                    "user": {
                        "_id": "66d86ae34eb2eb8dc078f808",
                        "avatarUrl": "/avatars/4fb926729092ce69087343ce767387c8.svg",
                        "isPro": false,
                        "fullname": "Yuxuan Li",
                        "user": "yuxuanli",
                        "type": "user"
                    },
                    "name": "Yuxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:05:30.760Z",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf89951a",
                    "name": "Jianyong Wang",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf89951b",
                    "user": {
                        "_id": "6310a3cd531cc21f9e06de6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
                        "isPro": false,
                        "fullname": "Zhiyuan Liu",
                        "user": "acharkq",
                        "type": "user"
                    },
                    "name": "Zhiyuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:06:05.421Z",
                    "hidden": false
                },
                {
                    "_id": "67bee83509a4524abf89951c",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-20T18:58:10.000Z",
            "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling",
            "summary": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12times speedup over the\nstate-of-the-art speculative sampling method EAGLE-2.",
            "upvotes": 6,
            "discussionId": "67bee83609a4524abf899550",
            "githubRepo": "https://github.com/thunlp/FR-Spec"
        },
        "publishedAt": "2025-03-05T00:36:34.146Z",
        "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14856.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b8ff3d95bd42c770878042",
            "avatarUrl": "/avatars/564a4dccdf9e5b813a99979b0ef58183.svg",
            "fullname": "Weilin Zhao",
            "name": "Achazwl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02537",
            "authors": [
                {
                    "_id": "67c808936f68759bf37a8eb8",
                    "user": {
                        "_id": "63f58403fcf95ecac2b33d78",
                        "avatarUrl": "/avatars/a77ea80784896502ae1cfa086a78ce66.svg",
                        "isPro": false,
                        "fullname": "Zhen Yang",
                        "user": "YZCS",
                        "type": "user"
                    },
                    "name": "Zhen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:11:01.696Z",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8eb9",
                    "user": {
                        "_id": "65d5aa45dca2a85f0fe895f3",
                        "avatarUrl": "/avatars/a3cbcade6ea101e99f58641aa409fdfe.svg",
                        "isPro": false,
                        "fullname": "Guibao SHEN",
                        "user": "PaulSHEN1",
                        "type": "user"
                    },
                    "name": "Guibao Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:20:13.342Z",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8eba",
                    "name": "Liang Hou",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8ebb",
                    "name": "Mushui Liu",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8ebc",
                    "user": {
                        "_id": "6340333a733f9eef46913dc8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6340333a733f9eef46913dc8/BwhKgkqbuuxWsJv2s_JEH.png",
                        "isPro": true,
                        "fullname": "luozhou wang",
                        "user": "wileewang",
                        "type": "user"
                    },
                    "name": "Luozhou Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:35:12.288Z",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8ebd",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8ebe",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8ebf",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:35:23.909Z",
                    "hidden": false
                },
                {
                    "_id": "67c808936f68759bf37a8ec0",
                    "name": "Ying-Cong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T12:03:26.000Z",
            "title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via\n  Energy Rectification",
            "summary": "Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an efficient and straightforward solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we propose an Energy Rectification strategy, where\nmodifying the hyperparameters of the classifier-free guidance effectively\nimproves the generation performance. Our method is entirely training-free and\nboasts a simple implementation logic. Through extensive comparisons with\nnumerous baseline methods, our RectifiedHR demonstrates superior effectiveness\nand efficiency.",
            "upvotes": 5,
            "discussionId": "67c808b76f68759bf37a96cf",
            "projectPage": "https://zhenyangcs.github.io/RectifiedHR-Diffusion/",
            "githubRepo": "https://github.com/EnVision-Research/RectifiedHR"
        },
        "publishedAt": "2025-03-05T05:53:05.272Z",
        "title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02537.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f58403fcf95ecac2b33d78",
            "avatarUrl": "/avatars/a77ea80784896502ae1cfa086a78ce66.svg",
            "fullname": "Zhen Yang",
            "name": "YZCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.01342",
            "authors": [
                {
                    "_id": "67c6b46e8389d77f5ba87179",
                    "user": {
                        "_id": "6585493b53c37507639fe3ba",
                        "avatarUrl": "/avatars/b7e71d4fa5ebb89a7ed6b2a8313687b5.svg",
                        "isPro": false,
                        "fullname": "Hao Tang",
                        "user": "kanashi6",
                        "type": "user"
                    },
                    "name": "Hao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-04T08:34:43.034Z",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba8717a",
                    "user": {
                        "_id": "66592c72f4124d863fd55574",
                        "avatarUrl": "/avatars/98f0d5e6ba3728e8a1164aa5188a3298.svg",
                        "isPro": false,
                        "fullname": "Chenwei Xie",
                        "user": "chenweix7",
                        "type": "user"
                    },
                    "name": "Chenwei Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:10:05.785Z",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba8717b",
                    "user": {
                        "_id": "65f43c3cc9940817caaf4434",
                        "avatarUrl": "/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg",
                        "isPro": false,
                        "fullname": "Haiyang Wang",
                        "user": "Haiyang-W",
                        "type": "user"
                    },
                    "name": "Haiyang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:10:27.485Z",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba8717c",
                    "user": {
                        "_id": "6615fc38d6af2bbe8bd86a72",
                        "avatarUrl": "/avatars/812277f2f5fd673f2e18988cd499c74d.svg",
                        "isPro": false,
                        "fullname": "Xiaoyi Bao",
                        "user": "Shawnee-bxy",
                        "type": "user"
                    },
                    "name": "Xiaoyi Bao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:10:34.475Z",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba8717d",
                    "user": {
                        "_id": "6489713d06a6dc54460725bb",
                        "avatarUrl": "/avatars/11b0e766eb8ccf67511e04a0c75e171e.svg",
                        "isPro": false,
                        "fullname": "Tingyu Weng",
                        "user": "windmillknight",
                        "type": "user"
                    },
                    "name": "Tingyu Weng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:10:41.931Z",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba8717e",
                    "name": "Pandeng Li",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba8717f",
                    "name": "Yun Zheng",
                    "hidden": false
                },
                {
                    "_id": "67c6b46e8389d77f5ba87180",
                    "user": {
                        "_id": "6497272b6401266eb6752b89",
                        "avatarUrl": "/avatars/d94a011660f4cad5344756027840dc30.svg",
                        "isPro": false,
                        "fullname": "Liwei Wang",
                        "user": "asdfg80",
                        "type": "user"
                    },
                    "name": "Liwei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:14:43.973Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T09:27:24.000Z",
            "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended\n  Language Interface",
            "summary": "Generalist models have achieved remarkable success in both language and\nvision-language tasks, showcasing the potential of unified modeling. However,\neffectively integrating fine-grained perception tasks like detection and\nsegmentation into these models remains a significant challenge. This is\nprimarily because these tasks often rely heavily on task-specific designs and\narchitectures that can complicate the modeling process. To address this\nchallenge, we present \\ours, a framework that Unifies\nFine-grained visual perception tasks through an Open-ended\nlanguage interface. By transforming all perception targets into the language\nspace, \\ours unifies object-level detection, pixel-level segmentation, and\nimage-level vision-language tasks into a single model. Additionally, we\nintroduce a novel embedding retrieval approach that relies solely on the\nlanguage interface to support segmentation tasks. Our framework bridges the gap\nbetween fine-grained perception and vision-language tasks, significantly\nsimplifying architectural design and training strategies while achieving\ncomparable or superior performance to methods with intricate task-specific\ndesigns. After multi-task training on five standard visual perception datasets,\n\\ours outperforms the previous state-of-the-art generalist models by 12.3 mAP\non COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation.\nFurthermore, our method seamlessly integrates with existing MLLMs, effectively\ncombining fine-grained perception capabilities with their advanced language\nabilities, thereby enabling more challenging tasks such as reasoning\nsegmentation. Code and models will be publicly available.",
            "upvotes": 5,
            "discussionId": "67c6b4728389d77f5ba8724d",
            "githubRepo": "https://github.com/nnnth/UFO"
        },
        "publishedAt": "2025-03-04T23:55:08.057Z",
        "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01342.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6585493b53c37507639fe3ba",
            "avatarUrl": "/avatars/b7e71d4fa5ebb89a7ed6b2a8313687b5.svg",
            "fullname": "Hao Tang",
            "name": "kanashi6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02197",
            "authors": [
                {
                    "_id": "67c7c183203958ca9c09171a",
                    "name": "Zhixun Chen",
                    "hidden": false
                },
                {
                    "_id": "67c7c183203958ca9c09171b",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67c7c183203958ca9c09171c",
                    "name": "Yuxuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67c7c183203958ca9c09171d",
                    "user": {
                        "_id": "646b3c09628e5b50b2dd1174",
                        "avatarUrl": "/avatars/f96a4d375ea44d10891f4b47b7d7986e.svg",
                        "isPro": false,
                        "fullname": "yali du",
                        "user": "YaliDU",
                        "type": "user"
                    },
                    "name": "Yali Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:15:28.895Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c183203958ca9c09171e",
                    "name": "Meng Fang",
                    "hidden": false
                },
                {
                    "_id": "67c7c183203958ca9c09171f",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:12:17.603Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T02:14:55.000Z",
            "title": "ATLaS: Agent Tuning via Learning Critical Steps",
            "summary": "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
            "upvotes": 5,
            "discussionId": "67c7c185203958ca9c091751"
        },
        "publishedAt": "2025-03-04T22:17:48.386Z",
        "title": "ATLaS: Agent Tuning via Learning Critical Steps",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02197.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02878",
            "authors": [
                {
                    "_id": "67c7bf7c40de8b1b534d23fa",
                    "user": {
                        "_id": "635d76ce94e5b275ca74b967",
                        "avatarUrl": "/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg",
                        "isPro": false,
                        "fullname": "Ethan Mendes",
                        "user": "emendes3",
                        "type": "user"
                    },
                    "name": "Ethan Mendes",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-05T03:06:00.588Z",
                    "hidden": false
                },
                {
                    "_id": "67c7bf7c40de8b1b534d23fb",
                    "user": {
                        "_id": "6377da9065ce6b04bd493cf8",
                        "avatarUrl": "/avatars/6be2632d6da49857928f8bcce8278f49.svg",
                        "isPro": false,
                        "fullname": "Alan Ritter",
                        "user": "rittera",
                        "type": "user"
                    },
                    "name": "Alan Ritter",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:19:56.686Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T18:58:11.000Z",
            "title": "Language Models can Self-Improve at State-Value Estimation for Better\n  Search",
            "summary": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
            "upvotes": 5,
            "discussionId": "67c7bf7e40de8b1b534d2491",
            "githubRepo": "https://github.com/ethanm88/self-taught-lookahead"
        },
        "publishedAt": "2025-03-04T22:07:18.480Z",
        "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02878.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635d76ce94e5b275ca74b967",
            "avatarUrl": "/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg",
            "fullname": "Ethan Mendes",
            "name": "emendes3",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.00876",
            "authors": [
                {
                    "_id": "67c8502f4b773d4f539fb5ac",
                    "user": {
                        "_id": "651c000fc4db16814c03d86a",
                        "avatarUrl": "/avatars/f1d61d5f9ffde4bb43402ff7b3c75332.svg",
                        "isPro": false,
                        "fullname": "Zijian Dong",
                        "user": "ZijianDD",
                        "type": "user"
                    },
                    "name": "Zijian Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T14:44:22.495Z",
                    "hidden": false
                },
                {
                    "_id": "67c8502f4b773d4f539fb5ad",
                    "user": {
                        "_id": "651c0afb26b2dee1ed2dfab8",
                        "avatarUrl": "/avatars/14a69bc1f0b877106cc39a19fcc9acdb.svg",
                        "isPro": false,
                        "fullname": "yilei wu",
                        "user": "yilei-nus",
                        "type": "user"
                    },
                    "name": "Yilei Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T14:44:30.902Z",
                    "hidden": false
                },
                {
                    "_id": "67c8502f4b773d4f539fb5ae",
                    "user": {
                        "_id": "66af930bfd80ab874984b511",
                        "avatarUrl": "/avatars/ed81bdc7a21e83462572ac731ab85a89.svg",
                        "isPro": false,
                        "fullname": "chenchongyao",
                        "user": "ccyhaha2",
                        "type": "user"
                    },
                    "name": "Chongyao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T14:44:44.533Z",
                    "hidden": false
                },
                {
                    "_id": "67c8502f4b773d4f539fb5af",
                    "name": "Yingtian Zou",
                    "hidden": false
                },
                {
                    "_id": "67c8502f4b773d4f539fb5b0",
                    "name": "Yichi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c8502f4b773d4f539fb5b1",
                    "name": "Juan Helen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-02T12:31:34.000Z",
            "title": "Improve Representation for Imbalanced Regression through Geometric\n  Constraints",
            "summary": "In representation learning, uniformity refers to the uniform feature\ndistribution in the latent space (i.e., unit hypersphere). Previous work has\nshown that improving uniformity contributes to the learning of\nunder-represented classes. However, most of the previous work focused on\nclassification; the representation space of imbalanced regression remains\nunexplored. Classification-based methods are not suitable for regression tasks\nbecause they cluster features into distinct groups without considering the\ncontinuous and ordered nature essential for regression. In a geometric aspect,\nwe uniquely focus on ensuring uniformity in the latent space for imbalanced\nregression through two key losses: enveloping and homogeneity. The enveloping\nloss encourages the induced trace to uniformly occupy the surface of a\nhypersphere, while the homogeneity loss ensures smoothness, with\nrepresentations evenly spaced at consistent intervals. Our method integrates\nthese geometric principles into the data representations via a Surrogate-driven\nRepresentation Learning (SRL) framework. Experiments with real-world regression\nand operator learning tasks highlight the importance of uniformity in\nimbalanced regression and validate the efficacy of our geometry-based loss\nfunctions.",
            "upvotes": 4,
            "discussionId": "67c850314b773d4f539fb645"
        },
        "publishedAt": "2025-03-05T08:25:03.453Z",
        "title": "Improve Representation for Imbalanced Regression through Geometric Constraints",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00876.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651c000fc4db16814c03d86a",
            "avatarUrl": "/avatars/f1d61d5f9ffde4bb43402ff7b3c75332.svg",
            "fullname": "Zijian Dong",
            "name": "ZijianDD",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02268",
            "authors": [
                {
                    "_id": "67c7ebafdf15f5978ac987c3",
                    "name": "Wenjia Jiang",
                    "hidden": false
                },
                {
                    "_id": "67c7ebafdf15f5978ac987c4",
                    "user": {
                        "_id": "673afd8879b5a48d37f38000",
                        "avatarUrl": "/avatars/8e552c13b37c46734ed7a5d72469c6ad.svg",
                        "isPro": false,
                        "fullname": "Zhuang Yangyang",
                        "user": "yyzhuang2",
                        "type": "user"
                    },
                    "name": "Yangyang Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:38:30.615Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ebafdf15f5978ac987c5",
                    "user": {
                        "_id": "65cf975bf5a15aa42146b208",
                        "avatarUrl": "/avatars/d62e3907331a24bb899b7a1fdf4299e7.svg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "ChenxiSong",
                        "type": "user"
                    },
                    "name": "Chenxi Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:38:35.946Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ebafdf15f5978ac987c6",
                    "user": {
                        "_id": "63104777cc8ed75decbd9500",
                        "avatarUrl": "/avatars/f9002d8bffa7304296718321d4c6f58a.svg",
                        "isPro": false,
                        "fullname": "xuyang",
                        "user": "xuyang",
                        "type": "user"
                    },
                    "name": "Xu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:38:43.389Z",
                    "hidden": false
                },
                {
                    "_id": "67c7ebafdf15f5978ac987c7",
                    "name": "Chi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T04:34:09.000Z",
            "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
            "summary": "Recent advancements in Large Language Models (LLMs) have led to the\ndevelopment of intelligent LLM-based agents capable of interacting with\ngraphical user interfaces (GUIs). These agents demonstrate strong reasoning and\nadaptability, enabling them to perform complex tasks that traditionally\nrequired predefined rules. However, the reliance on step-by-step reasoning in\nLLM-based agents often results in inefficiencies, particularly for routine\ntasks. In contrast, traditional rule-based systems excel in efficiency but lack\nthe intelligence and flexibility to adapt to novel scenarios. To address this\nchallenge, we propose a novel evolutionary framework for GUI agents that\nenhances operational efficiency while retaining intelligence and flexibility.\nOur approach incorporates a memory mechanism that records the agent's task\nexecution history. By analyzing this history, the agent identifies repetitive\naction sequences and evolves high-level actions that act as shortcuts,\nreplacing these low-level operations and improving efficiency. This allows the\nagent to focus on tasks requiring more complex reasoning, while simplifying\nroutine actions. Experimental results on multiple benchmark tasks demonstrate\nthat our approach significantly outperforms existing methods in both efficiency\nand accuracy. The code will be open-sourced to support further research.",
            "upvotes": 4,
            "discussionId": "67c7ebb5df15f5978ac98975"
        },
        "publishedAt": "2025-03-05T01:15:42.467Z",
        "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/dUDWK6xfRd9uVZz77V0K6.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02268.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64196320ed725fef64419c2a",
            "avatarUrl": "/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg",
            "fullname": "Chi Zhang",
            "name": "DrChiZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.02876",
            "authors": [
                {
                    "_id": "67c7bd7149b52e85403758f8",
                    "user": {
                        "_id": "6308bae5c038bf42d56a98e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yrslTwUe_vy_ZJha1H83m.png",
                        "isPro": false,
                        "fullname": "Dmitry Nechaev",
                        "user": "mgvz",
                        "type": "user"
                    },
                    "name": "Dmitry Nechaev",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-05T03:07:52.944Z",
                    "hidden": false
                },
                {
                    "_id": "67c7bd7149b52e85403758f9",
                    "user": {
                        "_id": "6655b0b9d6c043f39719eaaf",
                        "avatarUrl": "/avatars/66138e67ef3be41f29857b285b37adff.svg",
                        "isPro": false,
                        "fullname": "Alex Pchelnikov",
                        "user": "alpchel",
                        "type": "user"
                    },
                    "name": "Alexey Pchelnikov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-05T03:07:35.092Z",
                    "hidden": false
                },
                {
                    "_id": "67c7bd7149b52e85403758fa",
                    "name": "Ekaterina Ivanova",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T18:57:12.000Z",
            "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and\n  Baseline Models",
            "summary": "Advancing AI in computational pathology requires large, high-quality, and\ndiverse datasets, yet existing public datasets are often limited in organ\ndiversity, class coverage, or annotation quality. To bridge this gap, we\nintroduce SPIDER (Supervised Pathology Image-DEscription Repository), the\nlargest publicly available patch-level dataset covering multiple organ types,\nincluding Skin, Colorectal, and Thorax, with comprehensive class coverage for\neach organ. SPIDER provides high-quality annotations verified by expert\npathologists and includes surrounding context patches, which enhance\nclassification performance by providing spatial context.\n  Alongside the dataset, we present baseline models trained on SPIDER using the\nHibou-L foundation model as a feature extractor combined with an\nattention-based classification head. The models achieve state-of-the-art\nperformance across multiple tissue categories and serve as strong benchmarks\nfor future digital pathology research. Beyond patch classification, the model\nenables rapid identification of significant areas, quantitative tissue metrics,\nand establishes a foundation for multimodal approaches.\n  Both the dataset and trained models are publicly available to advance\nresearch, reproducibility, and AI-driven pathology development. Access them at:\nhttps://github.com/HistAI/SPIDER",
            "upvotes": 4,
            "discussionId": "67c7bd7649b52e8540375a34",
            "projectPage": "https://www.hist.ai/",
            "githubRepo": "https://github.com/HistAI/SPIDER"
        },
        "publishedAt": "2025-03-04T22:08:26.811Z",
        "title": "SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02876.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6308bae5c038bf42d56a98e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yrslTwUe_vy_ZJha1H83m.png",
            "fullname": "Dmitry Nechaev",
            "name": "mgvz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02357",
            "authors": [
                {
                    "_id": "67c830966f68759bf38567a5",
                    "name": "Zicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567a6",
                    "name": "Tengchuan Kou",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567a7",
                    "user": {
                        "_id": "64687fb52a180a1378359c46",
                        "avatarUrl": "/avatars/746ea4d52801227fd44b9a9f380738a2.svg",
                        "isPro": false,
                        "fullname": "shushi wang ",
                        "user": "wcain",
                        "type": "user"
                    },
                    "name": "Shushi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:36:18.075Z",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567a8",
                    "name": "Chunyi Li",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567a9",
                    "name": "Wei Sun",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567aa",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567ab",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567ac",
                    "user": {
                        "_id": "6459f4f9667a889f3d35f51c",
                        "avatarUrl": "/avatars/a4a407b8f0d6e3616c8e7ae7b4c78a7d.svg",
                        "isPro": false,
                        "fullname": "zongyu wang",
                        "user": "sl2782087",
                        "type": "user"
                    },
                    "name": "Zongyu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:36:37.427Z",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567ad",
                    "user": {
                        "_id": "647095cdcf0a68ad56aa7181",
                        "avatarUrl": "/avatars/65e87b38b90c57b6db82e51f48a15fcf.svg",
                        "isPro": false,
                        "fullname": "xuezhi cao",
                        "user": "a9108",
                        "type": "user"
                    },
                    "name": "Xuezhi Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:36:30.233Z",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567ae",
                    "name": "Xiongkuo Min",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567af",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "67c830966f68759bf38567b0",
                    "user": {
                        "_id": "65535b125413c1a54e6fb243",
                        "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
                        "isPro": false,
                        "fullname": "Guangtao Zhai",
                        "user": "GTZhai",
                        "type": "user"
                    },
                    "name": "Guangtao Zhai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:37:03.268Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T07:28:45.000Z",
            "title": "Q-Eval-100K: Evaluating Visual Quality and Alignment Level for\n  Text-to-Vision Content",
            "summary": "Evaluating text-to-vision content hinges on two crucial aspects: visual\nquality and alignment. While significant progress has been made in developing\nobjective models to assess these dimensions, the performance of such models\nheavily relies on the scale and quality of human annotations. According to\nScaling Law, increasing the number of human-labeled instances follows a\npredictable pattern that enhances the performance of evaluation models.\nTherefore, we introduce a comprehensive dataset designed to Evaluate Visual\nquality and Alignment Level for text-to-vision content (Q-EVAL-100K), featuring\nthe largest collection of human-labeled Mean Opinion Scores (MOS) for the\nmentioned two aspects. The Q-EVAL-100K dataset encompasses both text-to-image\nand text-to-video models, with 960K human annotations specifically focused on\nvisual quality and alignment for 100K instances (60K images and 40K videos).\nLeveraging this dataset with context prompt, we propose Q-Eval-Score, a unified\nmodel capable of evaluating both visual quality and alignment with special\nimprovements for handling long-text prompt alignment. Experimental results\nindicate that the proposed Q-Eval-Score achieves superior performance on both\nvisual quality and alignment, with strong generalization capabilities across\nother benchmarks. These findings highlight the significant value of the\nQ-EVAL-100K dataset. Data and codes will be available at\nhttps://github.com/zzc-1998/Q-Eval.",
            "upvotes": 3,
            "discussionId": "67c830976f68759bf38567f4"
        },
        "publishedAt": "2025-03-05T06:09:41.352Z",
        "title": "Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02357.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6526cc6bab4f5d98382f5603",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6526cc6bab4f5d98382f5603/NekYYb61I5nt4au6gXsVK.jpeg",
            "fullname": "Zicheng Zhang",
            "name": "zhangzicheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.02783",
            "authors": [
                {
                    "_id": "67c82d2202935d02b0ce0df1",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df2",
                    "user": {
                        "_id": "650be23ec4e52db6a4db63ef",
                        "avatarUrl": "/avatars/03af548029b38bee49ec295fefe74f9a.svg",
                        "isPro": false,
                        "fullname": "Haoling Li",
                        "user": "Ringo1110",
                        "type": "user"
                    },
                    "name": "Haoling Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T10:59:05.817Z",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df3",
                    "user": {
                        "_id": "641a9a4b05290a135041a3ed",
                        "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
                        "isPro": false,
                        "fullname": "Pluto",
                        "user": "CharonBony",
                        "type": "user"
                    },
                    "name": "Xin Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-05T10:53:22.999Z",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df4",
                    "user": {
                        "_id": "66adf5cc0c6056d9f4dc308f",
                        "avatarUrl": "/avatars/7689ba5538f6bbf235084d418b3065c1.svg",
                        "isPro": false,
                        "fullname": "Jianwen Luo",
                        "user": "Jianwen2003",
                        "type": "user"
                    },
                    "name": "Jianwen Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:37:40.552Z",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df5",
                    "user": {
                        "_id": "64c66647725ffa04b2fd6c94",
                        "avatarUrl": "/avatars/620f63f27fa1e90423b0dc22aa8e5809.svg",
                        "isPro": false,
                        "fullname": "yangyu huang",
                        "user": "yangyu90",
                        "type": "user"
                    },
                    "name": "Yangyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:37:46.285Z",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df6",
                    "user": {
                        "_id": "642e3bcb958faf258a40e89c",
                        "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
                        "isPro": false,
                        "fullname": "Ruihang Chu",
                        "user": "Ruihang",
                        "type": "user"
                    },
                    "name": "Ruihang Chu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:37:51.435Z",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df7",
                    "user": {
                        "_id": "64ca1fe838837b12d5e529b7",
                        "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg",
                        "isPro": false,
                        "fullname": "Yujiu Yang",
                        "user": "Thu-redrobot",
                        "type": "user"
                    },
                    "name": "Yujiu Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:37:57.005Z",
                    "hidden": false
                },
                {
                    "_id": "67c82d2202935d02b0ce0df8",
                    "user": {
                        "_id": "67366efb049bfa3a9084e8d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Bny5bUc6qFQ_RWpEKpRAW.jpeg",
                        "isPro": false,
                        "fullname": "Scarlett Li",
                        "user": "lisijia0504",
                        "type": "user"
                    },
                    "name": "Scarlett Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-05T11:38:04.131Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T16:56:34.000Z",
            "title": "IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging",
            "summary": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
            "upvotes": 3,
            "discussionId": "67c82d2302935d02b0ce0e3c"
        },
        "publishedAt": "2025-03-05T05:54:00.530Z",
        "title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02783.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650be23ec4e52db6a4db63ef",
            "avatarUrl": "/avatars/03af548029b38bee49ec295fefe74f9a.svg",
            "fullname": "Haoling Li",
            "name": "Ringo1110",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02812",
            "authors": [
                {
                    "_id": "67c87b12d62973a5d9f014b3",
                    "user": {
                        "_id": "61d2d6a5291690e1c7b4dd2d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666688353023-61d2d6a5291690e1c7b4dd2d.jpeg",
                        "isPro": false,
                        "fullname": "Nathan Godey",
                        "user": "nthngdy",
                        "type": "user"
                    },
                    "name": "Nathan Godey",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-05T16:25:56.974Z",
                    "hidden": false
                },
                {
                    "_id": "67c87b12d62973a5d9f014b4",
                    "name": "Alessio Devoto",
                    "hidden": false
                },
                {
                    "_id": "67c87b12d62973a5d9f014b5",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c87b12d62973a5d9f014b6",
                    "name": "Simone Scardapane",
                    "hidden": false
                },
                {
                    "_id": "67c87b12d62973a5d9f014b7",
                    "name": "Pasquale Minervini",
                    "hidden": false
                },
                {
                    "_id": "67c87b12d62973a5d9f014b8",
                    "name": "Éric de la Clergerie",
                    "hidden": false
                },
                {
                    "_id": "67c87b12d62973a5d9f014b9",
                    "name": "Benoît Sagot",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T17:37:49.000Z",
            "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
            "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
            "upvotes": 2,
            "discussionId": "67c87b14d62973a5d9f01561"
        },
        "publishedAt": "2025-03-05T11:29:36.628Z",
        "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02812.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61d2d6a5291690e1c7b4dd2d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666688353023-61d2d6a5291690e1c7b4dd2d.jpeg",
            "fullname": "Nathan Godey",
            "name": "nthngdy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02823",
            "authors": [
                {
                    "_id": "67c8522f8cc8a6393971a16c",
                    "user": {
                        "_id": "65b00f6bc9a5a7680f531c1d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00f6bc9a5a7680f531c1d/z4hzKR5tgdkft6c3Wxt9Y.jpeg",
                        "isPro": false,
                        "fullname": "Matteo Spanio",
                        "user": "matteospanio",
                        "type": "user"
                    },
                    "name": "Matteo Spanio",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T15:46:46.617Z",
                    "hidden": false
                },
                {
                    "_id": "67c8522f8cc8a6393971a16d",
                    "name": "Massimiliano Zampini",
                    "hidden": false
                },
                {
                    "_id": "67c8522f8cc8a6393971a16e",
                    "name": "Antonio Rodà",
                    "hidden": false
                },
                {
                    "_id": "67c8522f8cc8a6393971a16f",
                    "name": "Franco Pierucci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T17:48:48.000Z",
            "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
            "summary": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' (n=111) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
            "upvotes": 2,
            "discussionId": "67c852308cc8a6393971a19b"
        },
        "publishedAt": "2025-03-05T11:04:04.343Z",
        "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02823.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b00f6bc9a5a7680f531c1d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00f6bc9a5a7680f531c1d/z4hzKR5tgdkft6c3Wxt9Y.jpeg",
            "fullname": "Matteo Spanio",
            "name": "matteospanio",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02152",
            "authors": [
                {
                    "_id": "67c8691bb43e75ced9747fe6",
                    "user": {
                        "_id": "63865c918b5acae8d24ffff3",
                        "avatarUrl": "/avatars/7916f38ce38266a0a3b39a54f75636ff.svg",
                        "isPro": false,
                        "fullname": "Sonia",
                        "user": "sonicc",
                        "type": "user"
                    },
                    "name": "Sonia Cromp",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-05T15:57:21.571Z",
                    "hidden": false
                },
                {
                    "_id": "67c8691bb43e75ced9747fe7",
                    "name": "Satya Sai Srinath Namburi GNVV",
                    "hidden": false
                },
                {
                    "_id": "67c8691bb43e75ced9747fe8",
                    "name": "Mohammed Alkhudhayri",
                    "hidden": false
                },
                {
                    "_id": "67c8691bb43e75ced9747fe9",
                    "name": "Catherine Cao",
                    "hidden": false
                },
                {
                    "_id": "67c8691bb43e75ced9747fea",
                    "name": "Samuel Guo",
                    "hidden": false
                },
                {
                    "_id": "67c8691bb43e75ced9747feb",
                    "name": "Nicholas Roberts",
                    "hidden": false
                },
                {
                    "_id": "67c8691bb43e75ced9747fec",
                    "name": "Frederic Sala",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T00:32:15.000Z",
            "title": "Tabby: Tabular Data Synthesis with Language Models",
            "summary": "While advances in large language models (LLMs) have greatly improved the\nquality of synthetic text data in recent years, synthesizing tabular data has\nreceived relatively less attention. We address this disparity with Tabby, a\nsimple but powerful post-training modification to the standard Transformer\nlanguage model architecture, enabling its use for tabular dataset synthesis.\nTabby enables the representation of differences across columns using Gated\nMixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby\nresults in data quality near or equal to that of real data. By pairing our\nnovel LLM table training technique, Plain, with Tabby, we observe up to a 44%\nimprovement in quality over previous methods. We also show that Tabby extends\nbeyond tables to more general structured data, reaching parity with real data\non a nested JSON dataset as well.",
            "upvotes": 1,
            "discussionId": "67c8691db43e75ced974804f",
            "projectPage": "https://github.com/soCromp/tabby",
            "githubRepo": "https://github.com/soCromp/tabby"
        },
        "publishedAt": "2025-03-05T11:03:42.586Z",
        "title": "Tabby: Tabular Data Synthesis with Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02152.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63865c918b5acae8d24ffff3",
            "avatarUrl": "/avatars/7916f38ce38266a0a3b39a54f75636ff.svg",
            "fullname": "Sonia",
            "name": "sonicc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02304",
            "authors": [
                {
                    "_id": "67c7c17380815cf71fb9139d",
                    "user": {
                        "_id": "650d4a36cbd0c7d550d3b41b",
                        "avatarUrl": "/avatars/0f628ccbbad894098b201a188eb3bd0b.svg",
                        "isPro": false,
                        "fullname": "TongkunGuan",
                        "user": "TongkunGuan",
                        "type": "user"
                    },
                    "name": "Tongkun Guan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T15:47:05.706Z",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb9139e",
                    "name": "Zining Wang",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb9139f",
                    "name": "Pei Fu",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a0",
                    "name": "Zhengtao Guo",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a1",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a2",
                    "name": "Kai Zhou",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a3",
                    "name": "Tiezhu Yue",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a4",
                    "name": "Chen Duan",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a5",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a6",
                    "name": "Qianyi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a7",
                    "name": "Junfeng Luo",
                    "hidden": false
                },
                {
                    "_id": "67c7c17380815cf71fb913a8",
                    "name": "Xiaokang Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T06:05:33.000Z",
            "title": "A Token-level Text Image Foundation Model for Document Understanding",
            "summary": "In recent years, general visual foundation models (VFMs) have witnessed\nincreasing adoption, particularly as image encoders for popular multi-modal\nlarge language models (MLLMs). However, without semantically fine-grained\nsupervision, these models still encounter fundamental prediction errors in the\ncontext of downstream text-image-related tasks, i.e., perception, understanding\nand reasoning with images containing small and dense texts. To bridge this gap,\nwe develop TokenOCR, the first token-level visual foundation model specifically\ntailored for text-image-related tasks, designed to support a variety of\ntraditional downstream applications. To facilitate the pretraining of TokenOCR,\nwe also devise a high-quality data production pipeline that constructs the\nfirst token-level image text dataset, TokenIT, comprising 20 million images and\n1.8 billion token-mask pairs. Furthermore, leveraging this foundation with\nexceptional image-as-text capability, we seamlessly replace previous VFMs with\nTokenOCR to construct a document-level MLLM, TokenVL, for VQA-based document\nunderstanding tasks. Finally, extensive experiments demonstrate the\neffectiveness of TokenOCR and TokenVL. Code, datasets, and weights will be\navailable at https://token-family.github.io/TokenOCR_project.",
            "upvotes": 1,
            "discussionId": "67c7c17980815cf71fb915bf",
            "projectPage": "https://token-family.github.io/TokenOCR_project/",
            "githubRepo": "https://github.com/Token-family/TokenOCR"
        },
        "publishedAt": "2025-03-05T11:03:27.219Z",
        "title": "A Token-level Text Image Foundation Model for Document Understanding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02304.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "650d4a36cbd0c7d550d3b41b",
            "avatarUrl": "/avatars/0f628ccbbad894098b201a188eb3bd0b.svg",
            "fullname": "TongkunGuan",
            "name": "TongkunGuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.00200",
            "authors": [
                {
                    "_id": "67c8bdbbbece4af63c15f388",
                    "name": "Shuang Li",
                    "hidden": false
                },
                {
                    "_id": "67c8bdbbbece4af63c15f389",
                    "name": "Yihuai Gao",
                    "hidden": false
                },
                {
                    "_id": "67c8bdbbbece4af63c15f38a",
                    "name": "Dorsa Sadigh",
                    "hidden": false
                },
                {
                    "_id": "67c8bdbbbece4af63c15f38b",
                    "name": "Shuran Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T21:38:17.000Z",
            "title": "Unified Video Action Model",
            "summary": "A unified video and action model holds significant promise for robotics,\nwhere videos provide rich scene information for action prediction, and actions\nprovide dynamics information for video prediction. However, effectively\ncombining video generation and action prediction remains challenging, and\ncurrent video generation-based methods struggle to match the performance of\ndirect policy learning in action accuracy and inference speed. To bridge this\ngap, we introduce the Unified Video Action model (UVA), which jointly optimizes\nvideo and action predictions to achieve both high accuracy and efficient action\ninference. The key lies in learning a joint video-action latent representation\nand decoupling video-action decoding. The joint latent representation bridges\nthe visual and action domains, effectively modeling the relationship between\nvideo and action sequences. Meanwhile, the decoupled decoding, powered by two\nlightweight diffusion heads, enables high-speed action inference by bypassing\nvideo generation during inference. Such a unified framework further enables\nversatile functionality through masked input training. By selectively masking\nactions or videos, a single model can tackle diverse tasks beyond policy\nlearning, such as forward and inverse dynamics modeling and video generation.\nVia an extensive set of experiments, we demonstrate that UVA can serve as a\ngeneral-purpose solution for a wide range of robotics tasks, such as policy\nlearning, forward/inverse dynamics and video observation prediction, without\ncompromising performance compared to methods tailored for specific\napplications. Results are best viewed on\nhttps://unified-video-action-model.github.io/.",
            "upvotes": 0,
            "discussionId": "67c8bdbfbece4af63c15f4ac",
            "projectPage": "https://unified-video-action-model.github.io/",
            "githubRepo": "https://github.com/ShuangLI59/unified_video_action"
        },
        "publishedAt": "2025-03-05T16:12:24.781Z",
        "title": "Unified Video Action Model",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00200.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "629e4ad01fec321a2d2abfa3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629e4ad01fec321a2d2abfa3/LwMzgQr9RH5arKOBNR043.jpeg",
            "fullname": "Shuang Li",
            "name": "Shuang59",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01842",
            "authors": [
                {
                    "_id": "67c733280bfefe1ccd904e4b",
                    "user": {
                        "_id": "67ba6ecc3d75e6192e1a18e6",
                        "avatarUrl": "/avatars/ca8a31c00fa799d5da19dbc73e1d5faf.svg",
                        "isPro": false,
                        "fullname": "Hang Liu",
                        "user": "Hang917",
                        "type": "user"
                    },
                    "name": "Hang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-04T21:15:33.353Z",
                    "hidden": false
                },
                {
                    "_id": "67c733280bfefe1ccd904e4c",
                    "name": "Sangli Teng",
                    "hidden": false
                },
                {
                    "_id": "67c733280bfefe1ccd904e4d",
                    "name": "Ben Liu",
                    "hidden": false
                },
                {
                    "_id": "67c733280bfefe1ccd904e4e",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c733280bfefe1ccd904e4f",
                    "name": "Maani Ghaffari",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T18:59:23.000Z",
            "title": "Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets\n  Skateboarding",
            "summary": "This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a\nframework using on-policy Reinforcement Learning to identify and execute\nmode-switching without trajectory segmentation or event function learning.\nHybrid dynamical systems, which include continuous flow and discrete mode\nswitching, can model robotics tasks like legged robot locomotion. Model-based\nmethods usually depend on predefined gaits, while model-free approaches lack\nexplicit mode-switching knowledge. Current methods identify discrete modes via\nsegmentation before regressing continuous flow, but learning high-dimensional\ncomplex rigid body dynamics without trajectory labels or segmentation is a\nchallenging open problem. Our approach incorporates a beta policy distribution\nand a multi-critic architecture to model contact-guided motions, exemplified by\na challenging quadrupedal robot skateboard task. We validate our method through\nsimulations and real-world tests, demonstrating robust performance in hybrid\ndynamical systems.",
            "upvotes": 0,
            "discussionId": "67c7332b0bfefe1ccd904f05"
        },
        "publishedAt": "2025-03-05T10:37:18.358Z",
        "title": "Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ba6ecc3d75e6192e1a18e6/aT655oWePxaCbXTmP1Syd.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01842.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ba6ecc3d75e6192e1a18e6",
            "avatarUrl": "/avatars/ca8a31c00fa799d5da19dbc73e1d5faf.svg",
            "fullname": "Hang Liu",
            "name": "Hang917",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]