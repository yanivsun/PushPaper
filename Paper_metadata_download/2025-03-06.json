[
    {
        "paper": {
            "id": "2503.00865",
            "authors": [
                {
                    "_id": "67c666245e2443d7d5e9b76a",
                    "user": {
                        "_id": "64802face9ff472e30dc1ceb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
                        "isPro": false,
                        "fullname": "Yiran Zhao",
                        "user": "Yiran0924",
                        "type": "user"
                    },
                    "name": "Yiran Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76b",
                    "user": {
                        "_id": "61657b0b20606e5e73f611cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
                        "isPro": false,
                        "fullname": "CHAOQUN LIU",
                        "user": "lukecq",
                        "type": "user"
                    },
                    "name": "Chaoqun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:27:33.956Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76c",
                    "name": "Yue Deng",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76d",
                    "user": {
                        "_id": "671609f7664f44a151f1f0e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fEQLuH1kdW5Pd9Y_J64hN.png",
                        "isPro": false,
                        "fullname": "jiahao ying",
                        "user": "jhying",
                        "type": "user"
                    },
                    "name": "Jiahao Ying",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:39.926Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76e",
                    "user": {
                        "_id": "6539c87ba318a98bf0d15dd8",
                        "avatarUrl": "/avatars/beb9ba6eeacb61addc5897836bd59f55.svg",
                        "isPro": false,
                        "fullname": "Mahani Aljunied",
                        "user": "maljunied",
                        "type": "user"
                    },
                    "name": "Mahani Aljunied",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:33.285Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b76f",
                    "name": "Zhaodonghui Li",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b770",
                    "user": {
                        "_id": "6454685a548f22be598414c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
                        "isPro": false,
                        "fullname": "Lidong Bing",
                        "user": "LidongBing",
                        "type": "user"
                    },
                    "name": "Lidong Bing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:19.611Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b771",
                    "user": {
                        "_id": "604f67ef0fe8ff3ec13d71ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
                        "isPro": false,
                        "fullname": "Hou Pong (Ken) Chan",
                        "user": "kenchan0226",
                        "type": "user"
                    },
                    "name": "Hou Pong Chan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:35:50.272Z",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b772",
                    "name": "Yu Rong",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b773",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "67c666245e2443d7d5e9b774",
                    "user": {
                        "_id": "60dff6ae19a362a8c27862aa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60dff6ae19a362a8c27862aa/LIYzLB3cdPh-B3XIBgBCC.jpeg",
                        "isPro": false,
                        "fullname": "Wenxuan Zhang",
                        "user": "isakzhang",
                        "type": "user"
                    },
                    "name": "Wenxuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:27:36.769Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-02T11:53:55.000Z",
            "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of\n  Global Speakers",
            "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), yet open-source multilingual LLMs remain scarce, with existing models\noften limited in language coverage. Such models typically prioritize\nwell-resourced languages, while widely spoken but under-resourced languages are\noften overlooked. To address this disparity, we introduce Babel, an\nopen multilingual LLM that covers the top 25 languages by number of speakers,\nsupports over 90% of the global population, and includes many languages\nneglected by other open multilingual LLMs. Unlike traditional continue\npretraining approaches, Babel expands its parameter count through a layer\nextension technique that elevates Babel's performance ceiling. We introduce two\nvariants: Babel-9B, designed for efficient inference and\nfine-tuning, and Babel-83B, which sets a new standard for open\nmultilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its\nsuperior performance compared to open LLMs of comparable size. In addition,\nusing open-source supervised fine-tuning datasets, Babel achieves remarkable\nperformance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat\nsetting a new standard for multilingual tasks, reaching the same level of\ncommercial models.",
            "upvotes": 42,
            "discussionId": "67c666255e2443d7d5e9b7b3",
            "projectPage": "https://babel-llm.github.io/babel-llm/",
            "githubRepo": "https://github.com/babel-llm/babel-llm"
        },
        "publishedAt": "2025-03-05T21:49:03.700Z",
        "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64802face9ff472e30dc1ceb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
            "fullname": "Yiran Zhao",
            "name": "Yiran0924",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02003",
            "authors": [
                {
                    "_id": "67c9dc79b918b6494634f899",
                    "user": {
                        "_id": "639ac72e94cad09c5a16c169",
                        "avatarUrl": "/avatars/d676019ba2ccb6266791742c5a40a39e.svg",
                        "isPro": false,
                        "fullname": "Tin Nguyen",
                        "user": "ngthanhtinqn",
                        "type": "user"
                    },
                    "name": "Tin Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T19:50:18.537Z",
                    "hidden": false
                },
                {
                    "_id": "67c9dc79b918b6494634f89a",
                    "name": "Logan Bolton",
                    "hidden": false
                },
                {
                    "_id": "67c9dc79b918b6494634f89b",
                    "name": "Mohammad Reza Taesiri",
                    "hidden": false
                },
                {
                    "_id": "67c9dc79b918b6494634f89c",
                    "name": "Anh Totti Nguyen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T19:26:04.000Z",
            "title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from\n  Inputs",
            "summary": "An Achilles heel of Large Language Models (LLMs) is their tendency to\nhallucinate non-factual statements. A response mixed of factual and non-factual\nstatements poses a challenge for humans to verify and accurately base their\ndecisions on. To combat this problem, we propose Highlighted Chain-of-Thought\nPrompting (HoT), a technique for prompting LLMs to generate responses with XML\ntags that ground facts to those provided in the query. That is, given an input\nquestion, LLMs would first re-format the question to add XML tags highlighting\nkey facts, and then, generate a response with highlights over the facts\nreferenced from the input. Interestingly, in few-shot settings, HoT outperforms\nvanilla chain of thought prompting (CoT) on a wide range of 17 tasks from\narithmetic, reading comprehension to logical reasoning. When asking humans to\nverify LLM responses, highlights help time-limited participants to more\naccurately and efficiently recognize when LLMs are correct. Yet, surprisingly,\nwhen LLMs are wrong, HoTs tend to make users believe that an answer is correct.",
            "upvotes": 25,
            "discussionId": "67c9dc7bb918b6494634f946",
            "projectPage": "https://highlightedchainofthought.github.io/"
        },
        "publishedAt": "2025-03-06T17:46:32.865Z",
        "title": "HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02003.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "639ac72e94cad09c5a16c169",
            "avatarUrl": "/avatars/d676019ba2ccb6266791742c5a40a39e.svg",
            "fullname": "Tin Nguyen",
            "name": "ngthanhtinqn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.03746",
            "authors": [
                {
                    "_id": "67c991abe7dba9cdf9f41e67",
                    "user": {
                        "_id": "65080dc63fc966d1bbba485d",
                        "avatarUrl": "/avatars/347890233f2316e7f7a04d652b2378bb.svg",
                        "isPro": false,
                        "fullname": "Shimao Zhang",
                        "user": "Shimao-Zhang",
                        "type": "user"
                    },
                    "name": "Shimao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T19:50:45.146Z",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e68",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-06T12:14:52.016Z",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e69",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6a",
                    "name": "Junxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6b",
                    "name": "Zheheng Luo",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6c",
                    "name": "Shujian Huang",
                    "hidden": false
                },
                {
                    "_id": "67c991abe7dba9cdf9f41e6d",
                    "name": "Yeyun Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T18:58:44.000Z",
            "title": "Process-based Self-Rewarding Language Models",
            "summary": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.",
            "upvotes": 23,
            "discussionId": "67c991abe7dba9cdf9f41e96"
        },
        "publishedAt": "2025-03-06T07:15:20.638Z",
        "title": "Process-based Self-Rewarding Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03746.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
            "fullname": "Xiao Liu",
            "name": "lx865712528",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.00329",
            "authors": [
                {
                    "_id": "67c755f898a2e37274c62c96",
                    "user": {
                        "_id": "6669ffeecf27eed0f2f2581e",
                        "avatarUrl": "/avatars/6a1012cce15da12fa3bd6f9e0f87d100.svg",
                        "isPro": false,
                        "fullname": "Benjamin Schneider",
                        "user": "BenSchneider",
                        "type": "user"
                    },
                    "name": "Benjamin Schneider",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T19:50:52.715Z",
                    "hidden": false
                },
                {
                    "_id": "67c755f898a2e37274c62c97",
                    "name": "Florian Kerschbaum",
                    "hidden": false
                },
                {
                    "_id": "67c755f898a2e37274c62c98",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:50:07.881Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-01T03:29:02.000Z",
            "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
            "summary": "Visual embedding models excel at zero-shot tasks like visual retrieval and\nclassification. However, these models cannot be used for tasks that contain\nambiguity or require user instruction. These tasks necessitate a multimodal\nembedding model, which outputs embeddings that combine visual and natural\nlanguage input. Existing CLIP-based approaches embed images and text\nindependently, and fuse the result. We find that this results in weak\ninteractions between modalities, and poor user control over the representation.\nWe introduce ABC, an open-source multimodal embedding model that uses a\nvision-language model backbone to deeply integrate image features with natural\nlanguage instructions. ABC achieves bestfor-size performance on MSCOCO\nimage-to-text retrieval and is the top performing model on classification and\nVQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly\nunified vision-language representation, ABC can use natural language to solve\nsubtle and potentially ambiguous visual retrieval problems. To evaluate this\ncapability, we design CtrlBench, a benchmark that requires interleaving textual\ninstructions with image content for correct retrieval. ABC advances the state\nof multimodal embeddings by offering high-quality representations and flexible\nnatural language control. Our model and datasets are available at our project\npage.",
            "upvotes": 15,
            "discussionId": "67c7560298a2e37274c6311d",
            "projectPage": "https://tiger-ai-lab.github.io/ABC/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
        },
        "publishedAt": "2025-03-05T21:33:37.945Z",
        "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/b2vg-4UWwvcEboAZgK-Sv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 33
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.03751",
            "authors": [
                {
                    "_id": "67c912b1b5903dd437cc2370",
                    "user": {
                        "_id": "658529d61c461dfe88afe8e8",
                        "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
                        "isPro": false,
                        "fullname": "Xuanchi Ren",
                        "user": "xrenaa",
                        "type": "user"
                    },
                    "name": "Xuanchi Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T09:55:04.321Z",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2371",
                    "name": "Tianchang Shen",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2372",
                    "name": "Jiahui Huang",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2373",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2374",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2375",
                    "name": "Merlin Nimier-David",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2376",
                    "name": "Thomas Müller",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2377",
                    "name": "Alexander Keller",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2378",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "67c912b1b5903dd437cc2379",
                    "name": "Jun Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T18:59:50.000Z",
            "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
            "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
            "upvotes": 13,
            "discussionId": "67c912b9b5903dd437cc2505"
        },
        "publishedAt": "2025-03-05T22:13:22.552Z",
        "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6291
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.02951",
            "authors": [
                {
                    "_id": "67c907ea7568a12737ad4535",
                    "user": {
                        "_id": "653df1323479e9ebbe3eb6cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
                        "isPro": true,
                        "fullname": "Zhangchen Xu",
                        "user": "flydust",
                        "type": "user"
                    },
                    "name": "Zhangchen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:26:50.636Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4536",
                    "user": {
                        "_id": "637c88b6d55081513c5690d8",
                        "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
                        "isPro": false,
                        "fullname": "Yang Liu",
                        "user": "nlpyang",
                        "type": "user"
                    },
                    "name": "Yang Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4537",
                    "user": {
                        "_id": "605e8dfd5abeb13e714c4c18",
                        "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
                        "isPro": false,
                        "fullname": "yueqin yin",
                        "user": "yyqoni",
                        "type": "user"
                    },
                    "name": "Yueqin Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:26:48.614Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4538",
                    "user": {
                        "_id": "653b2524b77b5e255f2d29d2",
                        "avatarUrl": "/avatars/f69aea8de84c435295e7638bad5bd82e.svg",
                        "isPro": false,
                        "fullname": "Mingyuan Zhou",
                        "user": "mingyuanzhou",
                        "type": "user"
                    },
                    "name": "Mingyuan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T10:03:56.474Z",
                    "hidden": false
                },
                {
                    "_id": "67c907ea7568a12737ad4539",
                    "name": "Radha Poovendran",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T19:17:36.000Z",
            "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for\n  Coding",
            "summary": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.",
            "upvotes": 13,
            "discussionId": "67c907ee7568a12737ad4633",
            "projectPage": "https://kodcode-ai.github.io/",
            "githubRepo": "https://github.com/KodCode-AI/kodcode"
        },
        "publishedAt": "2025-03-05T21:31:01.626Z",
        "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "fullname": "Zhangchen Xu",
            "name": "flydust",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.03278",
            "authors": [
                {
                    "_id": "67c94e5f8c4ef8be73583f4b",
                    "name": "Jun Li",
                    "hidden": false
                },
                {
                    "_id": "67c94e5f8c4ef8be73583f4c",
                    "user": {
                        "_id": "631b9ff5824f2502e3557c7e",
                        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "che111",
                        "type": "user"
                    },
                    "name": "Che Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T10:03:06.882Z",
                    "hidden": false
                },
                {
                    "_id": "67c94e5f8c4ef8be73583f4d",
                    "name": "Wenjia Bai",
                    "hidden": false
                },
                {
                    "_id": "67c94e5f8c4ef8be73583f4e",
                    "name": "Rossella Arcucci",
                    "hidden": false
                },
                {
                    "_id": "67c94e5f8c4ef8be73583f4f",
                    "name": "Cosmin I. Bercea",
                    "hidden": false
                },
                {
                    "_id": "67c94e5f8c4ef8be73583f50",
                    "name": "Julia A. Schnabel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T09:02:33.000Z",
            "title": "Enhancing Abnormality Grounding for Vision Language Models with\n  Knowledge Descriptions",
            "summary": "Visual Language Models (VLMs) have demonstrated impressive capabilities in\nvisual grounding tasks. However, their effectiveness in the medical domain,\nparticularly for abnormality detection and localization within medical images,\nremains underexplored. A major challenge is the complex and abstract nature of\nmedical terminology, which makes it difficult to directly associate\npathological anomaly terms with their corresponding visual features. In this\nwork, we introduce a novel approach to enhance VLM performance in medical\nabnormality detection and localization by leveraging decomposed medical\nknowledge. Instead of directly prompting models to recognize specific\nabnormalities, we focus on breaking down medical concepts into fundamental\nattributes and common visual patterns. This strategy promotes a stronger\nalignment between textual descriptions and visual features, improving both the\nrecognition and localization of abnormalities in medical images.We evaluate our\nmethod on the 0.23B Florence-2 base model and demonstrate that it achieves\ncomparable performance in abnormality grounding to significantly larger 7B\nLLaVA-based medical VLMs, despite being trained on only 1.5% of the data used\nfor such models. Experimental results also demonstrate the effectiveness of our\napproach in both known and previously unseen abnormalities, suggesting its\nstrong generalization capabilities.",
            "upvotes": 11,
            "discussionId": "67c94e608c4ef8be73583f7b",
            "projectPage": "https://lijunrio.github.io/AG-KD/"
        },
        "publishedAt": "2025-03-06T02:29:15.964Z",
        "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.01836",
            "authors": [
                {
                    "_id": "67c94c32dd505e6a4db201a2",
                    "user": {
                        "_id": "65d9a453f20e4fc19480afba",
                        "avatarUrl": "/avatars/27bfa034a13a5e7cb5fc3b647515a201.svg",
                        "isPro": false,
                        "fullname": "yisen li",
                        "user": "yisenL",
                        "type": "user"
                    },
                    "name": "Yisen Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T10:04:09.687Z",
                    "hidden": false
                },
                {
                    "_id": "67c94c32dd505e6a4db201a3",
                    "name": "Lingfeng Yang",
                    "hidden": false
                },
                {
                    "_id": "67c94c32dd505e6a4db201a4",
                    "name": "Wenxuan Shen",
                    "hidden": false
                },
                {
                    "_id": "67c94c32dd505e6a4db201a5",
                    "name": "Pan Zhou",
                    "hidden": false
                },
                {
                    "_id": "67c94c32dd505e6a4db201a6",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "67c94c32dd505e6a4db201a7",
                    "name": "Weiwei Lin",
                    "hidden": false
                },
                {
                    "_id": "67c94c32dd505e6a4db201a8",
                    "user": {
                        "_id": "643be8879f5d314db2d9ed23",
                        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
                        "isPro": false,
                        "fullname": "Chen Dongping",
                        "user": "shuaishuaicdp",
                        "type": "user"
                    },
                    "name": "Dongping Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T10:09:00.496Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T18:56:44.000Z",
            "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
            "summary": "Distilling advanced Large Language Models' instruction-following capabilities\ninto smaller models using a selected subset has become a mainstream approach in\nmodel training. While existing synthetic instruction data selection strategies\nrely mainly on single-dimensional signals (i.e., reward scores, model\nperplexity), they fail to capture the complexity of instruction-following\nacross diverse fields. Therefore, we investigate more diverse signals to\ncapture comprehensive instruction-response pair characteristics and propose\nthree foundational metrics that leverage Multi-LLM wisdom, informed by (1)\ndiverse LLM responses and (2) reward model assessment. Building upon base\nmetrics, we propose CrowdSelect, an integrated metric incorporating a\nclustering-based approach to maintain response diversity. Our comprehensive\nexperiments demonstrate that our foundation metrics consistently improve\nperformance across 4 base models on MT-bench and Arena-Hard. CrowdSelect,\nefficiently incorporating all metrics, achieves state-of-the-art performance in\nboth Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and\n11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring\nvaluable insights for future research in this direction. Code are available at\nhttps://github.com/listentm/crowdselect.",
            "upvotes": 9,
            "discussionId": "67c94c33dd505e6a4db201f6",
            "githubRepo": "https://github.com/listentm/crowdselect"
        },
        "publishedAt": "2025-03-06T02:20:38.735Z",
        "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.01933",
            "authors": [
                {
                    "_id": "67c97d89ac48d4b9ebf73f21",
                    "user": {
                        "_id": "64e6103398cb11ef1023f1fd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/DVqwwilKU-ayTsThq6UrE.png",
                        "isPro": true,
                        "fullname": "Rakshit Aralimatti",
                        "user": "RakshitAralimatti",
                        "type": "user"
                    },
                    "name": "Rakshit Aralimatti",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-06T10:48:42.715Z",
                    "hidden": false
                },
                {
                    "_id": "67c97d89ac48d4b9ebf73f22",
                    "user": {
                        "_id": "63d9e09f1cae35c27bf80cb2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Syed Abdul Gaffar Shakhadri",
                        "user": "SyedAbdul",
                        "type": "user"
                    },
                    "name": "Syed Abdul Gaffar Shakhadri",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-06T10:50:31.321Z",
                    "hidden": false
                },
                {
                    "_id": "67c97d89ac48d4b9ebf73f23",
                    "user": {
                        "_id": "5fb7ae48e6ae537272bdeb3c",
                        "avatarUrl": "/avatars/e5d01cb428f4b22161e0d17895a5c678.svg",
                        "isPro": false,
                        "fullname": "Kruthika",
                        "user": "kruthika",
                        "type": "user"
                    },
                    "name": "Kruthika KR",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-06T10:48:42.715Z",
                    "hidden": false
                },
                {
                    "_id": "67c97d89ac48d4b9ebf73f24",
                    "user": {
                        "_id": "677cc34fe4cf361eedccd085",
                        "avatarUrl": "/avatars/e97a3f9a84ed258ab4b75c12865562d6.svg",
                        "isPro": false,
                        "fullname": "Kartik Basavaraj Angadi",
                        "user": "KartikAngadi",
                        "type": "user"
                    },
                    "name": "Kartik Basavaraj Angadi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-06T10:48:42.715Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T04:53:55.000Z",
            "title": "Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI\n  Perspective",
            "summary": "Deploying large scale language models on edge devices faces inherent\nchallenges such as high computational demands, energy consumption, and\npotential data privacy risks. This paper introduces the Shakti Small Language\nModels (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these\nconstraints headon. By combining efficient architectures, quantization\ntechniques, and responsible AI principles, the Shakti series enables on-device\nintelligence for smartphones, smart appliances, IoT systems, and beyond. We\nprovide comprehensive insights into their design philosophy, training\npipelines, and benchmark performance on both general tasks (e.g., MMLU,\nHellaswag) and specialized domains (healthcare, finance, and legal). Our\nfindings illustrate that compact models, when carefully engineered and\nfine-tuned, can meet and often exceed expectations in real-world edge-AI\nscenarios.",
            "upvotes": 6,
            "discussionId": "67c97d8aac48d4b9ebf73f3b"
        },
        "publishedAt": "2025-03-06T05:49:04.559Z",
        "title": "Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01933.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d9e09f1cae35c27bf80cb2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675223055197-noauth.jpeg",
            "fullname": "Syed Abdul Gaffar Shakhadri",
            "name": "SyedAbdul",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20317",
            "authors": [
                {
                    "_id": "67c95b2f1fcfdc62ba3a620b",
                    "name": "Yongjia Lei",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a620c",
                    "name": "Haoyu Han",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a620d",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a620e",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:24:22.970Z",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a620f",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a6210",
                    "user": {
                        "_id": "637c6d95a8716d642050b50f",
                        "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
                        "isPro": false,
                        "fullname": "Mahantesh Halappanavar",
                        "user": "mhalappa",
                        "type": "user"
                    },
                    "name": "Mahantesh M Halappanavar",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-06T08:22:09.090Z",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a6211",
                    "name": "Jiliang Tang",
                    "hidden": false
                },
                {
                    "_id": "67c95b2f1fcfdc62ba3a6212",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T17:42:52.000Z",
            "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph\n  Knowledge Bases",
            "summary": "Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for\nanswering queries by providing textual and structural knowledge. However,\ncurrent retrieval methods often retrieve these two types of knowledge in\nisolation without considering their mutual reinforcement and some hybrid\nmethods even bypass structural retrieval entirely after neighboring\naggregation. To fill in this gap, we propose a Mixture of\nStructural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge\nvia a Planning-Reasoning-Organizing framework. In the Planning stage, MoR\ngenerates textual planning graphs delineating the logic for answering queries.\nFollowing planning graphs, in the Reasoning stage, MoR interweaves structural\ntraversal and textual matching to obtain candidates from TG-KBs. In the\nOrganizing stage, MoR further reranks fetched candidates based on their\nstructural trajectory. Extensive experiments demonstrate the superiority of MoR\nin harmonizing structural and textual retrieval with insights, including uneven\nretrieving performance across different query logics and the benefits of\nintegrating structural trajectories for candidate reranking. Our code is\navailable at https://github.com/Yoega/MoR.",
            "upvotes": 6,
            "discussionId": "67c95b311fcfdc62ba3a62a5",
            "githubRepo": "https://github.com/Yoega/MoR/"
        },
        "publishedAt": "2025-03-06T03:22:14.664Z",
        "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20317.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.03044",
            "authors": [
                {
                    "_id": "67c94e6ad325e95d82f23433",
                    "user": {
                        "_id": "5e7749883d77a72421292d07",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
                        "isPro": false,
                        "fullname": "Gabriele Sarti",
                        "user": "gsarti",
                        "type": "user"
                    },
                    "name": "Gabriele Sarti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:24:30.601Z",
                    "hidden": false
                },
                {
                    "_id": "67c94e6ad325e95d82f23434",
                    "name": "Vilém Zouhar",
                    "hidden": false
                },
                {
                    "_id": "67c94e6ad325e95d82f23435",
                    "name": "Grzegorz Chrupała",
                    "hidden": false
                },
                {
                    "_id": "67c94e6ad325e95d82f23436",
                    "name": "Ana Guerberof-Arenas",
                    "hidden": false
                },
                {
                    "_id": "67c94e6ad325e95d82f23437",
                    "name": "Malvina Nissim",
                    "hidden": false
                },
                {
                    "_id": "67c94e6ad325e95d82f23438",
                    "name": "Arianna Bisazza",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T22:50:17.000Z",
            "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
            "summary": "Word-level quality estimation (QE) detects erroneous spans in machine\ntranslations, which can direct and facilitate human post-editing. While the\naccuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. Our QE4PE study investigates the impact\nof word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated by behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows.",
            "upvotes": 5,
            "discussionId": "67c94e6fd325e95d82f23524"
        },
        "publishedAt": "2025-03-06T02:30:17.431Z",
        "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "fullname": "Gabriele Sarti",
            "name": "gsarti",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 213
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.00307",
            "authors": [
                {
                    "_id": "67c9fcfc765ba582a758e083",
                    "name": "Guanghan Wang",
                    "hidden": false
                },
                {
                    "_id": "67c9fcfc765ba582a758e084",
                    "name": "Yair Schiff",
                    "hidden": false
                },
                {
                    "_id": "67c9fcfc765ba582a758e085",
                    "name": "Subham Sekhar Sahoo",
                    "hidden": false
                },
                {
                    "_id": "67c9fcfc765ba582a758e086",
                    "name": "Volodymyr Kuleshov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-01T02:37:51.000Z",
            "title": "Remasking Discrete Diffusion Models with Inference-Time Scaling",
            "summary": "Part of the success of diffusion models stems from their ability to perform\niterative refinement, i.e., repeatedly correcting outputs during generation.\nHowever, modern masked discrete diffusion lacks this capability: when a token\nis generated, it cannot be updated again, even when it introduces an error.\nHere, we address this limitation by introducing the remasking diffusion model\n(ReMDM) sampler, a method that can be applied to pretrained masked diffusion\nmodels in a principled way and that is derived from a discrete diffusion model\nwith a custom remasking backward process. Most interestingly, ReMDM endows\ndiscrete diffusion with a form of inference-time compute scaling. By increasing\nthe number of sampling steps, ReMDM generates natural language outputs that\napproach the quality of autoregressive models, whereas when the computation\nbudget is limited, ReMDM better maintains quality. ReMDM also improves sample\nquality of masked diffusion models for discretized images, and in scientific\ndomains such as molecule design, ReMDM facilitates diffusion guidance and\npushes the Pareto frontier of controllability relative to classical masking and\nuniform noise diffusion. We provide the code along with a blog post on the\nproject page: https://remdm.github.io.",
            "upvotes": 4,
            "discussionId": "67c9fcfd765ba582a758e0f0"
        },
        "publishedAt": "2025-03-06T14:57:45.551Z",
        "title": "Remasking Discrete Diffusion Models with Inference-Time Scaling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00307.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67c4c18ec88494ac4ed110a8",
            "avatarUrl": "/avatars/b44d2aca5eb2063b9fbce613c3b3fc1e.svg",
            "fullname": "Guanghan Wang",
            "name": "GuanghanWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2502.18860",
            "authors": [
                {
                    "_id": "67c95acf88c3b4201c10b9e9",
                    "name": "Md Mehrab Tanjim",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9ea",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9eb",
                    "name": "Mike Rimer",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9ec",
                    "name": "Xiang Chen",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9ed",
                    "name": "Sungchul Kim",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9ee",
                    "name": "Vaishnavi Muppala",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9ef",
                    "name": "Tong Yu",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9f0",
                    "name": "Zhengmian Hu",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9f1",
                    "name": "Ritwik Sinha",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9f2",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9f3",
                    "name": "Iftikhar Ahamath Burhanuddin",
                    "hidden": false
                },
                {
                    "_id": "67c95acf88c3b4201c10b9f4",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T09:24:24.968Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T06:05:29.000Z",
            "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
            "summary": "Conversational assistants often require a question rewriting algorithm that\nleverages a subset of past interactions to provide a more meaningful (accurate)\nanswer to the user's question or request. However, the exact rewriting approach\nmay often depend on the use case and application-specific tasks supported by\nthe conversational assistant, among other constraints. In this paper, we\nsystematically investigate two different approaches, denoted as rewriting and\nfusion, on two fundamentally different generation tasks, including a\ntext-to-text generation task and a multimodal generative task that takes as\ninput text and generates a visualization or data table that answers the user's\nquestion. Our results indicate that the specific rewriting or fusion approach\nhighly depends on the underlying use case and generative task. In particular,\nwe find that for a conversational question-answering assistant, the query\nrewriting approach performs best, whereas for a data analysis assistant that\ngenerates visualizations and data tables based on the user's conversation with\nthe assistant, the fusion approach works best. Notably, we explore two datasets\nfor the data analysis assistant use case, for short and long conversations, and\nwe find that query fusion always performs better, whereas for the\nconversational text-based question-answering, the query rewrite approach\nperforms best.",
            "upvotes": 4,
            "discussionId": "67c95acf88c3b4201c10ba22"
        },
        "publishedAt": "2025-03-06T03:20:40.127Z",
        "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18860.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01763",
            "authors": [
                {
                    "_id": "67c92e9c746bbcdbdfa8ebd4",
                    "name": "Zhengliang Shi",
                    "hidden": false
                },
                {
                    "_id": "67c92e9c746bbcdbdfa8ebd5",
                    "name": "Yuhan Wang",
                    "hidden": false
                },
                {
                    "_id": "67c92e9c746bbcdbdfa8ebd6",
                    "name": "Lingyong Yan",
                    "hidden": false
                },
                {
                    "_id": "67c92e9c746bbcdbdfa8ebd7",
                    "name": "Pengjie Ren",
                    "hidden": false
                },
                {
                    "_id": "67c92e9c746bbcdbdfa8ebd8",
                    "name": "Shuaiqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "67c92e9c746bbcdbdfa8ebd9",
                    "name": "Dawei Yin",
                    "hidden": false
                },
                {
                    "_id": "67c92e9c746bbcdbdfa8ebda",
                    "name": "Zhaochun Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T17:37:16.000Z",
            "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for\n  Large Language Models",
            "summary": "Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels.",
            "upvotes": 4,
            "discussionId": "67c92e9e746bbcdbdfa8ec57"
        },
        "publishedAt": "2025-03-06T00:12:07.867Z",
        "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d3eaa3623f6a56dde856d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
            "fullname": "vansin",
            "name": "vansin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01729",
            "authors": [
                {
                    "_id": "67c92e8b5650d7efeba5b48c",
                    "name": "Santiago Bou Betran",
                    "hidden": false
                },
                {
                    "_id": "67c92e8b5650d7efeba5b48d",
                    "name": "Alberta Longhini",
                    "hidden": false
                },
                {
                    "_id": "67c92e8b5650d7efeba5b48e",
                    "name": "Miguel Vasco",
                    "hidden": false
                },
                {
                    "_id": "67c92e8b5650d7efeba5b48f",
                    "name": "Yuchong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c92e8b5650d7efeba5b490",
                    "name": "Danica Kragic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T16:49:15.000Z",
            "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
            "summary": "Recent progress in robotic manipulation has been fueled by large-scale\ndatasets collected across diverse environments. Training robotic manipulation\npolicies on these datasets is traditionally performed in a centralized manner,\nraising concerns regarding scalability, adaptability, and data privacy. While\nfederated learning enables decentralized, privacy-preserving training, its\napplication to robotic manipulation remains largely unexplored. We introduce\nFLAME (Federated Learning Across Manipulation Environments), the first\nbenchmark designed for federated learning in robotic manipulation. FLAME\nconsists of: (i) a set of large-scale datasets of over 160,000 expert\ndemonstrations of multiple manipulation tasks, collected across a wide range of\nsimulated environments; (ii) a training and evaluation framework for robotic\npolicy learning in a federated setting. We evaluate standard federated learning\nalgorithms in FLAME, showing their potential for distributed policy learning\nand highlighting key challenges. Our benchmark establishes a foundation for\nscalable, adaptive, and privacy-aware robotic learning.",
            "upvotes": 4,
            "discussionId": "67c92e8d5650d7efeba5b519"
        },
        "publishedAt": "2025-03-06T00:11:48.501Z",
        "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d3eaa3623f6a56dde856d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
            "fullname": "vansin",
            "name": "vansin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01449",
            "authors": [
                {
                    "_id": "67c92e738d5fe8c860571103",
                    "name": "Ting Zhang",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c860571104",
                    "name": "Chengran Yang",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c860571105",
                    "name": "Yindu Su",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c860571106",
                    "name": "Martin Weyssow",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c860571107",
                    "name": "Hung Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c860571108",
                    "name": "Tan Bui",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c860571109",
                    "name": "Hong Jin Kang",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c86057110a",
                    "name": "Yikun Li",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c86057110b",
                    "name": "Eng Lieh Ouh",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c86057110c",
                    "name": "Lwin Khin Shar",
                    "hidden": false
                },
                {
                    "_id": "67c92e738d5fe8c86057110d",
                    "name": "David Lo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T11:56:00.000Z",
            "title": "Benchmarking Large Language Models for Multi-Language Software\n  Vulnerability Detection",
            "summary": "Recent advancements in generative AI have led to the widespread adoption of\nlarge language models (LLMs) in software engineering, addressing numerous\nlong-standing challenges. However, a comprehensive study examining the\ncapabilities of LLMs in software vulnerability detection (SVD), a crucial\naspect of software security, is currently lacking. Existing research primarily\nfocuses on evaluating LLMs using C/C++ datasets. It typically explores only one\nor two strategies among prompt engineering, instruction tuning, and sequence\nclassification fine-tuning for open-source LLMs. Consequently, there is a\nsignificant knowledge gap regarding the effectiveness of diverse LLMs in\ndetecting vulnerabilities across various programming languages. To address this\nknowledge gap, we present a comprehensive empirical study evaluating the\nperformance of LLMs on the SVD task. We have compiled a comprehensive dataset\ncomprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in\nJavaScript. We assess five open-source LLMs using multiple approaches,\nincluding prompt engineering, instruction tuning, and sequence classification\nfine-tuning. These LLMs are benchmarked against five fine-tuned small language\nmodels and two open-source static application security testing tools.\nFurthermore, we explore two avenues to improve LLM performance on SVD: a) Data\nperspective: Retraining models using downsampled balanced datasets. b) Model\nperspective: Investigating ensemble learning methods that combine predictions\nfrom multiple LLMs. Our comprehensive experiments demonstrate that SVD remains\na challenging task for LLMs. This study provides a thorough understanding of\nthe role of LLMs in SVD and offers practical insights for future advancements\nin leveraging generative AI to enhance software security practices.",
            "upvotes": 3,
            "discussionId": "67c92e748d5fe8c860571142"
        },
        "publishedAt": "2025-03-06T00:11:25.013Z",
        "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d3eaa3623f6a56dde856d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
            "fullname": "vansin",
            "name": "vansin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01378",
            "authors": [
                {
                    "_id": "67c92e537ae0115c7a7b9fa3",
                    "user": {
                        "_id": "641d7ac93509072bd5c29f23",
                        "avatarUrl": "/avatars/d64661cf7e4159932540c5f9e8f30b31.svg",
                        "isPro": false,
                        "fullname": "Artem Lykov",
                        "user": "ArtemLykov",
                        "type": "user"
                    },
                    "name": "Artem Lykov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T14:06:50.855Z",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9fa4",
                    "name": "Valerii Serpiva",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9fa5",
                    "name": "Muhammad Haris Khan",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9fa6",
                    "user": {
                        "_id": "67bde90dd254a3945474bfef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uRvk8mrmhy8mNlgEpRpw9.png",
                        "isPro": false,
                        "fullname": "Oleg Sautenkov",
                        "user": "Sauten",
                        "type": "user"
                    },
                    "name": "Oleg Sautenkov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T14:07:35.797Z",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9fa7",
                    "name": "Artyom Myshlyaev",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9fa8",
                    "name": "Grik Tadevosyan",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9fa9",
                    "user": {
                        "_id": "67a9c753b225716590053ea5",
                        "avatarUrl": "/avatars/75facb21fe2fb3a8055e15d75cda0ed8.svg",
                        "isPro": false,
                        "fullname": "Yasheerah Yaqoot",
                        "user": "yasheerahyaqoot",
                        "type": "user"
                    },
                    "name": "Yasheerah Yaqoot",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T14:07:22.304Z",
                    "hidden": false
                },
                {
                    "_id": "67c92e537ae0115c7a7b9faa",
                    "name": "Dzmitry Tsetserukou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T10:21:36.000Z",
            "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs",
            "summary": "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io",
            "upvotes": 2,
            "discussionId": "67c92e547ae0115c7a7b9fe6"
        },
        "publishedAt": "2025-03-06T00:10:56.364Z",
        "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d3eaa3623f6a56dde856d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
            "fullname": "vansin",
            "name": "vansin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01372",
            "authors": [
                {
                    "_id": "67c6bd6e8f3e7fd471affd06",
                    "user": {
                        "_id": "5fae5f68b8423e1d80b8a988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656078368144-5fae5f68b8423e1d80b8a988.jpeg",
                        "isPro": false,
                        "fullname": "Joel Niklaus",
                        "user": "joelniklaus",
                        "type": "user"
                    },
                    "name": "Joel Niklaus",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:39:30.778Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd07",
                    "name": "Jakob Merane",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd08",
                    "user": {
                        "_id": "66b46ea33a88f464e2842ee8",
                        "avatarUrl": "/avatars/a204b5828a96cf0d5334f238c6c96b13.svg",
                        "isPro": false,
                        "fullname": "Luka Nenadic",
                        "user": "lukanen",
                        "type": "user"
                    },
                    "name": "Luka Nenadic",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:39:40.122Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd09",
                    "user": {
                        "_id": "647530b3e0b188d3cb24b81b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647530b3e0b188d3cb24b81b/JzAMifLHLXhjkRSKeF5iQ.jpeg",
                        "isPro": false,
                        "fullname": "Sina Ahm",
                        "user": "SinaAhmadi",
                        "type": "user"
                    },
                    "name": "Sina Ahmadi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:39:55.817Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd0a",
                    "user": {
                        "_id": "645e1d5dad4ded42382ab518",
                        "avatarUrl": "/avatars/a33339d37e0ada45198452e48a918be2.svg",
                        "isPro": false,
                        "fullname": "Yingqiang Gao",
                        "user": "CharizardAcademy",
                        "type": "user"
                    },
                    "name": "Yingqiang Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:40:05.409Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd0b",
                    "name": "Cyrill A. H. Chevalley",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd0c",
                    "name": "Claude Humbel",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd0d",
                    "name": "Christophe Gösken",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd0e",
                    "name": "Lorenzo Tanzi",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd0f",
                    "name": "Thomas Lüthi",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd10",
                    "user": {
                        "_id": "6594ee0f57a556fbe1720c37",
                        "avatarUrl": "/avatars/662e965fbde86e53b86fd9c2c9dc325a.svg",
                        "isPro": false,
                        "fullname": "Stefan Palombo",
                        "user": "spalombo",
                        "type": "user"
                    },
                    "name": "Stefan Palombo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:52:02.245Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd11",
                    "user": {
                        "_id": "63c5cf2176c803a693f59763",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678689670011-63c5cf2176c803a693f59763.jpeg",
                        "isPro": false,
                        "fullname": "Spencer Poff",
                        "user": "spencerp",
                        "type": "user"
                    },
                    "name": "Spencer Poff",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:57:18.336Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd12",
                    "user": {
                        "_id": "65d505dd1e2597ff935d3b03",
                        "avatarUrl": "/avatars/8f7f2bdf0e93a2442b993d064983126f.svg",
                        "isPro": false,
                        "fullname": "Boling Yang",
                        "user": "BolingYang",
                        "type": "user"
                    },
                    "name": "Boling Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:57:11.493Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd13",
                    "name": "Nan Wu",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd14",
                    "user": {
                        "_id": "65d3ae44ea28ba508b5c62fd",
                        "avatarUrl": "/avatars/590cbb68787bd2706a2c2dd711274d61.svg",
                        "isPro": false,
                        "fullname": "Matthew Guillod",
                        "user": "mguillod",
                        "type": "user"
                    },
                    "name": "Matthew Guillod",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:57:04.689Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd15",
                    "user": {
                        "_id": "64df37df92f3bd97867e2122",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df37df92f3bd97867e2122/IfXh4jk5QZ_Pc6tlH8JqY.jpeg",
                        "isPro": false,
                        "fullname": "Robin Mamié",
                        "user": "robinmamie",
                        "type": "user"
                    },
                    "name": "Robin Mamié",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:56:57.175Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd16",
                    "user": {
                        "_id": "6630002ad2f21fa96c0780f2",
                        "avatarUrl": "/avatars/65b33ed0f4afb038fe23ebd98c655a53.svg",
                        "isPro": false,
                        "fullname": "Daniel Brunner",
                        "user": "ravecs",
                        "type": "user"
                    },
                    "name": "Daniel Brunner",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:56:50.083Z",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd17",
                    "name": "Julio Pereyra",
                    "hidden": false
                },
                {
                    "_id": "67c6bd6e8f3e7fd471affd18",
                    "user": {
                        "_id": "642c89fb3a6a3fa1ebaf95bc",
                        "avatarUrl": "/avatars/9a1da6fb5e0bf732e1ac29d0722f46db.svg",
                        "isPro": false,
                        "fullname": "Niko Grupen",
                        "user": "nikogrupen",
                        "type": "user"
                    },
                    "name": "Niko Grupen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:56:18.392Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T10:10:30.000Z",
            "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
            "summary": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments.",
            "upvotes": 2,
            "discussionId": "67c6bd708f3e7fd471affd5d"
        },
        "publishedAt": "2025-03-06T00:10:21.173Z",
        "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d3eaa3623f6a56dde856d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
            "fullname": "vansin",
            "name": "vansin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.00502",
            "authors": [
                {
                    "_id": "67c8427047c2aa135346dced",
                    "user": {
                        "_id": "66d3290364c1e9b73208af82",
                        "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
                        "isPro": false,
                        "fullname": "Shiyu Fang",
                        "user": "FanGShiYuu",
                        "type": "user"
                    },
                    "name": "Shiyu Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-05T15:46:48.691Z",
                    "hidden": false
                },
                {
                    "_id": "67c8427047c2aa135346dcee",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "67c8427047c2aa135346dcef",
                    "user": {
                        "_id": "6319e98c02fb3220373762f6",
                        "avatarUrl": "/avatars/1456665d2ae6a284e2505744fec0cb1d.svg",
                        "isPro": false,
                        "fullname": "Chengkai Xu",
                        "user": "Shinghoi",
                        "type": "user"
                    },
                    "name": "Chengkai Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:57:59.218Z",
                    "hidden": false
                },
                {
                    "_id": "67c8427047c2aa135346dcf0",
                    "user": {
                        "_id": "670bcbbcdb7d982eb6a7e027",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2aSOLBxM_MgGtK0EFJAKL.png",
                        "isPro": false,
                        "fullname": "Chen Lv",
                        "user": "zafkiellc",
                        "type": "user"
                    },
                    "name": "Chen Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T13:57:52.421Z",
                    "hidden": false
                },
                {
                    "_id": "67c8427047c2aa135346dcf1",
                    "name": "Peng Hang",
                    "hidden": false
                },
                {
                    "_id": "67c8427047c2aa135346dcf2",
                    "user": {
                        "_id": "6522245b54967a3a49b3b22b",
                        "avatarUrl": "/avatars/18d190808f91eaff4b81cec1e9690337.svg",
                        "isPro": false,
                        "fullname": "Jian SUN",
                        "user": "jiansun-china",
                        "type": "user"
                    },
                    "name": "Jian Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-06T14:06:29.074Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-01T14:15:52.000Z",
            "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner\n  Framework for Enhancing Autonomous Vehicle Interactions",
            "summary": "Autonomous Vehicles (AVs) have entered the commercialization stage, but their\nlimited ability to interact and express intentions still poses challenges in\ninteractions with Human-driven Vehicles (HVs). Recent advances in large\nlanguage models (LLMs) enable bidirectional human-machine communication, but\nthe conflict between slow inference speed and the need for real-time\ndecision-making challenges practical deployment. To address these issues, this\npaper introduces a parallel Actor-Reasoner framework designed to enable\nexplicit bidirectional AV-HV interactions across multiple scenarios. First, by\nfacilitating interactions between the LLM-driven Reasoner and heterogeneous\nsimulated HVs during training, an interaction memory database, referred to as\nthe Actor, is established. Then, by introducing the memory partition module and\nthe two-layer memory retrieval module, the Actor's ability to handle\nheterogeneous HVs is significantly enhanced. Ablation studies and comparisons\nwith other decision-making methods demonstrate that the proposed Actor-Reasoner\nframework significantly improves safety and efficiency. Finally, with the\ncombination of the external Human-Machine Interface (eHMI) information derived\nfrom Reasoner's reasoning and the feasible action solutions retrieved from the\nActor, the effectiveness of the proposed Actor-Reasoner is confirmed in\nmulti-scenario field interactions. Our code is available at\nhttps://github.com/FanGShiYuu/Actor-Reasoner.",
            "upvotes": 2,
            "discussionId": "67c8427247c2aa135346dd84",
            "projectPage": "https://fangshiyuu.github.io/Actor-Reasoner/",
            "githubRepo": "https://github.com/FanGShiYuu/Actor-Reasoner"
        },
        "publishedAt": "2025-03-05T21:37:18.981Z",
        "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d3290364c1e9b73208af82",
            "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
            "fullname": "Shiyu Fang",
            "name": "FanGShiYuu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02954",
            "authors": [
                {
                    "_id": "67c9d3c531e28a674096d786",
                    "user": {
                        "_id": "668e100b97171f3399e07f5d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
                        "isPro": false,
                        "fullname": "Yue Meng",
                        "user": "yuemithucsd",
                        "type": "user"
                    },
                    "name": "Yue Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T19:50:38.026Z",
                    "hidden": false
                },
                {
                    "_id": "67c9d3c531e28a674096d787",
                    "name": "Nathalie Majcherczyk",
                    "hidden": false
                },
                {
                    "_id": "67c9d3c531e28a674096d788",
                    "name": "Wenliang Liu",
                    "hidden": false
                },
                {
                    "_id": "67c9d3c531e28a674096d789",
                    "name": "Scott Kiesel",
                    "hidden": false
                },
                {
                    "_id": "67c9d3c531e28a674096d78a",
                    "name": "Chuchu Fan",
                    "hidden": false
                },
                {
                    "_id": "67c9d3c531e28a674096d78b",
                    "name": "Federico Pecora",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T19:20:11.000Z",
            "title": "Reliable and Efficient Multi-Agent Coordination via Graph Neural Network\n  Variational Autoencoders",
            "summary": "Multi-agent coordination is crucial for reliable multi-robot navigation in\nshared spaces such as automated warehouses. In regions of dense robot traffic,\nlocal coordination methods may fail to find a deadlock-free solution. In these\nscenarios, it is appropriate to let a central unit generate a global schedule\nthat decides the passing order of robots. However, the runtime of such\ncentralized coordination methods increases significantly with the problem\nscale. In this paper, we propose to leverage Graph Neural Network Variational\nAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale\nfaster than through centralized optimization. We formulate the coordination\nproblem as a graph problem and collect ground truth data using a Mixed-Integer\nLinear Program (MILP) solver. During training, our learning framework encodes\ngood quality solutions of the graph problem into a latent space. At inference\ntime, solution samples are decoded from the sampled latent variables, and the\nlowest-cost sample is selected for coordination. Finally, the feasible proposal\nwith the highest performance index is selected for the deployment. By\nconstruction, our GNN-VAE framework returns solutions that always respect the\nconstraints of the considered coordination problem. Numerical results show that\nour approach trained on small-scale problems can achieve high-quality solutions\neven for large-scale problems with 250 robots, being much faster than other\nbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord",
            "upvotes": 1,
            "discussionId": "67c9d3c631e28a674096d7be"
        },
        "publishedAt": "2025-03-06T16:16:32.387Z",
        "title": "Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02954.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668e100b97171f3399e07f5d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
            "fullname": "Yue Meng",
            "name": "yuemithucsd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02924",
            "authors": [
                {
                    "_id": "67c9d3801e7c4f57b44a958d",
                    "user": {
                        "_id": "668e100b97171f3399e07f5d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
                        "isPro": false,
                        "fullname": "Yue Meng",
                        "user": "yuemithucsd",
                        "type": "user"
                    },
                    "name": "Yue Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-06T19:50:39.656Z",
                    "hidden": false
                },
                {
                    "_id": "67c9d3801e7c4f57b44a958e",
                    "name": "Chuchu fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T18:59:00.000Z",
            "title": "Diverse Controllable Diffusion Policy with Signal Temporal Logic",
            "summary": "Generating realistic simulations is critical for autonomous system\napplications such as self-driving and human-robot interactions. However,\ndriving simulators nowadays still have difficulty in generating controllable,\ndiverse, and rule-compliant behaviors for road participants: Rule-based models\ncannot produce diverse behaviors and require careful tuning, whereas\nlearning-based methods imitate the policy from data but are not designed to\nfollow the rules explicitly. Besides, the real-world datasets are by nature\n\"single-outcome\", making the learning method hard to generate diverse\nbehaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion\nModels to learn controllable, diverse, and rule-aware policy. We first\ncalibrate the STL on the real-world data, then generate diverse synthetic data\nusing trajectory optimization, and finally learn the rectified diffusion policy\non the augmented dataset. We test on the NuScenes dataset and our approach can\nachieve the most diverse rule-compliant trajectories compared to other\nbaselines, with a runtime 1/17X to the second-best approach. In the closed-loop\ntesting, our approach reaches the highest diversity, rule satisfaction rate,\nand the least collision rate. Our method can generate varied characteristics\nconditional on different STL parameters in testing. A case study on human-robot\nencounter scenarios shows our approach can generate diverse and\nclosed-to-oracle trajectories. The annotation tool, augmented dataset, and code\nare available at https://github.com/mengyuest/pSTL-diffusion-policy.",
            "upvotes": 0,
            "discussionId": "67c9d3821e7c4f57b44a967e"
        },
        "publishedAt": "2025-03-06T16:10:16.064Z",
        "title": "Diverse Controllable Diffusion Policy with Signal Temporal Logic",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668e100b97171f3399e07f5d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
            "fullname": "Yue Meng",
            "name": "yuemithucsd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]