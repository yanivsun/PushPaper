[
    {
        "paper": {
            "id": "2502.21263",
            "authors": [
                {
                    "_id": "67ced7b271b2e0c1f985fb3a",
                    "user": {
                        "_id": "67471a8767d4205e5c6a2026",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qCPd8Das5IZ4GugFxpIIU.png",
                        "isPro": false,
                        "fullname": "Alexandr Nesterov",
                        "user": "newneonestor",
                        "type": "user"
                    },
                    "name": "Aleksandr Nesterov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T17:39:05.726Z",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb3b",
                    "user": {
                        "_id": "6504b078074aa8f31081f528",
                        "avatarUrl": "/avatars/63b88efd9a51c4912b98da0ea874e63b.svg",
                        "isPro": false,
                        "fullname": "Andrei Sakhovskii",
                        "user": "andorei",
                        "type": "user"
                    },
                    "name": "Andrey Sakhovskiy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T17:39:07.898Z",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb3c",
                    "user": {
                        "_id": "61dedb1b2066746d68b63adb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg",
                        "isPro": false,
                        "fullname": "Ivan Sviridov",
                        "user": "univanxx",
                        "type": "user"
                    },
                    "name": "Ivan Sviridov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:46:22.500Z",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb3d",
                    "name": "Airat Valiev",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb3e",
                    "user": {
                        "_id": "6563a8dd8fb38d71f77ee20a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WthtysRoxzNvT5AlIwPlR.jpeg",
                        "isPro": false,
                        "fullname": "Vladimir Makharev",
                        "user": "sm1rk",
                        "type": "user"
                    },
                    "name": "Vladimir Makharev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:46:24.513Z",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb3f",
                    "user": {
                        "_id": "64f734b08e7fa6529fb693f1",
                        "avatarUrl": "/avatars/a50c7e1d22174f06fbaf99e8caca7c37.svg",
                        "isPro": false,
                        "fullname": "Petr Anokhin",
                        "user": "petranokhin",
                        "type": "user"
                    },
                    "name": "Petr Anokhin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T17:39:10.948Z",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb40",
                    "name": "Galina Zubkova",
                    "hidden": false
                },
                {
                    "_id": "67ced7b271b2e0c1f985fb41",
                    "user": {
                        "_id": "662f8d645c4db70c77a203b0",
                        "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
                        "isPro": false,
                        "fullname": "Elena Tutubalina",
                        "user": "tlenusik",
                        "type": "user"
                    },
                    "name": "Elena Tutubalina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T13:35:18.779Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-28T17:40:24.000Z",
            "title": "RuCCoD: Towards Automated ICD Coding in Russian",
            "summary": "This study investigates the feasibility of automating clinical coding in\nRussian, a language with limited biomedical resources. We present a new dataset\nfor ICD coding, which includes diagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD\ncodes. This dataset serves as a benchmark for several state-of-the-art models,\nincluding BERT, LLaMA with LoRA, and RAG, with additional experiments examining\ntransfer learning across domains (from PubMed abstracts to medical diagnosis)\nand terminologies (from UMLS concepts to ICD codes). We then apply the\nbest-performing model to label an in-house EHR dataset containing patient\nhistories from 2017 to 2021. Our experiments, conducted on a carefully curated\ntest set, demonstrate that training with the automated predicted codes leads to\na significant improvement in accuracy compared to manually annotated data from\nphysicians. We believe our findings offer valuable insights into the potential\nfor automating clinical coding in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and data accuracy in these contexts.",
            "upvotes": 110,
            "discussionId": "67ced7b471b2e0c1f985fbb3",
            "ai_keywords": [
                "BERT",
                "LLaMA with LoRA",
                "RAG",
                "transfer learning",
                "electronic health records (EHRs)",
                "ICD coding",
                "UMLS concepts",
                "automated predicted codes"
            ]
        },
        "publishedAt": "2025-02-28T12:40:24.000Z",
        "title": "RuCCoD: Towards Automated ICD Coding in Russian",
        "summary": "This study investigates the feasibility of automating clinical coding in\nRussian, a language with limited biomedical resources. We present a new dataset\nfor ICD coding, which includes diagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD\ncodes. This dataset serves as a benchmark for several state-of-the-art models,\nincluding BERT, LLaMA with LoRA, and RAG, with additional experiments examining\ntransfer learning across domains (from PubMed abstracts to medical diagnosis)\nand terminologies (from UMLS concepts to ICD codes). We then apply the\nbest-performing model to label an in-house EHR dataset containing patient\nhistories from 2017 to 2021. Our experiments, conducted on a carefully curated\ntest set, demonstrate that training with the automated predicted codes leads to\na significant improvement in accuracy compared to manually annotated data from\nphysicians. We believe our findings offer valuable insights into the potential\nfor automating clinical coding in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and data accuracy in these contexts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.21263.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05236",
            "authors": [
                {
                    "_id": "67ce37239f9aaaae837f3894",
                    "user": {
                        "_id": "654c6845bac6e6e49895a5b5",
                        "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
                        "isPro": false,
                        "fullname": "Yibin Wang",
                        "user": "CodeGoat24",
                        "type": "user"
                    },
                    "name": "Yibin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:37:51.835Z",
                    "hidden": false
                },
                {
                    "_id": "67ce37239f9aaaae837f3895",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:01:24.660Z",
                    "hidden": false
                },
                {
                    "_id": "67ce37239f9aaaae837f3896",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "67ce37239f9aaaae837f3897",
                    "name": "Cheng Jin",
                    "hidden": false
                },
                {
                    "_id": "67ce37239f9aaaae837f3898",
                    "user": {
                        "_id": "64638c4d51fa6e63060521b5",
                        "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
                        "isPro": false,
                        "fullname": "JIaqi",
                        "user": "Jiaqiwang",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:38:17.938Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T08:36:05.000Z",
            "title": "Unified Reward Model for Multimodal Understanding and Generation",
            "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
            "upvotes": 87,
            "discussionId": "67ce37259f9aaaae837f3948",
            "projectPage": "https://codegoat24.github.io/UnifiedReward/",
            "githubRepo": "https://github.com/CodeGoat24/UnifiedReward",
            "ai_keywords": [
                "reward models",
                "preference optimization",
                "multimodal generation",
                "multimodal understanding",
                "UnifiedReward",
                "pairwise ranking",
                "pointwise scoring",
                "preference alignment",
                "vision model preference alignment",
                "Direct Preference Optimization (DPO)"
            ]
        },
        "publishedAt": "2025-03-07T03:36:05.000Z",
        "title": "Unified Reward Model for Multimodal Understanding and Generation",
        "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05500",
            "authors": [
                {
                    "_id": "67ce9626e5cdfda52b9e8839",
                    "user": {
                        "_id": "62be186a5f59ff2320e6e32b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
                        "isPro": false,
                        "fullname": "Nicolas-BZRD",
                        "user": "Nicolas-BZRD",
                        "type": "user"
                    },
                    "name": "Nicolas Boizard",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:42:06.860Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e883a",
                    "user": {
                        "_id": "65fa95405355a52c784633fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
                        "isPro": false,
                        "fullname": "Hippolyte Gisserot-Boukhlef",
                        "user": "hgissbkh",
                        "type": "user"
                    },
                    "name": "Hippolyte Gisserot-Boukhlef",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:42:13.176Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e883b",
                    "user": {
                        "_id": "64132452d8a418df415a6ded",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64132452d8a418df415a6ded/qkjL5G89uldHUXlCI3n4f.jpeg",
                        "isPro": false,
                        "fullname": "Duarte Alves",
                        "user": "DuarteMRAlves",
                        "type": "user"
                    },
                    "name": "Duarte M. Alves",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:42:23.055Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e883c",
                    "name": "André Martins",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e883d",
                    "user": {
                        "_id": "63937b399762cdd66be2a32f",
                        "avatarUrl": "/avatars/7aefd888a3c54673d5881dcef61f771b.svg",
                        "isPro": false,
                        "fullname": "Ayoub Hammal",
                        "user": "ayoubhammal",
                        "type": "user"
                    },
                    "name": "Ayoub Hammal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:42:42.527Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e883e",
                    "user": {
                        "_id": "677bedd522ca8585ede98470",
                        "avatarUrl": "/avatars/54bca410c446610f02aca55918c74518.svg",
                        "isPro": false,
                        "fullname": "Caio Corro",
                        "user": "caiocorro",
                        "type": "user"
                    },
                    "name": "Caio Corro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:42:48.603Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e883f",
                    "user": {
                        "_id": "61efea03a57920a251ec19b8",
                        "avatarUrl": "/avatars/f47c8e3cb17a2bf7d43f2c152bb86885.svg",
                        "isPro": false,
                        "fullname": "Celine Hudelot",
                        "user": "CelineH",
                        "type": "user"
                    },
                    "name": "Céline Hudelot",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:51:41.273Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8840",
                    "user": {
                        "_id": "66f2d6a684a241caac8e16dc",
                        "avatarUrl": "/avatars/81acb87c2b07bea938251b40a2139911.svg",
                        "isPro": false,
                        "fullname": "Emmanuel Malherbe",
                        "user": "emmanuelmalherbe",
                        "type": "user"
                    },
                    "name": "Emmanuel Malherbe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:51:47.996Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8841",
                    "name": "Etienne Malaboeuf",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8842",
                    "user": {
                        "_id": "6708db59caf70ddea8e1355d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6708db59caf70ddea8e1355d/C6T16AdpqoeWCk7Gg9wSH.jpeg",
                        "isPro": false,
                        "fullname": "Fanny Jourdan",
                        "user": "Fannyjrd",
                        "type": "user"
                    },
                    "name": "Fanny Jourdan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:09.223Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8843",
                    "user": {
                        "_id": "67cafedda972115e89972cd7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P_xComqG9IttvluN-6tyB.png",
                        "isPro": false,
                        "fullname": "Gabriel Hautreux",
                        "user": "GabrielHau",
                        "type": "user"
                    },
                    "name": "Gabriel Hautreux",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:15.512Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8844",
                    "user": {
                        "_id": "6772bde5c997eeb5550e80ea",
                        "avatarUrl": "/avatars/8134a4d9330317e748dc7b33e1bb25f6.svg",
                        "isPro": false,
                        "fullname": "João Alves",
                        "user": "albusonrails",
                        "type": "user"
                    },
                    "name": "João Alves",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:22.630Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8845",
                    "user": {
                        "_id": "66e2c22d7cc3edd60d725267",
                        "avatarUrl": "/avatars/b217c5708c7dba8b1c220f37984ccc1e.svg",
                        "isPro": false,
                        "fullname": "Kevin El Haddad",
                        "user": "kelhad",
                        "type": "user"
                    },
                    "name": "Kevin El-Haddad",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:31.191Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8846",
                    "user": {
                        "_id": "60f2e021adf471cbdf8bb660",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
                        "isPro": false,
                        "fullname": "Manuel Faysse",
                        "user": "manu",
                        "type": "user"
                    },
                    "name": "Manuel Faysse",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:38.114Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8847",
                    "user": {
                        "_id": "6369394dd322a76e1ea4bdf6",
                        "avatarUrl": "/avatars/a4e5ab0167025fbbfc970d54630ce754.svg",
                        "isPro": false,
                        "fullname": "Maxime Peyrard",
                        "user": "peyrardm",
                        "type": "user"
                    },
                    "name": "Maxime Peyrard",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:44.389Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8848",
                    "user": {
                        "_id": "67b622d2df3a86fbca306c43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lNlshrl56oaKslArMzSzj.png",
                        "isPro": false,
                        "fullname": "Nuno  Guerreiro",
                        "user": "nunogj",
                        "type": "user"
                    },
                    "name": "Nuno M. Guerreiro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:52:54.367Z",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e8849",
                    "name": "Patrick Fernandes",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e884a",
                    "name": "Ricardo Rei",
                    "hidden": false
                },
                {
                    "_id": "67ce9626e5cdfda52b9e884b",
                    "user": {
                        "_id": "644a900e3a619fe72b14af0f",
                        "avatarUrl": "/avatars/e2d5dac3d92757ed48e37e126a3464a3.svg",
                        "isPro": false,
                        "fullname": "Colombo",
                        "user": "PierreColombo",
                        "type": "user"
                    },
                    "name": "Pierre Colombo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:41:26.353Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T15:13:58.000Z",
            "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
            "summary": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
            "upvotes": 58,
            "discussionId": "67ce9627e5cdfda52b9e88a4",
            "ai_keywords": [
                "bidirectional encoder models",
                "generative decoder-only models",
                "multilingual encoders",
                "EuroBERT",
                "multilingual capabilities",
                "sequences of up to 8,192 tokens",
                "intermediate training checkpoints",
                "training framework"
            ]
        },
        "publishedAt": "2025-03-07T10:13:58.000Z",
        "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
        "summary": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png",
        "numComments": 9,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05085",
            "authors": [
                {
                    "_id": "67ced88fa0db83a841e09be6",
                    "user": {
                        "_id": "62d4cdbdb9e2ec814c0e0659",
                        "avatarUrl": "/avatars/616dd89be0fe8937b32c3f23c69a4e15.svg",
                        "isPro": false,
                        "fullname": "feng jiang",
                        "user": "liuxuan320",
                        "type": "user"
                    },
                    "name": "Feng Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:46:04.417Z",
                    "hidden": false
                },
                {
                    "_id": "67ced88fa0db83a841e09be7",
                    "user": {
                        "_id": "67061847e343e345b777d574",
                        "avatarUrl": "/avatars/6b6f35aa563dcf32937c4b6a3cdd870e.svg",
                        "isPro": false,
                        "fullname": "Zhiyu",
                        "user": "zylin1",
                        "type": "user"
                    },
                    "name": "Zhiyu Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T13:35:13.930Z",
                    "hidden": false
                },
                {
                    "_id": "67ced88fa0db83a841e09be8",
                    "user": {
                        "_id": "668e7f46c243a12604035758",
                        "avatarUrl": "/avatars/35bd20032fafb7d7603266cf9a72d1e0.svg",
                        "isPro": false,
                        "fullname": "Fan Bu",
                        "user": "FanBuCUHK",
                        "type": "user"
                    },
                    "name": "Fan Bu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T13:35:16.297Z",
                    "hidden": false
                },
                {
                    "_id": "67ced88fa0db83a841e09be9",
                    "name": "Yuhao Du",
                    "hidden": false
                },
                {
                    "_id": "67ced88fa0db83a841e09bea",
                    "user": {
                        "_id": "637c6703ca8542a0ba900ccb",
                        "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Benyou",
                        "type": "user"
                    },
                    "name": "Benyou Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T13:35:11.668Z",
                    "hidden": false
                },
                {
                    "_id": "67ced88fa0db83a841e09beb",
                    "name": "Haizhou Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T02:07:00.000Z",
            "title": "S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information",
            "summary": "The rapid development of large language models (LLMs) has brought significant\nattention to speech models, particularly recent progress in speech2speech\nprotocols supporting speech input and output. However, the existing benchmarks\nadopt automatic text-based evaluators for evaluating the instruction following\nability of these models lack consideration for paralinguistic information in\nboth speech understanding and generation. To address these issues, we introduce\nS2S-Arena, a novel arena-style S2S benchmark that evaluates\ninstruction-following capabilities with paralinguistic information in both\nspeech-in and speech-out across real-world tasks. We design 154 samples that\nfused TTS and live recordings in four domains with 21 tasks and manually\nevaluate existing popular speech models in an arena-style manner. The\nexperimental results show that: (1) in addition to the superior performance of\nGPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly\ntrained model after text-speech alignment in speech2speech protocols; (2)\nconsidering paralinguistic information, the knowledgeability of the speech\nmodel mainly depends on the LLM backbone, and the multilingual support of that\nis limited by the speech module; (3) excellent speech models can already\nunderstand the paralinguistic information in speech input, but generating\nappropriate audio with paralinguistic information is still a challenge.",
            "upvotes": 39,
            "discussionId": "67ced890a0db83a841e09c1b",
            "githubRepo": "https://github.com/FreedomIntelligence/S2S-Arena",
            "ai_keywords": [
                "large language models (LLMs)",
                "speech2speech protocols",
                "speech input and output",
                "speech models",
                "paralinguistic information",
                "speech understanding",
                "speech generation",
                "S2S-Arena",
                "arena-style S2S benchmark",
                "instruction-following capabilities",
                "TTS (Text-to-Speech)",
                "live recordings",
                "cascaded ASR (Automatic Speech Recognition)",
                "jointly trained model",
                "text-speech alignment",
                "knowledgeability",
                "multilingual support",
                "speech module"
            ]
        },
        "publishedAt": "2025-03-06T21:07:00.000Z",
        "title": "S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information",
        "summary": "The rapid development of large language models (LLMs) has brought significant\nattention to speech models, particularly recent progress in speech2speech\nprotocols supporting speech input and output. However, the existing benchmarks\nadopt automatic text-based evaluators for evaluating the instruction following\nability of these models lack consideration for paralinguistic information in\nboth speech understanding and generation. To address these issues, we introduce\nS2S-Arena, a novel arena-style S2S benchmark that evaluates\ninstruction-following capabilities with paralinguistic information in both\nspeech-in and speech-out across real-world tasks. We design 154 samples that\nfused TTS and live recordings in four domains with 21 tasks and manually\nevaluate existing popular speech models in an arena-style manner. The\nexperimental results show that: (1) in addition to the superior performance of\nGPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly\ntrained model after text-speech alignment in speech2speech protocols; (2)\nconsidering paralinguistic information, the knowledgeability of the speech\nmodel mainly depends on the LLM backbone, and the multilingual support of that\nis limited by the speech module; (3) excellent speech models can already\nunderstand the paralinguistic information in speech input, but generating\nappropriate audio with paralinguistic information is still a challenge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05085.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05179",
            "authors": [
                {
                    "_id": "67ce4bff5847e4787a7ebedd",
                    "user": {
                        "_id": "65f4060754ecda1ecb5797a0",
                        "avatarUrl": "/avatars/f8b44524d36b505673cb538fd7895a82.svg",
                        "isPro": false,
                        "fullname": "Simon Aytes",
                        "user": "saytes",
                        "type": "user"
                    },
                    "name": "Simon A. Aytes",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:01:10.363Z",
                    "hidden": false
                },
                {
                    "_id": "67ce4bff5847e4787a7ebede",
                    "user": {
                        "_id": "63036b6c5c70c21d0ea79d48",
                        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                        "isPro": false,
                        "fullname": "Jinheon Baek",
                        "user": "jinheon",
                        "type": "user"
                    },
                    "name": "Jinheon Baek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:01:13.328Z",
                    "hidden": false
                },
                {
                    "_id": "67ce4bff5847e4787a7ebedf",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T06:57:17.000Z",
            "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
            "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
            "upvotes": 37,
            "discussionId": "67ce4c035847e4787a7ebf4c",
            "projectPage": "https://huggingface.co/saytes/SoT_DistilBERT",
            "githubRepo": "https://github.com/SimonAytes/SoT",
            "ai_keywords": [
                "Sketch-of-Thought (SoT)",
                "Chain of Thought (CoT)",
                "token usage",
                "reasoning accuracy",
                "Conceptual Chaining",
                "Chunked Symbolism",
                "Expert Lexicons",
                "lightweight routing model",
                "multimodal scenarios",
                "reasoning datasets",
                "mathematical reasoning",
                "multi-hop reasoning"
            ]
        },
        "publishedAt": "2025-03-07T01:57:17.000Z",
        "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
        "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05132",
            "authors": [
                {
                    "_id": "67ce5ec17c6e6ea1cc5649c2",
                    "user": {
                        "_id": "6565ebeda0623adbd76642f3",
                        "avatarUrl": "/avatars/5b11f4aabd82ce543ad8db0fe016a0f9.svg",
                        "isPro": true,
                        "fullname": "Hengguang Zhou",
                        "user": "Dolphin42",
                        "type": "user"
                    },
                    "name": "Hengguang Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:25:19.828Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ec17c6e6ea1cc5649c3",
                    "user": {
                        "_id": "6534a434e778506c5b1e5be8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6534a434e778506c5b1e5be8/n_s7KqjSBOD8E31c1gEtw.jpeg",
                        "isPro": true,
                        "fullname": "Xirui Li",
                        "user": "AIcell",
                        "type": "user"
                    },
                    "name": "Xirui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:25:01.867Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ec17c6e6ea1cc5649c4",
                    "name": "Ruochen Wang",
                    "hidden": false
                },
                {
                    "_id": "67ce5ec17c6e6ea1cc5649c5",
                    "name": "Minhao Cheng",
                    "hidden": false
                },
                {
                    "_id": "67ce5ec17c6e6ea1cc5649c6",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:26:09.708Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ec17c6e6ea1cc5649c7",
                    "name": "Cho-Jui Hsieh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T04:21:47.000Z",
            "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
            "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
            "upvotes": 29,
            "discussionId": "67ce5ec27c6e6ea1cc564a01",
            "githubRepo": "https://github.com/turningpoint-ai/VisualThinker-R1-Zero",
            "ai_keywords": [
                "reinforcement learning",
                "self-reflection",
                "multimodal reasoning",
                "Qwen2-VL-2B",
                "SAT dataset",
                "CVBench",
                "instruct models",
                "trivial reasoning trajectories"
            ]
        },
        "publishedAt": "2025-03-06T23:21:47.000Z",
        "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
        "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.02130",
            "authors": [
                {
                    "_id": "67cc697fa029f09af72cca01",
                    "user": {
                        "_id": "6694cc1009326cb83f2d11bb",
                        "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
                        "isPro": false,
                        "fullname": "Zhixuan Lin",
                        "user": "zhixuan-lin",
                        "type": "user"
                    },
                    "name": "Zhixuan Lin",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-08T16:00:21.933Z",
                    "hidden": false
                },
                {
                    "_id": "67cc697fa029f09af72cca02",
                    "user": {
                        "_id": "64234eadd654afd6931a288b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UTU-XcO_ssKpIYr5MBujK.jpeg",
                        "isPro": false,
                        "fullname": "Evgenii Nikishin",
                        "user": "nikishin",
                        "type": "user"
                    },
                    "name": "Evgenii Nikishin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:54:09.945Z",
                    "hidden": false
                },
                {
                    "_id": "67cc697fa029f09af72cca03",
                    "user": {
                        "_id": "66906c4e37eadb9c577984d3",
                        "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
                        "isPro": false,
                        "fullname": "Owen He",
                        "user": "littleowen",
                        "type": "user"
                    },
                    "name": "Xu Owen He",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-08T16:00:01.392Z",
                    "hidden": false
                },
                {
                    "_id": "67cc697fa029f09af72cca04",
                    "name": "Aaron Courville",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T23:35:23.000Z",
            "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
            "summary": "An essential component of modern recurrent sequence models is the forget\ngate. While Transformers do not have an explicit recurrent form, we show that a\nforget gate can be naturally incorporated into Transformers by down-weighting\nthe unnormalized attention scores in a data-dependent way. We name this\nattention mechanism the Forgetting Attention and the resulting model the\nForgetting Transformer (FoX). We show that FoX outperforms the Transformer on\nlong-context language modeling, length extrapolation, and short-context\ndownstream tasks, while performing on par with the Transformer on long-context\ndownstream tasks. Moreover, it is compatible with the FlashAttention algorithm\nand does not require any positional embeddings. Several analyses, including the\nneedle-in-the-haystack test, show that FoX also retains the Transformer's\nsuperior long-context capabilities over recurrent sequence models such as\nMamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that\nincorporates some common architectural components in recurrent sequence models\nand find it significantly improves the performance of both FoX and the\nTransformer. Our code is available at\nhttps://github.com/zhixuan-lin/forgetting-transformer.",
            "upvotes": 19,
            "discussionId": "67cc6981a029f09af72ccac1",
            "githubRepo": "https://github.com/zhixuan-lin/forgetting-transformer",
            "ai_keywords": [
                "recurrent sequence models",
                "forget gate",
                "Transformers",
                "Forgetting Attention",
                "Forgetting Transformer (FoX)",
                "long-context language modeling",
                "length extrapolation",
                "short-context downstream tasks",
                "long-context downstream tasks",
                "FlashAttention algorithm",
                "positional embeddings",
                "needle-in-the-haystack test",
                "long-context capabilities",
                "Mamba-2",
                "HGRN2",
                "DeltaNet",
                "Pro block design"
            ]
        },
        "publishedAt": "2025-03-03T18:35:23.000Z",
        "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
        "summary": "An essential component of modern recurrent sequence models is the forget\ngate. While Transformers do not have an explicit recurrent form, we show that a\nforget gate can be naturally incorporated into Transformers by down-weighting\nthe unnormalized attention scores in a data-dependent way. We name this\nattention mechanism the Forgetting Attention and the resulting model the\nForgetting Transformer (FoX). We show that FoX outperforms the Transformer on\nlong-context language modeling, length extrapolation, and short-context\ndownstream tasks, while performing on par with the Transformer on long-context\ndownstream tasks. Moreover, it is compatible with the FlashAttention algorithm\nand does not require any positional embeddings. Several analyses, including the\nneedle-in-the-haystack test, show that FoX also retains the Transformer's\nsuperior long-context capabilities over recurrent sequence models such as\nMamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that\nincorporates some common architectural components in recurrent sequence models\nand find it significantly improves the performance of both FoX and the\nTransformer. Our code is available at\nhttps://github.com/zhixuan-lin/forgetting-transformer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05592",
            "authors": [
                {
                    "_id": "67ce5fd2e5cdfda52b9123a4",
                    "user": {
                        "_id": "66163dc8c7f45b3f893ff40b",
                        "avatarUrl": "/avatars/801043dac0caae90bbca8c9d3e2e203b.svg",
                        "isPro": false,
                        "fullname": "Song Huatong",
                        "user": "XXsongLALA",
                        "type": "user"
                    },
                    "name": "Huatong Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:03:49.730Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123a5",
                    "user": {
                        "_id": "61b8405b516a20acdf3b85ff",
                        "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg",
                        "isPro": false,
                        "fullname": "Jinhao Jiang",
                        "user": "Boru",
                        "type": "user"
                    },
                    "name": "Jinhao Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:04:22.446Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123a6",
                    "user": {
                        "_id": "6703ac76ea890f0ca5b225eb",
                        "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
                        "isPro": false,
                        "fullname": "Yingqian Min",
                        "user": "EliverQ",
                        "type": "user"
                    },
                    "name": "Yingqian Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T09:40:54.171Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123a7",
                    "user": {
                        "_id": "651a29d566e78720a78317ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651a29d566e78720a78317ec/icoHA_v1RPVfwTooN3fJb.png",
                        "isPro": false,
                        "fullname": "Jie Chen",
                        "user": "survivi",
                        "type": "user"
                    },
                    "name": "Jie Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:46:26.714Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123a8",
                    "user": {
                        "_id": "629b765ce1af194c641fcbc6",
                        "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
                        "isPro": false,
                        "fullname": "Zhipeng Chen",
                        "user": "TimothyCzp",
                        "type": "user"
                    },
                    "name": "Zhipeng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T11:08:49.613Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123a9",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123aa",
                    "name": "Lei Fang",
                    "hidden": false
                },
                {
                    "_id": "67ce5fd2e5cdfda52b9123ab",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:04:33.194Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T17:14:44.000Z",
            "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
            "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose R1-Searcher, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
            "upvotes": 18,
            "discussionId": "67ce5fd3e5cdfda52b912436",
            "githubRepo": "https://github.com/SsmallSong/R1-Searcher",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "Large Reasoning Models (LRMs)",
                "Large Language Models (LLMs)",
                "two-stage outcome-based RL",
                "external search systems",
                "R1-Searcher"
            ]
        },
        "publishedAt": "2025-03-07T12:14:44.000Z",
        "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
        "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose R1-Searcher, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.04957",
            "authors": [
                {
                    "_id": "67cf1071c6e7e15557f3814b",
                    "name": "Ada Defne Tur",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f3814c",
                    "name": "Nicholas Meade",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f3814d",
                    "name": "Xing Han Lù",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f3814e",
                    "name": "Alejandra Zambrano",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f3814f",
                    "name": "Arkil Patel",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f38150",
                    "name": "Esin Durmus",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f38151",
                    "name": "Spandana Gella",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f38152",
                    "name": "Karolina Stańczak",
                    "hidden": false
                },
                {
                    "_id": "67cf1071c6e7e15557f38153",
                    "user": {
                        "_id": "624734dc4c731bb6bfab8af7",
                        "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
                        "isPro": false,
                        "fullname": "Siva Reddy",
                        "user": "sivareddyg",
                        "type": "user"
                    },
                    "name": "Siva Reddy",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-10T16:16:51.311Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T20:43:14.000Z",
            "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
            "summary": "LLM-based agents are becoming increasingly proficient at solving web-based\ntasks. With this capability comes a greater risk of misuse for malicious\npurposes, such as posting misinformation in an online forum or selling illicit\nsubstances on a website. To evaluate these risks, we propose SafeArena, the\nfirst benchmark to focus on the deliberate misuse of web agents. SafeArena\ncomprises 250 safe and 250 harmful tasks across four websites. We classify the\nharmful tasks into five harm categories -- misinformation, illegal activity,\nharassment, cybercrime, and social bias, designed to assess realistic misuses\nof web agents. We evaluate leading LLM-based web agents, including GPT-4o,\nClaude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To\nsystematically assess their susceptibility to harmful tasks, we introduce the\nAgent Risk Assessment framework that categorizes agent behavior across four\nrisk levels. We find agents are surprisingly compliant with malicious requests,\nwith GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests,\nrespectively. Our findings highlight the urgent need for safety alignment\nprocedures for web agents. Our benchmark is available here:\nhttps://safearena.github.io",
            "upvotes": 16,
            "discussionId": "67cf1073c6e7e15557f38173",
            "projectPage": "https://safearena.github.io/",
            "githubRepo": "https://github.com/McGill-NLP/safearena",
            "ai_keywords": [
                "LLM-based agents",
                "SafeArena",
                "benchmark",
                "malicious requests",
                "misinformation",
                "illegal activity",
                "harassment",
                "cybercrime",
                "social bias",
                "Agent Risk Assessment framework"
            ]
        },
        "publishedAt": "2025-03-06T15:43:14.000Z",
        "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
        "summary": "LLM-based agents are becoming increasingly proficient at solving web-based\ntasks. With this capability comes a greater risk of misuse for malicious\npurposes, such as posting misinformation in an online forum or selling illicit\nsubstances on a website. To evaluate these risks, we propose SafeArena, the\nfirst benchmark to focus on the deliberate misuse of web agents. SafeArena\ncomprises 250 safe and 250 harmful tasks across four websites. We classify the\nharmful tasks into five harm categories -- misinformation, illegal activity,\nharassment, cybercrime, and social bias, designed to assess realistic misuses\nof web agents. We evaluate leading LLM-based web agents, including GPT-4o,\nClaude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To\nsystematically assess their susceptibility to harmful tasks, we introduce the\nAgent Risk Assessment framework that categorizes agent behavior across four\nrisk levels. We find agents are surprisingly compliant with malicious requests,\nwith GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests,\nrespectively. Our findings highlight the urgent need for safety alignment\nprocedures for web agents. Our benchmark is available here:\nhttps://safearena.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04957.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05639",
            "authors": [
                {
                    "_id": "67ce5ad85847e4787a82242d",
                    "user": {
                        "_id": "650447dd52ca06fef957f05d",
                        "avatarUrl": "/avatars/511c11ac9b3cc7a162bda5e07f6ee0a3.svg",
                        "isPro": true,
                        "fullname": "Yuxuan BIAN",
                        "user": "BianYx",
                        "type": "user"
                    },
                    "name": "Yuxuan Bian",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-10T03:22:04.947Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ad85847e4787a82242e",
                    "name": "Zhaoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ce5ad85847e4787a82242f",
                    "user": {
                        "_id": "62d4577bc85b0fcf7fde39bb",
                        "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
                        "isPro": false,
                        "fullname": "Xuan Ju",
                        "user": "juxuan27",
                        "type": "user"
                    },
                    "name": "Xuan Ju",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:57:06.955Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ad85847e4787a822430",
                    "user": {
                        "_id": "6374a02d0856ac905bfc6113",
                        "avatarUrl": "/avatars/2cbe75c9cc818a647ca6e416f129c96f.svg",
                        "isPro": false,
                        "fullname": "Mingdeng Cao",
                        "user": "Ljzycmd",
                        "type": "user"
                    },
                    "name": "Mingdeng Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:56:45.412Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ad85847e4787a822431",
                    "name": "Liangbin Xie",
                    "hidden": false
                },
                {
                    "_id": "67ce5ad85847e4787a822432",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T09:56:34.001Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5ad85847e4787a822433",
                    "name": "Qiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T17:59:46.000Z",
            "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control",
            "summary": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
            "upvotes": 13,
            "discussionId": "67ce5adc5847e4787a822524",
            "projectPage": "https://yxbian23.github.io/project/video-painter/",
            "githubRepo": "https://github.com/TencentARC/VideoPainter",
            "ai_keywords": [
                "dual-stream paradigm",
                "VideoPainter",
                "context encoder",
                "backbone parameters",
                "pre-trained video DiT",
                "semantic consistency",
                "target region ID resampling technique",
                "VPData",
                "VPBench",
                "segmentation-based inpainting",
                "video editing",
                "video editing pair data generation",
                "video quality",
                "mask region preservation",
                "textual coherence"
            ]
        },
        "publishedAt": "2025-03-07T12:59:46.000Z",
        "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control",
        "summary": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05639.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05379",
            "authors": [
                {
                    "_id": "67ce5f2389663abdbc364495",
                    "user": {
                        "_id": "6686044047f2a33570e59e31",
                        "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
                        "isPro": false,
                        "fullname": "Jiaxing Zhao",
                        "user": "StarJiaxing",
                        "type": "user"
                    },
                    "name": "Jiaxing Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:12:14.110Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5f2389663abdbc364496",
                    "name": "Xihan Wei",
                    "hidden": false
                },
                {
                    "_id": "67ce5f2389663abdbc364497",
                    "user": {
                        "_id": "63d0cc736b985b0f25d0412c",
                        "avatarUrl": "/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg",
                        "isPro": false,
                        "fullname": "Bo",
                        "user": "Liefeng",
                        "type": "user"
                    },
                    "name": "Liefeng Bo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:12:31.341Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T12:46:42.000Z",
            "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning",
            "summary": "In this work, we present the first application of Reinforcement Learning with\nVerifiable Reward (RLVR) to an Omni-multimodal large language model in the\ncontext of emotion recognition, a task where both visual and audio modalities\nplay crucial roles. We leverage RLVR to optimize the Omni model, significantly\nenhancing its performance in three key aspects: reasoning capability, emotion\nrecognition accuracy, and generalization ability. The introduction of RLVR not\nonly improves the model's overall performance on in-distribution data but also\ndemonstrates superior robustness when evaluated on out-of-distribution\ndatasets. More importantly, the improved reasoning capability enables clear\nanalysis of the contributions of different modalities, particularly visual and\naudio information, in the emotion recognition process. This provides valuable\ninsights into the optimization of multimodal large language models.",
            "upvotes": 12,
            "discussionId": "67ce5f2489663abdbc3644d0",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Reward (RLVR)",
                "Omni-multimodal large language model",
                "emotion recognition",
                "visual and audio modalities",
                "reasoning capability",
                "emotion recognition accuracy",
                "generalization ability",
                "out-of-distribution datasets",
                "multimodal large language models"
            ]
        },
        "publishedAt": "2025-03-07T07:46:42.000Z",
        "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning",
        "summary": "In this work, we present the first application of Reinforcement Learning with\nVerifiable Reward (RLVR) to an Omni-multimodal large language model in the\ncontext of emotion recognition, a task where both visual and audio modalities\nplay crucial roles. We leverage RLVR to optimize the Omni model, significantly\nenhancing its performance in three key aspects: reasoning capability, emotion\nrecognition accuracy, and generalization ability. The introduction of RLVR not\nonly improves the model's overall performance on in-distribution data but also\ndemonstrates superior robustness when evaluated on out-of-distribution\ndatasets. More importantly, the improved reasoning capability enables clear\nanalysis of the contributions of different modalities, particularly visual and\naudio information, in the emotion recognition process. This provides valuable\ninsights into the optimization of multimodal large language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png",
        "numComments": 3,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.04808",
            "authors": [
                {
                    "_id": "67ce5c7065b141ae6b0d3957",
                    "user": {
                        "_id": "67cef8b7d9f3ce4930069e10",
                        "avatarUrl": "/avatars/0b660e5ff13cf6cc27804bc722a70efc.svg",
                        "isPro": false,
                        "fullname": "Sephen Chung",
                        "user": "stephenchungmh",
                        "type": "user"
                    },
                    "name": "Stephen Chung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T17:39:13.472Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5c7065b141ae6b0d3958",
                    "user": {
                        "_id": "624c3d2ca19f20b197761ba9",
                        "avatarUrl": "/avatars/7a64b81c29f4f6700fa18effc5616865.svg",
                        "isPro": false,
                        "fullname": "Wenyu Du",
                        "user": "wydu",
                        "type": "user"
                    },
                    "name": "Wenyu Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:27:00.543Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5c7065b141ae6b0d3959",
                    "user": {
                        "_id": "641a6895fb5ffff5ac79d593",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/dFR_ofjbqCrcqGa9R3MMq.jpeg",
                        "isPro": false,
                        "fullname": "Jie Fu",
                        "user": "bigaidream",
                        "type": "user"
                    },
                    "name": "Jie Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:46:28.886Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T02:53:39.000Z",
            "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
            "summary": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
            "upvotes": 12,
            "discussionId": "67ce5c7165b141ae6b0d39c6",
            "projectPage": "https://gossamer-bookcase-8a7.notion.site/Learning-From-Failures-in-Multi-Attempt-Reinforcement-Learning-1a6215521f3a80df9b14d48306a9f7a2",
            "githubRepo": "https://github.com/DualityRL/multi-attempt",
            "ai_keywords": [
                "reinforcement learning",
                "reinforcement learning for large language models",
                "DeepSeek R1",
                "question-answering task",
                "multi-attempt setting",
                "response refinement",
                "search efficiency",
                "math benchmark"
            ]
        },
        "publishedAt": "2025-03-03T21:53:39.000Z",
        "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
        "summary": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.04872",
            "authors": [
                {
                    "_id": "67ce5deedb623d45a95deb72",
                    "user": {
                        "_id": "632c30576bcb864974cc40a8",
                        "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg",
                        "isPro": false,
                        "fullname": "sunlin",
                        "user": "lincharliesun",
                        "type": "user"
                    },
                    "name": "Lin Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:00:47.601Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb73",
                    "user": {
                        "_id": "67bc4f0bbcc20fb84d61f4fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9f8n0ZCSP1AktsYh8DK5b.jpeg",
                        "isPro": false,
                        "fullname": "Guangxiang Zhao",
                        "user": "zhaoguangxiang",
                        "type": "user"
                    },
                    "name": "Guangxiang Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:13:04.137Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb74",
                    "user": {
                        "_id": "64c7c3fb1c23fb9a2bc3950d",
                        "avatarUrl": "/avatars/84be70976902366201dd0349b1f2357c.svg",
                        "isPro": false,
                        "fullname": "xiaoqi",
                        "user": "xiaoqijiang",
                        "type": "user"
                    },
                    "name": "Xiaoqi Jian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:13:10.864Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb75",
                    "user": {
                        "_id": "660a71938695a785edd74d40",
                        "avatarUrl": "/avatars/41c96d1aec59d78d44145dc71d7b2e96.svg",
                        "isPro": false,
                        "fullname": "wuyuhan",
                        "user": "yuhanwuuu",
                        "type": "user"
                    },
                    "name": "Yuhan Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:13:31.059Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb76",
                    "user": {
                        "_id": "675a69699e086bd6250a36ef",
                        "avatarUrl": "/avatars/95c72e3975d1a37f8655a2fe629746ec.svg",
                        "isPro": false,
                        "fullname": "Weihong Lin",
                        "user": "lwher1996",
                        "type": "user"
                    },
                    "name": "Weihong Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:13:37.096Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb77",
                    "name": "Yongfu Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb78",
                    "name": "Change Jia",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb79",
                    "user": {
                        "_id": "664b0253156169f8724b8485",
                        "avatarUrl": "/avatars/4c1014b782ae298271911e7435cc5f6f.svg",
                        "isPro": false,
                        "fullname": "Linglin zhang",
                        "user": "LinglinZhang",
                        "type": "user"
                    },
                    "name": "Linglin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:13:58.854Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb7a",
                    "name": "Jinzhu Wu",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb7b",
                    "name": "Junfeng Ran",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb7c",
                    "user": {
                        "_id": "65a38f8b212d6aca9ac98573",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a38f8b212d6aca9ac98573/nN0XW8PujXkKy7pwQvlam.png",
                        "isPro": false,
                        "fullname": "Sai-er Hu",
                        "user": "Husserl233",
                        "type": "user"
                    },
                    "name": "Sai-er Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:14:32.009Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb7d",
                    "user": {
                        "_id": "665ee916b9e194a20c4769da",
                        "avatarUrl": "/avatars/cfb3ff26979bed1a0c65a6fa318af3ad.svg",
                        "isPro": false,
                        "fullname": "Zihan Jiang",
                        "user": "Jumbo0715",
                        "type": "user"
                    },
                    "name": "Zihan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:16:14.012Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb7e",
                    "user": {
                        "_id": "65a374a59acab1998092a9bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a374a59acab1998092a9bc/M3s_7bSf9G-6b9nLg7N3Z.jpeg",
                        "isPro": false,
                        "fullname": "Antonio",
                        "user": "JuntingZhou",
                        "type": "user"
                    },
                    "name": "Junting Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:14:47.371Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb7f",
                    "user": {
                        "_id": "67160ae2ea37ca179f4f87fe",
                        "avatarUrl": "/avatars/7fc126422cfedac79e85fa6f12db8a61.svg",
                        "isPro": false,
                        "fullname": "Wenrui Liu",
                        "user": "CarenceLiu",
                        "type": "user"
                    },
                    "name": "Wenrui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:15:11.742Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb80",
                    "user": {
                        "_id": "67b2795f0bd4ddcd84426bb4",
                        "avatarUrl": "/avatars/d4346ac5a0ebbaeb828d832cc6ca9f0b.svg",
                        "isPro": false,
                        "fullname": "Bin Cui",
                        "user": "lazybone128",
                        "type": "user"
                    },
                    "name": "Bin Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:15:19.684Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb81",
                    "name": "Tong Yang",
                    "hidden": false
                },
                {
                    "_id": "67ce5deedb623d45a95deb82",
                    "name": "Xiangzheng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T16:25:53.000Z",
            "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
            "summary": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\nselectively distilled into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.",
            "upvotes": 10,
            "discussionId": "67ce5df0db623d45a95dec1f",
            "ai_keywords": [
                "Branch-Merge distillation",
                "model distillation",
                "transfer learning",
                "selectively distilled",
                "domain-specific supervised fine-tuning (SFT)",
                "cross-domain knowledge transfer",
                "TinyR1-32B-Preview",
                "DeepSeek-R1",
                "DeepSeek-R1-Distill-Qwen-32B",
                "Mathematics",
                "Coding",
                "Science",
                "AIME 2024"
            ]
        },
        "publishedAt": "2025-03-06T11:25:53.000Z",
        "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
        "summary": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\nselectively distilled into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.05638",
            "authors": [
                {
                    "_id": "67ce8388764226f050ad18b3",
                    "name": "Mark YU",
                    "hidden": false
                },
                {
                    "_id": "67ce8388764226f050ad18b4",
                    "user": {
                        "_id": "657a7458afbb0117ba15c59f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                        "isPro": false,
                        "fullname": "Wenbo Hu",
                        "user": "wbhu-tc",
                        "type": "user"
                    },
                    "name": "Wenbo Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:00:42.681Z",
                    "hidden": false
                },
                {
                    "_id": "67ce8388764226f050ad18b5",
                    "user": {
                        "_id": "64770e86d7cf39f2e937ae9a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg",
                        "isPro": false,
                        "fullname": "Jinbo Xing",
                        "user": "Doubiiu",
                        "type": "user"
                    },
                    "name": "Jinbo Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:00:40.328Z",
                    "hidden": false
                },
                {
                    "_id": "67ce8388764226f050ad18b6",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:06:56.510Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T17:57:53.000Z",
            "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models",
            "summary": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
            "upvotes": 9,
            "discussionId": "67ce838a764226f050ad1952",
            "projectPage": "https://trajectorycrafter.github.io/",
            "githubRepo": "https://github.com/TrajectoryCrafter/TrajectoryCrafter",
            "ai_keywords": [
                "dual-stream conditional video diffusion model",
                "point cloud renders",
                "source videos",
                "deterministic view transformations",
                "stochastic content generation",
                "double-reprojection strategy"
            ]
        },
        "publishedAt": "2025-03-07T12:57:53.000Z",
        "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models",
        "summary": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04824",
            "authors": [
                {
                    "_id": "67cec02929ee2ca420c0c395",
                    "name": "Lei Ke",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c396",
                    "name": "Haohang Xu",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c397",
                    "name": "Xuefei Ning",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c398",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c399",
                    "name": "Jiajun Li",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c39a",
                    "user": {
                        "_id": "650be23ec4e52db6a4db63ef",
                        "avatarUrl": "/avatars/03af548029b38bee49ec295fefe74f9a.svg",
                        "isPro": false,
                        "fullname": "Haoling Li",
                        "user": "Ringo1110",
                        "type": "user"
                    },
                    "name": "Haoling Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T11:08:46.234Z",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c39b",
                    "name": "Yuxuan Lin",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c39c",
                    "name": "Dongsheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c39d",
                    "name": "Yujiu Yang",
                    "hidden": false
                },
                {
                    "_id": "67cec02929ee2ca420c0c39e",
                    "name": "Linfeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T04:50:53.000Z",
            "title": "ProReflow: Progressive Reflow with Decomposed Velocity",
            "summary": "Diffusion models have achieved significant progress in both image and video\ngeneration while still suffering from huge computation costs. As an effective\nsolution, flow matching aims to reflow the diffusion process of diffusion\nmodels into a straight line for a few-step and even one-step generation.\nHowever, in this paper, we suggest that the original training pipeline of flow\nmatching is not optimal and introduce two techniques to improve it. Firstly, we\nintroduce progressive reflow, which progressively reflows the diffusion models\nin local timesteps until the whole diffusion progresses, reducing the\ndifficulty of flow matching. Second, we introduce aligned v-prediction, which\nhighlights the importance of direction matching in flow matching over magnitude\nmatching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness\nof our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on\nMSCOCO2014 validation set with only 4 sampling steps, close to our teacher\nmodel (32 DDIM steps, FID = 10.05).",
            "upvotes": 9,
            "discussionId": "67cec02b29ee2ca420c0c447",
            "ai_keywords": [
                "diffusion models",
                "image generation",
                "video generation",
                "computation costs",
                "flow matching",
                "straight line",
                "progressive reflow",
                "local timesteps",
                "aligned v-prediction",
                "direction matching",
                "magnitude matching",
                "FID",
                "MSCOCO2014",
                "validation set",
                "sampling steps",
                "DDIM steps",
                "teacher model"
            ]
        },
        "publishedAt": "2025-03-04T23:50:53.000Z",
        "title": "ProReflow: Progressive Reflow with Decomposed Velocity",
        "summary": "Diffusion models have achieved significant progress in both image and video\ngeneration while still suffering from huge computation costs. As an effective\nsolution, flow matching aims to reflow the diffusion process of diffusion\nmodels into a straight line for a few-step and even one-step generation.\nHowever, in this paper, we suggest that the original training pipeline of flow\nmatching is not optimal and introduce two techniques to improve it. Firstly, we\nintroduce progressive reflow, which progressively reflows the diffusion models\nin local timesteps until the whole diffusion progresses, reducing the\ndifficulty of flow matching. Second, we introduce aligned v-prediction, which\nhighlights the importance of direction matching in flow matching over magnitude\nmatching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness\nof our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on\nMSCOCO2014 validation set with only 4 sampling steps, close to our teacher\nmodel (32 DDIM steps, FID = 10.05).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04824.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05652",
            "authors": [
                {
                    "_id": "67ce524ee969bc5fd69c9388",
                    "user": {
                        "_id": "61e9f5398c237a147a3f4ab5",
                        "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
                        "isPro": false,
                        "fullname": "Yunfan Jiang",
                        "user": "yunfanj",
                        "type": "user"
                    },
                    "name": "Yunfan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:01:07.901Z",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c9389",
                    "name": "Ruohan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c938a",
                    "name": "Josiah Wong",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c938b",
                    "name": "Chen Wang",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c938c",
                    "user": {
                        "_id": "63509bc859bfa9a85d4220aa",
                        "avatarUrl": "/avatars/ca2cc9b87f5ca5cd51606b2f9edf89d0.svg",
                        "isPro": false,
                        "fullname": "Yanjie Ze",
                        "user": "yjze",
                        "type": "user"
                    },
                    "name": "Yanjie Ze",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:01:05.242Z",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c938d",
                    "name": "Hang Yin",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c938e",
                    "user": {
                        "_id": "6487a3ccaa6b952c784eab16",
                        "avatarUrl": "/avatars/6d543110f169fbc0142b8485e2919f5f.svg",
                        "isPro": false,
                        "fullname": "Cem Gokmen",
                        "user": "cgokmen",
                        "type": "user"
                    },
                    "name": "Cem Gokmen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:11:18.669Z",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c938f",
                    "name": "Shuran Song",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c9390",
                    "user": {
                        "_id": "647b7b8345616c3de3627863",
                        "avatarUrl": "/avatars/8196364bffed71086ba9a0a4a62027d3.svg",
                        "isPro": false,
                        "fullname": "Jiajun Wu",
                        "user": "jiajunwu",
                        "type": "user"
                    },
                    "name": "Jiajun Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:11:41.722Z",
                    "hidden": false
                },
                {
                    "_id": "67ce524ee969bc5fd69c9391",
                    "name": "Li Fei-Fei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T18:15:21.000Z",
            "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities",
            "summary": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
            "upvotes": 8,
            "discussionId": "67ce5294e969bc5fd69c9a2c",
            "projectPage": "https://behavior-robot-suite.github.io/",
            "githubRepo": "https://github.com/behavior-robot-suite/brs-algo",
            "ai_keywords": [
                "bimanual coordination",
                "stable and precise navigation",
                "extensive end-effector reachability",
                "visuomotor policy learning",
                "whole-body teleoperation interface",
                "whole-body visuomotor policies",
                "long-range navigation",
                "articulated objects",
                "deformable objects",
                "confined spaces"
            ]
        },
        "publishedAt": "2025-03-07T13:15:21.000Z",
        "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities",
        "summary": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04548",
            "authors": [
                {
                    "_id": "67cbff8e4dedec48bdec8a99",
                    "user": {
                        "_id": "629b765ce1af194c641fcbc6",
                        "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
                        "isPro": false,
                        "fullname": "Zhipeng Chen",
                        "user": "TimothyCzp",
                        "type": "user"
                    },
                    "name": "Zhipeng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T11:08:52.643Z",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8a9a",
                    "user": {
                        "_id": "6703ac76ea890f0ca5b225eb",
                        "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
                        "isPro": false,
                        "fullname": "Yingqian Min",
                        "user": "EliverQ",
                        "type": "user"
                    },
                    "name": "Yingqian Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:02:04.349Z",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8a9b",
                    "user": {
                        "_id": "648e6a4567aa8ab0e0e4c30f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
                        "isPro": false,
                        "fullname": "Beichen Zhang",
                        "user": "ToheartZhang",
                        "type": "user"
                    },
                    "name": "Beichen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:29:55.450Z",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8a9c",
                    "user": {
                        "_id": "651a29d566e78720a78317ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651a29d566e78720a78317ec/icoHA_v1RPVfwTooN3fJb.png",
                        "isPro": false,
                        "fullname": "Jie Chen",
                        "user": "survivi",
                        "type": "user"
                    },
                    "name": "Jie Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:46:30.961Z",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8a9d",
                    "user": {
                        "_id": "61b8405b516a20acdf3b85ff",
                        "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg",
                        "isPro": false,
                        "fullname": "Jinhao Jiang",
                        "user": "Boru",
                        "type": "user"
                    },
                    "name": "Jinhao Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:30:03.904Z",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8a9e",
                    "user": {
                        "_id": "649e6761f9134a06ed1e0cea",
                        "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
                        "isPro": false,
                        "fullname": "Daixuan Cheng",
                        "user": "daixuancheng",
                        "type": "user"
                    },
                    "name": "Daixuan Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:30:09.972Z",
                    "hidden": true
                },
                {
                    "_id": "67cbff8e4dedec48bdec8a9f",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8aa0",
                    "name": "Zheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8aa1",
                    "name": "Xu Miao",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8aa2",
                    "name": "Yang Lu",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8aa3",
                    "name": "Lei Fang",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8aa4",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67cbff8e4dedec48bdec8aa5",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:30:44.799Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T15:34:27.000Z",
            "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
            "summary": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
            "upvotes": 7,
            "discussionId": "67cbff8f4dedec48bdec8af3",
            "ai_keywords": [
                "RL training",
                "Qwen2.5-32B",
                "DeepSeek-R1-Distill-Qwen-1.5B",
                "AIME 2024",
                "tool manipulation"
            ]
        },
        "publishedAt": "2025-03-06T10:34:27.000Z",
        "title": "An Empirical Study on Eliciting and Improving R1-like Reasoning Models",
        "summary": "In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04548.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05447",
            "authors": [
                {
                    "_id": "67ceab4132a6585cecad2c36",
                    "user": {
                        "_id": "6246bb33da617c00b48e4d92",
                        "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                        "isPro": false,
                        "fullname": "Weigao Sun",
                        "user": "weigao266",
                        "type": "user"
                    },
                    "name": "Weigao Sun",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-10T09:09:22.436Z",
                    "hidden": false
                },
                {
                    "_id": "67ceab4132a6585cecad2c37",
                    "user": {
                        "_id": "66ea643899af9ac3463639b1",
                        "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
                        "isPro": false,
                        "fullname": "Disen Lan",
                        "user": "landisen",
                        "type": "user"
                    },
                    "name": "Disen Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T10:13:18.306Z",
                    "hidden": false
                },
                {
                    "_id": "67ceab4132a6585cecad2c38",
                    "name": "Tong Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ceab4132a6585cecad2c39",
                    "user": {
                        "_id": "64cb54da1af278541d663708",
                        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                        "isPro": false,
                        "fullname": "Xiaoye Qu",
                        "user": "Xiaoye08",
                        "type": "user"
                    },
                    "name": "Xiaoye Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:19:56.928Z",
                    "hidden": false
                },
                {
                    "_id": "67ceab4132a6585cecad2c3a",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T14:17:45.000Z",
            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
            "summary": "Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
            "upvotes": 6,
            "discussionId": "67ceab4232a6585cecad2c82",
            "githubRepo": "https://github.com/OpenSparseLLMs/Linear-MoE",
            "ai_keywords": [
                "linear attention",
                "state space models",
                "linear RNNs",
                "Mixture-of-Experts (MoE)",
                "Linear-MoE",
                "linear-complexity sequence modeling",
                "sparsely activation",
                "Sequence Parallelism",
                "Transformer-MoE layers",
                "foundational model architecture"
            ]
        },
        "publishedAt": "2025-03-07T09:17:45.000Z",
        "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
        "summary": "Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05447.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04359",
            "authors": [
                {
                    "_id": "67ced7aa502759f20555f68d",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f68e",
                    "name": "Xuyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f68f",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f690",
                    "name": "Kechi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f691",
                    "name": "Ge Li",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f692",
                    "name": "Jia Li",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f693",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f694",
                    "name": "Fang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f695",
                    "name": "Chongyang Tao",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f696",
                    "name": "Yuqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ced7aa502759f20555f697",
                    "name": "Zhi Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T12:02:31.000Z",
            "title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding",
            "summary": "Current advanced long-context language models offer great potential for\nreal-world software engineering applications. However, progress in this\ncritical domain remains hampered by a fundamental limitation: the absence of a\nrigorous evaluation framework for long code understanding. To gap this\nobstacle, we propose a long code understanding benchmark LONGCODEU from four\naspects (8 tasks) to evaluate LCLMs' long code understanding ability required\nfor practical applications, including code unit perception, intra-code unit\nunderstanding, inter-code unit relation understanding, and long code\ndocumentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6\ngeneral models and 3 code models). Our experimental results reveal key\nlimitations in current LCLMs' capabilities for long code understanding.\nParticularly, the performance of LCLMs drops dramatically when the long code\nlength is greater than 32K, falling far short of their claimed 128K-1M context\nwindows. In the four aspects, inter-code unit relation understanding is the\nmost challenging for LCLMs. Our study provides valuable insights for optimizing\nLCLMs and driving advancements in software engineering.",
            "upvotes": 5,
            "discussionId": "67ced7ab502759f20555f6fc",
            "ai_keywords": [
                "long-context language models",
                "long code understanding",
                "benchmark",
                "LONGCODEU",
                "code unit perception",
                "intra-code unit understanding",
                "inter-code unit relation understanding",
                "long code documentation understanding",
                "LCLMs",
                "context windows"
            ]
        },
        "publishedAt": "2025-03-06T07:02:31.000Z",
        "title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding",
        "summary": "Current advanced long-context language models offer great potential for\nreal-world software engineering applications. However, progress in this\ncritical domain remains hampered by a fundamental limitation: the absence of a\nrigorous evaluation framework for long code understanding. To gap this\nobstacle, we propose a long code understanding benchmark LONGCODEU from four\naspects (8 tasks) to evaluate LCLMs' long code understanding ability required\nfor practical applications, including code unit perception, intra-code unit\nunderstanding, inter-code unit relation understanding, and long code\ndocumentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6\ngeneral models and 3 code models). Our experimental results reveal key\nlimitations in current LCLMs' capabilities for long code understanding.\nParticularly, the performance of LCLMs drops dramatically when the long code\nlength is greater than 32K, falling far short of their claimed 128K-1M context\nwindows. In the four aspects, inter-code unit relation understanding is the\nmost challenging for LCLMs. Our study provides valuable insights for optimizing\nLCLMs and driving advancements in software engineering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04359.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.01713",
            "authors": [
                {
                    "_id": "67c75e18cb29e2a4b0eb0293",
                    "user": {
                        "_id": "66c0a08bac74db25de8427ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                        "isPro": false,
                        "fullname": "Jintao Zhang",
                        "user": "jt-zhang",
                        "type": "user"
                    },
                    "name": "Jintao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-07T09:26:50.367Z",
                    "hidden": false
                },
                {
                    "_id": "67c75e18cb29e2a4b0eb0294",
                    "name": "Guoliang Li",
                    "hidden": false
                },
                {
                    "_id": "67c75e18cb29e2a4b0eb0295",
                    "name": "Jinyang Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T16:25:58.000Z",
            "title": "SAGE: A Framework of Precise Retrieval for RAG",
            "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.",
            "upvotes": 4,
            "discussionId": "67c75e1ccb29e2a4b0eb03a9",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "question-answering (QA)",
                "Large Language Models (LLMs)",
                "semantic segmentation model",
                "chunk selection algorithm",
                "relevance score",
                "SAGE (Semantic-Augmented Generation and Evaluation)"
            ]
        },
        "publishedAt": "2025-03-03T11:25:58.000Z",
        "title": "SAGE: A Framework of Precise Retrieval for RAG",
        "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.01840",
            "authors": [
                {
                    "_id": "67cef1d137e15ce6cd66a4fb",
                    "name": "Yuhui Li",
                    "hidden": false
                },
                {
                    "_id": "67cef1d137e15ce6cd66a4fc",
                    "name": "Fangyun Wei",
                    "hidden": false
                },
                {
                    "_id": "67cef1d137e15ce6cd66a4fd",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cef1d137e15ce6cd66a4fe",
                    "user": {
                        "_id": "67ccb8fff68cfbec9dfbb658",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HrIbjkCfydS7Ne2Cmj47E.png",
                        "isPro": false,
                        "fullname": "Hongyang Zhang",
                        "user": "hongyanz",
                        "type": "user"
                    },
                    "name": "Hongyang Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-10T14:06:09.903Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-03T18:59:04.000Z",
            "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test",
            "summary": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. The code is available at\nhttps://github.com/SafeAILab/EAGLE.",
            "upvotes": 3,
            "discussionId": "67cef1d137e15ce6cd66a545",
            "ai_keywords": [
                "LLMs",
                "speculative sampling",
                "EAGLE",
                "autoregression",
                "feature level",
                "top-layer features",
                "multi-layer feature fusion",
                "training-time test",
                "draft model",
                "chat models",
                "reasoning models"
            ]
        },
        "publishedAt": "2025-03-03T13:59:04.000Z",
        "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test",
        "summary": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. The code is available at\nhttps://github.com/SafeAILab/EAGLE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01840.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18968",
            "authors": [
                {
                    "_id": "67bfcc735db054ee3c57c717",
                    "user": {
                        "_id": "6693fba8e3a813f3bb3361ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6693fba8e3a813f3bb3361ba/oOJ3ImYPKDjk4odI_To05.png",
                        "isPro": false,
                        "fullname": "WangKuang",
                        "user": "wangkevin02",
                        "type": "user"
                    },
                    "name": "Kuang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-04T21:16:20.224Z",
                    "hidden": false
                },
                {
                    "_id": "67bfcc735db054ee3c57c718",
                    "name": "Xianfei Li",
                    "hidden": false
                },
                {
                    "_id": "67bfcc735db054ee3c57c719",
                    "name": "Shenghao Yang",
                    "hidden": false
                },
                {
                    "_id": "67bfcc735db054ee3c57c71a",
                    "name": "Li Zhou",
                    "hidden": false
                },
                {
                    "_id": "67bfcc735db054ee3c57c71b",
                    "name": "Feng Jiang",
                    "hidden": false
                },
                {
                    "_id": "67bfcc735db054ee3c57c71c",
                    "name": "Haizhou Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T09:26:54.000Z",
            "title": "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
            "summary": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
            "upvotes": 3,
            "discussionId": "67bfcc745db054ee3c57c793",
            "githubRepo": "https://github.com/wangkevin02/USP",
            "ai_keywords": [
                "large language models (LLMs)",
                "user simulators",
                "dialogue systems",
                "implicit user traits",
                "speaking style",
                "goals",
                "persona-based methods",
                "predefined profiles",
                "famous individuals",
                "archetypes",
                "User Simulator with implicit Profiles (USP)",
                "infer implicit user profiles",
                "personalized and realistic dialogues",
                "conditional supervised fine-tuning",
                "reinforcement learning",
                "cycle consistency",
                "user's latency profile schema",
                "diversity profile sampler",
                "dynamic multi-turn evaluations",
                "consistency benchmarks"
            ]
        },
        "publishedAt": "2025-02-26T04:26:54.000Z",
        "title": "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
        "summary": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18968.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05315",
            "authors": [
                {
                    "_id": "67ce6db07110b8bedb3344a7",
                    "user": {
                        "_id": "6432b9a62bfb2b0ec756b374",
                        "avatarUrl": "/avatars/5db525f2cd40fd077e751b250bbb30a5.svg",
                        "isPro": false,
                        "fullname": "Saumya Chaturvedi",
                        "user": "shollercoaster",
                        "type": "user"
                    },
                    "name": "Saumya Chaturvedi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:23:01.422Z",
                    "hidden": false
                },
                {
                    "_id": "67ce6db07110b8bedb3344a8",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:00:45.150Z",
                    "hidden": false
                },
                {
                    "_id": "67ce6db07110b8bedb3344a9",
                    "user": {
                        "_id": "6783105a17f0d939ebcdd86c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UKBXp_AGPVQjH9e_C-C5c.png",
                        "isPro": false,
                        "fullname": "Laurent Bindschaedler",
                        "user": "bindsch",
                        "type": "user"
                    },
                    "name": "Laurent Bindschaedler",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-10T10:23:07.597Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T10:50:45.000Z",
            "title": "LoRACode: LoRA Adapters for Code Embeddings",
            "summary": "Code embeddings are essential for semantic code search; however, current\napproaches often struggle to capture the precise syntactic and contextual\nnuances inherent in code. Open-source models such as CodeBERT and UniXcoder\nexhibit limitations in scalability and efficiency, while high-performing\nproprietary systems impose substantial computational costs. We introduce a\nparameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to\nconstruct task-specific adapters for code retrieval. Our approach reduces the\nnumber of trainable parameters to less than two percent of the base model,\nenabling rapid fine-tuning on extensive code corpora (2 million samples in 25\nminutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in\nMean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code\nsearch tasks across multiple programming languages. Distinction in task-wise\nand language-wise adaptation helps explore the sensitivity of code retrieval\nfor syntactical and linguistic variations.",
            "upvotes": 2,
            "discussionId": "67ce6db17110b8bedb3344c5",
            "ai_keywords": [
                "Code embeddings",
                "semantic code search",
                "syntactic nuances",
                "contextual nuances",
                "CodeBERT",
                "UniXcoder",
                "parameter-efficient fine-tuning",
                "Low-Rank Adaptation (LoRA)",
                "task-specific adapters",
                "code retrieval",
                "Mean Reciprocal Rank (MRR)",
                "Code2Code search",
                "Text2Code search"
            ]
        },
        "publishedAt": "2025-03-07T05:50:45.000Z",
        "title": "LoRACode: LoRA Adapters for Code Embeddings",
        "summary": "Code embeddings are essential for semantic code search; however, current\napproaches often struggle to capture the precise syntactic and contextual\nnuances inherent in code. Open-source models such as CodeBERT and UniXcoder\nexhibit limitations in scalability and efficiency, while high-performing\nproprietary systems impose substantial computational costs. We introduce a\nparameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to\nconstruct task-specific adapters for code retrieval. Our approach reduces the\nnumber of trainable parameters to less than two percent of the base model,\nenabling rapid fine-tuning on extensive code corpora (2 million samples in 25\nminutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in\nMean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code\nsearch tasks across multiple programming languages. Distinction in task-wise\nand language-wise adaptation helps explore the sensitivity of code retrieval\nfor syntactical and linguistic variations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04504",
            "authors": [
                {
                    "_id": "67cb8e882cfa481bcee9455e",
                    "user": {
                        "_id": "66a07c07b7f0bb64d3b35497",
                        "avatarUrl": "/avatars/c38af1ddb7a5b625e26b7ff05957ff7c.svg",
                        "isPro": false,
                        "fullname": "SunghyunAhn",
                        "user": "SkiddieAhn",
                        "type": "user"
                    },
                    "name": "Sunghyun Ahn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:02:12.798Z",
                    "hidden": false
                },
                {
                    "_id": "67cb8e882cfa481bcee9455f",
                    "user": {
                        "_id": "673d7b70713e4b8db2d5ca94",
                        "avatarUrl": "/avatars/b9e89eba62eb939ddd93c1cb91744e93.svg",
                        "isPro": false,
                        "fullname": "Youngwan Jo",
                        "user": "jyy1551",
                        "type": "user"
                    },
                    "name": "Youngwan Jo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:02:08.710Z",
                    "hidden": false
                },
                {
                    "_id": "67cb8e882cfa481bcee94560",
                    "name": "Kijung Lee",
                    "hidden": false
                },
                {
                    "_id": "67cb8e882cfa481bcee94561",
                    "name": "Sein Kwon",
                    "hidden": false
                },
                {
                    "_id": "67cb8e882cfa481bcee94562",
                    "name": "Inpyo Hong",
                    "hidden": false
                },
                {
                    "_id": "67cb8e882cfa481bcee94563",
                    "name": "Sanghyun Park",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T14:52:34.000Z",
            "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
            "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
            "upvotes": 1,
            "discussionId": "67cb8e8a2cfa481bcee945cd",
            "githubRepo": "https://github.com/SkiddieAhn/Paper-AnyAnomaly",
            "ai_keywords": [
                "context-aware visual question answering",
                "large vision language model",
                "C-VAD datasets",
                "UBnormal dataset",
                "VAD benchmark datasets"
            ]
        },
        "publishedAt": "2025-03-06T09:52:34.000Z",
        "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
        "summary": "Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04504.png",
        "numComments": 2,
        "isAuthorParticipating": true
    }
]