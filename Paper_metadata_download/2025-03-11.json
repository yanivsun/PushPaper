[
    {
        "paper": {
            "id": "2503.03601",
            "authors": [
                {
                    "_id": "67cbfff12cc05acaab147f07",
                    "name": "Kristian Kuznetsov",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f08",
                    "user": {
                        "_id": "636254dc2691058b19d9276a",
                        "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
                        "isPro": false,
                        "fullname": "Kushnareva",
                        "user": "Kushnareva",
                        "type": "user"
                    },
                    "name": "Laida Kushnareva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:23:18.630Z",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f09",
                    "user": {
                        "_id": "65d5e094cd05bc1eaa0fafc9",
                        "avatarUrl": "/avatars/ea3d52def6ef4d9af07728a76a499a9f.svg",
                        "isPro": false,
                        "fullname": "Polina Druzhinina",
                        "user": "plina2polina",
                        "type": "user"
                    },
                    "name": "Polina Druzhinina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:23:35.842Z",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f0a",
                    "user": {
                        "_id": "6172aaeec8e66e2aa84c06b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
                        "isPro": false,
                        "fullname": "Anton Razzhigaev",
                        "user": "razzant",
                        "type": "user"
                    },
                    "name": "Anton Razzhigaev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:23:21.197Z",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f0b",
                    "user": {
                        "_id": "64cbfc39a81988d0735867a1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cbfc39a81988d0735867a1/JLlYTaRa70H4MzLXOHhEB.jpeg",
                        "isPro": false,
                        "fullname": "Anastasia Voznyuk",
                        "user": "natriistorm",
                        "type": "user"
                    },
                    "name": "Anastasia Voznyuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:23:50.503Z",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f0c",
                    "name": "Irina Piontkovskaya",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f0d",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                },
                {
                    "_id": "67cbfff12cc05acaab147f0e",
                    "name": "Serguei Barannikov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T15:33:52.000Z",
            "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
            "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
            "upvotes": 137,
            "discussionId": "67cbfff22cc05acaab147f4d",
            "ai_keywords": [
                "Sparse Autoencoders",
                "Gemma-2-2b",
                "residual stream",
                "interpretability",
                "domain-specific statistics",
                "model-specific statistics",
                "steering approach",
                "LLM-based interpretation",
                "writing style",
                "information-dense domains",
                "human-like outputs"
            ]
        },
        "publishedAt": "2025-03-05T10:33:52.000Z",
        "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
        "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03601.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07605",
            "authors": [
                {
                    "_id": "67cfa0c1edb742caa3572982",
                    "name": "Xun Liang",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572983",
                    "user": {
                        "_id": "669e0b93c7cb0568dac6e92e",
                        "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
                        "isPro": false,
                        "fullname": "hanyu Wang",
                        "user": "UglyToilet",
                        "type": "user"
                    },
                    "name": "Hanyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:46.104Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572984",
                    "name": "Huayi Lai",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572985",
                    "user": {
                        "_id": "66daea8776dbaaa372eabec5",
                        "avatarUrl": "/avatars/1e5fbe4ff06bb6121c7029253b76b79f.svg",
                        "isPro": false,
                        "fullname": "siminniu",
                        "user": "siminniu",
                        "type": "user"
                    },
                    "name": "Simin Niu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:32:27.196Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572986",
                    "user": {
                        "_id": "656f339a5273668d5b946b33",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656f339a5273668d5b946b33/o2nBvQiOKKP5IfDmnpHP2.jpeg",
                        "isPro": false,
                        "fullname": "Shichao Song",
                        "user": "Ki-Seki",
                        "type": "user"
                    },
                    "name": "Shichao Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:32:34.461Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572987",
                    "name": "Jiawei Yang",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572988",
                    "name": "Jihao Zhao",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa3572989",
                    "name": "Feiyu Xiong",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa357298a",
                    "user": {
                        "_id": "66b02a2642c34e7a212133c0",
                        "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
                        "isPro": false,
                        "fullname": "Bo Tang",
                        "user": "BO1022",
                        "type": "user"
                    },
                    "name": "Bo Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:43:43.972Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa0c1edb742caa357298b",
                    "name": "Zhiyu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:59:03.000Z",
            "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
            "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
            "upvotes": 61,
            "discussionId": "67cfa0c2edb742caa35729dc",
            "githubRepo": "https://github.com/IAAR-Shanghai/SEAP",
            "ai_keywords": [
                "Sparse Expert Activation Pruning (SEAP)",
                "hidden states",
                "activations",
                "task-specific expert activation patterns",
                "computational efficiency"
            ]
        },
        "publishedAt": "2025-03-10T13:59:03.000Z",
        "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
        "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07605.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07365",
            "authors": [
                {
                    "_id": "67cf9cd037bc7273882147a3",
                    "user": {
                        "_id": "640b37b2bab5ca8fbe7df8f2",
                        "avatarUrl": "/avatars/c7bef45efad6a0d911a720e2236fcba5.svg",
                        "isPro": false,
                        "fullname": "fanqing meng",
                        "user": "FanqingM",
                        "type": "user"
                    },
                    "name": "Fanqing Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:25:13.253Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147a4",
                    "user": {
                        "_id": "666fe1a5b07525f0bde69c27",
                        "avatarUrl": "/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg",
                        "isPro": false,
                        "fullname": "Lingxiao Du",
                        "user": "Cierra0506",
                        "type": "user"
                    },
                    "name": "Lingxiao Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:25:22.523Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147a5",
                    "name": "Zongkai Liu",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147a6",
                    "name": "Zhixiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147a7",
                    "user": {
                        "_id": "653a483dacdeea08424ef55d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tYG2isIZBLCaBbWABatgK.png",
                        "isPro": false,
                        "fullname": "Quanfeng Lu",
                        "user": "hflqf88888",
                        "type": "user"
                    },
                    "name": "Quanfeng Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:30:58.512Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147a8",
                    "name": "Daocheng Fu",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147a9",
                    "user": {
                        "_id": "643df87f7cd64d872cb9fabd",
                        "avatarUrl": "/avatars/c53bfabcee08de448dde973915e8b31d.svg",
                        "isPro": false,
                        "fullname": "Botian Shi",
                        "user": "friskit",
                        "type": "user"
                    },
                    "name": "Botian Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:31:14.118Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147aa",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:31:26.926Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147ab",
                    "user": {
                        "_id": "66b593026cef22e6ba6adb8a",
                        "avatarUrl": "/avatars/8a94d55b85177e84d65dd0bd537e335f.svg",
                        "isPro": false,
                        "fullname": "JunjunHe",
                        "user": "JunjunHe",
                        "type": "user"
                    },
                    "name": "Junjun He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:31:39.917Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147ac",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:31:47.538Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147ad",
                    "user": {
                        "_id": "67cb7d55560c3dcbb1adeaa3",
                        "avatarUrl": "/avatars/0b616d3655b0b54a621c2608b2f14379.svg",
                        "isPro": false,
                        "fullname": "Ping Luo",
                        "user": "appleluo",
                        "type": "user"
                    },
                    "name": "Ping Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:31:54.446Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147ae",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147af",
                    "user": {
                        "_id": "63cf4ecdc1dedf59c8f8362e",
                        "avatarUrl": "/avatars/cede885854d6a1551860080d55c87568.svg",
                        "isPro": false,
                        "fullname": "Qiaosheng ZHANG",
                        "user": "Domingo12",
                        "type": "user"
                    },
                    "name": "Qiaosheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:32:02.499Z",
                    "hidden": false
                },
                {
                    "_id": "67cf9cd037bc7273882147b0",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:25:01.021Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T14:23:12.000Z",
            "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
            "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
            "upvotes": 47,
            "discussionId": "67cf9cd137bc7273882147e2",
            "ai_keywords": [
                "multimodal reasoning",
                "rule-based reinforcement learning (RL)",
                "large-scale rule-based reinforcement learning (RL)",
                "DeepSeek-R1",
                "multimodal space",
                "accuracy reward",
                "response length",
                "reflection behaviors",
                "instruction-tuned",
                "pre-trained models",
                "multimodal reasoning capabilities",
                "rule-based RL",
                "supervised fine-tuning",
                "data efficiency"
            ]
        },
        "publishedAt": "2025-03-10T10:23:12.000Z",
        "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
        "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07365.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07002",
            "authors": [
                {
                    "_id": "67cfa814d212c9c5048845a0",
                    "name": "Jiazheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67cfa814d212c9c5048845a1",
                    "user": {
                        "_id": "64eac1f496f42afd627d439c",
                        "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg",
                        "isPro": false,
                        "fullname": "Sipeng Zheng",
                        "user": "sipeng9527",
                        "type": "user"
                    },
                    "name": "Sipeng Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T10:59:45.667Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa814d212c9c5048845a2",
                    "user": {
                        "_id": "61e52be53d6dbb1da842316a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
                        "isPro": false,
                        "fullname": "Börje Karlsson",
                        "user": "tellarin",
                        "type": "user"
                    },
                    "name": "Börje F. Karlsson",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:32.095Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa814d212c9c5048845a3",
                    "name": "Zongqing Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T07:32:53.000Z",
            "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
            "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
            "upvotes": 33,
            "discussionId": "67cfa818d212c9c504884689",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "vision towers",
                "multi-turn vision question-answering tasks",
                "multi-turn multimodal dialogue dataset (MMDiag)",
                "GPT assistant",
                "multimodal dialogue learning",
                "grounding",
                "reasoning capabilities",
                "Deliberate module",
                "Gaze module",
                "Chain-of-Thought"
            ]
        },
        "publishedAt": "2025-03-10T03:32:53.000Z",
        "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
        "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07002.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07314",
            "authors": [
                {
                    "_id": "67cfa750c8f2a661dc9798fe",
                    "user": {
                        "_id": "6345a93afe134dfd7a0cfabd",
                        "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
                        "isPro": false,
                        "fullname": "wu weijia",
                        "user": "weijiawu",
                        "type": "user"
                    },
                    "name": "Weijia Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:00:27.031Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa750c8f2a661dc9798ff",
                    "name": "Zeyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "67cfa750c8f2a661dc979900",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:00:52.927Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T13:33:27.000Z",
            "title": "Automated Movie Generation via Multi-Agent CoT Planning",
            "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
            "upvotes": 30,
            "discussionId": "67cfa752c8f2a661dc9799b8",
            "projectPage": "https://weijiawu.github.io/MovieAgent/",
            "githubRepo": "https://github.com/showlab/MovieAgent",
            "ai_keywords": [
                "MovieAgent",
                "Chain of Thought (CoT)",
                "automated movie/long-video generation",
                "multi-scene, multi-shot long-form videos",
                "coherent narrative",
                "character consistency",
                "synchronized subtitles",
                "stable audio",
                "hierarchical CoT-based reasoning",
                "multiple LLM agents",
                "director",
                "screenwriter",
                "storyboard artist",
                "location manager",
                "script faithfulness",
                "narrative coherence",
                "fully automated movie generation"
            ]
        },
        "publishedAt": "2025-03-10T09:33:27.000Z",
        "title": "Automated Movie Generation via Multi-Agent CoT Planning",
        "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07314.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07216",
            "authors": [
                {
                    "_id": "67cfa6fcd77496ce0c154bdc",
                    "user": {
                        "_id": "638716c14e00d7fc0902fef4",
                        "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg",
                        "isPro": false,
                        "fullname": "Sangwoo Park",
                        "user": "Sangsang",
                        "type": "user"
                    },
                    "name": "Sangwoo Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T11:43:53.415Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa6fcd77496ce0c154bdd",
                    "user": {
                        "_id": "64ad5f59b7e4b2c1ce47eb43",
                        "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
                        "isPro": false,
                        "fullname": "Seanie Lee",
                        "user": "Seanie-lee",
                        "type": "user"
                    },
                    "name": "Seanie Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:02:25.920Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa6fcd77496ce0c154bde",
                    "user": {
                        "_id": "65f2ab29d83aee7a6492b7b7",
                        "avatarUrl": "/avatars/d0ff817329ee9a534a936dad41a1943e.svg",
                        "isPro": false,
                        "fullname": "Byungjoo Kim",
                        "user": "matbambbang",
                        "type": "user"
                    },
                    "name": "Byungjoo Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:02:08.406Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa6fcd77496ce0c154bdf",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T11:55:50.000Z",
            "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
            "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
            "upvotes": 26,
            "discussionId": "67cfa6fdd77496ce0c154c18",
            "ai_keywords": [
                "Federated Learning (FL)",
                "vision-language models (VLMs)",
                "membership inference attacks (MIAs)",
                "FedRand framework",
                "Low-Rank Adaptation (LoRA)",
                "subparameters",
                "non-private client parameters",
                "client parameters",
                "aggregation",
                "robustness"
            ]
        },
        "publishedAt": "2025-03-10T07:55:50.000Z",
        "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
        "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07216.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07067",
            "authors": [
                {
                    "_id": "67cfa99b7c95194db8d75468",
                    "user": {
                        "_id": "64b7628af902508f0d7ae112",
                        "avatarUrl": "/avatars/83c155254486e80c1dfd14676fdf9215.svg",
                        "isPro": false,
                        "fullname": "Jongwoo Ko",
                        "user": "jongwooko",
                        "type": "user"
                    },
                    "name": "Jongwoo Ko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:29.622Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa99b7c95194db8d75469",
                    "user": {
                        "_id": "64ad94f05a4a60156925ec96",
                        "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
                        "isPro": false,
                        "fullname": "Tianyi Chen",
                        "user": "tianyic",
                        "type": "user"
                    },
                    "name": "Tianyi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:27.139Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa99b7c95194db8d7546a",
                    "user": {
                        "_id": "63f0c2ac9cf89c9ed1bdd25c",
                        "avatarUrl": "/avatars/856b2cb482250fb83c6fe793e29dfd74.svg",
                        "isPro": false,
                        "fullname": "Sungnyun Kim",
                        "user": "sungnyun",
                        "type": "user"
                    },
                    "name": "Sungnyun Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:03:03.613Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa99b7c95194db8d7546b",
                    "user": {
                        "_id": "646b0c6d3e2a7b0659452fc7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b0c6d3e2a7b0659452fc7/aqT9nbrJtOk38tDw_12xW.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Ding",
                        "user": "tding1",
                        "type": "user"
                    },
                    "name": "Tianyu Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:03:09.967Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa99b7c95194db8d7546c",
                    "name": "Luming Liang",
                    "hidden": false
                },
                {
                    "_id": "67cfa99b7c95194db8d7546d",
                    "name": "Ilya Zharkov",
                    "hidden": false
                },
                {
                    "_id": "67cfa99b7c95194db8d7546e",
                    "name": "Se-Young Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T08:51:32.000Z",
            "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
            "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
            "upvotes": 24,
            "discussionId": "67cfa99c7c95194db8d754bf",
            "githubRepo": "https://github.com/jongwooko/distillm-2",
            "ai_keywords": [
                "contrastive approach",
                "likelihood",
                "DistiLLM-2",
                "instruction-following",
                "code generation",
                "preference alignment",
                "vision-language extensions"
            ]
        },
        "publishedAt": "2025-03-10T04:51:32.000Z",
        "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
        "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07067.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07027",
            "authors": [
                {
                    "_id": "67cf98fd59dbba733d8c531e",
                    "user": {
                        "_id": "636b3f9ce3ad78bc68b67541",
                        "avatarUrl": "/avatars/2b7e745953ae39e01222e99fb63b279e.svg",
                        "isPro": false,
                        "fullname": "yuxuan",
                        "user": "zzyx",
                        "type": "user"
                    },
                    "name": "Yuxuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:57.021Z",
                    "hidden": false
                },
                {
                    "_id": "67cf98fd59dbba733d8c531f",
                    "name": "Yirui Yuan",
                    "hidden": false
                },
                {
                    "_id": "67cf98fd59dbba733d8c5320",
                    "user": {
                        "_id": "64311a95034ecbefddd141ef",
                        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                        "isPro": false,
                        "fullname": "Yiren Song",
                        "user": "yiren98",
                        "type": "user"
                    },
                    "name": "Yiren Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:12:12.348Z",
                    "hidden": false
                },
                {
                    "_id": "67cf98fd59dbba733d8c5321",
                    "user": {
                        "_id": "637745113a63a2983ffbde13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                        "isPro": false,
                        "fullname": "Haofan Wang",
                        "user": "wanghaofan",
                        "type": "user"
                    },
                    "name": "Haofan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:54.821Z",
                    "hidden": false
                },
                {
                    "_id": "67cf98fd59dbba733d8c5322",
                    "name": "Jiaming Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T08:07:17.000Z",
            "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
            "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
            "upvotes": 19,
            "discussionId": "67cf990359dbba733d8c545d",
            "ai_keywords": [
                "Unet-based diffusion models",
                "ControlNet",
                "IP-Adapter",
                "DiT (Diffusion Transformer)",
                "Condition Injection LoRA Module",
                "Condition Injection",
                "zero-shot multi-condition generalization",
                "Position-Aware Training Paradigm",
                "Position-Aware",
                "Causal Attention Mechanism",
                "KV Cache",
                "conditional generation tasks",
                "image synthesis"
            ]
        },
        "publishedAt": "2025-03-10T04:07:17.000Z",
        "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
        "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07027.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06680",
            "authors": [
                {
                    "_id": "67cf94d9f2b1fe815db6db40",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db41",
                    "user": {
                        "_id": "641a9a4b05290a135041a3ed",
                        "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
                        "isPro": false,
                        "fullname": "Pluto",
                        "user": "CharonBony",
                        "type": "user"
                    },
                    "name": "Xin Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-11T01:41:47.194Z",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db42",
                    "name": "Zhongxin Guo",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db43",
                    "user": {
                        "_id": "64af8d4b2cda6a37a4927d72",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/o37G6VkKSyP4N9xktFErV.png",
                        "isPro": false,
                        "fullname": "Shaoguang Mao",
                        "user": "dawnmsg",
                        "type": "user"
                    },
                    "name": "Shaoguang Mao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:04:01.624Z",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db44",
                    "name": "Wen Luo",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db45",
                    "name": "Guangyue Peng",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db46",
                    "user": {
                        "_id": "64c66647725ffa04b2fd6c94",
                        "avatarUrl": "/avatars/620f63f27fa1e90423b0dc22aa8e5809.svg",
                        "isPro": false,
                        "fullname": "yangyu huang",
                        "user": "yangyu90",
                        "type": "user"
                    },
                    "name": "Yangyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:04:18.816Z",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db47",
                    "name": "Houfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "67cf94d9f2b1fe815db6db48",
                    "user": {
                        "_id": "67366efb049bfa3a9084e8d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Bny5bUc6qFQ_RWpEKpRAW.jpeg",
                        "isPro": false,
                        "fullname": "Scarlett Li",
                        "user": "lisijia0504",
                        "type": "user"
                    },
                    "name": "Scarlett Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:04:34.252Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T16:11:57.000Z",
            "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
            "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
            "upvotes": 17,
            "discussionId": "67cf94dbf2b1fe815db6db9e"
        },
        "publishedAt": "2025-03-09T12:11:57.000Z",
        "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
        "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06680.png",
        "numComments": 6,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07608",
            "authors": [
                {
                    "_id": "67cfa5bcb17ca92d24da9033",
                    "user": {
                        "_id": "65a4a180c8a09bd5e8e900b8",
                        "avatarUrl": "/avatars/c135db68f6ff2c40119acd2e9ddce968.svg",
                        "isPro": false,
                        "fullname": "Bo Jiang",
                        "user": "rb93dett",
                        "type": "user"
                    },
                    "name": "Bo Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:43.665Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa5bcb17ca92d24da9034",
                    "user": {
                        "_id": "67adc154a266b54c2835cceb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HxrPMqiiTWE3z8qhMlW_m.png",
                        "isPro": false,
                        "fullname": "Shaoyu Chen",
                        "user": "Atan-0221",
                        "type": "user"
                    },
                    "name": "Shaoyu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:16:49.557Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa5bcb17ca92d24da9035",
                    "name": "Qian Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cfa5bcb17ca92d24da9036",
                    "user": {
                        "_id": "66c2e7fc934e2f07753542ac",
                        "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
                        "isPro": false,
                        "fullname": "LWY",
                        "user": "wenyuliu",
                        "type": "user"
                    },
                    "name": "Wenyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:17:02.692Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa5bcb17ca92d24da9037",
                    "user": {
                        "_id": "62600de6d47e3dbae32ce1ce",
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:16:56.180Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:59:42.000Z",
            "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
            "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
            "upvotes": 14,
            "discussionId": "67cfa5bdb17ca92d24da9064",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "reasoning",
                "end-to-end models",
                "vision-language models (VLMs)",
                "supervised fine-tuning (SFT)",
                "GRPO-based RL rewards",
                "two-stage planning reasoning training strategy",
                "emergent multimodal planning capabilities"
            ]
        },
        "publishedAt": "2025-03-10T13:59:42.000Z",
        "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
        "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07608.png",
        "numComments": 1,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.06749",
            "authors": [
                {
                    "_id": "67cfb6495944a8e54f24cd9a",
                    "user": {
                        "_id": "67c1e934c46bc617b5ea7a3c",
                        "avatarUrl": "/avatars/ea208034285d30c9df61e6ca4752b2eb.svg",
                        "isPro": false,
                        "fullname": "Hwx",
                        "user": "HuangWenXuan",
                        "type": "user"
                    },
                    "name": "Wenxuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:32:39.856Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cd9b",
                    "user": {
                        "_id": "667507c64b1e6619167123b0",
                        "avatarUrl": "/avatars/f101cb9eb1acc0a2cb46f8320907b06f.svg",
                        "isPro": false,
                        "fullname": "Bohan Jia",
                        "user": "BohanJia",
                        "type": "user"
                    },
                    "name": "Bohan Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:32:47.826Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cd9c",
                    "user": {
                        "_id": "6669981b9d56215237eaa25f",
                        "avatarUrl": "/avatars/ba8bd9b34637d1c52cc9ff17cab79da1.svg",
                        "isPro": false,
                        "fullname": "Zijie Zhai",
                        "user": "skyz1010",
                        "type": "user"
                    },
                    "name": "Zijie Zhai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:32:54.315Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cd9d",
                    "name": "Shaosheng Cao",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cd9e",
                    "user": {
                        "_id": "61eeecc16f01506a837664ce",
                        "avatarUrl": "/avatars/61ef427943e2945f512e6e3c705991c7.svg",
                        "isPro": false,
                        "fullname": "Zheyu Ye",
                        "user": "zheyuye",
                        "type": "user"
                    },
                    "name": "Zheyu Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:33:09.475Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cd9f",
                    "name": "Fei Zhao",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cda0",
                    "name": "Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "67cfb6495944a8e54f24cda1",
                    "user": {
                        "_id": "6481ace6578646b5c2370b7e",
                        "avatarUrl": "/avatars/218ffacc705c1fe0e23ef1217f76b374.svg",
                        "isPro": false,
                        "fullname": "linshaohui",
                        "user": "kimlin123",
                        "type": "user"
                    },
                    "name": "Shaohui Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:33:21.819Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T20:06:45.000Z",
            "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
            "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
            "upvotes": 14,
            "discussionId": "67cfb64f5944a8e54f24cf33",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "MLLMs",
                "multimodal reasoning",
                "CoT dataset",
                "modality bridging",
                "data filtering",
                "Vision-R1-cold dataset",
                "Progressive Thinking Suppression Training (PTST)",
                "Group Relative Policy Optimization (GRPO)",
                "hard formatting result reward function",
                "multimodal math dataset",
                "MathVista benchmark",
                "OpenAI O1"
            ]
        },
        "publishedAt": "2025-03-09T16:06:45.000Z",
        "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
        "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06749.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.05244",
            "authors": [
                {
                    "_id": "67cfebe18a4265f3656a50aa",
                    "user": {
                        "_id": "642d430a7f9efee76b8713c0",
                        "avatarUrl": "/avatars/4981f166a6df8e2ea60cd4c41c2f44d4.svg",
                        "isPro": false,
                        "fullname": "YuningWu",
                        "user": "AQuarterMile",
                        "type": "user"
                    },
                    "name": "Yuning Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:24:29.900Z",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50ab",
                    "name": "Jiahao Mei",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50ac",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50ad",
                    "name": "Chenliang Li",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50ae",
                    "name": "SHaopeng Lai",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50af",
                    "name": "Yuran Ren",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50b0",
                    "name": "Zijia Wang",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50b1",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50b2",
                    "name": "Mengyue Wu",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50b3",
                    "name": "Qin Jin",
                    "hidden": false
                },
                {
                    "_id": "67cfebe18a4265f3656a50b4",
                    "name": "Fei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T08:56:20.000Z",
            "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
            "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
            "upvotes": 14,
            "discussionId": "67cfebe38a4265f3656a5136",
            "githubRepo": "https://github.com/X-PLUG/WritingBench",
            "ai_keywords": [
                "large language models (LLMs)",
                "text generation",
                "generative writing",
                "benchmarks",
                "writing domains",
                "subdomains",
                "creative writing",
                "persuasive writing",
                "informative writing",
                "technical writing",
                "query-dependent evaluation framework",
                "instance-specific assessment criteria",
                "critic model",
                "criteria-aware scoring",
                "data curation"
            ]
        },
        "publishedAt": "2025-03-07T03:56:20.000Z",
        "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
        "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05244.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04629",
            "authors": [
                {
                    "_id": "67cfbab6607797f40c6d4164",
                    "user": {
                        "_id": "65b88b92e0bde92c176a888a",
                        "avatarUrl": "/avatars/fc1cb54328ca93860e97fc73a3c1eb2f.svg",
                        "isPro": false,
                        "fullname": "Xiangchao Yan",
                        "user": "yxc97",
                        "type": "user"
                    },
                    "name": "Xiangchao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:39:44.870Z",
                    "hidden": false
                },
                {
                    "_id": "67cfbab6607797f40c6d4165",
                    "name": "Shiyang Feng",
                    "hidden": false
                },
                {
                    "_id": "67cfbab6607797f40c6d4166",
                    "user": {
                        "_id": "64a3d1ddb3239f3e3892b24b",
                        "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
                        "isPro": false,
                        "fullname": "Jiakang Yuan",
                        "user": "JiakangYuan",
                        "type": "user"
                    },
                    "name": "Jiakang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:39:59.177Z",
                    "hidden": false
                },
                {
                    "_id": "67cfbab6607797f40c6d4167",
                    "user": {
                        "_id": "65b7ae76768464877cdb2e39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
                        "isPro": false,
                        "fullname": "Renqiu Xia",
                        "user": "renqiux0302",
                        "type": "user"
                    },
                    "name": "Renqiu Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:40:04.859Z",
                    "hidden": false
                },
                {
                    "_id": "67cfbab6607797f40c6d4168",
                    "user": {
                        "_id": "5e49e8cf37cb5b49818287ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e49e8cf37cb5b49818287ae/IV9b5Z70NhgmBNfAlc_co.jpeg",
                        "isPro": false,
                        "fullname": "Bin Wang",
                        "user": "binwang",
                        "type": "user"
                    },
                    "name": "Bin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:40:11.608Z",
                    "hidden": false
                },
                {
                    "_id": "67cfbab6607797f40c6d4169",
                    "user": {
                        "_id": "643dfd235aafbdca3a5792c0",
                        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
                        "isPro": false,
                        "fullname": "Bo Zhang",
                        "user": "BoZhang",
                        "type": "user"
                    },
                    "name": "Bo Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:40:19.430Z",
                    "hidden": false
                },
                {
                    "_id": "67cfbab6607797f40c6d416a",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T17:15:48.000Z",
            "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
            "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
            "upvotes": 14,
            "discussionId": "67cfbab9607797f40c6d4206",
            "ai_keywords": [
                "LLMs (Large Language Models)",
                "SurveyForge",
                "SurveyBench",
                "AutoSurvey"
            ]
        },
        "publishedAt": "2025-03-06T12:15:48.000Z",
        "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
        "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04629.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07602",
            "authors": [
                {
                    "_id": "67cfb2efb77bc8e7d415f904",
                    "user": {
                        "_id": "637f70d6fab5db9101c3dfc8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f70d6fab5db9101c3dfc8/NgkYNXWLDavLbrnCby2Fl.jpeg",
                        "isPro": false,
                        "fullname": "Yujie Wei",
                        "user": "weilllllls",
                        "type": "user"
                    },
                    "name": "Yujie Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:17:35.633Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f905",
                    "name": "Shiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f906",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:21:32.780Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f907",
                    "name": "Biao Gong",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f908",
                    "user": {
                        "_id": "6492a0d8d4ae24c933ace44d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DXIky2sdPwmiCOR9p-JBQ.png",
                        "isPro": false,
                        "fullname": "Longxiang Tang",
                        "user": "lloong",
                        "type": "user"
                    },
                    "name": "Longxiang Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:21:30.700Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f909",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f90a",
                    "name": "Haonan Qiu",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f90b",
                    "name": "Hengjia Li",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f90c",
                    "user": {
                        "_id": "6697b62a27bf71ffe289492f",
                        "avatarUrl": "/avatars/f48095f1e2ca69836294343301be1700.svg",
                        "isPro": false,
                        "fullname": "Shuai Tan",
                        "user": "Shuaishuai0219",
                        "type": "user"
                    },
                    "name": "Shuai Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T10:22:02.278Z",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f90d",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cfb2efb77bc8e7d415f90e",
                    "name": "Hongming Shan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:58:03.000Z",
            "title": "DreamRelation: Relation-Centric Video Customization",
            "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
            "upvotes": 12,
            "discussionId": "67cfb2f1b77bc8e7d415f96b",
            "ai_keywords": [
                "Relational Decoupling Learning",
                "Relational Dynamics Enhancement",
                "relation LoRA triplet",
                "hybrid mask training strategy",
                "attention mechanism",
                "space-time relational contrastive loss",
                "MM-DiT"
            ]
        },
        "publishedAt": "2025-03-10T13:58:03.000Z",
        "title": "DreamRelation: Relation-Centric Video Customization",
        "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07602.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07459",
            "authors": [
                {
                    "_id": "67cfd1934fed2b7e3e4cbb34",
                    "user": {
                        "_id": "63357c608adfa81faf2ac180",
                        "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
                        "isPro": false,
                        "fullname": "Xiangru Tang",
                        "user": "RTT1",
                        "type": "user"
                    },
                    "name": "Xiangru Tang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-11T06:00:52.457Z",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb35",
                    "name": "Daniel Shao",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb36",
                    "name": "Jiwoong Sohn",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb37",
                    "name": "Jiapeng Chen",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb38",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb39",
                    "name": "Jinyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb3a",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb3b",
                    "name": "Yilun Zhao",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb3c",
                    "name": "Chenglin Wu",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb3d",
                    "user": {
                        "_id": "65cae89119683f9817c049ea",
                        "avatarUrl": "/avatars/b08b10d7c72e2cf1108147e659411b32.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shi",
                        "user": "wshi83",
                        "type": "user"
                    },
                    "name": "Wenqi Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:21:16.321Z",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb3e",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "67cfd1934fed2b7e3e4cbb3f",
                    "name": "Mark Gerstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T15:38:44.000Z",
            "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
            "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
            "upvotes": 12,
            "discussionId": "67cfd1944fed2b7e3e4cbb81",
            "ai_keywords": [
                "MedAgentsBench",
                "multi-step clinical reasoning",
                "diagnosis formulation",
                "treatment planning",
                "MedAgentsBench",
                "DeepSeek R1",
                "OpenAI o3",
                "advanced search-based agent methods"
            ]
        },
        "publishedAt": "2025-03-10T11:38:44.000Z",
        "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
        "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07459.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04973",
            "authors": [
                {
                    "_id": "67d01f99a47b429698a2b34e",
                    "user": {
                        "_id": "62c88e75a5ac2974c0a5c8ea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666106981514-62c88e75a5ac2974c0a5c8ea.jpeg",
                        "isPro": true,
                        "fullname": "Giulio Corallo",
                        "user": "giulio98",
                        "type": "user"
                    },
                    "name": "Giulio Corallo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T11:43:02.035Z",
                    "hidden": false
                },
                {
                    "_id": "67d01f99a47b429698a2b34f",
                    "user": {
                        "_id": "6362d9712691058b19de1ba4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6362d9712691058b19de1ba4/c9QrA2oE6lcs_46ShaTY1.jpeg",
                        "isPro": true,
                        "fullname": "Orion Weller",
                        "user": "orionweller",
                        "type": "user"
                    },
                    "name": "Orion Weller",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:17:49.755Z",
                    "hidden": false
                },
                {
                    "_id": "67d01f99a47b429698a2b350",
                    "user": {
                        "_id": "6707ed4d371a647d6c8b5f04",
                        "avatarUrl": "/avatars/e9cb4c9caa3943588891e41e7f62cd69.svg",
                        "isPro": false,
                        "fullname": "Fabio Petroni",
                        "user": "fabiopetroni",
                        "type": "user"
                    },
                    "name": "Fabio Petroni",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:17:56.241Z",
                    "hidden": false
                },
                {
                    "_id": "67d01f99a47b429698a2b351",
                    "user": {
                        "_id": "67cfe779293bf2fc0c7163f2",
                        "avatarUrl": "/avatars/6d719eafd43a214d34d61bc6d9908511.svg",
                        "isPro": false,
                        "fullname": "Paolo Papotti",
                        "user": "papotti",
                        "type": "user"
                    },
                    "name": "Paolo Papotti",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-11T12:35:45.283Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T21:07:41.000Z",
            "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
            "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
            "upvotes": 12,
            "discussionId": "67d01f9aa47b429698a2b39d",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "key-value (KV) cache compression",
                "zero-shot setup",
                "few-shot setup",
                "LongBench v2",
                "inference latency"
            ]
        },
        "publishedAt": "2025-03-06T16:07:41.000Z",
        "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
        "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04973.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06580",
            "authors": [
                {
                    "_id": "67cfa71827c7f0b2db19f7c2",
                    "user": {
                        "_id": "645b4a2978730bcc103dfe4d",
                        "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Zhang",
                        "user": "TokerZ",
                        "type": "user"
                    },
                    "name": "Yuxiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:40.336Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa71827c7f0b2db19f7c3",
                    "user": {
                        "_id": "65b5b586d2ae98fd67c76d6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RR_mNkUAG4zP902WLgeYv.jpeg",
                        "isPro": false,
                        "fullname": "yuqi yang",
                        "user": "tzteyang",
                        "type": "user"
                    },
                    "name": "Yuqi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T11:09:10.879Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa71827c7f0b2db19f7c4",
                    "name": "Jiangming Shu",
                    "hidden": false
                },
                {
                    "_id": "67cfa71827c7f0b2db19f7c5",
                    "name": "Xinyan Wen",
                    "hidden": false
                },
                {
                    "_id": "67cfa71827c7f0b2db19f7c6",
                    "name": "Jitao Sang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T12:19:47.000Z",
            "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
            "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
            "upvotes": 11,
            "discussionId": "67cfa71927c7f0b2db19f817",
            "githubRepo": "https://github.com/ADaM-BJTU/AutoCoA",
            "ai_keywords": [
                "Large Agent Models (LAMs)",
                "Chain-of-Action (CoA)",
                "AutoCoA framework",
                "supervised fine-tuning (SFT)",
                "reinforcement learning (RL)",
                "step-level action triggering",
                "trajectory-level CoA optimization",
                "internal world model"
            ]
        },
        "publishedAt": "2025-03-09T08:19:47.000Z",
        "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
        "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06580.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04812",
            "authors": [
                {
                    "_id": "67ce5542818e1825dea7440b",
                    "user": {
                        "_id": "6626449503e1f561573d30e9",
                        "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
                        "isPro": false,
                        "fullname": "Zhibin Lan",
                        "user": "zhibinlan",
                        "type": "user"
                    },
                    "name": "Zhibin Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:00:54.535Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5542818e1825dea7440c",
                    "user": {
                        "_id": "635239137d071f23d083b056",
                        "avatarUrl": "/avatars/1f1a0ed38d8de499d4b78922801c6d95.svg",
                        "isPro": false,
                        "fullname": "liqiang niu",
                        "user": "lqniu",
                        "type": "user"
                    },
                    "name": "Liqiang Niu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:00:51.713Z",
                    "hidden": false
                },
                {
                    "_id": "67ce5542818e1825dea7440d",
                    "name": "Fandong Meng",
                    "hidden": false
                },
                {
                    "_id": "67ce5542818e1825dea7440e",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67ce5542818e1825dea7440f",
                    "name": "Jinsong Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T10:21:57.000Z",
            "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
            "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
            "upvotes": 10,
            "discussionId": "67ce5543818e1825dea74480",
            "githubRepo": "https://github.com/DeepLearnXMU/LLaVE",
            "ai_keywords": [
                "multimodal embedding models",
                "interleaved image-text retrieval",
                "multimodal RAG",
                "multimodal clustering",
                "LMM-based embedding models",
                "InfoNCE loss",
                "similarity distribution",
                "hard negative pairs",
                "representation learning",
                "LLaVE",
                "MMEB benchmark",
                "meta-tasks",
                "datasets",
                "state-of-the-art (SOTA)",
                "scalability",
                "efficiency",
                "text-video retrieval tasks",
                "zero-shot manner",
                "transfer"
            ]
        },
        "publishedAt": "2025-03-04T05:21:57.000Z",
        "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
        "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04812.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07334",
            "authors": [
                {
                    "_id": "67d04cecf9bcb732f0880c96",
                    "user": {
                        "_id": "67887a429ede23ff0a964c98",
                        "avatarUrl": "/avatars/12dea48e4b37e8a7f5c9bbbc733f2348.svg",
                        "isPro": false,
                        "fullname": "xing xie",
                        "user": "xing0916",
                        "type": "user"
                    },
                    "name": "Xing Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T14:55:23.006Z",
                    "hidden": false
                },
                {
                    "_id": "67d04cecf9bcb732f0880c97",
                    "name": "Jiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "67d04cecf9bcb732f0880c98",
                    "name": "Ziyue Lin",
                    "hidden": false
                },
                {
                    "_id": "67d04cecf9bcb732f0880c99",
                    "name": "Huijie Fan",
                    "hidden": false
                },
                {
                    "_id": "67d04cecf9bcb732f0880c9a",
                    "name": "Zhi Han",
                    "hidden": false
                },
                {
                    "_id": "67d04cecf9bcb732f0880c9b",
                    "name": "Yandong Tang",
                    "hidden": false
                },
                {
                    "_id": "67d04cecf9bcb732f0880c9c",
                    "name": "Liangqiong Qu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T13:49:28.000Z",
            "title": "Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment",
            "summary": "We present Autoregressive Representation Alignment (ARRA), a new training\nframework that unlocks global-coherent text-to-image generation in\nautoregressive LLMs without architectural changes. Unlike prior work that\nrequires complex architectural redesigns, ARRA aligns LLM hidden states with\nvisual representations from external visual foundational models via a global\nvisual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual\nconstraints: local next-token prediction and global semantic distillation,\nenabling LLMs to implicitly learn spatial and contextual coherence while\nretaining their original autoregressive paradigm. Extensive experiments\nvalidate ARRA's plug-and-play versatility. When training from\ntext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%\n(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive\nLLMs like Chameleon and LlamaGen, all without framework modifications. For\ndomain adaption, ARRA aligns general-purpose LLMs with specialized models\n(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on\nmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign\n-- not just architectural innovation -- can resolve cross-modal global\ncoherence challenges, ARRA offers a complementary paradigm for advancing\nautoregressive models. Code and models will be released to advance\nautoregressive image generation.",
            "upvotes": 9,
            "discussionId": "67d04cf0f9bcb732f0880df7",
            "ai_keywords": [
                "Autoregressive Representation Alignment (ARRA)",
                "global-coherent text-to-image generation",
                "autoregressive LLMs",
                "global visual alignment loss",
                "hybrid token",
                "local next-token prediction",
                "global semantic distillation",
                "spatial and contextual coherence",
                "FID",
                "MIMIC-CXR",
                "DeepEyeNet",
                "ImageNet",
                "Chameleon",
                "LlamaGen",
                "domain adaption",
                "BioMedCLIP",
                "training objective redesign",
                "cross-modal global coherence"
            ]
        },
        "publishedAt": "2025-03-10T09:49:28.000Z",
        "title": "Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment",
        "summary": "We present Autoregressive Representation Alignment (ARRA), a new training\nframework that unlocks global-coherent text-to-image generation in\nautoregressive LLMs without architectural changes. Unlike prior work that\nrequires complex architectural redesigns, ARRA aligns LLM hidden states with\nvisual representations from external visual foundational models via a global\nvisual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual\nconstraints: local next-token prediction and global semantic distillation,\nenabling LLMs to implicitly learn spatial and contextual coherence while\nretaining their original autoregressive paradigm. Extensive experiments\nvalidate ARRA's plug-and-play versatility. When training from\ntext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%\n(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive\nLLMs like Chameleon and LlamaGen, all without framework modifications. For\ndomain adaption, ARRA aligns general-purpose LLMs with specialized models\n(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on\nmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign\n-- not just architectural innovation -- can resolve cross-modal global\ncoherence challenges, ARRA offers a complementary paradigm for advancing\nautoregressive models. Code and models will be released to advance\nautoregressive image generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07334.png",
        "numComments": 1,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.07507",
            "authors": [
                {
                    "_id": "67cfa44c3a9d50150f59ffe1",
                    "name": "Jie Hu",
                    "hidden": false
                },
                {
                    "_id": "67cfa44c3a9d50150f59ffe2",
                    "user": {
                        "_id": "63492af03caab0136df08d25",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63492af03caab0136df08d25/-aZzajKl38jT29A-kziCI.jpeg",
                        "isPro": false,
                        "fullname": "Shizun Wang",
                        "user": "littlepure2333",
                        "type": "user"
                    },
                    "name": "Shizun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:40:56.536Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa44c3a9d50150f59ffe3",
                    "user": {
                        "_id": "63fc03a50aab060792ffef39",
                        "avatarUrl": "/avatars/9d5b1bb2a41928e08176b703935133ab.svg",
                        "isPro": false,
                        "fullname": "Wangxinchao",
                        "user": "wxcTest",
                        "type": "user"
                    },
                    "name": "Xinchao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:40:50.238Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T16:29:10.000Z",
            "title": "PE3R: Perception-Efficient 3D Reconstruction",
            "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
            "upvotes": 8,
            "discussionId": "67cfa4503a9d50150f5a0137",
            "githubRepo": "https://github.com/hujiecpp/PE3R",
            "ai_keywords": [
                "feed-forward architecture",
                "3D semantic field reconstruction",
                "zero-shot generalization",
                "2D-to-3D open-vocabulary segmentation",
                "perception accuracy",
                "reconstruction precision",
                "speedup"
            ]
        },
        "publishedAt": "2025-03-10T12:29:10.000Z",
        "title": "PE3R: Perception-Efficient 3D Reconstruction",
        "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07507.png",
        "numComments": 1,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.07197",
            "authors": [
                {
                    "_id": "67cfa76cf36e4221c5009654",
                    "user": {
                        "_id": "624f909eac5dd186b01ac3f5",
                        "avatarUrl": "/avatars/71a5c93c491064ef9e1eda80fda90665.svg",
                        "isPro": false,
                        "fullname": "Zebin You",
                        "user": "yyyou",
                        "type": "user"
                    },
                    "name": "Zebin You",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:22:38.059Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa76cf36e4221c5009655",
                    "user": {
                        "_id": "656949b71d7c2ca7b7aae5f2",
                        "avatarUrl": "/avatars/e7b23e260eb348cc26b849aaa601a503.svg",
                        "isPro": false,
                        "fullname": "Jingyang Ou",
                        "user": "JingyangOu",
                        "type": "user"
                    },
                    "name": "Jingyang Ou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:51:53.871Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa76cf36e4221c5009656",
                    "user": {
                        "_id": "67513d6d3b8586521cda5d76",
                        "avatarUrl": "/avatars/0f95cc5c23a0a1da289aa785bd33b616.svg",
                        "isPro": false,
                        "fullname": "Xiaolu  Zhang",
                        "user": "xiaolu0714",
                        "type": "user"
                    },
                    "name": "Xiaolu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:52:08.690Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa76cf36e4221c5009657",
                    "name": "Jun Hu",
                    "hidden": false
                },
                {
                    "_id": "67cfa76cf36e4221c5009658",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "67cfa76cf36e4221c5009659",
                    "user": {
                        "_id": "64c07b488e2612254361153b",
                        "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg",
                        "isPro": false,
                        "fullname": "chongxuan li",
                        "user": "zhenxuan00",
                        "type": "user"
                    },
                    "name": "Chongxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:52:20.109Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T11:27:12.000Z",
            "title": "Effective and Efficient Masked Image Generation Models",
            "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
            "upvotes": 8,
            "discussionId": "67cfa76df36e4221c5009686",
            "githubRepo": "https://github.com/ML-GSAI/eMIGM",
            "ai_keywords": [
                "masked image generation models",
                "masked diffusion models",
                "training and sampling",
                "Fr\\'echet Inception Distance (FID)",
                "function evaluations (NFEs)",
                "VAR",
                "continuous diffusion models"
            ]
        },
        "publishedAt": "2025-03-10T07:27:12.000Z",
        "title": "Effective and Efficient Masked Image Generation Models",
        "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07197.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05856",
            "authors": [
                {
                    "_id": "67d000d9113c8d298a25100c",
                    "user": {
                        "_id": "6627bb750723599e5c2fdfcf",
                        "avatarUrl": "/avatars/391f72a7a9a9c79f8bf87b55000610cd.svg",
                        "isPro": false,
                        "fullname": "Lorenz Wolf",
                        "user": "Llwo",
                        "type": "user"
                    },
                    "name": "Lorenz Wolf",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T10:22:08.605Z",
                    "hidden": false
                },
                {
                    "_id": "67d000d9113c8d298a25100d",
                    "name": "Sangwoong Yoon",
                    "hidden": false
                },
                {
                    "_id": "67d000d9113c8d298a25100e",
                    "name": "Ilija Bogunovic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T14:46:39.000Z",
            "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs",
            "summary": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a single carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
            "upvotes": 7,
            "discussionId": "67d000da113c8d298a251042",
            "ai_keywords": [
                "Mixture of large language model (LLMs) Agents (MoA)",
                "large language model (LLMs)",
                "AlpacaEval 2.0",
                "inference time",
                "deceptive LLM agents",
                "propagation of deceptive information",
                "model size",
                "information availability",
                "LLaMA 3.1-70B",
                "length-controlled Win Rate (LC WR)",
                "unsupervised defense mechanisms",
                "Doge of Venice voting process"
            ]
        },
        "publishedAt": "2025-03-07T09:46:39.000Z",
        "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs",
        "summary": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a single carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05856.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06520",
            "authors": [
                {
                    "_id": "67cf990ca80a73999cc816c3",
                    "name": "Yuqi Liu",
                    "hidden": false
                },
                {
                    "_id": "67cf990ca80a73999cc816c4",
                    "user": {
                        "_id": "673a10f911b7efeeedabc252",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T7ySn7F0pTVCvRdcvMz3d.png",
                        "isPro": false,
                        "fullname": "Bohao Peng",
                        "user": "BoHao0326",
                        "type": "user"
                    },
                    "name": "Bohao Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:51:05.435Z",
                    "hidden": false
                },
                {
                    "_id": "67cf990ca80a73999cc816c5",
                    "user": {
                        "_id": "65d882d30f35ed3f52d3ae2c",
                        "avatarUrl": "/avatars/22cda67c3fcd7150320ec3551eda90f5.svg",
                        "isPro": false,
                        "fullname": "Zhisheng Zhong",
                        "user": "zszhong",
                        "type": "user"
                    },
                    "name": "Zhisheng Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:50:58.642Z",
                    "hidden": false
                },
                {
                    "_id": "67cf990ca80a73999cc816c6",
                    "name": "Zihao Yue",
                    "hidden": false
                },
                {
                    "_id": "67cf990ca80a73999cc816c7",
                    "user": {
                        "_id": "645b6094bc7518912e1fbc34",
                        "avatarUrl": "/avatars/5e1a875ba3ce350e71fe7049ca6a44c1.svg",
                        "isPro": false,
                        "fullname": "Fanbin Lu",
                        "user": "Fanbin",
                        "type": "user"
                    },
                    "name": "Fanbin Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:51:21.227Z",
                    "hidden": false
                },
                {
                    "_id": "67cf990ca80a73999cc816c8",
                    "name": "Bei Yu",
                    "hidden": false
                },
                {
                    "_id": "67cf990ca80a73999cc816c9",
                    "name": "Jiaya Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T08:48:51.000Z",
            "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
            "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
            "upvotes": 6,
            "discussionId": "67cf990da80a73999cc81723",
            "ai_keywords": [
                "Seg-Zero",
                "decoupled architecture",
                "reasoning model",
                "segmentation model",
                "positional prompts",
                "pixel-level masks",
                "cognitive reinforcement",
                "reward mechanism",
                "reinforcement learning",
                "GRPO",
                "zero-shot generalization",
                "ReasonSeg benchmark",
                "emergent test-time reasoning"
            ]
        },
        "publishedAt": "2025-03-09T04:48:51.000Z",
        "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
        "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06520.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06121",
            "authors": [
                {
                    "_id": "67cfa436d37b8309603da1ee",
                    "user": {
                        "_id": "66de61d7174e9c6971dbb253",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sM0xfS7HAkf_6GmkEGjDk.png",
                        "isPro": false,
                        "fullname": "Alic Li",
                        "user": "Alic-Li",
                        "type": "user"
                    },
                    "name": "Li weile",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-11T04:29:57.207Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa436d37b8309603da1ef",
                    "user": {
                        "_id": "6176b32847ee6431f632981e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
                        "isPro": false,
                        "fullname": "IvanD",
                        "user": "xiaol",
                        "type": "user"
                    },
                    "name": "Liu Xiao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-11T03:33:06.087Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-08T08:31:18.000Z",
            "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
            "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
            "upvotes": 5,
            "discussionId": "67cfa437d37b8309603da253",
            "ai_keywords": [
                "Transformers",
                "LSTMs",
                "GRUs",
                "RWKV-7",
                "meta-learning",
                "state update mechanism",
                "time mix",
                "channel mix",
                "Timer",
                "performance improvement",
                "training time",
                "parameters"
            ]
        },
        "publishedAt": "2025-03-08T03:31:18.000Z",
        "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
        "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06121.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.03499",
            "authors": [
                {
                    "_id": "67cb02680a2a716f25805cb4",
                    "user": {
                        "_id": "63e1d5247fbb6ae4d4f4cc8e",
                        "avatarUrl": "/avatars/8a8f700adf9e8000641c2c2f6bd56080.svg",
                        "isPro": false,
                        "fullname": "Wonjun Kang",
                        "user": "wjkang",
                        "type": "user"
                    },
                    "name": "Wonjun Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:54:53.750Z",
                    "hidden": false
                },
                {
                    "_id": "67cb02680a2a716f25805cb5",
                    "user": {
                        "_id": "630c90123dc31beba6e8f406",
                        "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
                        "isPro": false,
                        "fullname": "Kevin Galim",
                        "user": "kev95",
                        "type": "user"
                    },
                    "name": "Kevin Galim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:55:00.348Z",
                    "hidden": false
                },
                {
                    "_id": "67cb02680a2a716f25805cb6",
                    "user": {
                        "_id": "65ded6ec6efc2a4ebb18d306",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/SPsUBPd_CKMBzACSCgwP5.png",
                        "isPro": false,
                        "fullname": "Yuchen Zeng",
                        "user": "yzeng58",
                        "type": "user"
                    },
                    "name": "Yuchen Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:55:07.129Z",
                    "hidden": false
                },
                {
                    "_id": "67cb02680a2a716f25805cb7",
                    "name": "Minjae Lee",
                    "hidden": false
                },
                {
                    "_id": "67cb02680a2a716f25805cb8",
                    "name": "Hyung Il Koo",
                    "hidden": false
                },
                {
                    "_id": "67cb02680a2a716f25805cb9",
                    "name": "Nam Ik Cho",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T13:44:42.000Z",
            "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
            "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
            "upvotes": 5,
            "discussionId": "67cb02690a2a716f25805cfd",
            "githubRepo": "https://github.com/furiosa-ai/ssm-state-tuning",
            "ai_keywords": [
                "State Space Models (SSMs)",
                "Parameter-Efficient Fine-Tuning (PEFT)",
                "Prompt Tuning",
                "Prefix-Tuning",
                "State-based methods",
                "State-offset Tuning",
                "timesteps",
                "state-related features",
                "state-at-the-current-step"
            ]
        },
        "publishedAt": "2025-03-05T08:44:42.000Z",
        "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
        "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03499.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02199",
            "authors": [
                {
                    "_id": "67c90dad6f3ef3c2c77689b0",
                    "name": "Ailin Deng",
                    "hidden": false
                },
                {
                    "_id": "67c90dad6f3ef3c2c77689b1",
                    "name": "Tri Cao",
                    "hidden": false
                },
                {
                    "_id": "67c90dad6f3ef3c2c77689b2",
                    "user": {
                        "_id": "67cbb6ea2cc05acaab023f75",
                        "avatarUrl": "/avatars/79272c8889a8c472cf75172ead72daea.svg",
                        "isPro": false,
                        "fullname": "Zhirui Chen",
                        "user": "ryanchen42",
                        "type": "user"
                    },
                    "name": "Zhirui Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:03:34.042Z",
                    "hidden": false
                },
                {
                    "_id": "67c90dad6f3ef3c2c77689b3",
                    "name": "Bryan Hooi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T02:21:07.000Z",
            "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
            "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
            "upvotes": 5,
            "discussionId": "67c90dae6f3ef3c2c77689ec",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "blind faith in text",
                "modality preferences",
                "textual variations",
                "vision-centric tasks",
                "text bias",
                "instruction prompts",
                "language model size",
                "token order",
                "positional biases",
                "multi-modal data",
                "supervised fine-tuning",
                "text augmentation",
                "balanced training",
                "modality interactions"
            ]
        },
        "publishedAt": "2025-03-03T21:21:07.000Z",
        "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
        "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02199.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07595",
            "authors": [
                {
                    "_id": "67cfa440aff9c98bb3f45a56",
                    "user": {
                        "_id": "64b3fc1fa24816979609dcb3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b3fc1fa24816979609dcb3/cHRMs4YegRcgbZO8_bBaZ.jpeg",
                        "isPro": false,
                        "fullname": "Sinclair Schneider",
                        "user": "SinclairSchneider",
                        "type": "user"
                    },
                    "name": "Sinclair Schneider",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-11T02:48:03.261Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa440aff9c98bb3f45a57",
                    "user": {
                        "_id": "64de10ee5e19298505465326",
                        "avatarUrl": "/avatars/3ae7c6604bac70a6db62de08d08e8a2c.svg",
                        "isPro": false,
                        "fullname": "Florian Steuber",
                        "user": "steuber",
                        "type": "user"
                    },
                    "name": "Florian Steuber",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:50:06.762Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa440aff9c98bb3f45a58",
                    "user": {
                        "_id": "64dadb5a311afacb53a7c2d0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64dadb5a311afacb53a7c2d0/bPH3B3B0WbADCS_k9ilAU.png",
                        "isPro": false,
                        "fullname": "Joao Schneider",
                        "user": "JoaoSchneider",
                        "type": "user"
                    },
                    "name": "Joao A. G. Schneider",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:50:18.046Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa440aff9c98bb3f45a59",
                    "name": "Gabi Dreo Rodosek",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:56:25.000Z",
            "title": "Detection Avoidance Techniques for Large Language Models",
            "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
            "upvotes": 4,
            "discussionId": "67cfa441aff9c98bb3f45a95",
            "ai_keywords": [
                "large language models",
                "fake news",
                "classification systems",
                "DetectGPT",
                "evasion techniques",
                "generative models",
                "temperature",
                "shallow learning-detectors",
                "fine-tuning",
                "reinforcement learning",
                "BERT-based-detectors",
                "zero-shot-detectors",
                "rephrasing"
            ]
        },
        "publishedAt": "2025-03-10T13:56:25.000Z",
        "title": "Detection Avoidance Techniques for Large Language Models",
        "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07595.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07465",
            "authors": [
                {
                    "_id": "67cfaaed7f229132171f596b",
                    "name": "Ao Wang",
                    "hidden": false
                },
                {
                    "_id": "67cfaaed7f229132171f596c",
                    "name": "Lihao Liu",
                    "hidden": false
                },
                {
                    "_id": "67cfaaed7f229132171f596d",
                    "name": "Hui Chen",
                    "hidden": false
                },
                {
                    "_id": "67cfaaed7f229132171f596e",
                    "name": "Zijia Lin",
                    "hidden": false
                },
                {
                    "_id": "67cfaaed7f229132171f596f",
                    "name": "Jungong Han",
                    "hidden": false
                },
                {
                    "_id": "67cfaaed7f229132171f5970",
                    "name": "Guiguang Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T15:42:59.000Z",
            "title": "YOLOE: Real-Time Seeing Anything",
            "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3times\nless training cost and 1.4times inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
            "upvotes": 4,
            "discussionId": "67cfaaf27f229132171f5ab4",
            "ai_keywords": [
                "Object detection",
                "Segmentation",
                "YOLO series",
                "Open-set methods",
                "Text prompts",
                "Visual cues",
                "Prompt-free paradigm",
                "YOLOE",
                "Rep-parameterizable Region-Text Alignment (RepRTA)",
                "Pretrained textual embeddings",
                "Re-parameterizable lightweight auxiliary network",
                "Semantic-Activated Visual Prompt Encoder (SAVPE)",
                "Decoupled semantic and activation branches",
                "Visual embedding",
                "Lazy Region-Prompt Contrast (LRPC)",
                "Large vocabulary",
                "Specialized embedding",
                "LVIS",
                "Zero-shot performance",
                "Transferability",
                "Inference efficiency",
                "Training cost",
                "AP",
                "COCO",
                "Closed-set YOLOv8-L",
                "Inference speedup",
                "Training time"
            ]
        },
        "publishedAt": "2025-03-10T11:42:59.000Z",
        "title": "YOLOE: Real-Time Seeing Anything",
        "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3times\nless training cost and 1.4times inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07465.png",
        "numComments": 1,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.07274",
            "authors": [
                {
                    "_id": "67d010ecdf1cf0ec0b961fd1",
                    "user": {
                        "_id": "65e1863062dc883757896f98",
                        "avatarUrl": "/avatars/125c56f4c605a09485e0b17e76897bdf.svg",
                        "isPro": false,
                        "fullname": "Cristian Perez Jensen",
                        "user": "cristianpjensen",
                        "type": "user"
                    },
                    "name": "Cristian Perez Jensen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T10:57:59.848Z",
                    "hidden": false
                },
                {
                    "_id": "67d010ecdf1cf0ec0b961fd2",
                    "user": {
                        "_id": "63b4b02a103617b0a5b0ee2e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                        "isPro": false,
                        "fullname": "Seyedmorteza Sadat",
                        "user": "msadat97",
                        "type": "user"
                    },
                    "name": "Seyedmorteza Sadat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T10:37:28.600Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T12:55:08.000Z",
            "title": "Efficient Distillation of Classifier-Free Guidance using Adapters",
            "summary": "While classifier-free guidance (CFG) is essential for conditional diffusion\nmodels, it doubles the number of neural function evaluations (NFEs) per\ninference step. To mitigate this inefficiency, we introduce adapter guidance\ndistillation (AGD), a novel approach that simulates CFG in a single forward\npass. AGD leverages lightweight adapters to approximate CFG, effectively\ndoubling the sampling speed while maintaining or even improving sample quality.\nUnlike prior guidance distillation methods that tune the entire model, AGD\nkeeps the base model frozen and only trains minimal additional parameters\n(sim2%) to significantly reduce the resource requirement of the distillation\nphase. Additionally, this approach preserves the original model weights and\nenables the adapters to be seamlessly combined with other checkpoints derived\nfrom the same base model. We also address a key mismatch between training and\ninference in existing guidance distillation methods by training on CFG-guided\ntrajectories instead of standard diffusion trajectories. Through extensive\nexperiments, we show that AGD achieves comparable or superior FID to CFG across\nmultiple architectures with only half the NFEs. Notably, our method enables the\ndistillation of large models (sim2.6B parameters) on a single consumer GPU\nwith 24 GB of VRAM, making it more accessible than previous approaches that\nrequire multiple high-end GPUs. We will publicly release the implementation of\nour method.",
            "upvotes": 4,
            "discussionId": "67d010f1df1cf0ec0b96215f",
            "ai_keywords": [
                "classifier-free guidance (CFG)",
                "diffusion models",
                "neural function evaluations (NFEs)",
                "adapter guidance distillation (AGD)",
                "lightweight adapters",
                "sampling speed",
                "sample quality",
                "guidance distillation methods",
                "base model",
                "minimal additional parameters",
                "resource requirement",
                "training trajectories",
                "diffusion trajectories",
                "FID",
                "large models",
                "consumer GPU",
                "VRAM"
            ]
        },
        "publishedAt": "2025-03-10T08:55:08.000Z",
        "title": "Efficient Distillation of Classifier-Free Guidance using Adapters",
        "summary": "While classifier-free guidance (CFG) is essential for conditional diffusion\nmodels, it doubles the number of neural function evaluations (NFEs) per\ninference step. To mitigate this inefficiency, we introduce adapter guidance\ndistillation (AGD), a novel approach that simulates CFG in a single forward\npass. AGD leverages lightweight adapters to approximate CFG, effectively\ndoubling the sampling speed while maintaining or even improving sample quality.\nUnlike prior guidance distillation methods that tune the entire model, AGD\nkeeps the base model frozen and only trains minimal additional parameters\n(sim2%) to significantly reduce the resource requirement of the distillation\nphase. Additionally, this approach preserves the original model weights and\nenables the adapters to be seamlessly combined with other checkpoints derived\nfrom the same base model. We also address a key mismatch between training and\ninference in existing guidance distillation methods by training on CFG-guided\ntrajectories instead of standard diffusion trajectories. Through extensive\nexperiments, we show that AGD achieves comparable or superior FID to CFG across\nmultiple architectures with only half the NFEs. Notably, our method enables the\ndistillation of large models (sim2.6B parameters) on a single consumer GPU\nwith 24 GB of VRAM, making it more accessible than previous approaches that\nrequire multiple high-end GPUs. We will publicly release the implementation of\nour method.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07274.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07603",
            "authors": [
                {
                    "_id": "67cfc310f2b1fe815dc24ebf",
                    "user": {
                        "_id": "6365c1158b6bd0d9f6605d3d",
                        "avatarUrl": "/avatars/1e3e6573e0cb2bb1b9d8043045120ee6.svg",
                        "isPro": false,
                        "fullname": "Sedrick Keh",
                        "user": "sedrickkeh",
                        "type": "user"
                    },
                    "name": "Sedrick Keh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:52:53.915Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec0",
                    "user": {
                        "_id": "62be399f01dc22b4d22fc990",
                        "avatarUrl": "/avatars/3e1f3ed30773cbb410b8a9d3c854cac7.svg",
                        "isPro": false,
                        "fullname": "Jean Mercat",
                        "user": "jmercat",
                        "type": "user"
                    },
                    "name": "Jean Mercat",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:53:00.187Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec1",
                    "user": {
                        "_id": "62be1cd77c0388414f012f23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656626356583-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Samir Gadre",
                        "user": "samirg",
                        "type": "user"
                    },
                    "name": "Samir Yitzhak Gadre",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:53:10.097Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec2",
                    "name": "Kushal Arora",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec3",
                    "user": {
                        "_id": "633b2d317af633cbcd03a53e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633b2d317af633cbcd03a53e/awtbCqUE4qBusxuu17iFg.png",
                        "isPro": false,
                        "fullname": "Igor Vasiljevic",
                        "user": "ivas-tri",
                        "type": "user"
                    },
                    "name": "Igor Vasiljevic",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:54:33.031Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec4",
                    "name": "Benjamin Burchfiel",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec5",
                    "name": "Shuran Song",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec6",
                    "user": {
                        "_id": "668924108c440fe1951e2d2a",
                        "avatarUrl": "/avatars/97e60f1ecb0b7d8f7238a4e92944e370.svg",
                        "isPro": false,
                        "fullname": "Russ Tedrake",
                        "user": "RussTedrake",
                        "type": "user"
                    },
                    "name": "Russ Tedrake",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:53:34.722Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec7",
                    "user": {
                        "_id": "64358e662d0ed796668cb438",
                        "avatarUrl": "/avatars/6f86547d8691e061e289dd906deedfdb.svg",
                        "isPro": false,
                        "fullname": "Thomas Kollar",
                        "user": "thomas-kollar",
                        "type": "user"
                    },
                    "name": "Thomas Kollar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:53:47.408Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec8",
                    "user": {
                        "_id": "6252375c818a5dc29ab9d945",
                        "avatarUrl": "/avatars/cd4f0b220ea36e13ff71157cffe9d9d1.svg",
                        "isPro": false,
                        "fullname": "Ludwig Schmidt",
                        "user": "ludwigschmidt",
                        "type": "user"
                    },
                    "name": "Ludwig Schmidt",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:54:00.333Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc310f2b1fe815dc24ec9",
                    "user": {
                        "_id": "6488e29b78570cedad71eb2d",
                        "avatarUrl": "/avatars/0d8bb52c1e2be8d6aa7c732cf92fc5a9.svg",
                        "isPro": false,
                        "fullname": "Achal Dave",
                        "user": "achal-tri",
                        "type": "user"
                    },
                    "name": "Achal Dave",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:54:08.423Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:58:19.000Z",
            "title": "Should VLMs be Pre-trained with Image Data?",
            "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
            "upvotes": 3,
            "discussionId": "67cfc314f2b1fe815dc24fe3",
            "ai_keywords": [
                "pre-trained LLMs",
                "vision-language tasks",
                "fine-tune",
                "vision tokens"
            ]
        },
        "publishedAt": "2025-03-10T13:58:19.000Z",
        "title": "Should VLMs be Pre-trained with Image Data?",
        "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07603.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07598",
            "authors": [
                {
                    "_id": "67d04a5d7c367a40ae3e417a",
                    "name": "Zeyinzi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67d04a5d7c367a40ae3e417b",
                    "name": "Zhen Han",
                    "hidden": false
                },
                {
                    "_id": "67d04a5d7c367a40ae3e417c",
                    "name": "Chaojie Mao",
                    "hidden": false
                },
                {
                    "_id": "67d04a5d7c367a40ae3e417d",
                    "name": "Jingfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d04a5d7c367a40ae3e417e",
                    "name": "Yulin Pan",
                    "hidden": false
                },
                {
                    "_id": "67d04a5d7c367a40ae3e417f",
                    "name": "Yu Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:57:04.000Z",
            "title": "VACE: All-in-One Video Creation and Editing",
            "summary": "Diffusion Transformer has demonstrated powerful capability and scalability in\ngenerating high-quality images and videos. Further pursuing the unification of\ngeneration and editing tasks has yielded significant progress in the domain of\nimage content creation. However, due to the intrinsic demands for consistency\nacross both temporal and spatial dynamics, achieving a unified approach for\nvideo synthesis remains challenging. We introduce VACE, which enables users to\nperform Video tasks within an All-in-one framework for Creation and Editing.\nThese tasks include reference-to-video generation, video-to-video editing, and\nmasked video-to-video editing. Specifically, we effectively integrate the\nrequirements of various tasks by organizing video task inputs, such as editing,\nreference, and masking, into a unified interface referred to as the Video\nCondition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we\ninject different task concepts into the model using formalized representations\nof temporal and spatial dimensions, allowing it to handle arbitrary video\nsynthesis tasks flexibly. Extensive experiments demonstrate that the unified\nmodel of VACE achieves performance on par with task-specific models across\nvarious subtasks. Simultaneously, it enables diverse applications through\nversatile task combinations. Project page:\nhttps://ali-vilab.github.io/VACE-Page/.",
            "upvotes": 3,
            "discussionId": "67d04a607c367a40ae3e423f",
            "projectPage": "https://ali-vilab.github.io/VACE-Page/",
            "githubRepo": "https://github.com/ali-vilab/VACE",
            "ai_keywords": [
                "Diffusion Transformer",
                "high-quality images and videos",
                "generation and editing tasks",
                "temporal and spatial dynamics",
                "video synthesis",
                "VACE (Video tasks within an All-in-one framework for Creation and Editing)",
                "reference-to-video generation",
                "video-to-video editing",
                "masked video-to-video editing",
                "Video Condition Unit (VCU)",
                "Context Adapter structure",
                "temporal and spatial dimensions",
                "unified model"
            ]
        },
        "publishedAt": "2025-03-10T13:57:04.000Z",
        "title": "VACE: All-in-One Video Creation and Editing",
        "summary": "Diffusion Transformer has demonstrated powerful capability and scalability in\ngenerating high-quality images and videos. Further pursuing the unification of\ngeneration and editing tasks has yielded significant progress in the domain of\nimage content creation. However, due to the intrinsic demands for consistency\nacross both temporal and spatial dynamics, achieving a unified approach for\nvideo synthesis remains challenging. We introduce VACE, which enables users to\nperform Video tasks within an All-in-one framework for Creation and Editing.\nThese tasks include reference-to-video generation, video-to-video editing, and\nmasked video-to-video editing. Specifically, we effectively integrate the\nrequirements of various tasks by organizing video task inputs, such as editing,\nreference, and masking, into a unified interface referred to as the Video\nCondition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we\ninject different task concepts into the model using formalized representations\nof temporal and spatial dimensions, allowing it to handle arbitrary video\nsynthesis tasks flexibly. Extensive experiments demonstrate that the unified\nmodel of VACE achieves performance on par with task-specific models across\nvarious subtasks. Simultaneously, it enables diverse applications through\nversatile task combinations. Project page:\nhttps://ali-vilab.github.io/VACE-Page/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07598.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.07265",
            "authors": [
                {
                    "_id": "67d01cf289beb7795a51f111",
                    "name": "Yuwei Niu",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f112",
                    "user": {
                        "_id": "65e14c28b1a6de8a71e70172",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e14c28b1a6de8a71e70172/D097SILGsqoufpp3sG8tV.jpeg",
                        "isPro": false,
                        "fullname": "Munan Ning",
                        "user": "MunanNing",
                        "type": "user"
                    },
                    "name": "Munan Ning",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:56:00.839Z",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f113",
                    "user": {
                        "_id": "66c16d5d780d735f17bccfdc",
                        "avatarUrl": "/avatars/6bff393b61a78b425d6f429a6d75ad56.svg",
                        "isPro": false,
                        "fullname": "Mengren Zheng",
                        "user": "zmr66z6xx6",
                        "type": "user"
                    },
                    "name": "Mengren Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:56:12.978Z",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f114",
                    "name": "Bin Lin",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f115",
                    "user": {
                        "_id": "63ad0b04e3b217fb36d36c13",
                        "avatarUrl": "/avatars/5a3715ba20859052ba04c048db9e03c2.svg",
                        "isPro": false,
                        "fullname": "Peng Jin",
                        "user": "Pengjin",
                        "type": "user"
                    },
                    "name": "Peng Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:56:20.825Z",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f116",
                    "user": {
                        "_id": "6630b287ce65a66ed8f04eba",
                        "avatarUrl": "/avatars/18ad0e3b453021ea6006e23333e92c12.svg",
                        "isPro": false,
                        "fullname": "liaojiaqi",
                        "user": "ljq940913",
                        "type": "user"
                    },
                    "name": "Jiaqi Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:56:32.257Z",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f117",
                    "name": "Kunpeng Ning",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f118",
                    "user": {
                        "_id": "6476e69d99a5ce743ccde294",
                        "avatarUrl": "/avatars/12955bcec780ff34a7e78281b45b016d.svg",
                        "isPro": false,
                        "fullname": "Bin Zhu",
                        "user": "binzhu2023",
                        "type": "user"
                    },
                    "name": "Bin Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:56:47.289Z",
                    "hidden": false
                },
                {
                    "_id": "67d01cf289beb7795a51f119",
                    "user": {
                        "_id": "614030bd8cbdb613b82f36a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631596713749-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Li Yuan",
                        "user": "LiYuan",
                        "type": "user"
                    },
                    "name": "Li Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:56:54.455Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T12:47:53.000Z",
            "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation",
            "summary": "Text-to-Image (T2I) models are capable of generating high-quality artistic\ncreations and visual content. However, existing research and evaluation\nstandards predominantly focus on image realism and shallow text-image\nalignment, lacking a comprehensive assessment of complex semantic understanding\nand world knowledge integration in text to image generation. To address this\nchallenge, we propose WISE, the first benchmark specifically\ndesigned for World Knowledge-Informed Semantic\nEvaluation. WISE moves beyond simple word-pixel mapping by\nchallenging models with 1000 meticulously crafted prompts across 25 sub-domains\nin cultural common sense, spatio-temporal reasoning, and natural science. To\novercome the limitations of traditional CLIP metric, we introduce\nWiScore, a novel quantitative metric for assessing knowledge-image\nalignment. Through comprehensive testing of 20 models (10 dedicated T2I models\nand 10 unified multimodal models) using 1,000 structured prompts spanning 25\nsubdomains, our findings reveal significant limitations in their ability to\neffectively integrate and apply world knowledge during image generation,\nhighlighting critical pathways for enhancing knowledge incorporation and\napplication in next-generation T2I models. Code and data are available at\nhttps://github.com/PKU-YuanGroup/WISE.",
            "upvotes": 3,
            "discussionId": "67d01cf489beb7795a51f186",
            "githubRepo": "https://github.com/PKU-YuanGroup/WISE",
            "ai_keywords": [
                "WISE (World Knowledge-Informed Semantic Evaluation)",
                "WiScore",
                "Text-to-Image (T2I) models",
                "unified multimodal models",
                "knowledge-image alignment",
                "cultural common sense",
                "spatio-temporal reasoning",
                "natural science",
                "CLIP metric"
            ]
        },
        "publishedAt": "2025-03-10T08:47:53.000Z",
        "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation",
        "summary": "Text-to-Image (T2I) models are capable of generating high-quality artistic\ncreations and visual content. However, existing research and evaluation\nstandards predominantly focus on image realism and shallow text-image\nalignment, lacking a comprehensive assessment of complex semantic understanding\nand world knowledge integration in text to image generation. To address this\nchallenge, we propose WISE, the first benchmark specifically\ndesigned for World Knowledge-Informed Semantic\nEvaluation. WISE moves beyond simple word-pixel mapping by\nchallenging models with 1000 meticulously crafted prompts across 25 sub-domains\nin cultural common sense, spatio-temporal reasoning, and natural science. To\novercome the limitations of traditional CLIP metric, we introduce\nWiScore, a novel quantitative metric for assessing knowledge-image\nalignment. Through comprehensive testing of 20 models (10 dedicated T2I models\nand 10 unified multimodal models) using 1,000 structured prompts spanning 25\nsubdomains, our findings reveal significant limitations in their ability to\neffectively integrate and apply world knowledge during image generation,\nhighlighting critical pathways for enhancing knowledge incorporation and\napplication in next-generation T2I models. Code and data are available at\nhttps://github.com/PKU-YuanGroup/WISE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07265.png",
        "numComments": 1,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.06885",
            "authors": [
                {
                    "_id": "67cfc1c5182d970d40896a5e",
                    "user": {
                        "_id": "655b813476e4fad5529f3256",
                        "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
                        "isPro": false,
                        "fullname": "Yan Yang",
                        "user": "HelloKKMe",
                        "type": "user"
                    },
                    "name": "Yan Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:21:28.574Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc1c5182d970d40896a5f",
                    "user": {
                        "_id": "6357362f811ee2fa05070f64",
                        "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
                        "isPro": false,
                        "fullname": "Dongxu Li",
                        "user": "dxli1",
                        "type": "user"
                    },
                    "name": "Dongxu Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-11T04:53:27.837Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc1c5182d970d40896a60",
                    "user": {
                        "_id": "632c7a0d1d303f5f9acf01b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
                        "isPro": false,
                        "fullname": "Haoning Wu",
                        "user": "haoningwu",
                        "type": "user"
                    },
                    "name": "Haoning Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:32:00.625Z",
                    "hidden": false
                },
                {
                    "_id": "67cfc1c5182d970d40896a61",
                    "name": "Bei Chen",
                    "hidden": false
                },
                {
                    "_id": "67cfc1c5182d970d40896a62",
                    "name": "Liu Liu",
                    "hidden": false
                },
                {
                    "_id": "67cfc1c5182d970d40896a63",
                    "name": "Liyuan Pan",
                    "hidden": false
                },
                {
                    "_id": "67cfc1c5182d970d40896a64",
                    "user": {
                        "_id": "61f9d3b54ac99e8a1bae85f4",
                        "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
                        "isPro": false,
                        "fullname": "JunnanLi",
                        "user": "JunnanLi",
                        "type": "user"
                    },
                    "name": "Junnan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:31:33.001Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T03:29:18.000Z",
            "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
            "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
            "upvotes": 3,
            "discussionId": "67cfc1c7182d970d40896b1d",
            "projectPage": "https://yan98.github.io/ProBench/",
            "githubRepo": "https://github.com/Yan98/ProBench_eval",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "benchmark",
                "user queries",
                "professional expertise",
                "advanced reasoning",
                "high-quality samples",
                "daily productivity demands",
                "fields",
                "sub-fields",
                "visual perception",
                "textual understanding",
                "domain knowledge"
            ]
        },
        "publishedAt": "2025-03-09T23:29:18.000Z",
        "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
        "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06885.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06626",
            "authors": [
                {
                    "_id": "67d02a6da2b648cac0c37359",
                    "user": {
                        "_id": "642b51385bf2355d02a23d15",
                        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
                        "isPro": true,
                        "fullname": "Hasan Abed Al Kader Hammoud",
                        "user": "hammh0a",
                        "type": "user"
                    },
                    "name": "Hasan Abed Al Kader Hammoud",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:30:32.989Z",
                    "hidden": false
                },
                {
                    "_id": "67d02a6da2b648cac0c3735a",
                    "name": "Bernard Ghanem",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T14:04:09.000Z",
            "title": "DiffCLIP: Differential Attention Meets CLIP",
            "summary": "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps://github.com/hammoudhasan/DiffCLIP.",
            "upvotes": 3,
            "discussionId": "67d02a6ea2b648cac0c373ac",
            "projectPage": "https://hammoudhasan.github.io/DiffCLIP/",
            "githubRepo": "https://github.com/hammoudhasan/DiffCLIP",
            "ai_keywords": [
                "DiffCLIP",
                "differential attention mechanism",
                "CLIP architectures",
                "dual encoder",
                "image-text understanding tasks",
                "zero-shot classification",
                "retrieval",
                "robustness benchmarks"
            ]
        },
        "publishedAt": "2025-03-09T10:04:09.000Z",
        "title": "DiffCLIP: Differential Attention Meets CLIP",
        "summary": "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps://github.com/hammoudhasan/DiffCLIP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06626.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06273",
            "authors": [
                {
                    "_id": "67cfa7ca62df3125810efc27",
                    "user": {
                        "_id": "65bb389bc60df511e142ba82",
                        "avatarUrl": "/avatars/5884ea11d2682ed71585a4025a4248c1.svg",
                        "isPro": false,
                        "fullname": "Jeong Hun Yeo",
                        "user": "JeongHun0716",
                        "type": "user"
                    },
                    "name": "Jeong Hun Yeo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T10:57:57.858Z",
                    "hidden": false
                },
                {
                    "_id": "67cfa7ca62df3125810efc28",
                    "name": "Minsu Kim",
                    "hidden": false
                },
                {
                    "_id": "67cfa7ca62df3125810efc29",
                    "name": "Chae Won Kim",
                    "hidden": false
                },
                {
                    "_id": "67cfa7ca62df3125810efc2a",
                    "name": "Stavros Petridis",
                    "hidden": false
                },
                {
                    "_id": "67cfa7ca62df3125810efc2b",
                    "name": "Yong Man Ro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-08T16:40:13.000Z",
            "title": "Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations",
            "summary": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR)\nframework, dubbed Zero-AVSR, which enables speech recognition in target\nlanguages without requiring any audio-visual speech data in those languages.\nSpecifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer),\nwhich learns language-agnostic speech representations by predicting Roman text.\nThen, by leveraging the strong multilingual modeling capabilities of Large\nLanguage Models (LLMs), we propose converting the predicted Roman text into\nlanguage-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it\na step further, we explore a unified Zero-AVSR approach by directly integrating\nthe audio-visual speech representations encoded by the AV-Romanizer into the\nLLM. This is achieved through finetuning the adapter and the LLM using our\nproposed multi-task learning scheme. To capture the wide spectrum of phonetic\nand linguistic diversity, we also introduce a Multilingual Audio-Visual\nRomanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data\nacross 82 languages, along with transcriptions in both language-specific\ngraphemes and Roman text. Extensive analysis and experiments confirm that the\nproposed Zero-AVSR framework has the potential to expand language support\nbeyond the languages seen during the training of the AV-Romanizer.",
            "upvotes": 3,
            "discussionId": "67cfa7cb62df3125810efc6a",
            "ai_keywords": [
                "Audio-Visual Speech Recognition (AVSR)",
                "Zero-AVSR",
                "Audio-Visual Speech Romanizer (AV-Romanizer)",
                "language-agnostic speech representations",
                "Roman text",
                "Large Language Models (LLMs)",
                "Cascaded Zero-AVSR",
                "adapter",
                "multi-task learning scheme",
                "Multilingual Audio-Visual Romanized Corpus (MARC)",
                "audio-visual speech data",
                "language-specific graphemes",
                "phonetic diversity",
                "linguistic diversity",
                "zero-shot framework"
            ]
        },
        "publishedAt": "2025-03-08T11:40:13.000Z",
        "title": "Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations",
        "summary": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR)\nframework, dubbed Zero-AVSR, which enables speech recognition in target\nlanguages without requiring any audio-visual speech data in those languages.\nSpecifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer),\nwhich learns language-agnostic speech representations by predicting Roman text.\nThen, by leveraging the strong multilingual modeling capabilities of Large\nLanguage Models (LLMs), we propose converting the predicted Roman text into\nlanguage-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it\na step further, we explore a unified Zero-AVSR approach by directly integrating\nthe audio-visual speech representations encoded by the AV-Romanizer into the\nLLM. This is achieved through finetuning the adapter and the LLM using our\nproposed multi-task learning scheme. To capture the wide spectrum of phonetic\nand linguistic diversity, we also introduce a Multilingual Audio-Visual\nRomanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data\nacross 82 languages, along with transcriptions in both language-specific\ngraphemes and Roman text. Extensive analysis and experiments confirm that the\nproposed Zero-AVSR framework has the potential to expand language support\nbeyond the languages seen during the training of the AV-Romanizer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06273.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07389",
            "authors": [
                {
                    "_id": "67d03821fe8f90f34f04973d",
                    "user": {
                        "_id": "6602650e1737e5cd4a8367a6",
                        "avatarUrl": "/avatars/6c9271879739364fce22ece91412451b.svg",
                        "isPro": false,
                        "fullname": "chendongdong",
                        "user": "ddgoodgood",
                        "type": "user"
                    },
                    "name": "Ruidong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T13:42:30.572Z",
                    "hidden": false
                },
                {
                    "_id": "67d03821fe8f90f34f04973e",
                    "name": "Honglin Guo",
                    "hidden": false
                },
                {
                    "_id": "67d03821fe8f90f34f04973f",
                    "name": "Lanjun Wang",
                    "hidden": false
                },
                {
                    "_id": "67d03821fe8f90f34f049740",
                    "name": "Chenyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d03821fe8f90f34f049741",
                    "name": "Weizhi Nie",
                    "hidden": false
                },
                {
                    "_id": "67d03821fe8f90f34f049742",
                    "name": "An-An Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T14:37:53.000Z",
            "title": "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models",
            "summary": "Recent advances in text-to-image diffusion models enable photorealistic image\ngeneration, but they also risk producing malicious content, such as NSFW\nimages. To mitigate risk, concept erasure methods are studied to facilitate the\nmodel to unlearn specific concepts. However, current studies struggle to fully\nerase malicious concepts implicitly embedded in prompts (e.g., metaphorical\nexpressions or adversarial prompts) while preserving the model's normal\ngeneration capability. To address this challenge, our study proposes TRCE,\nusing a two-stage concept erasure strategy to achieve an effective trade-off\nbetween reliable erasure and knowledge preservation. Firstly, TRCE starts by\nerasing the malicious semantics implicitly embedded in textual prompts. By\nidentifying a critical mapping objective(i.e., the [EoT] embedding), we\noptimize the cross-attention layers to map malicious prompts to contextually\nsimilar prompts but with safe concepts. This step prevents the model from being\noverly influenced by malicious semantics during the denoising process.\nFollowing this, considering the deterministic properties of the sampling\ntrajectory of the diffusion model, TRCE further steers the early denoising\nprediction toward the safe direction and away from the unsafe one through\ncontrastive learning, thus further avoiding the generation of malicious\ncontent. Finally, we conduct comprehensive evaluations of TRCE on multiple\nmalicious concept erasure benchmarks, and the results demonstrate its\neffectiveness in erasing malicious concepts while better preserving the model's\noriginal generation ability. The code is available at:\nhttp://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated\ncontent that may contain offensive material.",
            "upvotes": 2,
            "discussionId": "67d03823fe8f90f34f0497ec",
            "githubRepo": "https://github.com/ddgoodgood/TRCE",
            "ai_keywords": [
                "text-to-image diffusion models",
                "photorealistic image generation",
                "NSFW images",
                "concept erasure methods",
                "prompts",
                "metaphorical expressions",
                "adversarial prompts",
                "cross-attention layers",
                "[EoT] embedding",
                "denoising process",
                "sampling trajectory",
                "contrastive learning"
            ]
        },
        "publishedAt": "2025-03-10T10:37:53.000Z",
        "title": "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models",
        "summary": "Recent advances in text-to-image diffusion models enable photorealistic image\ngeneration, but they also risk producing malicious content, such as NSFW\nimages. To mitigate risk, concept erasure methods are studied to facilitate the\nmodel to unlearn specific concepts. However, current studies struggle to fully\nerase malicious concepts implicitly embedded in prompts (e.g., metaphorical\nexpressions or adversarial prompts) while preserving the model's normal\ngeneration capability. To address this challenge, our study proposes TRCE,\nusing a two-stage concept erasure strategy to achieve an effective trade-off\nbetween reliable erasure and knowledge preservation. Firstly, TRCE starts by\nerasing the malicious semantics implicitly embedded in textual prompts. By\nidentifying a critical mapping objective(i.e., the [EoT] embedding), we\noptimize the cross-attention layers to map malicious prompts to contextually\nsimilar prompts but with safe concepts. This step prevents the model from being\noverly influenced by malicious semantics during the denoising process.\nFollowing this, considering the deterministic properties of the sampling\ntrajectory of the diffusion model, TRCE further steers the early denoising\nprediction toward the safe direction and away from the unsafe one through\ncontrastive learning, thus further avoiding the generation of malicious\ncontent. Finally, we conduct comprehensive evaluations of TRCE on multiple\nmalicious concept erasure benchmarks, and the results demonstrate its\neffectiveness in erasing malicious concepts while better preserving the model's\noriginal generation ability. The code is available at:\nhttp://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated\ncontent that may contain offensive material.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07389.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06960",
            "authors": [
                {
                    "_id": "67d0434c9507f2114701cf96",
                    "name": "Xin Wen",
                    "hidden": false
                },
                {
                    "_id": "67d0434c9507f2114701cf97",
                    "user": {
                        "_id": "62dcd71075e9787ec5aa41ba",
                        "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
                        "isPro": true,
                        "fullname": "Bingchen Zhao",
                        "user": "tennant",
                        "type": "user"
                    },
                    "name": "Bingchen Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:20:55.624Z",
                    "hidden": false
                },
                {
                    "_id": "67d0434c9507f2114701cf98",
                    "user": {
                        "_id": "6647a9eb658c482f961680dc",
                        "avatarUrl": "/avatars/7cdd80f7e78224efc4426c3911ccfc52.svg",
                        "isPro": false,
                        "fullname": "Yilun Chen",
                        "user": "allenchenyilun",
                        "type": "user"
                    },
                    "name": "Yilun Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:23:40.661Z",
                    "hidden": false
                },
                {
                    "_id": "67d0434c9507f2114701cf99",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:20:26.954Z",
                    "hidden": false
                },
                {
                    "_id": "67d0434c9507f2114701cf9a",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T06:18:31.000Z",
            "title": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning",
            "summary": "Pre-trained vision models (PVMs) are fundamental to modern robotics, yet\ntheir optimal configuration remains unclear. Through systematic evaluation, we\nfind that while DINO and iBOT outperform MAE across visuomotor control and\nperception tasks, they struggle when trained on non-(single-)object-centric\n(NOC) data--a limitation strongly correlated with their diminished ability to\nlearn object-centric representations. This investigation indicates that the\nability to form object-centric representations from the non-object-centric\nrobotics dataset is the key to success for PVMs. Motivated by this discovery,\nwe designed SlotMIM, a method that induces object-centric representations by\nintroducing a semantic bottleneck to reduce the number of prototypes to\nencourage the emergence of objectness as well as cross-view consistency\nregularization for encouraging multiview invariance. Our experiments encompass\npre-training on object-centric, scene-centric, web-crawled, and ego-centric\ndata. Across all settings, our approach learns transferrable representations\nand achieves significant improvements over prior work in image recognition,\nscene understanding, and robot learning evaluations. When scaled up with\nmillion-scale datasets, our method also demonstrates superior data efficiency\nand scalability. Our code and models are publicly available at\nhttps://github.com/CVMI-Lab/SlotMIM.",
            "upvotes": 2,
            "discussionId": "67d0434e9507f2114701cfeb",
            "githubRepo": "https://github.com/CVMI-Lab/SlotMIM",
            "ai_keywords": [
                "DINO",
                "iBOT",
                "MAE",
                "object-centric representations",
                "non-(single-)object-centric (NOC) data",
                "SlotMIM",
                "semantic bottleneck",
                "prototypes",
                "objectness",
                "cross-view consistency regularization",
                "multiview invariance",
                "pre-training",
                "scene-centric data",
                "web-crawled data",
                "ego-centric data",
                "transferrable representations",
                "image recognition",
                "scene understanding",
                "robot learning",
                "data efficiency",
                "scalability"
            ]
        },
        "publishedAt": "2025-03-10T02:18:31.000Z",
        "title": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning",
        "summary": "Pre-trained vision models (PVMs) are fundamental to modern robotics, yet\ntheir optimal configuration remains unclear. Through systematic evaluation, we\nfind that while DINO and iBOT outperform MAE across visuomotor control and\nperception tasks, they struggle when trained on non-(single-)object-centric\n(NOC) data--a limitation strongly correlated with their diminished ability to\nlearn object-centric representations. This investigation indicates that the\nability to form object-centric representations from the non-object-centric\nrobotics dataset is the key to success for PVMs. Motivated by this discovery,\nwe designed SlotMIM, a method that induces object-centric representations by\nintroducing a semantic bottleneck to reduce the number of prototypes to\nencourage the emergence of objectness as well as cross-view consistency\nregularization for encouraging multiview invariance. Our experiments encompass\npre-training on object-centric, scene-centric, web-crawled, and ego-centric\ndata. Across all settings, our approach learns transferrable representations\nand achieves significant improvements over prior work in image recognition,\nscene understanding, and robot learning evaluations. When scaled up with\nmillion-scale datasets, our method also demonstrates superior data efficiency\nand scalability. Our code and models are publicly available at\nhttps://github.com/CVMI-Lab/SlotMIM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06960.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.06362",
            "authors": [
                {
                    "_id": "67cffd119f703990a8e25925",
                    "user": {
                        "_id": "64903f017b630c141867877f",
                        "avatarUrl": "/avatars/3c7b248baed446ecc2b6adc2c444320d.svg",
                        "isPro": false,
                        "fullname": "Umberto Cappellazzo",
                        "user": "hisoka94",
                        "type": "user"
                    },
                    "name": "Umberto Cappellazzo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:19:16.209Z",
                    "hidden": false
                },
                {
                    "_id": "67cffd119f703990a8e25926",
                    "name": "Minsu Kim",
                    "hidden": false
                },
                {
                    "_id": "67cffd119f703990a8e25927",
                    "name": "Stavros Petridis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T00:02:10.000Z",
            "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
            "summary": "Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.",
            "upvotes": 2,
            "discussionId": "67cffd129f703990a8e25990",
            "ai_keywords": [
                "Audio-Visual Speech Recognition (AVSR)",
                "Large Language Models (LLMs)",
                "speech representations",
                "computational costs",
                "audio-visual token allocation",
                "Matryoshka-based Multimodal LLM",
                "Matryoshka Representation Learning",
                "global LoRA modules",
                "scale-specific LoRA modules",
                "LoRA-based Matryoshka strategies"
            ]
        },
        "publishedAt": "2025-03-08T19:02:10.000Z",
        "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
        "summary": "Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06362.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05578",
            "authors": [
                {
                    "_id": "67d01e7b66fafa3aa91677e1",
                    "user": {
                        "_id": "67d01cc5211e07ebcf89efd0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/P2Czu4w0Zz-POjteCtEm_.jpeg",
                        "isPro": false,
                        "fullname": "Jian Liu",
                        "user": "JianLiu99",
                        "type": "user"
                    },
                    "name": "Jian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T11:43:04.337Z",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e2",
                    "name": "Wei Sun",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e3",
                    "name": "Kai Zeng",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e4",
                    "name": "Jin Zheng",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e5",
                    "name": "Hui Yang",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e6",
                    "name": "Lin Wang",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e7",
                    "name": "Hossein Rahmani",
                    "hidden": false
                },
                {
                    "_id": "67d01e7b66fafa3aa91677e8",
                    "name": "Ajmal Mian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T17:00:41.000Z",
            "title": "Novel Object 6D Pose Estimation with a Single Reference View",
            "summary": "Existing novel object 6D pose estimation methods typically rely on CAD models\nor dense reference views, which are both difficult to acquire. Using only a\nsingle reference view is more scalable, but challenging due to large pose\ndiscrepancies and limited geometric and spatial information. To address these\nissues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose\nestimation method. Our key idea is to iteratively establish point-wise\nalignment in the camera coordinate system based on state space models (SSMs).\nSpecifically, iterative camera-space point-wise alignment can effectively\nhandle large pose discrepancies, while our proposed RGB and Points SSMs can\ncapture long-range dependencies and spatial information from a single view,\noffering linear complexity and superior spatial modeling capability. Once\npre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel\nobject using only a single reference view, without requiring retraining or a\nCAD model. Extensive experiments on six popular datasets and real-world robotic\nscenes demonstrate that we achieve on-par performance with CAD-based and dense\nreference view-based methods, despite operating in the more challenging single\nreference setting. Code will be released at\nhttps://github.com/CNJianLiu/SinRef-6D.",
            "upvotes": 2,
            "discussionId": "67d01e7d66fafa3aa916788b",
            "projectPage": "https://github.com/CNJianLiu/SinRef-6D",
            "githubRepo": "https://github.com/CNJianLiu/SinRef-6D",
            "ai_keywords": [
                "state space models (SSMs)",
                "iterative camera-space point-wise alignment",
                "RGB and Points SSMs",
                "long-range dependencies",
                "spatial modeling capability",
                "pre-trained",
                "synthetic data",
                "6D pose estimation",
                "novel object 6D estimation",
                "single reference view",
                "CAD-based methods",
                "dense reference view-based methods",
                "real-world robotic scenes"
            ]
        },
        "publishedAt": "2025-03-07T12:00:41.000Z",
        "title": "Novel Object 6D Pose Estimation with a Single Reference View",
        "summary": "Existing novel object 6D pose estimation methods typically rely on CAD models\nor dense reference views, which are both difficult to acquire. Using only a\nsingle reference view is more scalable, but challenging due to large pose\ndiscrepancies and limited geometric and spatial information. To address these\nissues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose\nestimation method. Our key idea is to iteratively establish point-wise\nalignment in the camera coordinate system based on state space models (SSMs).\nSpecifically, iterative camera-space point-wise alignment can effectively\nhandle large pose discrepancies, while our proposed RGB and Points SSMs can\ncapture long-range dependencies and spatial information from a single view,\noffering linear complexity and superior spatial modeling capability. Once\npre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel\nobject using only a single reference view, without requiring retraining or a\nCAD model. Extensive experiments on six popular datasets and real-world robotic\nscenes demonstrate that we achieve on-par performance with CAD-based and dense\nreference view-based methods, despite operating in the more challenging single\nreference setting. Code will be released at\nhttps://github.com/CNJianLiu/SinRef-6D.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05578.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.20475",
            "authors": [
                {
                    "_id": "67cbd2a9101af374ebe1ee20",
                    "user": {
                        "_id": "63102e82236215d0b7120b0e",
                        "avatarUrl": "/avatars/232bc42247990c0c4efe5e57176cba85.svg",
                        "isPro": false,
                        "fullname": "Tianyi Lorena Yan",
                        "user": "LorenaYannnnn",
                        "type": "user"
                    },
                    "name": "Tianyi Lorena Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:02:06.336Z",
                    "hidden": false
                },
                {
                    "_id": "67cbd2a9101af374ebe1ee21",
                    "name": "Robin Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-27T19:23:15.000Z",
            "title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries",
            "summary": "To answer one-to-many factual queries (e.g., listing cities of a country), a\nlanguage model (LM) must simultaneously recall knowledge and avoid repeating\nprevious answers. How are these two subtasks implemented and integrated\ninternally? Across multiple datasets and models, we identify a\npromote-then-suppress mechanism: the model first recalls all answers, and then\nsuppresses previously generated ones. Specifically, LMs use both the subject\nand previous answer tokens to perform knowledge recall, with attention\npropagating subject information and MLPs promoting the answers. Then, attention\nattends to and suppresses previous answer tokens, while MLPs amplify the\nsuppression signal. Our mechanism is corroborated by extensive experimental\nevidence: in addition to using early decoding and causal tracing, we analyze\nhow components use different tokens by introducing both Token Lens, which\ndecodes aggregated attention updates from specified tokens, and a knockout\nmethod that analyzes changes in MLP outputs after removing attention to\nspecified tokens. Overall, we provide new insights into how LMs' internal\ncomponents interact with different input tokens to support complex factual\nrecall. Code is available at\nhttps://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.",
            "upvotes": 2,
            "discussionId": "67cbd2ad101af374ebe1ef00",
            "ai_keywords": [
                "attention",
                "MLPs (Multi-Layer Perceptrons)",
                "Token Lens",
                "causal tracing",
                "knockout method"
            ]
        },
        "publishedAt": "2025-02-27T14:23:15.000Z",
        "title": "Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries",
        "summary": "To answer one-to-many factual queries (e.g., listing cities of a country), a\nlanguage model (LM) must simultaneously recall knowledge and avoid repeating\nprevious answers. How are these two subtasks implemented and integrated\ninternally? Across multiple datasets and models, we identify a\npromote-then-suppress mechanism: the model first recalls all answers, and then\nsuppresses previously generated ones. Specifically, LMs use both the subject\nand previous answer tokens to perform knowledge recall, with attention\npropagating subject information and MLPs promoting the answers. Then, attention\nattends to and suppresses previous answer tokens, while MLPs amplify the\nsuppression signal. Our mechanism is corroborated by extensive experimental\nevidence: in addition to using early decoding and causal tracing, we analyze\nhow components use different tokens by introducing both Token Lens, which\ndecodes aggregated attention updates from specified tokens, and a knockout\nmethod that analyzes changes in MLP outputs after removing attention to\nspecified tokens. Overall, we provide new insights into how LMs' internal\ncomponents interact with different input tokens to support complex factual\nrecall. Code is available at\nhttps://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20475.png",
        "numComments": 4,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07597",
            "authors": [
                {
                    "_id": "67d029c7f8577b7e578cc5ff",
                    "user": {
                        "_id": "65f8281209cf7381af189aa7",
                        "avatarUrl": "/avatars/fc7d570d1ff4df5b87b38a082b16f1ab.svg",
                        "isPro": false,
                        "fullname": "Yuhong Zhang",
                        "user": "YuhongZhang",
                        "type": "user"
                    },
                    "name": "Yuhong Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:24:04.517Z",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc600",
                    "user": {
                        "_id": "6523f611db79e539485521b1",
                        "avatarUrl": "/avatars/64aca195312084a98162b08152841429.svg",
                        "isPro": false,
                        "fullname": "Guanlin WU",
                        "user": "Frank0930",
                        "type": "user"
                    },
                    "name": "Guanlin Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:24:11.123Z",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc601",
                    "user": {
                        "_id": "63109a4d61cab0446e48c83b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
                        "isPro": false,
                        "fullname": "Ling-Hao Chen",
                        "user": "EvanTHU",
                        "type": "user"
                    },
                    "name": "Ling-Hao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:24:17.437Z",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc602",
                    "user": {
                        "_id": "63af25605fe9db73f67a0fb7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63af25605fe9db73f67a0fb7/mcQHbTdJVxi2GdHzRB9ad.jpeg",
                        "isPro": false,
                        "fullname": "Zhuokai Zhao",
                        "user": "zhuokai",
                        "type": "user"
                    },
                    "name": "Zhuokai Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:24:37.175Z",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc603",
                    "name": "Jing Lin",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc604",
                    "name": "Xiaoke Jiang",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc605",
                    "user": {
                        "_id": "61a48038cd3eb3043f38e402",
                        "avatarUrl": "/avatars/21f33c82fcbbf49434345d522c8865d9.svg",
                        "isPro": false,
                        "fullname": "Jiamin Wu",
                        "user": "Bunny",
                        "type": "user"
                    },
                    "name": "Jiamin Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:24:52.948Z",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc606",
                    "user": {
                        "_id": "66d9e708dc8d21114944e4f8",
                        "avatarUrl": "/avatars/1f22d26eeb5b70d56a7031dca2d7b790.svg",
                        "isPro": false,
                        "fullname": "Zhuoheng Li",
                        "user": "Andy-LZH",
                        "type": "user"
                    },
                    "name": "Zhuoheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:24:59.235Z",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc607",
                    "name": "Hao Frank Yang",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc608",
                    "name": "Haoqian Wang",
                    "hidden": false
                },
                {
                    "_id": "67d029c7f8577b7e578cc609",
                    "name": "Lei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:57:03.000Z",
            "title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
            "summary": "In this paper, we present a novel framework designed to reconstruct\nlong-sequence 3D human motion in the world coordinates from in-the-wild videos\nwith multiple shot transitions. Such long-sequence in-the-wild motions are\nhighly valuable to applications such as motion generation and motion\nunderstanding, but are of great challenge to be recovered due to abrupt shot\ntransitions, partial occlusions, and dynamic backgrounds presented in such\nvideos. Existing methods primarily focus on single-shot videos, where\ncontinuity is maintained within a single camera view, or simplify multi-shot\nalignment in camera space only. In this work, we tackle the challenges by\nintegrating an enhanced camera pose estimation with Human Motion Recovery (HMR)\nby incorporating a shot transition detector and a robust alignment module for\naccurate pose and orientation continuity across shots. By leveraging a custom\nmotion integrator, we effectively mitigate the problem of foot sliding and\nensure temporal consistency in human pose. Extensive evaluations on our created\nmulti-shot dataset from public 3D human datasets demonstrate the robustness of\nour method in reconstructing realistic human motion in world coordinates.",
            "upvotes": 1,
            "discussionId": "67d029caf8577b7e578cc6d6",
            "ai_keywords": [
                "enhanced camera pose estimation",
                "Human Motion Recovery (HMR)",
                "shot transition detector",
                "robust alignment module",
                "motion integrator",
                "temporal consistency",
                "multi-shot dataset",
                "public 3D human datasets",
                "realistic human motion",
                "world coordinates"
            ]
        },
        "publishedAt": "2025-03-10T13:57:03.000Z",
        "title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
        "summary": "In this paper, we present a novel framework designed to reconstruct\nlong-sequence 3D human motion in the world coordinates from in-the-wild videos\nwith multiple shot transitions. Such long-sequence in-the-wild motions are\nhighly valuable to applications such as motion generation and motion\nunderstanding, but are of great challenge to be recovered due to abrupt shot\ntransitions, partial occlusions, and dynamic backgrounds presented in such\nvideos. Existing methods primarily focus on single-shot videos, where\ncontinuity is maintained within a single camera view, or simplify multi-shot\nalignment in camera space only. In this work, we tackle the challenges by\nintegrating an enhanced camera pose estimation with Human Motion Recovery (HMR)\nby incorporating a shot transition detector and a robust alignment module for\naccurate pose and orientation continuity across shots. By leveraging a custom\nmotion integrator, we effectively mitigate the problem of foot sliding and\nensure temporal consistency in human pose. Extensive evaluations on our created\nmulti-shot dataset from public 3D human datasets demonstrate the robustness of\nour method in reconstructing realistic human motion in world coordinates.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07597.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07426",
            "authors": [
                {
                    "_id": "67cff321f2b1fe815dce3722",
                    "user": {
                        "_id": "664764e5d834283e7ff96d37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664764e5d834283e7ff96d37/JBi3dvAUKqqGf_DjVORK7.jpeg",
                        "isPro": false,
                        "fullname": "Junkang Wu",
                        "user": "junkang0909",
                        "type": "user"
                    },
                    "name": "Junkang Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:16:45.903Z",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3723",
                    "user": {
                        "_id": "631fc126788f34547a9d7ee1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674202999503-631fc126788f34547a9d7ee1.png",
                        "isPro": false,
                        "fullname": "Kexin Huang",
                        "user": "kexinhuang",
                        "type": "user"
                    },
                    "name": "Kexin Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:16:08.313Z",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3724",
                    "name": "Xue Wang",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3725",
                    "name": "Jinyang Gao",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3726",
                    "name": "Bolin Ding",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3727",
                    "name": "Jiancan Wu",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3728",
                    "name": "Xiangnan He",
                    "hidden": false
                },
                {
                    "_id": "67cff321f2b1fe815dce3729",
                    "user": {
                        "_id": "65fca775fa59bdf4737b1a84",
                        "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
                        "isPro": false,
                        "fullname": "Xiang Wang",
                        "user": "xiangwang1223",
                        "type": "user"
                    },
                    "name": "Xiang Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-11T08:24:02.839Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T15:11:07.000Z",
            "title": "RePO: ReLU-based Preference Optimization",
            "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter beta, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters (beta, gamma). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates beta via two\nadvances: (1) retaining SimPO's reference-free margins but removing beta\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case (beta to infty), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
            "upvotes": 1,
            "discussionId": "67cff322f2b1fe815dce3787",
            "ai_keywords": [
                "LLMS",
                "RLHF",
                "DPO",
                "SimPO",
                "ReLU-based Preference Optimization (RePO)",
                "reference-free margins",
                "gradient analysis",
                "ReLU-based max-margin loss",
                "convex envelope",
                "0-1 loss"
            ]
        },
        "publishedAt": "2025-03-10T11:11:07.000Z",
        "title": "RePO: ReLU-based Preference Optimization",
        "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter beta, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters (beta, gamma). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates beta via two\nadvances: (1) retaining SimPO's reference-free margins but removing beta\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case (beta to infty), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07426.png",
        "numComments": 1,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06141",
            "authors": [
                {
                    "_id": "67cf981ec8f2a661dc93a6fd",
                    "user": {
                        "_id": "6513a0f14f1682e4407758a9",
                        "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
                        "isPro": false,
                        "fullname": "Mingxing Li",
                        "user": "MingxingLi",
                        "type": "user"
                    },
                    "name": "Mingxing Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:23:00.614Z",
                    "hidden": false
                },
                {
                    "_id": "67cf981ec8f2a661dc93a6fe",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67cf981ec8f2a661dc93a6ff",
                    "name": "Lei Sun",
                    "hidden": false
                },
                {
                    "_id": "67cf981ec8f2a661dc93a700",
                    "name": "Yancheng Bai",
                    "hidden": false
                },
                {
                    "_id": "67cf981ec8f2a661dc93a701",
                    "name": "Xiangxiang Chu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-08T09:49:10.000Z",
            "title": "Next Token Is Enough: Realistic Image Quality and Aesthetic Scoring with\n  Multimodal Large Language Model",
            "summary": "The rapid expansion of mobile internet has resulted in a substantial increase\nin user-generated content (UGC) images, thereby making the thorough assessment\nof UGC images both urgent and essential. Recently, multimodal large language\nmodels (MLLMs) have shown great potential in image quality assessment (IQA) and\nimage aesthetic assessment (IAA). Despite this progress, effectively scoring\nthe quality and aesthetics of UGC images still faces two main challenges: 1) A\nsingle score is inadequate to capture the hierarchical human perception. 2) How\nto use MLLMs to output numerical scores, such as mean opinion scores (MOS),\nremains an open question. To address these challenges, we introduce a novel\ndataset, named Realistic image Quality and Aesthetic (RealQA), including 14,715\nUGC images, each of which is annoted with 10 fine-grained attributes. These\nattributes span three levels: low level (e.g., image clarity), middle level\n(e.g., subject integrity) and high level (e.g., composition). Besides, we\nconduct a series of in-depth and comprehensive investigations into how to\neffectively predict numerical scores using MLLMs. Surprisingly, by predicting\njust two extra significant digits, the next token paradigm can achieve SOTA\nperformance. Furthermore, with the help of chain of thought (CoT) combined with\nthe learnt fine-grained attributes, the proposed method can outperform SOTA\nmethods on five public datasets for IQA and IAA with superior interpretability\nand show strong zero-shot generalization for video quality assessment (VQA).\nThe code and dataset will be released.",
            "upvotes": 1,
            "discussionId": "67cf9820c8f2a661dc93a76f",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "image quality assessment (IQA)",
                "image aesthetic assessment (IAA)",
                "hierarchical human perception",
                "mean opinion scores (MOS)",
                "next token paradigm",
                "chain of thought (CoT)",
                "zero-shot generalization",
                "video quality assessment (VQA)"
            ]
        },
        "publishedAt": "2025-03-08T04:49:10.000Z",
        "title": "Next Token Is Enough: Realistic Image Quality and Aesthetic Scoring with\n  Multimodal Large Language Model",
        "summary": "The rapid expansion of mobile internet has resulted in a substantial increase\nin user-generated content (UGC) images, thereby making the thorough assessment\nof UGC images both urgent and essential. Recently, multimodal large language\nmodels (MLLMs) have shown great potential in image quality assessment (IQA) and\nimage aesthetic assessment (IAA). Despite this progress, effectively scoring\nthe quality and aesthetics of UGC images still faces two main challenges: 1) A\nsingle score is inadequate to capture the hierarchical human perception. 2) How\nto use MLLMs to output numerical scores, such as mean opinion scores (MOS),\nremains an open question. To address these challenges, we introduce a novel\ndataset, named Realistic image Quality and Aesthetic (RealQA), including 14,715\nUGC images, each of which is annoted with 10 fine-grained attributes. These\nattributes span three levels: low level (e.g., image clarity), middle level\n(e.g., subject integrity) and high level (e.g., composition). Besides, we\nconduct a series of in-depth and comprehensive investigations into how to\neffectively predict numerical scores using MLLMs. Surprisingly, by predicting\njust two extra significant digits, the next token paradigm can achieve SOTA\nperformance. Furthermore, with the help of chain of thought (CoT) combined with\nthe learnt fine-grained attributes, the proposed method can outperform SOTA\nmethods on five public datasets for IQA and IAA with superior interpretability\nand show strong zero-shot generalization for video quality assessment (VQA).\nThe code and dataset will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06141.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05641",
            "authors": [
                {
                    "_id": "67d0446aa9ae7682cd1c62b4",
                    "user": {
                        "_id": "62ce26129f723d34cf1f595a",
                        "avatarUrl": "/avatars/8e305ac7c170d70fbf83c109789b40d9.svg",
                        "isPro": false,
                        "fullname": "Justin Chen",
                        "user": "dinobby",
                        "type": "user"
                    },
                    "name": "Justin Chih-Yao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:15:18.833Z",
                    "hidden": false
                },
                {
                    "_id": "67d0446aa9ae7682cd1c62b5",
                    "name": "Sukwon Yun",
                    "hidden": false
                },
                {
                    "_id": "67d0446aa9ae7682cd1c62b6",
                    "user": {
                        "_id": "61781c4caf41befe8ff060e8",
                        "avatarUrl": "/avatars/8871d7b046fc28cbc8638228da8e9737.svg",
                        "isPro": false,
                        "fullname": "Elias Stengel-Eskin",
                        "user": "esteng",
                        "type": "user"
                    },
                    "name": "Elias Stengel-Eskin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:14:40.165Z",
                    "hidden": false
                },
                {
                    "_id": "67d0446aa9ae7682cd1c62b7",
                    "name": "Tianlong Chen",
                    "hidden": false
                },
                {
                    "_id": "67d0446aa9ae7682cd1c62b8",
                    "user": {
                        "_id": "665d9d3a057f7c508f98c625",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
                        "isPro": false,
                        "fullname": "Mohit Bansal",
                        "user": "mohitbansal",
                        "type": "user"
                    },
                    "name": "Mohit Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:14:22.014Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T18:03:13.000Z",
            "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning",
            "summary": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
            "upvotes": 1,
            "discussionId": "67d0446ba9ae7682cd1c62ea",
            "projectPage": "https://symbolic-moe.github.io",
            "githubRepo": "https://github.com/dinobby/Symbolic-MoE/",
            "ai_keywords": [
                "Symbolic-MoE",
                "Mixture-of-Experts",
                "skill-based recruiting strategy",
                "instance-level expert selection",
                "aggregator",
                "batch inference strategy",
                "MMLU-Pro",
                "GPQA",
                "AIME",
                "MedMCQA",
                "GPT4o-mini"
            ]
        },
        "publishedAt": "2025-03-07T13:03:13.000Z",
        "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning",
        "summary": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05641.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05283",
            "authors": [
                {
                    "_id": "67cef721e5ab8ec0550b7a66",
                    "user": {
                        "_id": "6542cb9602409f0994fbb6c6",
                        "avatarUrl": "/avatars/c0ba12b349e61998f36f2701ac3acb30.svg",
                        "isPro": false,
                        "fullname": "Souhail Hadgi",
                        "user": "Squall01",
                        "type": "user"
                    },
                    "name": "Souhail Hadgi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:18:09.155Z",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a67",
                    "user": {
                        "_id": "607021f450a44430c625a937",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617961457504-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Luca Moschella",
                        "user": "lucmos",
                        "type": "user"
                    },
                    "name": "Luca Moschella",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:18:16.141Z",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a68",
                    "user": {
                        "_id": "5e8ef1f14957053f606489e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
                        "isPro": false,
                        "fullname": "Andrea Santilli",
                        "user": "teelinsan",
                        "type": "user"
                    },
                    "name": "Andrea Santilli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T14:35:44.397Z",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a69",
                    "name": "Diego Gomez",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a6a",
                    "name": "Qixing Huang",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a6b",
                    "user": {
                        "_id": "652681664e066bf73f8e2bd1",
                        "avatarUrl": "/avatars/084dec4765d9996d74901b8df95ec35f.svg",
                        "isPro": false,
                        "fullname": "Emanuele Rodola'",
                        "user": "erodola",
                        "type": "user"
                    },
                    "name": "Emanuele Rodolà",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:18:47.736Z",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a6c",
                    "name": "Simone Melzi",
                    "hidden": false
                },
                {
                    "_id": "67cef721e5ab8ec0550b7a6d",
                    "name": "Maks Ovsjanikov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T09:51:56.000Z",
            "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces",
            "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations.",
            "upvotes": 1,
            "discussionId": "67cef723e5ab8ec0550b7ac8",
            "ai_keywords": [
                "uni-modal 2D vision",
                "text encoders",
                "learned features",
                "3D encoders",
                "3D foundation models",
                "alignment objectives",
                "feature alignment",
                "subspaces",
                "lower-dimensional subspaces",
                "semantic data representations",
                "geometric data representations",
                "matching tasks",
                "retrieval tasks"
            ]
        },
        "publishedAt": "2025-03-07T04:51:56.000Z",
        "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces",
        "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05283.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05265",
            "authors": [
                {
                    "_id": "67cf6e6f59dbba733d819ca5",
                    "user": {
                        "_id": "63753c6a9ef4e3b87a5fa22c",
                        "avatarUrl": "/avatars/91dd3209284758d29ac59718f08ee32d.svg",
                        "isPro": false,
                        "fullname": "Rumi Allbert",
                        "user": "raaec",
                        "type": "user"
                    },
                    "name": "Rumi A. Allbert",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T08:23:05.438Z",
                    "hidden": false
                },
                {
                    "_id": "67cf6e6f59dbba733d819ca6",
                    "name": "Makai L. Allbert",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T09:30:16.000Z",
            "title": "PhiloBERTA: A Transformer-Based Cross-Lingual Analysis of Greek and\n  Latin Lexicons",
            "summary": "We present PhiloBERTA, a cross-lingual transformer model that measures\nsemantic relationships between ancient Greek and Latin lexicons. Through\nanalysis of selected term pairs from classical texts, we use contextual\nembeddings and angular similarity metrics to identify precise semantic\nalignments. Our results show that etymologically related pairs demonstrate\nsignificantly higher similarity scores, particularly for abstract philosophical\nconcepts such as epist\\=em\\=e (scientia) and dikaiosyn\\=e (iustitia).\nStatistical analysis reveals consistent patterns in these relationships (p =\n0.012), with etymologically related pairs showing remarkably stable semantic\npreservation compared to control pairs. These findings establish a quantitative\nframework for examining how philosophical concepts moved between Greek and\nLatin traditions, offering new methods for classical philological research.",
            "upvotes": 1,
            "discussionId": "67cf6e7159dbba733d819d50",
            "githubRepo": "https://github.com/RumiAllbert/PhiloBERTA",
            "ai_keywords": [
                "cross-lingual transformer model",
                "contextual embeddings",
                "angular similarity metrics",
                "semantic relationships",
                "statistical analysis",
                "quantitative framework",
                "classical texts",
                "episteme",
                "scientia",
                "dikaiosyne",
                "iustitia",
                "etymologically related pairs",
                "abstract philosophical concepts",
                "semantic preservation"
            ]
        },
        "publishedAt": "2025-03-07T04:30:16.000Z",
        "title": "PhiloBERTA: A Transformer-Based Cross-Lingual Analysis of Greek and\n  Latin Lexicons",
        "summary": "We present PhiloBERTA, a cross-lingual transformer model that measures\nsemantic relationships between ancient Greek and Latin lexicons. Through\nanalysis of selected term pairs from classical texts, we use contextual\nembeddings and angular similarity metrics to identify precise semantic\nalignments. Our results show that etymologically related pairs demonstrate\nsignificantly higher similarity scores, particularly for abstract philosophical\nconcepts such as epist\\=em\\=e (scientia) and dikaiosyn\\=e (iustitia).\nStatistical analysis reveals consistent patterns in these relationships (p =\n0.012), with etymologically related pairs showing remarkably stable semantic\npreservation compared to control pairs. These findings establish a quantitative\nframework for examining how philosophical concepts moved between Greek and\nLatin traditions, offering new methods for classical philological research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05265.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.02819",
            "authors": [
                {
                    "_id": "67d0b23d33256c62f20dbc1f",
                    "name": "Marta Skreta",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc20",
                    "name": "Tara Akhound-Sadegh",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc21",
                    "name": "Viktor Ohanesian",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc22",
                    "name": "Roberto Bondesan",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc23",
                    "name": "Alán Aspuru-Guzik",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc24",
                    "name": "Arnaud Doucet",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc25",
                    "name": "Rob Brekelmans",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc26",
                    "name": "Alexander Tong",
                    "hidden": false
                },
                {
                    "_id": "67d0b23d33256c62f20dbc27",
                    "name": "Kirill Neklyudov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-04T17:46:51.000Z",
            "title": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts",
            "summary": "While score-based generative models are the model of choice across diverse\ndomains, there are limited tools available for controlling inference-time\nbehavior in a principled manner, e.g. for composing multiple pretrained models.\nExisting classifier-free guidance methods use a simple heuristic to mix\nconditional and unconditional scores to approximately sample from conditional\ndistributions. However, such methods do not approximate the intermediate\ndistributions, necessitating additional 'corrector' steps. In this work, we\nprovide an efficient and principled method for sampling from a sequence of\nannealed, geometric-averaged, or product distributions derived from pretrained\nscore-based models. We derive a weighted simulation scheme which we call\nFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by\ncarefully accounting for terms in the appropriate partial differential\nequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo\n(SMC) resampling algorithms that leverage inference-time scaling to improve\nsampling quality. We empirically demonstrate the utility of our methods by\nproposing amortized sampling via inference-time temperature annealing,\nimproving multi-objective molecule generation using pretrained models, and\nimproving classifier-free guidance for text-to-image generation. Our code is\navailable at https://github.com/martaskrt/fkc-diffusion.",
            "upvotes": 1,
            "discussionId": "67d0b24233256c62f20dbd7e",
            "ai_keywords": [
                "score-based generative models",
                "inference-time behavior",
                "pretrained models",
                "classifier-free guidance",
                "conditional scores",
                "unconditional scores",
                "conditional distributions",
                "intermediate distributions",
                "corrected steps",
                "annealed distributions",
                "geometric-averaged distributions",
                "product distributions",
                "Feynman-Kac Correctors (FKCs)",
                "Feynman-Kac formula",
                "partial differential equations (PDEs)",
                "Sequential Monte Carlo (SMC)",
                "resampling algorithms",
                "inference-time scaling",
                "amortized sampling",
                "inference-time temperature annealing",
                "multi-objective molecule generation",
                "text-to-image generation"
            ]
        },
        "publishedAt": "2025-03-04T12:46:51.000Z",
        "title": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts",
        "summary": "While score-based generative models are the model of choice across diverse\ndomains, there are limited tools available for controlling inference-time\nbehavior in a principled manner, e.g. for composing multiple pretrained models.\nExisting classifier-free guidance methods use a simple heuristic to mix\nconditional and unconditional scores to approximately sample from conditional\ndistributions. However, such methods do not approximate the intermediate\ndistributions, necessitating additional 'corrector' steps. In this work, we\nprovide an efficient and principled method for sampling from a sequence of\nannealed, geometric-averaged, or product distributions derived from pretrained\nscore-based models. We derive a weighted simulation scheme which we call\nFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by\ncarefully accounting for terms in the appropriate partial differential\nequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo\n(SMC) resampling algorithms that leverage inference-time scaling to improve\nsampling quality. We empirically demonstrate the utility of our methods by\nproposing amortized sampling via inference-time temperature annealing,\nimproving multi-objective molecule generation using pretrained models, and\nimproving classifier-free guidance for text-to-image generation. Our code is\navailable at https://github.com/martaskrt/fkc-diffusion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02819.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.07413",
            "authors": [
                {
                    "_id": "67d050c28db3ce9654a56a96",
                    "name": "Yan Tai",
                    "hidden": false
                },
                {
                    "_id": "67d050c28db3ce9654a56a97",
                    "name": "Luhao Zhu",
                    "hidden": false
                },
                {
                    "_id": "67d050c28db3ce9654a56a98",
                    "name": "Zhiqiang Chen",
                    "hidden": false
                },
                {
                    "_id": "67d050c28db3ce9654a56a99",
                    "name": "Ynan Ding",
                    "hidden": false
                },
                {
                    "_id": "67d050c28db3ce9654a56a9a",
                    "name": "Yiying Dong",
                    "hidden": false
                },
                {
                    "_id": "67d050c28db3ce9654a56a9b",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "67d050c28db3ce9654a56a9c",
                    "name": "Guodong Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T14:59:14.000Z",
            "title": "REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding",
            "summary": "Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot\ncapabilities across diverse vision-language tasks after training on mega-scale\ndatasets. However, dense prediction tasks, such as semantic segmentation and\nkeypoint detection, pose significant challenges for MLLMs when represented\nsolely as text outputs. Simultaneously, current MLLMs utilizing latent\nembeddings for visual task decoding generally demonstrate limited adaptability\nto both multi-task learning and multi-granularity scenarios. In this work, we\npresent REF-VLM, an end-to-end framework for unified training of various visual\ndecoding tasks. To address complex visual decoding scenarios, we introduce the\nTriplet-Based Referring Paradigm (TRP), which explicitly decouples three\ncritical dimensions in visual decoding tasks through a triplet structure:\nconcepts, decoding types, and targets. TRP employs symbolic delimiters to\nenforce structured representation learning, enhancing the parsability and\ninterpretability of model outputs. Additionally, we construct Visual-Task\nInstruction Following Dataset (VTInstruct), a large-scale multi-task dataset\ncontaining over 100 million multimodal dialogue samples across 25 task types.\nBeyond text inputs and outputs, VT-Instruct incorporates various visual prompts\nsuch as point, box, scribble, and mask, and generates outputs composed of text\nand visual units like box, keypoint, depth and mask. The combination of\ndifferent visual prompts and visual units generates a wide variety of task\ntypes, expanding the applicability of REF-VLM significantly. Both qualitative\nand quantitative experiments demonstrate that our REF-VLM outperforms other\nMLLMs across a variety of standard benchmarks. The code, dataset, and demo\navailable at https://github.com/MacavityT/REF-VLM.",
            "upvotes": 0,
            "discussionId": "67d050c88db3ce9654a56c59",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "zero-shot capabilities",
                "vision-language tasks",
                "dense prediction tasks",
                "semantic segmentation",
                "keypoint detection",
                "latent embeddings",
                "multi-task learning",
                "multi-granularity scenarios",
                "REF-VLM",
                "end-to-end framework",
                "Triplet-Based Referring Paradigm (TRP)",
                "concepts",
                "decoding types",
                "targets",
                "symbolic delimiters",
                "structured representation learning",
                "parsability",
                "interpretability",
                "Visual-Task Instruction Following Dataset (VTInstruct)",
                "multimodal dialogue samples",
                "point",
                "box",
                "scribble",
                "mask",
                "visual units",
                "depth"
            ]
        },
        "publishedAt": "2025-03-10T10:59:14.000Z",
        "title": "REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding",
        "summary": "Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot\ncapabilities across diverse vision-language tasks after training on mega-scale\ndatasets. However, dense prediction tasks, such as semantic segmentation and\nkeypoint detection, pose significant challenges for MLLMs when represented\nsolely as text outputs. Simultaneously, current MLLMs utilizing latent\nembeddings for visual task decoding generally demonstrate limited adaptability\nto both multi-task learning and multi-granularity scenarios. In this work, we\npresent REF-VLM, an end-to-end framework for unified training of various visual\ndecoding tasks. To address complex visual decoding scenarios, we introduce the\nTriplet-Based Referring Paradigm (TRP), which explicitly decouples three\ncritical dimensions in visual decoding tasks through a triplet structure:\nconcepts, decoding types, and targets. TRP employs symbolic delimiters to\nenforce structured representation learning, enhancing the parsability and\ninterpretability of model outputs. Additionally, we construct Visual-Task\nInstruction Following Dataset (VTInstruct), a large-scale multi-task dataset\ncontaining over 100 million multimodal dialogue samples across 25 task types.\nBeyond text inputs and outputs, VT-Instruct incorporates various visual prompts\nsuch as point, box, scribble, and mask, and generates outputs composed of text\nand visual units like box, keypoint, depth and mask. The combination of\ndifferent visual prompts and visual units generates a wide variety of task\ntypes, expanding the applicability of REF-VLM significantly. Both qualitative\nand quantitative experiments demonstrate that our REF-VLM outperforms other\nMLLMs across a variety of standard benchmarks. The code, dataset, and demo\navailable at https://github.com/MacavityT/REF-VLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07413.png",
        "numComments": 1,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.06698",
            "authors": [
                {
                    "_id": "67cf9d9ebe9366d1ae96de5e",
                    "name": "Xavier Thomas",
                    "hidden": false
                },
                {
                    "_id": "67cf9d9ebe9366d1ae96de5f",
                    "name": "Deepti Ghadiyaram",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T17:29:01.000Z",
            "title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain\n  Generalization",
            "summary": "Domain Generalization aims to develop models that can generalize to novel and\nunseen data distributions. In this work, we study how model architectures and\npre-training objectives impact feature richness and propose a method to\neffectively leverage them for domain generalization. Specifically, given a\npre-trained feature space, we first discover latent domain structures, referred\nto as pseudo-domains, that capture domain-specific variations in an\nunsupervised manner. Next, we augment existing classifiers with these\ncomplementary pseudo-domain representations making them more amenable to\ndiverse unseen test domains. We analyze how different pre-training feature\nspaces differ in the domain-specific variances they capture. Our empirical\nstudies reveal that features from diffusion models excel at separating domains\nin the absence of explicit domain labels and capture nuanced domain-specific\ninformation. On 5 datasets, we show that our very simple framework improves\ngeneralization to unseen domains by a maximum test accuracy improvement of over\n4% compared to the standard baseline Empirical Risk Minimization (ERM).\nCrucially, our method outperforms most algorithms that access domain labels\nduring training.",
            "upvotes": 0,
            "discussionId": "67cf9da5be9366d1ae96e00e",
            "ai_keywords": [
                "diffusion models",
                "pseudo-domains",
                "domain-specific variations",
                "feature richness",
                "latent domain structures",
                "pre-training objectives",
                "feature spaces",
                "empirical studies",
                "Empirical Risk Minimization (ERM)"
            ]
        },
        "publishedAt": "2025-03-09T13:29:01.000Z",
        "title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain\n  Generalization",
        "summary": "Domain Generalization aims to develop models that can generalize to novel and\nunseen data distributions. In this work, we study how model architectures and\npre-training objectives impact feature richness and propose a method to\neffectively leverage them for domain generalization. Specifically, given a\npre-trained feature space, we first discover latent domain structures, referred\nto as pseudo-domains, that capture domain-specific variations in an\nunsupervised manner. Next, we augment existing classifiers with these\ncomplementary pseudo-domain representations making them more amenable to\ndiverse unseen test domains. We analyze how different pre-training feature\nspaces differ in the domain-specific variances they capture. Our empirical\nstudies reveal that features from diffusion models excel at separating domains\nin the absence of explicit domain labels and capture nuanced domain-specific\ninformation. On 5 datasets, we show that our very simple framework improves\ngeneralization to unseen domains by a maximum test accuracy improvement of over\n4% compared to the standard baseline Empirical Risk Minimization (ERM).\nCrucially, our method outperforms most algorithms that access domain labels\nduring training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06698.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.03511",
            "authors": [
                {
                    "_id": "67cd7ace999766d8cd73fb18",
                    "user": {
                        "_id": "6732f2c24c2f18a60e76b915",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732f2c24c2f18a60e76b915/W6oozAjM-zu7E3SL9uQ97.jpeg",
                        "isPro": false,
                        "fullname": "Fan",
                        "user": "KianYale",
                        "type": "user"
                    },
                    "name": "Qingyu Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T08:01:41.233Z",
                    "hidden": false
                },
                {
                    "_id": "67cd7ace999766d8cd73fb19",
                    "user": {
                        "_id": "64f53882bc54e7f4eeede46a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f53882bc54e7f4eeede46a/R43aKnfpmyHtUbwZWT64p.jpeg",
                        "isPro": false,
                        "fullname": "Yinghao Cai",
                        "user": "paparika",
                        "type": "user"
                    },
                    "name": "Yinghao Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:12:56.722Z",
                    "hidden": false
                },
                {
                    "_id": "67cd7ace999766d8cd73fb1a",
                    "name": "Chao Li",
                    "hidden": false
                },
                {
                    "_id": "67cd7ace999766d8cd73fb1b",
                    "name": "Wenzhe He",
                    "hidden": false
                },
                {
                    "_id": "67cd7ace999766d8cd73fb1c",
                    "user": {
                        "_id": "65638259330b667bb2416ab9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/IuOfh4z_hDuJ3XL0jqvVx.png",
                        "isPro": false,
                        "fullname": "Xudong Zheng",
                        "user": "xudongz",
                        "type": "user"
                    },
                    "name": "Xudong Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-11T14:13:12.216Z",
                    "hidden": true
                },
                {
                    "_id": "67cd7ace999766d8cd73fb1d",
                    "name": "Tao Lu",
                    "hidden": false
                },
                {
                    "_id": "67cd7ace999766d8cd73fb1e",
                    "name": "Bin Liang",
                    "hidden": false
                },
                {
                    "_id": "67cd7ace999766d8cd73fb1f",
                    "name": "Shuo Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-05T13:57:37.000Z",
            "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
            "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
            "upvotes": 0,
            "discussionId": "67cd7ad0999766d8cd73fb77",
            "ai_keywords": [
                "neural surface reconstruction",
                "background priors",
                "material-agnostic grasp detection",
                "transformers",
                "global prior volumes",
                "multi-view features",
                "spatial encoding",
                "narrow and sparse viewing conditions",
                "residual feature enhancement",
                "occupancy-prior volume",
                "transparent objects",
                "specular surfaces"
            ]
        },
        "publishedAt": "2025-03-05T08:57:37.000Z",
        "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
        "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03511.png",
        "numComments": 2,
        "isAuthorParticipating": true
    }
]