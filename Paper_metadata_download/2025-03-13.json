[
    {
        "paper": {
            "id": "2503.09566",
            "authors": [
                {
                    "_id": "67d274c467e782a7eeb4ab70",
                    "name": "Lingmin Ran",
                    "hidden": false
                },
                {
                    "_id": "67d274c467e782a7eeb4ab71",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:33:22.000Z",
            "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
            "summary": "The development of video diffusion models unveils a significant challenge:\nthe substantial computational demands. To mitigate this challenge, we note that\nthe reverse process of diffusion exhibits an inherent entropy-reducing nature.\nGiven the inter-frame redundancy in video modality, maintaining full frame\nrates in high-entropy stages is unnecessary. Based on this insight, we propose\nTPDiff, a unified framework to enhance training and inference efficiency. By\ndividing diffusion into several stages, our framework progressively increases\nframe rate along the diffusion process with only the last stage operating on\nfull frame rate, thereby optimizing computational efficiency. To train the\nmulti-stage diffusion model, we introduce a dedicated training framework:\nstage-wise diffusion. By solving the partitioned probability flow ordinary\ndifferential equations (ODE) of diffusion under aligned data and noise, our\ntraining strategy is applicable to various diffusion forms and further enhances\ntraining efficiency. Comprehensive experimental evaluations validate the\ngenerality of our method, demonstrating 50% reduction in training cost and 1.5x\nimprovement in inference efficiency.",
            "upvotes": 36,
            "discussionId": "67d274c567e782a7eeb4abb0",
            "projectPage": "https://showlab.github.io/TPDiff/",
            "githubRepo": "https://github.com/showlab/TPDiff",
            "ai_keywords": [
                "video diffusion models",
                "entropy-reducing nature",
                "inter-frame redundancy",
                "TPDiff",
                "unified framework",
                "frame rate",
                "diffusion stages",
                "stage-wise diffusion",
                "partitioned probability flow ordinary differential equations (ODE)",
                "diffusion forms"
            ]
        },
        "publishedAt": "2025-03-12T13:33:22.000Z",
        "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
        "summary": "The development of video diffusion models unveils a significant challenge:\nthe substantial computational demands. To mitigate this challenge, we note that\nthe reverse process of diffusion exhibits an inherent entropy-reducing nature.\nGiven the inter-frame redundancy in video modality, maintaining full frame\nrates in high-entropy stages is unnecessary. Based on this insight, we propose\nTPDiff, a unified framework to enhance training and inference efficiency. By\ndividing diffusion into several stages, our framework progressively increases\nframe rate along the diffusion process with only the last stage operating on\nfull frame rate, thereby optimizing computational efficiency. To train the\nmulti-stage diffusion model, we introduce a dedicated training framework:\nstage-wise diffusion. By solving the partitioned probability flow ordinary\ndifferential equations (ODE) of diffusion under aligned data and noise, our\ntraining strategy is applicable to various diffusion forms and further enhances\ntraining efficiency. Comprehensive experimental evaluations validate the\ngenerality of our method, demonstrating 50% reduction in training cost and 1.5x\nimprovement in inference efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09566.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09573",
            "authors": [
                {
                    "_id": "67d2511e7d0fc37e67269f85",
                    "name": "Marianne Arriola",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f86",
                    "name": "Aaron Gokaslan",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f87",
                    "name": "Justin T Chiu",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f88",
                    "name": "Zhihan Yang",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f89",
                    "name": "Zhixuan Qi",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f8a",
                    "name": "Jiaqi Han",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f8b",
                    "name": "Subham Sekhar Sahoo",
                    "hidden": false
                },
                {
                    "_id": "67d2511e7d0fc37e67269f8c",
                    "name": "Volodymyr Kuleshov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:43:40.000Z",
            "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
            "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
            "upvotes": 34,
            "discussionId": "67d2511e7d0fc37e67269fbf",
            "projectPage": "https://m-arriola.com/bd3lms/",
            "ai_keywords": [
                "diffusion language models",
                "autoregressive models",
                "parallelized generation",
                "controllability",
                "likelihood modeling",
                "fixed-length generation",
                "block diffusion language models",
                "discrete denoising diffusion",
                "flexible-length generation",
                "inference efficiency",
                "KV caching",
                "parallel token sampling",
                "efficient training algorithm",
                "gradient variance estimators",
                "data-driven noise schedules",
                "arbitrary-length sequences"
            ]
        },
        "publishedAt": "2025-03-12T13:43:40.000Z",
        "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
        "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09573.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09151",
            "authors": [
                {
                    "_id": "67d2784fbe3b4e06086d8eec",
                    "user": {
                        "_id": "656ee8008bb9f4f8d95bd8f7",
                        "avatarUrl": "/avatars/4069d70f1279d928da521211c495d638.svg",
                        "isPro": true,
                        "fullname": "Hyeonho Jeong",
                        "user": "hyeonho-jeong-video",
                        "type": "user"
                    },
                    "name": "Hyeonho Jeong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:23:54.861Z",
                    "hidden": false
                },
                {
                    "_id": "67d2784fbe3b4e06086d8eed",
                    "name": "Suhyeon Lee",
                    "hidden": false
                },
                {
                    "_id": "67d2784fbe3b4e06086d8eee",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T08:26:15.000Z",
            "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
            "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
            "upvotes": 26,
            "discussionId": "67d27857be3b4e06086d9160",
            "projectPage": "https://hyeonho99.github.io/reangle-a-video/",
            "githubRepo": "https://github.com/HyeonHo99/Reangle-Video",
            "ai_keywords": [
                "image-to-video diffusion transformer",
                "self-supervised manner",
                "view-invariant motion",
                "warped videos",
                "Multi-View Consistent Image-to-Images Translation",
                "cross-view consistency",
                "DUSt3R",
                "multi-view video generation",
                "static view transport",
                "dynamic camera control"
            ]
        },
        "publishedAt": "2025-03-12T04:26:15.000Z",
        "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
        "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09151.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09601",
            "authors": [
                {
                    "_id": "67d29b617d0fc37e673c7e65",
                    "name": "Itay Chachy",
                    "hidden": false
                },
                {
                    "_id": "67d29b617d0fc37e673c7e66",
                    "name": "Guy Yariv",
                    "hidden": false
                },
                {
                    "_id": "67d29b617d0fc37e673c7e67",
                    "name": "Sagie Benaim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:59:47.000Z",
            "title": "RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling",
            "summary": "Score Distillation Sampling (SDS) has emerged as an effective technique for\nleveraging 2D diffusion priors for tasks such as text-to-3D generation. While\npowerful, SDS struggles with achieving fine-grained alignment to user intent.\nTo overcome this, we introduce RewardSDS, a novel approach that weights noise\nsamples based on alignment scores from a reward model, producing a weighted SDS\nloss. This loss prioritizes gradients from noise samples that yield aligned\nhigh-reward output. Our approach is broadly applicable and can extend SDS-based\nmethods. In particular, we demonstrate its applicability to Variational Score\nDistillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and\nRewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,\nshowing significant improvements over SDS and VSD on a diverse set of metrics\nmeasuring generation quality and alignment to desired reward models, enabling\nstate-of-the-art performance. Project page is available at https://itaychachy.\ngithub.io/reward-sds/.",
            "upvotes": 11,
            "discussionId": "67d29b637d0fc37e673c7efc",
            "projectPage": "https://itaychachy.github.io/reward-sds/",
            "githubRepo": "https://github.com/itaychachy/RewardSDS",
            "ai_keywords": [
                "score distillation sampling (SDS)",
                "2D diffusion priors",
                "text-to-3D generation",
                "reward model",
                "weighted SDS loss",
                "gradients",
                "high-reward output",
                "variational score distillation (VSD)",
                "RewardVSD",
                "text-to-image",
                "2D editing",
                "generation quality",
                "alignment to desired reward models"
            ]
        },
        "publishedAt": "2025-03-12T13:59:47.000Z",
        "title": "RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling",
        "summary": "Score Distillation Sampling (SDS) has emerged as an effective technique for\nleveraging 2D diffusion priors for tasks such as text-to-3D generation. While\npowerful, SDS struggles with achieving fine-grained alignment to user intent.\nTo overcome this, we introduce RewardSDS, a novel approach that weights noise\nsamples based on alignment scores from a reward model, producing a weighted SDS\nloss. This loss prioritizes gradients from noise samples that yield aligned\nhigh-reward output. Our approach is broadly applicable and can extend SDS-based\nmethods. In particular, we demonstrate its applicability to Variational Score\nDistillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and\nRewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,\nshowing significant improvements over SDS and VSD on a diverse set of metrics\nmeasuring generation quality and alignment to desired reward models, enabling\nstate-of-the-art performance. Project page is available at https://itaychachy.\ngithub.io/reward-sds/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09601.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.08525",
            "authors": [
                {
                    "_id": "67d280f20a6a6dd4a0ffe9e8",
                    "name": "Tong Wei",
                    "hidden": false
                },
                {
                    "_id": "67d280f20a6a6dd4a0ffe9e9",
                    "name": "Yijun Yang",
                    "hidden": false
                },
                {
                    "_id": "67d280f20a6a6dd4a0ffe9ea",
                    "name": "Junliang Xing",
                    "hidden": false
                },
                {
                    "_id": "67d280f20a6a6dd4a0ffe9eb",
                    "name": "Yuanchun Shi",
                    "hidden": false
                },
                {
                    "_id": "67d280f20a6a6dd4a0ffe9ec",
                    "name": "Zongqing Lu",
                    "hidden": false
                },
                {
                    "_id": "67d280f20a6a6dd4a0ffe9ed",
                    "name": "Deheng Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T15:17:02.000Z",
            "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
            "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
            "upvotes": 11,
            "discussionId": "67d280f30a6a6dd4a0ffea45",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable outcome rewards",
                "chain-of-thought (CoT) reasoning",
                "large language models (LLMs)",
                "vision-language model (VLM)",
                "goal-directed action reasoning",
                "visual environments",
                "complex card games",
                "24 points",
                "embodied tasks",
                "ALFWorld",
                "action outcomes",
                "thought collapse",
                "diversity",
                "state-irrelevant",
                "incomplete reasoning",
                "invalid actions",
                "negative rewards",
                "process guidance",
                "automated corrector",
                "GTR (Guided Thought Reinforcement)",
                "simultaneous training",
                "human labeling",
                "performance",
                "generalization",
                "task success rates",
                "state-of-the-art (SoTA) models",
                "model sizes"
            ]
        },
        "publishedAt": "2025-03-11T11:17:02.000Z",
        "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
        "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08525.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.04388",
            "authors": [
                {
                    "_id": "67d05aa2348bae81a8ae572e",
                    "name": "Shahar Levy",
                    "hidden": false
                },
                {
                    "_id": "67d05aa2348bae81a8ae572f",
                    "name": "Nir Mazor",
                    "hidden": false
                },
                {
                    "_id": "67d05aa2348bae81a8ae5730",
                    "user": {
                        "_id": "63b433ee7af2e415f25b1a7b",
                        "avatarUrl": "/avatars/0b03f66d263bffd22ed864d1241fe28b.svg",
                        "isPro": false,
                        "fullname": "Lihi Shalmon",
                        "user": "LihiShalmon",
                        "type": "user"
                    },
                    "name": "Lihi Shalmon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-11T16:09:23.471Z",
                    "hidden": false
                },
                {
                    "_id": "67d05aa2348bae81a8ae5731",
                    "name": "Michael Hassid",
                    "hidden": false
                },
                {
                    "_id": "67d05aa2348bae81a8ae5732",
                    "name": "Gabriel Stanovsky",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T12:38:17.000Z",
            "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
            "summary": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .",
            "upvotes": 11,
            "discussionId": "67d05aa3348bae81a8ae5780",
            "githubRepo": "https://github.com/shaharl6000/MoreDocsSameLen",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "LLMs",
                "relevant documents",
                "multi-hop QA task",
                "document count",
                "long contexts"
            ]
        },
        "publishedAt": "2025-03-06T07:38:17.000Z",
        "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
        "summary": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04388.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06955",
            "authors": [
                {
                    "_id": "67d2748117d5cbff7c23621e",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:23:56.553Z",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c23621f",
                    "name": "Yiran Wang",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236220",
                    "name": "Wei Mao",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236221",
                    "name": "Danning Li",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236222",
                    "name": "Rui Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236223",
                    "name": "Biao Wu",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236224",
                    "name": "Zirui Song",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236225",
                    "name": "Bohan Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236226",
                    "name": "Ian Reid",
                    "hidden": false
                },
                {
                    "_id": "67d2748117d5cbff7c236227",
                    "name": "Richard Hartley",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T06:04:31.000Z",
            "title": "Motion Anything: Any to Motion Generation",
            "summary": "Conditional motion generation has been extensively studied in computer\nvision, yet two critical challenges remain. First, while masked autoregressive\nmethods have recently outperformed diffusion-based approaches, existing masking\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\ngiven conditions. Second, existing methods for different conditioning\nmodalities often fail to integrate multiple modalities effectively, limiting\ncontrol and coherence in generated motion. To address these challenges, we\npropose Motion Anything, a multimodal motion generation framework that\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\nspatial and temporal control over key frames and actions. Our model adaptively\nencodes multimodal conditions, including text and music, improving\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\ntwice the size of AIST++, thereby filling a critical gap in the community.\nExtensive experiments demonstrate that Motion Anything surpasses\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\nimprovement in FID on HumanML3D and showing consistent performance gains on\nAIST++ and TMD. See our project website\nhttps://steve-zeyu-zhang.github.io/MotionAnything",
            "upvotes": 10,
            "discussionId": "67d2748317d5cbff7c2362f2",
            "projectPage": "https://steve-zeyu-zhang.github.io/MotionAnything",
            "githubRepo": "https://github.com/steve-zeyu-zhang/MotionAnything",
            "ai_keywords": [
                "masked autoregressive methods",
                "diffusion-based approaches",
                "masking models",
                "dynamic frames",
                "body parts",
                "conditional motion generation",
                "multimodal motion generation framework",
                "Attention-based Mask Modeling",
                "key frames",
                "actions",
                "multimodal conditions",
                "Text-Music-Dance (TMD)",
                "FID",
                "HumanML3D",
                "AIST++"
            ]
        },
        "publishedAt": "2025-03-10T02:04:31.000Z",
        "title": "Motion Anything: Any to Motion Generation",
        "summary": "Conditional motion generation has been extensively studied in computer\nvision, yet two critical challenges remain. First, while masked autoregressive\nmethods have recently outperformed diffusion-based approaches, existing masking\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\ngiven conditions. Second, existing methods for different conditioning\nmodalities often fail to integrate multiple modalities effectively, limiting\ncontrol and coherence in generated motion. To address these challenges, we\npropose Motion Anything, a multimodal motion generation framework that\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\nspatial and temporal control over key frames and actions. Our model adaptively\nencodes multimodal conditions, including text and music, improving\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\ntwice the size of AIST++, thereby filling a critical gap in the community.\nExtensive experiments demonstrate that Motion Anything surpasses\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\nimprovement in FID on HumanML3D and showing consistent performance gains on\nAIST++ and TMD. See our project website\nhttps://steve-zeyu-zhang.github.io/MotionAnything",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06955.png",
        "numComments": 4,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07103",
            "authors": [
                {
                    "_id": "67d27d4c6fbfb8ee21886b60",
                    "user": {
                        "_id": "663486a1f64712540644cb68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
                        "isPro": true,
                        "fullname": "Alessandro",
                        "user": "Devy1",
                        "type": "user"
                    },
                    "name": "Alessandro Giagnorio",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:23:50.840Z",
                    "hidden": false
                },
                {
                    "_id": "67d27d4c6fbfb8ee21886b61",
                    "name": "Antonio Mastropaolo",
                    "hidden": false
                },
                {
                    "_id": "67d27d4c6fbfb8ee21886b62",
                    "name": "Saima Afrin",
                    "hidden": false
                },
                {
                    "_id": "67d27d4c6fbfb8ee21886b63",
                    "user": {
                        "_id": "63f378004745321de351a554",
                        "avatarUrl": "/avatars/3be79f0f6eb0bcfdc9f31b46d2bafc14.svg",
                        "isPro": false,
                        "fullname": "Max Di Penta",
                        "user": "mdiipenta",
                        "type": "user"
                    },
                    "name": "Massimiliano Di Penta",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-13T06:38:05.744Z",
                    "hidden": false
                },
                {
                    "_id": "67d27d4c6fbfb8ee21886b64",
                    "name": "Gabriele Bavota",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T09:26:08.000Z",
            "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
            "summary": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance.",
            "upvotes": 6,
            "discussionId": "67d27d4d6fbfb8ee21886bab",
            "githubRepo": "https://github.com/Devy99/lowbit-quantization",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "code generation",
                "trainable parameters",
                "memory footprint",
                "quantization techniques",
                "precision",
                "floating point 32 bits",
                "int 8 bits",
                "code generation performance",
                "calibration datasets",
                "code-specific calibration datasets"
            ]
        },
        "publishedAt": "2025-03-10T05:26:08.000Z",
        "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
        "summary": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07103.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09402",
            "authors": [
                {
                    "_id": "67d27a9117d5cbff7c2519f6",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": true,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:23:53.060Z",
                    "hidden": false
                },
                {
                    "_id": "67d27a9117d5cbff7c2519f7",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T13:53:30.000Z",
            "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
            "summary": "Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's efficient similarity search. (ii) A\nhierarchical vocabulary derived from large-scale video narrations using our\nnarration pair encoding algorithm, enabling efficient indexing of specific\nevents (e.g., cutting a tomato) by identifying broader scenarios (e.g.,\nkitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary\nupdate strategy leveraging generative models to extend the vocabulary for novel\nevents encountered during inference. To validate our approach, we introduce\nVidCap-Eval, a development set requiring concise narrations with reasoning\nrelationships (e.g., before and after). Experiments on EgoSchema, COIN, and\nHiREST further demonstrate the effectiveness of VLog, highlighting its ability\nto generate concise, contextually accurate, and efficient narrations, offering\na novel perspective on video understanding. Codes are released at\nhttps://github.com/showlab/VLog.",
            "upvotes": 5,
            "discussionId": "67d27a9517d5cbff7c251b41",
            "githubRepo": "https://github.com/showlab/VLog",
            "ai_keywords": [
                "generative retrieval model",
                "contrastive retrieval",
                "hierarchical vocabulary",
                "narration pair encoding algorithm",
                "expressive postfixes",
                "vocabulary update strategy",
                "generative models",
                "VidCap-Eval",
                "EgoSchema",
                "COIN",
                "HiREST",
                "concise narrations",
                "reasoning relationships"
            ]
        },
        "publishedAt": "2025-03-12T09:53:30.000Z",
        "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
        "summary": "Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's efficient similarity search. (ii) A\nhierarchical vocabulary derived from large-scale video narrations using our\nnarration pair encoding algorithm, enabling efficient indexing of specific\nevents (e.g., cutting a tomato) by identifying broader scenarios (e.g.,\nkitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary\nupdate strategy leveraging generative models to extend the vocabulary for novel\nevents encountered during inference. To validate our approach, we introduce\nVidCap-Eval, a development set requiring concise narrations with reasoning\nrelationships (e.g., before and after). Experiments on EgoSchema, COIN, and\nHiREST further demonstrate the effectiveness of VLog, highlighting its ability\nto generate concise, contextually accurate, and efficient narrations, offering\na novel perspective on video understanding. Codes are released at\nhttps://github.com/showlab/VLog.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09402.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06573",
            "authors": [
                {
                    "_id": "67d02a147d82f613a31ed396",
                    "name": "Gili Lior",
                    "hidden": false
                },
                {
                    "_id": "67d02a147d82f613a31ed397",
                    "name": "Asaf Yehudai",
                    "hidden": false
                },
                {
                    "_id": "67d02a147d82f613a31ed398",
                    "name": "Ariel Gera",
                    "hidden": false
                },
                {
                    "_id": "67d02a147d82f613a31ed399",
                    "name": "Liat Ein-Dor",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T12:06:29.000Z",
            "title": "WildIFEval: Instruction Following in the Wild",
            "summary": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, in natural user prompts. We categorize these constraints into\neight high-level classes to capture their distribution and dynamics in\nreal-world scenarios. Leveraging WildIFEval, we conduct extensive experiments\nto benchmark the instruction-following capabilities of leading LLMs. Our\nfindings reveal that all evaluated models experience performance degradation\nwith an increasing number of constraints. Thus, we show that all models have a\nlarge room for improvement on such tasks. Moreover, we observe that the\nspecific type of constraint plays a critical role in model performance. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.",
            "upvotes": 5,
            "discussionId": "67d02a167d82f613a31ed44b"
        },
        "publishedAt": "2025-03-09T08:06:29.000Z",
        "title": "WildIFEval: Instruction Following in the Wild",
        "summary": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, in natural user prompts. We categorize these constraints into\neight high-level classes to capture their distribution and dynamics in\nreal-world scenarios. Leveraging WildIFEval, we conduct extensive experiments\nto benchmark the instruction-following capabilities of leading LLMs. Our\nfindings reveal that all evaluated models experience performance degradation\nwith an increasing number of constraints. Thus, we show that all models have a\nlarge room for improvement on such tasks. Moreover, we observe that the\nspecific type of constraint plays a critical role in model performance. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06573.png",
        "numComments": 3,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09419",
            "authors": [
                {
                    "_id": "67d27032825fbe674d2e4109",
                    "user": {
                        "_id": "64a63f9449b08110f761cd73",
                        "avatarUrl": "/avatars/61860202fc818b105ef24e74dd4f7d3c.svg",
                        "isPro": false,
                        "fullname": "Yifan Zhou",
                        "user": "SingleZombie",
                        "type": "user"
                    },
                    "name": "Yifan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:24:00.485Z",
                    "hidden": false
                },
                {
                    "_id": "67d27032825fbe674d2e410a",
                    "name": "Zeqi Xiao",
                    "hidden": false
                },
                {
                    "_id": "67d27032825fbe674d2e410b",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "67d27032825fbe674d2e410c",
                    "name": "Xingang Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T14:16:30.000Z",
            "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
            "summary": "Latent Diffusion Models (LDMs) are known to have an unstable generation\nprocess, where even small perturbations or shifts in the input noise can lead\nto significantly different outputs. This hinders their applicability in\napplications requiring consistent results. In this work, we redesign LDMs to\nenhance consistency by making them shift-equivariant. While introducing\nanti-aliasing operations can partially improve shift-equivariance, significant\naliasing and inconsistency persist due to the unique challenges in LDMs,\nincluding 1) aliasing amplification during VAE training and multiple U-Net\ninferences, and 2) self-attention modules that inherently lack\nshift-equivariance. To address these issues, we redesign the attention modules\nto be shift-equivariant and propose an equivariance loss that effectively\nsuppresses the frequency bandwidth of the features in the continuous domain.\nThe resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is\nalso robust to irregular warping. Extensive experiments demonstrate that AF-LDM\nproduces significantly more consistent results than vanilla LDM across various\napplications, including video editing and image-to-image translation. Code is\navailable at: https://github.com/SingleZombie/AFLDM",
            "upvotes": 4,
            "discussionId": "67d27034825fbe674d2e4185",
            "projectPage": "https://zhouyifan.net/AF-LDM-Page/",
            "githubRepo": "https://github.com/SingleZombie/AFLDM",
            "ai_keywords": [
                "Latent Diffusion Models (LDMs)",
                "shift-equivariant",
                "anti-aliasing operations",
                "aliasing amplification",
                "VAE training",
                "U-Net",
                "self-attention modules",
                "attention modules",
                "equivariance loss",
                "alias-free LDM (AF-LDM)",
                "video editing",
                "image-to-image translation"
            ]
        },
        "publishedAt": "2025-03-12T10:16:30.000Z",
        "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
        "summary": "Latent Diffusion Models (LDMs) are known to have an unstable generation\nprocess, where even small perturbations or shifts in the input noise can lead\nto significantly different outputs. This hinders their applicability in\napplications requiring consistent results. In this work, we redesign LDMs to\nenhance consistency by making them shift-equivariant. While introducing\nanti-aliasing operations can partially improve shift-equivariance, significant\naliasing and inconsistency persist due to the unique challenges in LDMs,\nincluding 1) aliasing amplification during VAE training and multiple U-Net\ninferences, and 2) self-attention modules that inherently lack\nshift-equivariance. To address these issues, we redesign the attention modules\nto be shift-equivariant and propose an equivariance loss that effectively\nsuppresses the frequency bandwidth of the features in the continuous domain.\nThe resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is\nalso robust to irregular warping. Extensive experiments demonstrate that AF-LDM\nproduces significantly more consistent results than vanilla LDM across various\napplications, including video editing and image-to-image translation. Code is\navailable at: https://github.com/SingleZombie/AFLDM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09419.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09579",
            "authors": [
                {
                    "_id": "67d26ae40a955727b9687d8f",
                    "user": {
                        "_id": "6144e4667f2544bb450787b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png",
                        "isPro": false,
                        "fullname": "Yingfa Chen",
                        "user": "chen-yingfa",
                        "type": "user"
                    },
                    "name": "Yingfa Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:24:03.831Z",
                    "hidden": false
                },
                {
                    "_id": "67d26ae40a955727b9687d90",
                    "name": "Yutong Wu",
                    "hidden": false
                },
                {
                    "_id": "67d26ae40a955727b9687d91",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "67d26ae40a955727b9687d92",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67d26ae40a955727b9687d93",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:50:42.000Z",
            "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
            "summary": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
            "upvotes": 3,
            "discussionId": "67d26ae90a955727b9687eff",
            "githubRepo": "https://github.com/thunlp/cost-optimal-gqa",
            "ai_keywords": [
                "Transformer-based",
                "large language models (LLMs)",
                "parameter size",
                "context length",
                "attention head configuration",
                "grouped-query attention",
                "query and key-value heads",
                "model performance",
                "computational cost",
                "memory cost",
                "cost-optimal LLMs"
            ]
        },
        "publishedAt": "2025-03-12T13:50:42.000Z",
        "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
        "summary": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09579.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.08681",
            "authors": [
                {
                    "_id": "67d2c04b5d0737ce36ec58c8",
                    "user": {
                        "_id": "63bbfd74141c7d395c471768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Viktor Moskvoretskii",
                        "user": "VityaVitalich",
                        "type": "user"
                    },
                    "name": "Viktor Moskvoretskii",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-13T11:23:55.970Z",
                    "hidden": false
                },
                {
                    "_id": "67d2c04b5d0737ce36ec58c9",
                    "name": "Chris Biemann",
                    "hidden": false
                },
                {
                    "_id": "67d2c04b5d0737ce36ec58ca",
                    "name": "Irina Nikishina",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T17:57:44.000Z",
            "title": "Self-Taught Self-Correction for Small Language Models",
            "summary": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels.",
            "upvotes": 3,
            "discussionId": "67d2c04b5d0737ce36ec5901",
            "githubRepo": "https://github.com/VityaVitalich/STASC/",
            "ai_keywords": [
                "large language models (LLMs)",
                "small language models (SLMs)",
                "self-correct",
                "iterative fine-tuning",
                "self-generated data",
                "Self-Taught Self-Correction (STaSC)",
                "algorithmic design choices",
                "question-answering task",
                "self-correction",
                "learning dynamics"
            ]
        },
        "publishedAt": "2025-03-11T13:57:44.000Z",
        "title": "Self-Taught Self-Correction for Small Language Models",
        "summary": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08681.png",
        "numComments": 2,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07588",
            "authors": [
                {
                    "_id": "67d1808e13d7b3f8c6ea9147",
                    "name": "Junwei Luo",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea9148",
                    "name": "Yingying Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea9149",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea914a",
                    "name": "Kang Wu",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea914b",
                    "name": "Qi Zhu",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea914c",
                    "name": "Lei Liang",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea914d",
                    "name": "Jingdong Chen",
                    "hidden": false
                },
                {
                    "_id": "67d1808e13d7b3f8c6ea914e",
                    "name": "Yansheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T17:51:16.000Z",
            "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning",
            "summary": "Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.",
            "upvotes": 3,
            "discussionId": "67d1809013d7b3f8c6ea9271",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "gigapixel RSIs",
                "Dynamic Image Pyramid (DIP)",
                "text-guided token pruning",
                "Region Focus Module (RFM)",
                "text-aware region localization",
                "vision tokens",
                "coarse-to-fine image tile selection",
                "vision token pruning strategy",
                "large imagery",
                "LRS-VQA",
                "QA pairs",
                "high-resolution strategies",
                "token reduction methods"
            ]
        },
        "publishedAt": "2025-03-10T13:51:16.000Z",
        "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning",
        "summary": "Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07588.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09590",
            "authors": [
                {
                    "_id": "67d2f46bbc86adc45acd3888",
                    "name": "Md Mohaiminul Islam",
                    "hidden": false
                },
                {
                    "_id": "67d2f46bbc86adc45acd3889",
                    "name": "Tushar Nagarajan",
                    "hidden": false
                },
                {
                    "_id": "67d2f46bbc86adc45acd388a",
                    "name": "Huiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67d2f46bbc86adc45acd388b",
                    "name": "Gedas Bertasius",
                    "hidden": false
                },
                {
                    "_id": "67d2f46bbc86adc45acd388c",
                    "name": "Lorenzo Torresani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:57:32.000Z",
            "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
            "summary": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.",
            "upvotes": 2,
            "discussionId": "67d2f470bc86adc45acd39c7",
            "ai_keywords": [
                "self-attention mechanism",
                "sequence modeling",
                "spatiotemporal tokens",
                "sparse frame sampling",
                "space-time pooling",
                "state-space model",
                "selective scan algorithm",
                "reduced token sequence",
                "large language model (LLM)",
                "long-form VQA benchmarks",
                "PerceptionTest",
                "NExT-QA",
                "EgoSchema",
                "VNBench",
                "LongVideoBench",
                "Video-MME"
            ]
        },
        "publishedAt": "2025-03-12T13:57:32.000Z",
        "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
        "summary": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09590.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09427",
            "authors": [
                {
                    "_id": "67d247c2a48964532e40e78e",
                    "name": "Yaorui Shi",
                    "hidden": false
                },
                {
                    "_id": "67d247c2a48964532e40e78f",
                    "name": "Jiaqi Yang",
                    "hidden": false
                },
                {
                    "_id": "67d247c2a48964532e40e790",
                    "name": "Sihang Li",
                    "hidden": false
                },
                {
                    "_id": "67d247c2a48964532e40e791",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "67d247c2a48964532e40e792",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "67d247c2a48964532e40e793",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67d247c2a48964532e40e794",
                    "name": "Yang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T14:26:16.000Z",
            "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
            "summary": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in k-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
            "upvotes": 2,
            "discussionId": "67d247c4a48964532e40e802",
            "ai_keywords": [
                "single-cell RNA sequencing data",
                "pre-trained language models (PLMs)",
                "cell PLMs",
                "cross-modal tasks",
                "information loss",
                "single-modal pre-training",
                "Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT)",
                "cross-modal projectors",
                "multimodal cell-text PLMs",
                "cell description generation",
                "cell type annotation",
                "$k$-NN accuracy",
                "text-conditioned pseudo-cell generation"
            ]
        },
        "publishedAt": "2025-03-12T10:26:16.000Z",
        "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
        "summary": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in k-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09427.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.05397",
            "authors": [
                {
                    "_id": "67d2b618919e3b981ada89ce",
                    "name": "Sakharam Gawade",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89cf",
                    "name": "Shivam Akhouri",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89d0",
                    "name": "Chinmay Kulkarni",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89d1",
                    "name": "Jagdish Samant",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89d2",
                    "name": "Pragya Sahu",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89d3",
                    "name": "Aastik",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89d4",
                    "name": "Jai Pahal",
                    "hidden": false
                },
                {
                    "_id": "67d2b618919e3b981ada89d5",
                    "name": "Saswat Meher",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T13:20:12.000Z",
            "title": "Multi Agent based Medical Assistant for Edge Devices",
            "summary": "Large Action Models (LAMs) have revolutionized intelligent automation, but\ntheir application in healthcare faces challenges due to privacy concerns,\nlatency, and dependency on internet access. This report introduces an ondevice,\nmulti-agent healthcare assistant that overcomes these limitations. The system\nutilizes smaller, task-specific agents to optimize resources, ensure\nscalability and high performance. Our proposed system acts as a one-stop\nsolution for health care needs with features like appointment booking, health\nmonitoring, medication reminders, and daily health reporting. Powered by the\nQwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an\naverage RougeL score of 85.5 for planning and 96.5 for calling for our tasks\nwhile being lightweight for on-device deployment. This innovative approach\ncombines the benefits of ondevice systems with multi-agent architectures,\npaving the way for user-centric healthcare solutions.",
            "upvotes": 2,
            "discussionId": "67d2b61a919e3b981ada8a5f",
            "ai_keywords": [
                "Qwen Code Instruct 2.5 7B model",
                "RougeL score"
            ]
        },
        "publishedAt": "2025-03-07T08:20:12.000Z",
        "title": "Multi Agent based Medical Assistant for Edge Devices",
        "summary": "Large Action Models (LAMs) have revolutionized intelligent automation, but\ntheir application in healthcare faces challenges due to privacy concerns,\nlatency, and dependency on internet access. This report introduces an ondevice,\nmulti-agent healthcare assistant that overcomes these limitations. The system\nutilizes smaller, task-specific agents to optimize resources, ensure\nscalability and high performance. Our proposed system acts as a one-stop\nsolution for health care needs with features like appointment booking, health\nmonitoring, medication reminders, and daily health reporting. Powered by the\nQwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an\naverage RougeL score of 85.5 for planning and 96.5 for calling for our tasks\nwhile being lightweight for on-device deployment. This innovative approach\ncombines the benefits of ondevice systems with multi-agent architectures,\npaving the way for user-centric healthcare solutions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05397.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09600",
            "authors": [
                {
                    "_id": "67d270d88f3def6bcbb87b6b",
                    "user": {
                        "_id": "658e85bb5b7553ca5c29ba89",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
                        "isPro": false,
                        "fullname": "Jihao Zhao",
                        "user": "Robot2050",
                        "type": "user"
                    },
                    "name": "Jihao Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:23:58.864Z",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b6c",
                    "name": "Zhiyuan Ji",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b6d",
                    "name": "Zhaoxin Fan",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b6e",
                    "name": "Hanyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b6f",
                    "name": "Simin Niu",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b70",
                    "name": "Bo Tang",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b71",
                    "name": "Feiyu Xiong",
                    "hidden": false
                },
                {
                    "_id": "67d270d88f3def6bcbb87b72",
                    "name": "Zhiyu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:59:42.000Z",
            "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
            "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
            "upvotes": 1,
            "discussionId": "67d270d98f3def6bcbb87bfb",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "Large language models (LLMs)",
                "text chunking",
                "dual-metric evaluation method",
                "Boundary Clarity",
                "Chunk Stickiness",
                "chunking quality",
                "traditional chunking",
                "semantic chunking",
                "contextual nuances",
                "granularity-aware Mixture-of-Chunkers (MoC)",
                "three-stage processing mechanism",
                "chunking regular expressions",
                "chunk extraction",
                "chunking task",
                "chunking kernel"
            ]
        },
        "publishedAt": "2025-03-12T13:59:42.000Z",
        "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
        "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09600.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09516",
            "authors": [
                {
                    "_id": "67d238ab7d0fc37e671feaa8",
                    "user": {
                        "_id": "64b8222749bde5d948104627",
                        "avatarUrl": "/avatars/e179dc68aebae503dcd7ea6b65b4a4b7.svg",
                        "isPro": false,
                        "fullname": "Bowen",
                        "user": "PeterJinGo",
                        "type": "user"
                    },
                    "name": "Bowen Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-13T08:24:22.835Z",
                    "hidden": false
                },
                {
                    "_id": "67d238ab7d0fc37e671feaa9",
                    "name": "Hansi Zeng",
                    "hidden": false
                },
                {
                    "_id": "67d238ab7d0fc37e671feaaa",
                    "name": "Zhenrui Yue",
                    "hidden": false
                },
                {
                    "_id": "67d238ab7d0fc37e671feaab",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "67d238ab7d0fc37e671feaac",
                    "name": "Hamed Zamani",
                    "hidden": false
                },
                {
                    "_id": "67d238ab7d0fc37e671feaad",
                    "name": "Jiawei Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T16:26:39.000Z",
            "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning",
            "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
            "upvotes": 0,
            "discussionId": "67d238ae7d0fc37e671feb7c",
            "ai_keywords": [
                "retrieval augmentation",
                "tool-use training",
                "search engine",
                "reinforcement learning (RL)",
                "multi-turn retrieval",
                "token masking",
                "outcome-based reward function",
                "retrieval-augmented reasoning"
            ]
        },
        "publishedAt": "2025-03-12T12:26:39.000Z",
        "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning",
        "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09516.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09410",
            "authors": [
                {
                    "_id": "67d2c2a8876050b1ae724978",
                    "name": "Jiale Wang",
                    "hidden": false
                },
                {
                    "_id": "67d2c2a8876050b1ae724979",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d2c2a8876050b1ae72497a",
                    "name": "Wei Ke",
                    "hidden": false
                },
                {
                    "_id": "67d2c2a8876050b1ae72497b",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T14:01:18.000Z",
            "title": "Monte Carlo Diffusion for Generalizable Learning-Based RANSAC",
            "summary": "Random Sample Consensus (RANSAC) is a fundamental approach for robustly\nestimating parametric models from noisy data. Existing learning-based RANSAC\nmethods utilize deep learning to enhance the robustness of RANSAC against\noutliers. However, these approaches are trained and tested on the data\ngenerated by the same algorithms, leading to limited generalization to\nout-of-distribution data during inference. Therefore, in this paper, we\nintroduce a novel diffusion-based paradigm that progressively injects noise\ninto ground-truth data, simulating the noisy conditions for training\nlearning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo\nsampling into the diffusion paradigm, approximating diverse data distributions\nby introducing different types of randomness at multiple stages. We evaluate\nour approach in the context of feature matching through comprehensive\nexperiments on the ScanNet and MegaDepth datasets. The experimental results\ndemonstrate that our Monte Carlo diffusion mechanism significantly improves the\ngeneralization ability of learning-based RANSAC. We also develop extensive\nablation studies that highlight the effectiveness of key components in our\nframework.",
            "upvotes": 0,
            "discussionId": "67d2c2aa876050b1ae724a20",
            "ai_keywords": [
                "diffusion-based paradigm",
                "Monte Carlo sampling"
            ]
        },
        "publishedAt": "2025-03-12T10:01:18.000Z",
        "title": "Monte Carlo Diffusion for Generalizable Learning-Based RANSAC",
        "summary": "Random Sample Consensus (RANSAC) is a fundamental approach for robustly\nestimating parametric models from noisy data. Existing learning-based RANSAC\nmethods utilize deep learning to enhance the robustness of RANSAC against\noutliers. However, these approaches are trained and tested on the data\ngenerated by the same algorithms, leading to limited generalization to\nout-of-distribution data during inference. Therefore, in this paper, we\nintroduce a novel diffusion-based paradigm that progressively injects noise\ninto ground-truth data, simulating the noisy conditions for training\nlearning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo\nsampling into the diffusion paradigm, approximating diverse data distributions\nby introducing different types of randomness at multiple stages. We evaluate\nour approach in the context of feature matching through comprehensive\nexperiments on the ScanNet and MegaDepth datasets. The experimental results\ndemonstrate that our Monte Carlo diffusion mechanism significantly improves the\ngeneralization ability of learning-based RANSAC. We also develop extensive\nablation studies that highlight the effectiveness of key components in our\nframework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09410.png",
        "numComments": 2,
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.08674",
            "authors": [
                {
                    "_id": "67d323231d5d1fd92c35ad1c",
                    "user": {
                        "_id": "64bece3e01f1983a86a3e043",
                        "avatarUrl": "/avatars/7f6372d409e8eb0474f7ff6282cfd4e9.svg",
                        "isPro": false,
                        "fullname": "Toby Kreiman",
                        "user": "tkreiman",
                        "type": "user"
                    },
                    "name": "Tobias Kreiman",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-13T18:32:48.171Z",
                    "hidden": false
                },
                {
                    "_id": "67d323231d5d1fd92c35ad1d",
                    "name": "Aditi S. Krishnapriyan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T17:54:29.000Z",
            "title": "Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields",
            "summary": "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps://tkreiman.github.io/projects/mlff_distribution_shifts/.",
            "upvotes": 0,
            "discussionId": "67d323251d5d1fd92c35ad99",
            "projectPage": "https://tkreiman.github.io/projects/mlff_distribution_shifts/",
            "githubRepo": "https://github.com/ASK-Berkeley/MLFF-distribution-shifts?tab=readme-ov-file",
            "ai_keywords": [
                "Machine Learning Force Fields (MLFFs)",
                "ab initio quantum mechanical molecular simulations",
                "chemical spaces",
                "distribution shifts",
                "diagnostic experiments",
                "supervised training methods",
                "overfitting",
                "poor representations",
                "spectral graph theory",
                "graph structures",
                "gradient steps",
                "auxiliary objective",
                "physical prior",
                "test-time refinement strategies"
            ]
        },
        "publishedAt": "2025-03-11T13:54:29.000Z",
        "title": "Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields",
        "summary": "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps://tkreiman.github.io/projects/mlff_distribution_shifts/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08674.png",
        "numComments": 3,
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05333",
            "authors": [
                {
                    "_id": "67cede23eefee37944850e24",
                    "user": {
                        "_id": "6628d1b6fbbb8665257e2d3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628d1b6fbbb8665257e2d3a/yXU5idS0-t3X5d1SserUv.png",
                        "isPro": false,
                        "fullname": "Martin Spitznagel",
                        "user": "mspitzna",
                        "type": "user"
                    },
                    "name": "Martin Spitznagel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-10T12:45:56.622Z",
                    "hidden": false
                },
                {
                    "_id": "67cede23eefee37944850e25",
                    "name": "Jan Vaillant",
                    "hidden": false
                },
                {
                    "_id": "67cede23eefee37944850e26",
                    "name": "Janis Keuper",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T11:19:13.000Z",
            "title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex\n  Physical Relations?",
            "summary": "The image-to-image translation abilities of generative learning models have\nrecently made significant progress in the estimation of complex (steered)\nmappings between image distributions. While appearance based tasks like image\nin-painting or style transfer have been studied at length, we propose to\ninvestigate the potential of generative models in the context of physical\nsimulations. Providing a dataset of 300k image-pairs and baseline evaluations\nfor three different physical simulation tasks, we propose a benchmark to\ninvestigate the following research questions: i) are generative models able to\nlearn complex physical relations from input-output image pairs? ii) what\nspeedups can be achieved by replacing differential equation based simulations?\nWhile baseline evaluations of different current models show the potential for\nhigh speedups (ii), these results also show strong limitations toward the\nphysical correctness (i). This underlines the need for new methods to enforce\nphysical correctness. Data, baseline models and evaluation code\nhttp://www.physics-gen.org.",
            "upvotes": 0,
            "discussionId": "67cede2aeefee37944851028",
            "projectPage": "https://www.physics-gen.org/",
            "githubRepo": "https://github.com/physicsgen/physicsgen",
            "ai_keywords": [
                "image-to-image translation",
                "generative learning models",
                "image in-painting",
                "style transfer",
                "physical simulations",
                "image-pairs",
                "differential equation based simulations",
                "physical correctness"
            ]
        },
        "publishedAt": "2025-03-07T06:19:13.000Z",
        "title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex\n  Physical Relations?",
        "summary": "The image-to-image translation abilities of generative learning models have\nrecently made significant progress in the estimation of complex (steered)\nmappings between image distributions. While appearance based tasks like image\nin-painting or style transfer have been studied at length, we propose to\ninvestigate the potential of generative models in the context of physical\nsimulations. Providing a dataset of 300k image-pairs and baseline evaluations\nfor three different physical simulation tasks, we propose a benchmark to\ninvestigate the following research questions: i) are generative models able to\nlearn complex physical relations from input-output image pairs? ii) what\nspeedups can be achieved by replacing differential equation based simulations?\nWhile baseline evaluations of different current models show the potential for\nhigh speedups (ii), these results also show strong limitations toward the\nphysical correctness (i). This underlines the need for new methods to enforce\nphysical correctness. Data, baseline models and evaluation code\nhttp://www.physics-gen.org.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05333.png",
        "numComments": 2,
        "isAuthorParticipating": true
    }
]