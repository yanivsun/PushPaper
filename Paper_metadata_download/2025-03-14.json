[
    {
        "paper": {
            "id": "2503.10613",
            "authors": [
                {
                    "_id": "67d393ca336d57afb21bbf63",
                    "user": {
                        "_id": "67a99ec47b754f038d110926",
                        "avatarUrl": "/avatars/e1ff318a42ccb75b094bbe7dae0cabec.svg",
                        "isPro": false,
                        "fullname": "Advait Gupta",
                        "user": "advaitgupta",
                        "type": "user"
                    },
                    "name": "Advait Gupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:36.855Z",
                    "hidden": false
                },
                {
                    "_id": "67d393ca336d57afb21bbf64",
                    "user": {
                        "_id": "672f89e6d7f4171f374dacea",
                        "avatarUrl": "/avatars/4a8b378e13e862586bb428fdf000b3cc.svg",
                        "isPro": false,
                        "fullname": "NandaKiran Velaga",
                        "user": "nandakiran09",
                        "type": "user"
                    },
                    "name": "NandaKiran Velaga",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:34.327Z",
                    "hidden": false
                },
                {
                    "_id": "67d393ca336d57afb21bbf65",
                    "name": "Dang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67d393ca336d57afb21bbf66",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:39.157Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:55:45.000Z",
            "submittedOnDailyAt": "2025-03-14T01:33:20.201Z",
            "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
            "upvotes": 52,
            "discussionId": "67d393cf336d57afb21bc0db",
            "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/CoSTAR",
            "githubRepo": "https://github.com/tianyi-lab/CoSTAR",
            "ai_keywords": [
                "text-to-image models",
                "stable diffusion",
                "DALLE-3",
                "multi-turn image editing",
                "agentic workflow",
                "tool use",
                "subtasks",
                "AI tools",
                "cost-efficient",
                "large language models (LLMs)",
                "subtask planning",
                "graph search",
                "three-stage approach",
                "CoSTA*",
                "subtask tree",
                "pruning",
                "A* search",
                "subgraph",
                "cost-quality trade-off",
                "vision-language model (VLM)",
                "failure",
                "total cost",
                "quality",
                "modality switching",
                "benchmark",
                "state-of-the-art image-editing models",
                "user preference"
            ]
        },
        "publishedAt": "2025-03-13T13:55:45.000Z",
        "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
        "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10613.png",
        "numComments": 10,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10622",
            "authors": [
                {
                    "_id": "67d3b0a87443e648e8aa1ea6",
                    "user": {
                        "_id": "6552126dd8a8835b66653767",
                        "avatarUrl": "/avatars/0b1dad9ebaeada8f5e7ebe453123960b.svg",
                        "isPro": false,
                        "fullname": "Jiachen Zhu",
                        "user": "JiachenZhu",
                        "type": "user"
                    },
                    "name": "Jiachen Zhu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-14T05:13:31.648Z",
                    "hidden": false
                },
                {
                    "_id": "67d3b0a87443e648e8aa1ea7",
                    "user": {
                        "_id": "63e58e3a006a775275e59e41",
                        "avatarUrl": "/avatars/75262a35b27a2ae1939df9118120d99e.svg",
                        "isPro": false,
                        "fullname": "Xinlei Chen",
                        "user": "endernewton",
                        "type": "user"
                    },
                    "name": "Xinlei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:45:29.927Z",
                    "hidden": false
                },
                {
                    "_id": "67d3b0a87443e648e8aa1ea8",
                    "name": "Kaiming He",
                    "hidden": false
                },
                {
                    "_id": "67d3b0a87443e648e8aa1ea9",
                    "user": {
                        "_id": "64ed0b8c2203a126eb1a5b9a",
                        "avatarUrl": "/avatars/9156dc406ed3f9ee62b73657ac20f5ed.svg",
                        "isPro": false,
                        "fullname": "Yann LeCun",
                        "user": "ylecun",
                        "type": "user"
                    },
                    "name": "Yann LeCun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:45:43.974Z",
                    "hidden": false
                },
                {
                    "_id": "67d3b0a87443e648e8aa1eaa",
                    "name": "Zhuang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:06.000Z",
            "submittedOnDailyAt": "2025-03-14T02:59:49.783Z",
            "title": "Transformers without Normalization",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
            "upvotes": 44,
            "discussionId": "67d3b0a97443e648e8aa1f22",
            "ai_keywords": [
                "Dynamic Tanh (DyT)",
                "Transformers",
                "normalization layers",
                "layer normalization",
                "hyperparameter tuning",
                "supervised learning",
                "self-supervised learning",
                "computer vision",
                "language models"
            ]
        },
        "publishedAt": "2025-03-13T13:59:06.000Z",
        "title": "Transformers without Normalization",
        "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10622.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6367
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10633",
            "authors": [
                {
                    "_id": "67d3ba5e4d3a41ed9f8651eb",
                    "user": {
                        "_id": "630dd4218df86f1e5beb2ed7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
                        "isPro": false,
                        "fullname": "Eliahu Horwitz",
                        "user": "Eliahu",
                        "type": "user"
                    },
                    "name": "Eliahu Horwitz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:04.270Z",
                    "hidden": false
                },
                {
                    "_id": "67d3ba5e4d3a41ed9f8651ec",
                    "user": {
                        "_id": "674ec6d1ce68874ee4f2d53b",
                        "avatarUrl": "/avatars/4c15c9bdcf51d4bf5e6fceb86195e480.svg",
                        "isPro": false,
                        "fullname": "Nitzan Kurer",
                        "user": "nitzankur",
                        "type": "user"
                    },
                    "name": "Nitzan Kurer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:09:18.093Z",
                    "hidden": false
                },
                {
                    "_id": "67d3ba5e4d3a41ed9f8651ed",
                    "user": {
                        "_id": "6465fd33dac127ac80f0b334",
                        "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
                        "isPro": false,
                        "fullname": "Jonathan Kahana",
                        "user": "jonkahana",
                        "type": "user"
                    },
                    "name": "Jonathan Kahana",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:09:25.338Z",
                    "hidden": false
                },
                {
                    "_id": "67d3ba5e4d3a41ed9f8651ee",
                    "user": {
                        "_id": "669ffff5944b597ce2a1aa5b",
                        "avatarUrl": "/avatars/559ca0ad82b1a52208510f09492fafa6.svg",
                        "isPro": false,
                        "fullname": "Liel Amar",
                        "user": "LielAmar",
                        "type": "user"
                    },
                    "name": "Liel Amar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:01.634Z",
                    "hidden": false
                },
                {
                    "_id": "67d3ba5e4d3a41ed9f8651ef",
                    "user": {
                        "_id": "646cfc3b4220471ca0c56b20",
                        "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
                        "isPro": false,
                        "fullname": "Yedid Hoshen",
                        "user": "yedid",
                        "type": "user"
                    },
                    "name": "Yedid Hoshen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:09:39.859Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
            ],
            "publishedAt": "2025-03-13T17:59:53.000Z",
            "submittedOnDailyAt": "2025-03-14T03:51:41.703Z",
            "title": "Charting and Navigating Hugging Face's Model Atlas",
            "submittedOnDailyBy": {
                "_id": "630dd4218df86f1e5beb2ed7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
                "isPro": false,
                "fullname": "Eliahu Horwitz",
                "user": "Eliahu",
                "type": "user"
            },
            "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
            "upvotes": 36,
            "discussionId": "67d3ba634d3a41ed9f86533a",
            "projectPage": "https://horwitz.ai/model-atlas",
            "githubRepo": "https://github.com/eliahuhorwitz/Model-Atlas",
            "ai_keywords": [
                "neural networks",
                "model repositories",
                "atlas",
                "model landscape",
                "model evolution",
                "predicting model attributes",
                "trends in computer vision models",
                "high-confidence structural priors",
                "dominant real-world model training practices",
                "interactive atlas"
            ]
        },
        "publishedAt": "2025-03-13T13:59:53.000Z",
        "title": "Charting and Navigating Hugging Face's Model Atlas",
        "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10633.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "630dd4218df86f1e5beb2ed7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
            "fullname": "Eliahu Horwitz",
            "name": "Eliahu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10480",
            "authors": [
                {
                    "_id": "67d38a42d3d16e1166d81bed",
                    "user": {
                        "_id": "64c3c631e77ea9f28111172a",
                        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                        "isPro": false,
                        "fullname": "Siyin Wang",
                        "user": "sinwang",
                        "type": "user"
                    },
                    "name": "Siyin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:44.686Z",
                    "hidden": false
                },
                {
                    "_id": "67d38a42d3d16e1166d81bee",
                    "user": {
                        "_id": "629ef8544313a7c1dd671130",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
                        "isPro": false,
                        "fullname": "Zhaoye Fei",
                        "user": "ngc7293",
                        "type": "user"
                    },
                    "name": "Zhaoye Fei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:05:04.607Z",
                    "hidden": false
                },
                {
                    "_id": "67d38a42d3d16e1166d81bef",
                    "name": "Qinyuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "67d38a42d3d16e1166d81bf0",
                    "user": {
                        "_id": "64196e45060a651c415d5cf7",
                        "avatarUrl": "/avatars/71a43232a7bae851eca252782387a63d.svg",
                        "isPro": false,
                        "fullname": "Shiduo Zhang",
                        "user": "CyberDJ",
                        "type": "user"
                    },
                    "name": "Shiduo Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:05:20.440Z",
                    "hidden": false
                },
                {
                    "_id": "67d38a42d3d16e1166d81bf1",
                    "name": "Panpan Cai",
                    "hidden": false
                },
                {
                    "_id": "67d38a42d3d16e1166d81bf2",
                    "user": {
                        "_id": "618497ea8aaadc9253c2dfa9",
                        "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
                        "isPro": false,
                        "fullname": "Fu Jinlan",
                        "user": "Jinlan",
                        "type": "user"
                    },
                    "name": "Jinlan Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:05:38.981Z",
                    "hidden": false
                },
                {
                    "_id": "67d38a42d3d16e1166d81bf3",
                    "user": {
                        "_id": "61457b8deff2c9fdb4de4988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
                        "isPro": false,
                        "fullname": "Xipeng Qiu",
                        "user": "xpqiu",
                        "type": "user"
                    },
                    "name": "Xipeng Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:05:46.041Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T15:49:56.000Z",
            "submittedOnDailyAt": "2025-03-14T01:42:40.120Z",
            "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
            "submittedOnDailyBy": {
                "_id": "64c3c631e77ea9f28111172a",
                "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                "isPro": false,
                "fullname": "Siyin Wang",
                "user": "sinwang",
                "type": "user"
            },
            "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
            "upvotes": 33,
            "discussionId": "67d38a44d3d16e1166d81c54",
            "ai_keywords": [
                "Dual Preference Optimization (D$^2$PO)",
                "preference learning",
                "state prediction",
                "action selection",
                "environment dynamics",
                "tree search mechanism",
                "VoTa-Bench",
                "Qwen2-VL",
                "LLaVA-1.6",
                "LLaMA-3.2",
                "task success rates",
                "efficient execution paths"
            ]
        },
        "publishedAt": "2025-03-13T11:49:56.000Z",
        "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
        "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10480.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "fullname": "Siyin Wang",
            "name": "sinwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09669",
            "authors": [
                {
                    "_id": "67d37754e07f664c7325f236",
                    "user": {
                        "_id": "63bbf972d8d676a2299cdb44",
                        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
                        "isPro": false,
                        "fullname": "Sangwon",
                        "user": "agwmon",
                        "type": "user"
                    },
                    "name": "Sangwon Jang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:49.038Z",
                    "hidden": false
                },
                {
                    "_id": "67d37754e07f664c7325f237",
                    "user": {
                        "_id": "66c6edcc91dced946471bc13",
                        "avatarUrl": "/avatars/55cc8593da6540e1566e1de9d7133f9f.svg",
                        "isPro": false,
                        "fullname": "June Suk Choi",
                        "user": "wchoi403",
                        "type": "user"
                    },
                    "name": "June Suk Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:05:59.636Z",
                    "hidden": false
                },
                {
                    "_id": "67d37754e07f664c7325f238",
                    "user": {
                        "_id": "65e5bd4568234ef5d6decadc",
                        "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
                        "isPro": false,
                        "fullname": "Jaehyeong Jo",
                        "user": "harryjo97",
                        "type": "user"
                    },
                    "name": "Jaehyeong Jo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:06:19.773Z",
                    "hidden": false
                },
                {
                    "_id": "67d37754e07f664c7325f239",
                    "user": {
                        "_id": "635097ec59bfa9a85d4207b2",
                        "avatarUrl": "/avatars/787085894e9e6538b6b3e3051efe9eea.svg",
                        "isPro": false,
                        "fullname": "Kimin Lee",
                        "user": "kiminle2",
                        "type": "user"
                    },
                    "name": "Kimin Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:06:26.649Z",
                    "hidden": false
                },
                {
                    "_id": "67d37754e07f664c7325f23a",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T17:21:57.000Z",
            "submittedOnDailyAt": "2025-03-14T02:05:42.787Z",
            "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "63bbf972d8d676a2299cdb44",
                "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
                "isPro": false,
                "fullname": "Sangwon",
                "user": "agwmon",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
            "upvotes": 28,
            "discussionId": "67d37759e07f664c7325f3c5",
            "projectPage": "https://silent-branding.github.io/",
            "ai_keywords": [
                "text-to-image diffusion models",
                "high-quality contents",
                "text prompts",
                "data poisoning attacks",
                "Silent Branding Attack",
                "visual patterns",
                "data poisoning algorithm",
                "logos",
                "style personalization datasets",
                "logo detection"
            ]
        },
        "publishedAt": "2025-03-12T13:21:57.000Z",
        "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
        "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09669.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bbf972d8d676a2299cdb44",
            "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
            "fullname": "Sangwon",
            "name": "agwmon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09662",
            "authors": [
                {
                    "_id": "67d3daf40034469b0d6cc872",
                    "name": "Shitong Shao",
                    "hidden": false
                },
                {
                    "_id": "67d3daf40034469b0d6cc873",
                    "name": "Zikai Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d3daf40034469b0d6cc874",
                    "name": "Dian Xie",
                    "hidden": false
                },
                {
                    "_id": "67d3daf40034469b0d6cc875",
                    "name": "Yuetong Fang",
                    "hidden": false
                },
                {
                    "_id": "67d3daf40034469b0d6cc876",
                    "name": "Tian Ye",
                    "hidden": false
                },
                {
                    "_id": "67d3daf40034469b0d6cc877",
                    "user": {
                        "_id": "6424772d956c16097c2745b4",
                        "avatarUrl": "/avatars/469af721009b9825ae6ac49112f58fdb.svg",
                        "isPro": false,
                        "fullname": "Bai LiChen",
                        "user": "indulgeBai",
                        "type": "user"
                    },
                    "name": "Lichen Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:43:18.852Z",
                    "hidden": false
                },
                {
                    "_id": "67d3daf40034469b0d6cc878",
                    "name": "Zeke Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T15:15:25.000Z",
            "submittedOnDailyAt": "2025-03-14T06:07:49.038Z",
            "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
            "submittedOnDailyBy": {
                "_id": "66015e8aa4d296af07de538e",
                "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
                "isPro": false,
                "fullname": "Ye",
                "user": "Owen777",
                "type": "user"
            },
            "summary": "Making text-to-image (T2I) generative model sample both fast and well\nrepresents a promising research direction. Previous studies have typically\nfocused on either enhancing the visual quality of synthesized images at the\nexpense of sampling efficiency or dramatically accelerating sampling without\nimproving the base model's generative capacity. Moreover, nearly all inference\nmethods have not been able to ensure stable performance simultaneously on both\ndiffusion models (DMs) and visual autoregressive models (ARMs). In this paper,\nwe introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises\nthree subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects\nclassifier-free guidance (CFG) trajectories, and then use collected data to\ntrain a weak model that reflects the easy-to-learn contents while reducing\nnumber of function evaluations during inference by half. Subsequently, CoRe^2\nemploys weak-to-strong guidance to refine the conditional output, thereby\nimproving the model's capacity to generate high-frequency and realistic\ncontent, which is difficult for the base model to capture. To the best of our\nknowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness\nacross a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs\nlike LlamaGen. It has exhibited significant performance improvements on HPD v2,\nPick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be\nseamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by\n0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using\nSD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
            "upvotes": 27,
            "discussionId": "67d3dafb0034469b0d6ccac0",
            "ai_keywords": [
                "diffusion models (DMs)",
                "visual autoregressive models (ARMs)",
                "classifier-free guidance (CFG)",
                "HPD v2",
                "Pick-of-Pic",
                "Drawbench",
                "GenEval",
                "T2I-Compbench",
                "PickScore",
                "AES",
                "Z-Sampling",
                "SDXL",
                "SD3.5",
                "FLUX",
                "LlamaGen"
            ]
        },
        "publishedAt": "2025-03-12T11:15:25.000Z",
        "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
        "summary": "Making text-to-image (T2I) generative model sample both fast and well\nrepresents a promising research direction. Previous studies have typically\nfocused on either enhancing the visual quality of synthesized images at the\nexpense of sampling efficiency or dramatically accelerating sampling without\nimproving the base model's generative capacity. Moreover, nearly all inference\nmethods have not been able to ensure stable performance simultaneously on both\ndiffusion models (DMs) and visual autoregressive models (ARMs). In this paper,\nwe introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises\nthree subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects\nclassifier-free guidance (CFG) trajectories, and then use collected data to\ntrain a weak model that reflects the easy-to-learn contents while reducing\nnumber of function evaluations during inference by half. Subsequently, CoRe^2\nemploys weak-to-strong guidance to refine the conditional output, thereby\nimproving the model's capacity to generate high-frequency and realistic\ncontent, which is difficult for the base model to capture. To the best of our\nknowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness\nacross a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs\nlike LlamaGen. It has exhibited significant performance improvements on HPD v2,\nPick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be\nseamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by\n0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using\nSD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09662.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "fullname": "Ye",
            "name": "Owen777",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10639",
            "authors": [
                {
                    "_id": "67d3a632db36a4d5d95dbcff",
                    "user": {
                        "_id": "65b8724123d948d884b379b1",
                        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
                        "isPro": false,
                        "fullname": "Rongyao Fang",
                        "user": "LucasFang",
                        "type": "user"
                    },
                    "name": "Rongyao Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:14.110Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd00",
                    "user": {
                        "_id": "64a2b496e2e19de17db7de65",
                        "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
                        "isPro": false,
                        "fullname": "Duan Chengqi",
                        "user": "gogoduan",
                        "type": "user"
                    },
                    "name": "Chengqi Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:07:01.612Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd01",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd02",
                    "user": {
                        "_id": "65fc7c824d36be78e66ba92d",
                        "avatarUrl": "/avatars/d4a55c820cae533f91724e062427516a.svg",
                        "isPro": false,
                        "fullname": "Linjiang Huang",
                        "user": "LjHuang",
                        "type": "user"
                    },
                    "name": "Linjiang Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:07:08.707Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd03",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd04",
                    "user": {
                        "_id": "65273fea0ef49cfb783fa5c1",
                        "avatarUrl": "/avatars/0c9e204bc2151c8cc533311900d05a36.svg",
                        "isPro": false,
                        "fullname": "shilinyan",
                        "user": "shilinyan",
                        "type": "user"
                    },
                    "name": "Shilin Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:07:18.212Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd05",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd06",
                    "user": {
                        "_id": "666d4a0fe70e5838d95aebee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6dkjoFA_sOjCkjvcvozZ5.jpeg",
                        "isPro": false,
                        "fullname": "zengxingyu",
                        "user": "zengxingyu",
                        "type": "user"
                    },
                    "name": "Xingyu Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:07:42.264Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd07",
                    "name": "Rui Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd08",
                    "user": {
                        "_id": "64686f7172d9180d4ac8b4e4",
                        "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
                        "isPro": false,
                        "fullname": "Jifeng Dai",
                        "user": "daijifeng",
                        "type": "user"
                    },
                    "name": "Jifeng Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:14:59.460Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd09",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:14:50.924Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a632db36a4d5d95dbd0a",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T09:14:43.402Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:59.000Z",
            "submittedOnDailyAt": "2025-03-14T02:16:04.349Z",
            "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
            "submittedOnDailyBy": {
                "_id": "65b8724123d948d884b379b1",
                "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
                "isPro": false,
                "fullname": "Rongyao Fang",
                "user": "LucasFang",
                "type": "user"
            },
            "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
            "upvotes": 24,
            "discussionId": "67d3a636db36a4d5d95dbdeb",
            "githubRepo": "https://github.com/rongyaofang/GoT",
            "ai_keywords": [
                "Generation Chain-of-Thought (GoT)",
                "text-to-image generation",
                "editing tasks",
                "reasoning chain generation",
                "end-to-end diffusion model",
                "Semantic-Spatial Guidance Module",
                "semantic relationships",
                "spatial arrangements",
                "large-scale GoT datasets",
                "detailed reasoning chains",
                "semantic-spatial relationships",
                "interactive visual generation",
                "reasoning steps",
                "human intent"
            ]
        },
        "publishedAt": "2025-03-13T13:59:59.000Z",
        "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
        "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10639.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b8724123d948d884b379b1",
            "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
            "fullname": "Rongyao Fang",
            "name": "LucasFang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10291",
            "authors": [
                {
                    "_id": "67d3cbea16d1ecea57ed096c",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:13:00.956Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed096d",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed096e",
                    "user": {
                        "_id": "67640bdb1a42694eaf04f748",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nJ4aJWbAj155ZD3H9hw8g.png",
                        "isPro": false,
                        "fullname": "Lianjie Chen",
                        "user": "chenlj22",
                        "type": "user"
                    },
                    "name": "Lianjie Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:13:45.138Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed096f",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0970",
                    "name": "Jinguo Zhu",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0971",
                    "user": {
                        "_id": "66a11a38933ef3c9844903c3",
                        "avatarUrl": "/avatars/e1b4fa38f1bae79272827194044279f2.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Zhao",
                        "user": "Decaux",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:13:57.533Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0972",
                    "name": "Yangzhou Liu",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0973",
                    "name": "Yue Cao",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0974",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:14:16.758Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0975",
                    "user": {
                        "_id": "64ae2359179421d320b1694b",
                        "avatarUrl": "/avatars/c387a75191005bcaa473091de5383a10.svg",
                        "isPro": false,
                        "fullname": "Xizhou Zhu",
                        "user": "Einsiedler",
                        "type": "user"
                    },
                    "name": "Xizhou Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:14:23.213Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0976",
                    "user": {
                        "_id": "65ead3ea908526a39082e641",
                        "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
                        "isPro": false,
                        "fullname": "Lewei Lu",
                        "user": "luotto",
                        "type": "user"
                    },
                    "name": "Lewei Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:14:29.188Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0977",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:14:35.014Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0978",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed0979",
                    "user": {
                        "_id": "64686f7172d9180d4ac8b4e4",
                        "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
                        "isPro": false,
                        "fullname": "Jifeng Dai",
                        "user": "daijifeng",
                        "type": "user"
                    },
                    "name": "Jifeng Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:14:54.752Z",
                    "hidden": false
                },
                {
                    "_id": "67d3cbea16d1ecea57ed097a",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:14:48.768Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
            ],
            "publishedAt": "2025-03-13T12:03:37.000Z",
            "submittedOnDailyAt": "2025-03-14T06:40:16.854Z",
            "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
            "submittedOnDailyBy": {
                "_id": "619507e7b74b6c591f794340",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                "isPro": false,
                "fullname": "Weiyun Wang",
                "user": "Weiyun1025",
                "type": "user"
            },
            "summary": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.",
            "upvotes": 21,
            "discussionId": "67d3cbed16d1ecea57ed0a75",
            "projectPage": "https://internvl.github.io/blog/2025-03-13-VisualPRM/",
            "githubRepo": "https://github.com/OpenGVLab/InternVL",
            "ai_keywords": [
                "Process Reward Model (PRM)",
                "Multimodal Large Language Models (MLLMs)",
                "Best-of-N (BoN)",
                "multimodal reasoning benchmarks",
                "Automated data pipeline",
                "VisualPRM400K",
                "VisualProcessBench",
                "human-annotated step-wise correctness labels",
                "multimodal PRMs",
                "erroneous steps in multimodal reasoning tasks"
            ]
        },
        "publishedAt": "2025-03-13T08:03:37.000Z",
        "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
        "summary": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10291.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "619507e7b74b6c591f794340",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "fullname": "Weiyun Wang",
            "name": "Weiyun1025",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.08677",
            "authors": [
                {
                    "_id": "67d3007727a57c422730858e",
                    "user": {
                        "_id": "635b6bdfd63bcb528acefa78",
                        "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
                        "isPro": false,
                        "fullname": "Yongsheng Yu",
                        "user": "yeates",
                        "type": "user"
                    },
                    "name": "Yongsheng Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:57:07.420Z",
                    "hidden": false
                },
                {
                    "_id": "67d3007727a57c422730858f",
                    "name": "Ziyun Zeng",
                    "hidden": false
                },
                {
                    "_id": "67d3007727a57c4227308590",
                    "name": "Haitian Zheng",
                    "hidden": false
                },
                {
                    "_id": "67d3007727a57c4227308591",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/635b6bdfd63bcb528acefa78/zOFRFWpFL9GyoHsvpWmei.gif"
            ],
            "publishedAt": "2025-03-11T17:55:27.000Z",
            "submittedOnDailyAt": "2025-03-14T12:04:03.313Z",
            "title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting",
            "submittedOnDailyBy": {
                "_id": "635b6bdfd63bcb528acefa78",
                "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
                "isPro": false,
                "fullname": "Yongsheng Yu",
                "user": "yeates",
                "type": "user"
            },
            "summary": "Diffusion-based generative models have revolutionized object-oriented image\nediting, yet their deployment in realistic object removal and insertion remains\nhampered by challenges such as the intricate interplay of physical effects and\ninsufficient paired training data. In this work, we introduce OmniPaint, a\nunified framework that re-conceptualizes object removal and insertion as\ninterdependent processes rather than isolated tasks. Leveraging a pre-trained\ndiffusion prior along with a progressive training pipeline comprising initial\npaired sample optimization and subsequent large-scale unpaired refinement via\nCycleFlow, OmniPaint achieves precise foreground elimination and seamless\nobject insertion while faithfully preserving scene geometry and intrinsic\nproperties. Furthermore, our novel CFD metric offers a robust, reference-free\nevaluation of context consistency and object hallucination, establishing a new\nbenchmark for high-fidelity image editing. Project page:\nhttps://yeates.github.io/OmniPaint-Page/",
            "upvotes": 19,
            "discussionId": "67d3007a27a57c422730865d",
            "projectPage": "https://www.yongshengyu.com/OmniPaint-Page/",
            "githubRepo": "https://github.com/yeates/OmniPaint",
            "ai_keywords": [
                "diffusion-based generative models",
                "OmniPaint",
                "diffusion prior",
                "CycleFlow",
                "foreground elimination",
                "seamless object insertion",
                "scene geometry",
                "intrinsic properties",
                "CFD metric",
                "context consistency",
                "object hallucination",
                "high-fidelity image editing"
            ]
        },
        "publishedAt": "2025-03-11T13:55:27.000Z",
        "title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting",
        "summary": "Diffusion-based generative models have revolutionized object-oriented image\nediting, yet their deployment in realistic object removal and insertion remains\nhampered by challenges such as the intricate interplay of physical effects and\ninsufficient paired training data. In this work, we introduce OmniPaint, a\nunified framework that re-conceptualizes object removal and insertion as\ninterdependent processes rather than isolated tasks. Leveraging a pre-trained\ndiffusion prior along with a progressive training pipeline comprising initial\npaired sample optimization and subsequent large-scale unpaired refinement via\nCycleFlow, OmniPaint achieves precise foreground elimination and seamless\nobject insertion while faithfully preserving scene geometry and intrinsic\nproperties. Furthermore, our novel CFD metric offers a robust, reference-free\nevaluation of context consistency and object hallucination, establishing a new\nbenchmark for high-fidelity image editing. Project page:\nhttps://yeates.github.io/OmniPaint-Page/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/635b6bdfd63bcb528acefa78/zOFRFWpFL9GyoHsvpWmei.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635b6bdfd63bcb528acefa78",
            "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
            "fullname": "Yongsheng Yu",
            "name": "yeates",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04723",
            "authors": [
                {
                    "_id": "67d39576de5ce3cc428b1909",
                    "user": {
                        "_id": "63369da91ba5d5ece24118a4",
                        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
                        "isPro": false,
                        "fullname": "wuyuhao",
                        "user": "mozhu",
                        "type": "user"
                    },
                    "name": "Yuhao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:48:49.288Z",
                    "hidden": false
                },
                {
                    "_id": "67d39576de5ce3cc428b190a",
                    "user": {
                        "_id": "64ed568ccf6118a9379a61b8",
                        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                        "isPro": false,
                        "fullname": "Yushi Bai",
                        "user": "bys0318",
                        "type": "user"
                    },
                    "name": "Yushi Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:48:17.625Z",
                    "hidden": false
                },
                {
                    "_id": "67d39576de5ce3cc428b190b",
                    "user": {
                        "_id": "637f228152229c63921119c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
                        "isPro": false,
                        "fullname": "Zhiqiang Hu",
                        "user": "Zhiqiang007",
                        "type": "user"
                    },
                    "name": "Zhiqing Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:31.682Z",
                    "hidden": false
                },
                {
                    "_id": "67d39576de5ce3cc428b190c",
                    "user": {
                        "_id": "648c48d8c0ddeee6df5b6d22",
                        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                        "isPro": false,
                        "fullname": "Shangqing Tu",
                        "user": "tsq2000",
                        "type": "user"
                    },
                    "name": "Shangqing Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:48:09.780Z",
                    "hidden": false
                },
                {
                    "_id": "67d39576de5ce3cc428b190d",
                    "user": {
                        "_id": "64ba81d60a30149d57cfadb8",
                        "avatarUrl": "/avatars/da8f9813975606ea6b1dec0c9767831d.svg",
                        "isPro": false,
                        "fullname": "Hee Ming Shan",
                        "user": "mingshan",
                        "type": "user"
                    },
                    "name": "Ming Shan Hee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:48:03.137Z",
                    "hidden": false
                },
                {
                    "_id": "67d39576de5ce3cc428b190e",
                    "user": {
                        "_id": "65df8cbc2705d9672f55d1aa",
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:47:53.216Z",
                    "hidden": false
                },
                {
                    "_id": "67d39576de5ce3cc428b190f",
                    "user": {
                        "_id": "64b94cc0e3d41dbd6974ae45",
                        "avatarUrl": "/avatars/5edb9d9465addfceccef04c4465a34e6.svg",
                        "isPro": false,
                        "fullname": "Roy Ka-Wei lee",
                        "user": "sroylee",
                        "type": "user"
                    },
                    "name": "Roy Ka-Wei Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:47:46.961Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T18:59:37.000Z",
            "submittedOnDailyAt": "2025-03-14T01:04:48.148Z",
            "title": "Shifting Long-Context LLMs Research from Input to Output",
            "submittedOnDailyBy": {
                "_id": "63369da91ba5d5ece24118a4",
                "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
                "isPro": false,
                "fullname": "wuyuhao",
                "user": "mozhu",
                "type": "user"
            },
            "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
            "upvotes": 17,
            "discussionId": "67d39577de5ce3cc428b194f",
            "ai_keywords": [
                "long-context Large Language Models (LLMs)",
                "long-context comprehension",
                "long-output generation",
                "novel writing",
                "long-term planning",
                "complex reasoning",
                "coherent",
                "contextually rich",
                "logically consistent",
                "extended text",
                "high-quality",
                "long-form outputs"
            ]
        },
        "publishedAt": "2025-03-06T13:59:37.000Z",
        "title": "Shifting Long-Context LLMs Research from Input to Output",
        "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04723.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63369da91ba5d5ece24118a4",
            "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
            "fullname": "wuyuhao",
            "name": "mozhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10596",
            "authors": [
                {
                    "_id": "67d3a8950ada3dfbf617fc23",
                    "user": {
                        "_id": "655bcabecfe086853e2aa22b",
                        "avatarUrl": "/avatars/7fd2e7a5ccbbf25643e349682d2f655f.svg",
                        "isPro": false,
                        "fullname": "Rui Hu",
                        "user": "RuiHu",
                        "type": "user"
                    },
                    "name": "Rui Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:45:13.299Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc24",
                    "user": {
                        "_id": "643632c6a4bd75c62cc2d3b4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643632c6a4bd75c62cc2d3b4/rCqEb3_w54-QH2QSP2JhM.jpeg",
                        "isPro": false,
                        "fullname": "Lianghui Zhu",
                        "user": "LianghuiZhu",
                        "type": "user"
                    },
                    "name": "Lianghui Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:45:06.399Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc25",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc26",
                    "user": {
                        "_id": "646b3db131968a60a01e4cf5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
                        "isPro": false,
                        "fullname": "Tianheng Cheng",
                        "user": "wondervictor",
                        "type": "user"
                    },
                    "name": "Tianheng Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:44:40.138Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc27",
                    "name": "Lei Liu",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc28",
                    "name": "Heng Liu",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc29",
                    "name": "Longjin Ran",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc2a",
                    "user": {
                        "_id": "65389a669c474315d7425f96",
                        "avatarUrl": "/avatars/2fa3828ca489cfe1948129a0eccf264f.svg",
                        "isPro": false,
                        "fullname": "chenxiaoxin",
                        "user": "steelozazala",
                        "type": "user"
                    },
                    "name": "Xiaoxin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:44:23.180Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc2b",
                    "user": {
                        "_id": "66c2e7fc934e2f07753542ac",
                        "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
                        "isPro": false,
                        "fullname": "LWY",
                        "user": "wenyuliu",
                        "type": "user"
                    },
                    "name": "Wenyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:43:44.312Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a8950ada3dfbf617fc2c",
                    "user": {
                        "_id": "62600de6d47e3dbae32ce1ce",
                        "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
                        "isPro": false,
                        "fullname": "Xinggang Wang",
                        "user": "xinggangw",
                        "type": "user"
                    },
                    "name": "Xinggang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:43:37.668Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:43:10.000Z",
            "submittedOnDailyAt": "2025-03-14T02:31:30.611Z",
            "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
            "submittedOnDailyBy": {
                "_id": "646b3db131968a60a01e4cf5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
                "isPro": false,
                "fullname": "Tianheng Cheng",
                "user": "wondervictor",
                "type": "user"
            },
            "summary": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., 4.5 times faster than\nthe GLaMM.",
            "upvotes": 16,
            "discussionId": "67d3a8960ada3dfbf617fc8d",
            "ai_keywords": [
                "Referring Expression Segmentation (RES)",
                "Vision-Language Model (VLM)",
                "GroundingSuite",
                "cIoU",
                "gIoU",
                "gRefCOCO",
                "RefCOCOm",
                "GLaMM"
            ]
        },
        "publishedAt": "2025-03-13T13:43:10.000Z",
        "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
        "summary": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., 4.5 times faster than\nthe GLaMM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10596.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646b3db131968a60a01e4cf5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "fullname": "Tianheng Cheng",
            "name": "wondervictor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 27
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10437",
            "authors": [
                {
                    "_id": "67d3adf57360ea908cf5f0bc",
                    "user": {
                        "_id": "658bb7e47459b6e471b9d2e6",
                        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
                        "isPro": false,
                        "fullname": "Wanhua Li",
                        "user": "EthanTaylor",
                        "type": "user"
                    },
                    "name": "Wanhua Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:56:10.748Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0bd",
                    "user": {
                        "_id": "66f0d2036a483077eed42bfb",
                        "avatarUrl": "/avatars/f7f3f726842c26b8e52c9bdd48774b8e.svg",
                        "isPro": false,
                        "fullname": "Renping Zhou",
                        "user": "rpzhou",
                        "type": "user"
                    },
                    "name": "Renping Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:08.159Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0be",
                    "name": "Jiawei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0bf",
                    "user": {
                        "_id": "6578f49ba87010c9f8a30e2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9XvodI5B4-ghh1dJr84lQ.jpeg",
                        "isPro": false,
                        "fullname": "Yingwei Song",
                        "user": "wrencanfly",
                        "type": "user"
                    },
                    "name": "Yingwei Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:56:39.206Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0c0",
                    "name": "Johannes Herter",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0c1",
                    "user": {
                        "_id": "642002b51ccd411979d72b18",
                        "avatarUrl": "/avatars/0a1e7b632fb30ad39e363c3838117322.svg",
                        "isPro": false,
                        "fullname": "Minghan Qin",
                        "user": "Qmh",
                        "type": "user"
                    },
                    "name": "Minghan Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:56:49.663Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0c2",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "67d3adf57360ea908cf5f0c3",
                    "user": {
                        "_id": "62acc69e36f7c7b7f65fccca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62acc69e36f7c7b7f65fccca/S8o0XE6TaQwLU8q3QPkct.png",
                        "isPro": false,
                        "fullname": "Hanspeter Pfister",
                        "user": "hpfister",
                        "type": "user"
                    },
                    "name": "Hanspeter Pfister",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:56:55.960Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
            ],
            "publishedAt": "2025-03-13T14:58:22.000Z",
            "submittedOnDailyAt": "2025-03-14T02:49:34.475Z",
            "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "658bb7e47459b6e471b9d2e6",
                "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
                "isPro": false,
                "fullname": "Wanhua Li",
                "user": "EthanTaylor",
                "type": "user"
            },
            "summary": "Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.",
            "upvotes": 15,
            "discussionId": "67d3adf87360ea908cf5f182",
            "projectPage": "https://4d-langsplat.github.io/",
            "githubRepo": "https://github.com/zrporz/4DLangSplat",
            "ai_keywords": [
                "4D language fields",
                "time-sensitive queries",
                "open-ended language queries",
                "LangSplat",
                "CLIP features",
                "3D Gaussian representations",
                "dynamic 4D fields",
                "temporal dynamics",
                "pixel-aligned",
                "object-wise video features",
                "4D LangSplat",
                "time-agnostic queries",
                "Multimodal Large Language Models (MLLMs)",
                "multimodal object-wise video prompting",
                "visual prompts",
                "text prompts",
                "detailed captions",
                "temporally consistent captions",
                "sentence embeddings",
                "shared embedding spaces",
                "status deformable network",
                "continuous changes"
            ]
        },
        "publishedAt": "2025-03-13T10:58:22.000Z",
        "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
        "summary": "Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10437.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658bb7e47459b6e471b9d2e6",
            "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
            "fullname": "Wanhua Li",
            "name": "EthanTaylor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10351",
            "authors": [
                {
                    "_id": "67d39b35acb72b994659d4fd",
                    "user": {
                        "_id": "635504620d6e89270d440050",
                        "avatarUrl": "/avatars/3790bf4a68f943a122af59b1362b07f2.svg",
                        "isPro": false,
                        "fullname": "LiuSinuo",
                        "user": "SNF",
                        "type": "user"
                    },
                    "name": "Sinuo Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:46:07.508Z",
                    "hidden": false
                },
                {
                    "_id": "67d39b35acb72b994659d4fe",
                    "user": {
                        "_id": "6527d8b077bceabaab382a75",
                        "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
                        "isPro": false,
                        "fullname": "Chenyang Lyu",
                        "user": "ChenyangLyu",
                        "type": "user"
                    },
                    "name": "Chenyang Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:46:14.863Z",
                    "hidden": false
                },
                {
                    "_id": "67d39b35acb72b994659d4ff",
                    "user": {
                        "_id": "62d4bf8c97ab9eb08762a975",
                        "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
                        "isPro": false,
                        "fullname": "Minghao Wu",
                        "user": "minghaowu",
                        "type": "user"
                    },
                    "name": "Minghao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:46:28.934Z",
                    "hidden": false
                },
                {
                    "_id": "67d39b35acb72b994659d500",
                    "user": {
                        "_id": "636b030c328133bdb3a523bc",
                        "avatarUrl": "/avatars/15d5d5403fef2f1368bb4185b199061d.svg",
                        "isPro": false,
                        "fullname": "Longyue Wang",
                        "user": "longyuewang",
                        "type": "user"
                    },
                    "name": "Longyue Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:46:48.590Z",
                    "hidden": false
                },
                {
                    "_id": "67d39b35acb72b994659d501",
                    "user": {
                        "_id": "66b03cedd59c09785e39711e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N5yfQBSP3oAKPCz4ylR09.png",
                        "isPro": false,
                        "fullname": "Weihua Luo",
                        "user": "acecamel1977",
                        "type": "user"
                    },
                    "name": "Weihua Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:46:55.026Z",
                    "hidden": false
                },
                {
                    "_id": "67d39b35acb72b994659d502",
                    "user": {
                        "_id": "63f87ebadf053017d1acbfdd",
                        "avatarUrl": "/avatars/e497ba5f41a2587837b4a6118d9367bb.svg",
                        "isPro": false,
                        "fullname": "Kaifu Zhang",
                        "user": "zhangkaifu314",
                        "type": "user"
                    },
                    "name": "Kaifu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:47:02.246Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
            ],
            "publishedAt": "2025-03-13T13:27:53.000Z",
            "submittedOnDailyAt": "2025-03-14T01:29:07.562Z",
            "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "6527d8b077bceabaab382a75",
                "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
                "isPro": false,
                "fullname": "Chenyang Lyu",
                "user": "ChenyangLyu",
                "type": "user"
            },
            "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
            "upvotes": 15,
            "discussionId": "67d39b40acb72b994659d916",
            "ai_keywords": [
                "Chain-of-Thought reasoning (CoT)",
                "Large Reasoning Models (LRMs)",
                "Neural MT",
                "Contextual coherence",
                "Cultural intentionality",
                "Self-reflection",
                "Stylized translation",
                "Document-level translation",
                "Multimodal translation",
                "Auto-pivot translation",
                "Over-localisation",
                "Inference efficiency",
                "Multilingual cognitive agents"
            ]
        },
        "publishedAt": "2025-03-13T09:27:53.000Z",
        "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
        "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10351.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527d8b077bceabaab382a75",
            "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
            "fullname": "Chenyang Lyu",
            "name": "ChenyangLyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10618",
            "authors": [
                {
                    "_id": "67d3d2dec4a225b653154b3a",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b3b",
                    "name": "Rui Qian",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b3c",
                    "user": {
                        "_id": "638807bd5c2df0349f443de9",
                        "avatarUrl": "/avatars/8593af4c3b716dfe17227618873dd3f2.svg",
                        "isPro": false,
                        "fullname": "wenze",
                        "user": "wenzehu",
                        "type": "user"
                    },
                    "name": "Wenze Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:51:01.388Z",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b3d",
                    "user": {
                        "_id": "638cb2efc8912be69c149a46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638cb2efc8912be69c149a46/qxMNOWoSUG525DgNPpHcK.jpeg",
                        "isPro": false,
                        "fullname": "Tsu-Jui Fu",
                        "user": "tsujuifu",
                        "type": "user"
                    },
                    "name": "Tsu-Jui Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:51:09.453Z",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b3e",
                    "name": "Lezhi Li",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b3f",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b40",
                    "name": "Alex Schwing",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b41",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "67d3d2dec4a225b653154b42",
                    "user": {
                        "_id": "64b762568c632fbca942a405",
                        "avatarUrl": "/avatars/1eb737ec169967872f1ebf5ff29f1e6b.svg",
                        "isPro": false,
                        "fullname": "Yinfei Yang",
                        "user": "yinfeiy",
                        "type": "user"
                    },
                    "name": "Yinfei Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:50:24.690Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:57:25.000Z",
            "submittedOnDailyAt": "2025-03-14T05:26:46.588Z",
            "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
            "submittedOnDailyBy": {
                "_id": "656c2fa772c19de72367bd69",
                "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
                "isPro": false,
                "fullname": "Alex Yang",
                "user": "yyf86",
                "type": "user"
            },
            "summary": "In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.",
            "upvotes": 14,
            "discussionId": "67d3d302c4a225b6531556d6",
            "ai_keywords": [
                "Diffusion Transformers (DiTs)",
                "text-to-image generation",
                "architectural choices",
                "text-conditioning strategies",
                "training protocols",
                "PixArt-style",
                "MMDiT variants",
                "concatenated text and noise inputs",
                "parameter-efficiency",
                "layer-wise parameter sharing strategy",
                "Variational Auto-Encoders (VAEs)",
                "DiT-Air",
                "DiT-Air-Lite",
                "supervised and reward fine-tuning",
                "GenEval",
                "T2I CompBench",
                "state-of-the-art performance"
            ]
        },
        "publishedAt": "2025-03-13T13:57:25.000Z",
        "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
        "summary": "In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10618.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656c2fa772c19de72367bd69",
            "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
            "fullname": "Alex Yang",
            "name": "yyf86",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10582",
            "authors": [
                {
                    "_id": "67d387ff45b17e31c16d05d1",
                    "user": {
                        "_id": "6721451d41cc176331607843",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HQ6_0b2Y-X1ORS6tAz2cq.png",
                        "isPro": false,
                        "fullname": "Yiming Jia",
                        "user": "jymmmmm",
                        "type": "user"
                    },
                    "name": "Yiming Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:49:09.988Z",
                    "hidden": false
                },
                {
                    "_id": "67d387ff45b17e31c16d05d2",
                    "user": {
                        "_id": "640d7ec5fdeaae13907fc488",
                        "avatarUrl": "/avatars/acf43ea155105a51c8612dacc4725091.svg",
                        "isPro": false,
                        "fullname": "Jiachen Li",
                        "user": "jiachenli-ucsb",
                        "type": "user"
                    },
                    "name": "Jiachen Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:49:35.163Z",
                    "hidden": false
                },
                {
                    "_id": "67d387ff45b17e31c16d05d3",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "67d387ff45b17e31c16d05d4",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "67d387ff45b17e31c16d05d5",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "67d387ff45b17e31c16d05d6",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "67d387ff45b17e31c16d05d7",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-14T01:36:13.720Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
            ],
            "publishedAt": "2025-03-13T17:32:48.000Z",
            "submittedOnDailyAt": "2025-03-14T00:47:38.699Z",
            "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Vision-Language Models have made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focused\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900K\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.",
            "upvotes": 13,
            "discussionId": "67d3880d45b17e31c16d09d1",
            "projectPage": "https://tiger-ai-lab.github.io/VisualWebInstruct/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/VisualWebInstruct",
            "ai_keywords": [
                "Vision-Language Models",
                "VisualWebInstruct",
                "search engine",
                "question-answer pairs",
                "visual QA pairs",
                "text QA pairs",
                "fine-tuned",
                "Llava-OV-mid",
                "MAmmoTH-VL",
                "MAmmoTH-VL2",
                "MMMU-Pro-std",
                "MathVerse",
                "DynaMath"
            ]
        },
        "publishedAt": "2025-03-13T13:32:48.000Z",
        "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
        "summary": "Vision-Language Models have made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focused\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900K\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10582.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 33
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10460",
            "authors": [
                {
                    "_id": "67d3a87df6ea55297c3cd071",
                    "name": "Liang Wen",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd072",
                    "name": "Yunke Cai",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd073",
                    "user": {
                        "_id": "643cbf3aae8d93dc395383e3",
                        "avatarUrl": "/avatars/3e719374cc14483e9cdfb174ab99ad26.svg",
                        "isPro": false,
                        "fullname": "Fenrui Xiao",
                        "user": "cizhenshi",
                        "type": "user"
                    },
                    "name": "Fenrui Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:07:18.770Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd074",
                    "name": "Xin He",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd075",
                    "name": "Qi An",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd076",
                    "user": {
                        "_id": "646c2f4d628e5b50b2e22511",
                        "avatarUrl": "/avatars/da64cfd3320f00a1718ab40165ed35dd.svg",
                        "isPro": false,
                        "fullname": "zhenyu duan",
                        "user": "duan901010",
                        "type": "user"
                    },
                    "name": "Zhenyu Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:07:33.157Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd077",
                    "user": {
                        "_id": "66fe5e256990f417165835e8",
                        "avatarUrl": "/avatars/8d3d0a2922f2e17610cb36e84af73584.svg",
                        "isPro": false,
                        "fullname": "Yimin DU",
                        "user": "yyy99",
                        "type": "user"
                    },
                    "name": "Yimin Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:07:12.002Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd078",
                    "user": {
                        "_id": "663a9ae58cf658ffaf3d02e5",
                        "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg",
                        "isPro": false,
                        "fullname": "Junchen Liu",
                        "user": "JunchenLiu",
                        "type": "user"
                    },
                    "name": "Junchen Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:07:42.920Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd079",
                    "user": {
                        "_id": "641d1c7045487810d137866c",
                        "avatarUrl": "/avatars/dc1140875c23213db4991714bdb1a3a7.svg",
                        "isPro": false,
                        "fullname": "tlf",
                        "user": "tanglifu",
                        "type": "user"
                    },
                    "name": "Lifu Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:07:54.728Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd07a",
                    "name": "Xiaowei Lv",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd07b",
                    "user": {
                        "_id": "65685c676c1ae87d4ee8dcfb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65685c676c1ae87d4ee8dcfb/YjUejWm43ODaiA4gAxvp1.jpeg",
                        "isPro": false,
                        "fullname": "Haosheng Zou",
                        "user": "zhs12",
                        "type": "user"
                    },
                    "name": "Haosheng Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:06:40.468Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd07c",
                    "name": "Yongchao Deng",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd07d",
                    "user": {
                        "_id": "660a339940e346fba5b66f1d",
                        "avatarUrl": "/avatars/ae8e29f0ab259257af3e888553ef5946.svg",
                        "isPro": false,
                        "fullname": "jiashousheng",
                        "user": "shousheng",
                        "type": "user"
                    },
                    "name": "Shousheng Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:06:25.241Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a87df6ea55297c3cd07e",
                    "name": "Xiangzheng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T15:29:22.000Z",
            "submittedOnDailyAt": "2025-03-14T02:25:27.538Z",
            "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.",
            "upvotes": 11,
            "discussionId": "67d3a87ef6ea55297c3cd0d0",
            "ai_keywords": [
                "COT models",
                "curriculum training",
                "two-stage SFT",
                "semi-on-policy DPO",
                "long COT capabilities",
                "SOTA",
                "generalization",
                "3k dataset",
                "fine-tuning",
                "GRPO",
                "reasoning performance",
                "AIME24 & 25 scores",
                "response length",
                "reward score"
            ]
        },
        "publishedAt": "2025-03-13T11:29:22.000Z",
        "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
        "summary": "This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10460.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6367
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09642",
            "authors": [
                {
                    "_id": "67d3ab8d032b54ab876cb7ec",
                    "name": "Xiangyu Peng",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7ed",
                    "name": "Zangwei Zheng",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7ee",
                    "name": "Chenhui Shen",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7ef",
                    "name": "Tom Young",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f0",
                    "name": "Xinying Guo",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f1",
                    "name": "Binluo Wang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f2",
                    "name": "Hang Xu",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f3",
                    "name": "Hongxin Liu",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f4",
                    "name": "Mingyan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f5",
                    "name": "Wenjun Li",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f6",
                    "name": "Yuhui Wang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f7",
                    "name": "Anbang Ye",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f8",
                    "name": "Gang Ren",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7f9",
                    "name": "Qianran Ma",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7fa",
                    "name": "Wanying Liang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7fb",
                    "name": "Xiang Lian",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7fc",
                    "name": "Xiwen Wu",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7fd",
                    "name": "Yuting Zhong",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7fe",
                    "name": "Zhuangyan Li",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb7ff",
                    "name": "Chaoyu Gong",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb800",
                    "name": "Guojun Lei",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb801",
                    "name": "Leijun Cheng",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb802",
                    "name": "Limin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb803",
                    "name": "Minghao Li",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb804",
                    "name": "Ruijie Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb805",
                    "name": "Silan Hu",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb806",
                    "name": "Shijie Huang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb807",
                    "name": "Xiaokang Wang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb808",
                    "name": "Yuanheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb809",
                    "name": "Yuqi Wang",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb80a",
                    "name": "Ziang Wei",
                    "hidden": false
                },
                {
                    "_id": "67d3ab8d032b54ab876cb80b",
                    "name": "Yang You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T05:00:07.000Z",
            "submittedOnDailyAt": "2025-03-14T02:39:06.938Z",
            "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
            "upvotes": 11,
            "discussionId": "67d3ab93032b54ab876cb9b0",
            "ai_keywords": [
                "video generation models",
                "data curation",
                "model architecture",
                "training strategy",
                "system optimization",
                "human evaluation",
                "VBench scores",
                "HunyuanVideo",
                "Runway Gen-3 Alpha",
                "open-source"
            ]
        },
        "publishedAt": "2025-03-12T01:00:07.000Z",
        "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
        "summary": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09642.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6367
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10589",
            "authors": [
                {
                    "_id": "67d3adc5c14480a46038cecf",
                    "name": "Yuwei Guo",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced0",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced1",
                    "user": {
                        "_id": "63c98de85fdc575773c52098",
                        "avatarUrl": "/avatars/b66675e781695daed7c00a7c802c91f5.svg",
                        "isPro": false,
                        "fullname": "Ziyan Yang",
                        "user": "ziyany",
                        "type": "user"
                    },
                    "name": "Ziyan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:54:45.430Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced2",
                    "user": {
                        "_id": "645d5a44680734460f9e7742",
                        "avatarUrl": "/avatars/66687e2c413acce55436d967071f8786.svg",
                        "isPro": false,
                        "fullname": "Zhibei Ma",
                        "user": "Brightmzb",
                        "type": "user"
                    },
                    "name": "Zhibei Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:54:24.552Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced3",
                    "user": {
                        "_id": "64415957bd0c9726529802f6",
                        "avatarUrl": "/avatars/1132d1ee68fb58ec635d57c8175caacd.svg",
                        "isPro": false,
                        "fullname": "Zhijie Lin",
                        "user": "Ikuinen",
                        "type": "user"
                    },
                    "name": "Zhijie Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:53:03.195Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced4",
                    "user": {
                        "_id": "6421183b69a2c2933882d652",
                        "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
                        "isPro": false,
                        "fullname": "Zhenheng Yang",
                        "user": "zhenheny",
                        "type": "user"
                    },
                    "name": "Zhenheng Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:52:40.696Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced5",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:52:10.238Z",
                    "hidden": false
                },
                {
                    "_id": "67d3adc5c14480a46038ced6",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:40:07.000Z",
            "submittedOnDailyAt": "2025-03-14T02:47:38.511Z",
            "title": "Long Context Tuning for Video Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
            "upvotes": 10,
            "discussionId": "67d3adc7c14480a46038cf5e",
            "ai_keywords": [
                "scalable diffusion transformers",
                "context window",
                "scene-level consistency",
                "Long Context Tuning (LCT)",
                "full attention mechanisms",
                "interleaved 3D position embedding",
                "asynchronous noise strategy",
                "joint and auto-regressive shot generation",
                "bidirectional attention",
                "context-causal attention",
                "efficient KV-cache",
                "compositional generation",
                "interactive shot extension"
            ]
        },
        "publishedAt": "2025-03-13T13:40:07.000Z",
        "title": "Long Context Tuning for Video Generation",
        "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10589.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6367
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10357",
            "authors": [
                {
                    "_id": "67d3da3ce0b767b3d0fae2a4",
                    "user": {
                        "_id": "63bbfd74141c7d395c471768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Viktor Moskvoretskii",
                        "user": "VityaVitalich",
                        "type": "user"
                    },
                    "name": "Viktor Moskvoretskii",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-14T07:38:54.624Z",
                    "hidden": false
                },
                {
                    "_id": "67d3da3ce0b767b3d0fae2a5",
                    "name": "Alina Lobanova",
                    "hidden": false
                },
                {
                    "_id": "67d3da3ce0b767b3d0fae2a6",
                    "name": "Ekaterina Neminova",
                    "hidden": false
                },
                {
                    "_id": "67d3da3ce0b767b3d0fae2a7",
                    "name": "Chris Biemann",
                    "hidden": false
                },
                {
                    "_id": "67d3da3ce0b767b3d0fae2a8",
                    "user": {
                        "_id": "605473729d7c1d4d81b7e52b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
                        "isPro": false,
                        "fullname": "Alexander Panchenko",
                        "user": "apanc",
                        "type": "user"
                    },
                    "name": "Alexander Panchenko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:57:38.630Z",
                    "hidden": false
                },
                {
                    "_id": "67d3da3ce0b767b3d0fae2a9",
                    "user": {
                        "_id": "6458a9ed6fa580137af4c710",
                        "avatarUrl": "/avatars/8b9f99537adf30fee6aa7ae61ab7235b.svg",
                        "isPro": true,
                        "fullname": "Irina Nikishina",
                        "user": "lilaspourpre",
                        "type": "user"
                    },
                    "name": "Irina Nikishina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T10:57:44.989Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
            ],
            "publishedAt": "2025-03-13T13:37:54.000Z",
            "submittedOnDailyAt": "2025-03-14T05:58:28.512Z",
            "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
            "submittedOnDailyBy": {
                "_id": "63bbfd74141c7d395c471768",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
                "isPro": false,
                "fullname": "Viktor Moskvoretskii",
                "user": "VityaVitalich",
                "type": "user"
            },
            "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
            "upvotes": 10,
            "discussionId": "67d3da42e0b767b3d0fae455",
            "projectPage": "https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
            "ai_keywords": [
                "text-to-image models",
                "zero-shot setup",
                "taxonomy concepts",
                "text-based methods",
                "taxonomy enrichment",
                "comprehensive benchmark",
                "WordNet",
                "LLM generated predictions",
                "taxonomy-related text-to-image metrics",
                "pairwise evaluation",
                "GPT-4 feedback",
                "image generation",
                "ranking of models",
                "retrieval-based approach",
                "automating the curation of structured data resources"
            ]
        },
        "publishedAt": "2025-03-13T09:37:54.000Z",
        "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
        "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10357.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "fullname": "Viktor Moskvoretskii",
            "name": "VityaVitalich",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09799",
            "authors": [
                {
                    "_id": "67d456e2f624d5a396490fa3",
                    "user": {
                        "_id": "65f1c0bf22ea2a37a28b6466",
                        "avatarUrl": "/avatars/d9a11158bad338ada0d044476d8681c1.svg",
                        "isPro": false,
                        "fullname": "Zachary Charles",
                        "user": "zcharles8",
                        "type": "user"
                    },
                    "name": "Zachary Charles",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-14T16:18:43.760Z",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490fa4",
                    "name": "Gabriel Teston",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490fa5",
                    "name": "Lucio Dery",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490fa6",
                    "name": "Keith Rush",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490fa7",
                    "name": "Nova Fallen",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490fa8",
                    "name": "Zachary Garrett",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490fa9",
                    "name": "Arthur Szlam",
                    "hidden": false
                },
                {
                    "_id": "67d456e2f624d5a396490faa",
                    "name": "Arthur Douillard",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T20:04:38.000Z",
            "submittedOnDailyAt": "2025-03-14T14:49:14.021Z",
            "title": "Communication-Efficient Language Model Training Scales Reliably and\n  Robustly: Scaling Laws for DiLoCo",
            "submittedOnDailyBy": {
                "_id": "622792366303bf1dc304f49f",
                "avatarUrl": "/avatars/975c1cc3eb2f97cf8e848162056d5bea.svg",
                "isPro": false,
                "fullname": "Arthur Douillard",
                "user": "ArthurDouillard",
                "type": "user"
            },
            "summary": "As we scale to more massive machine learning models, the frequent\nsynchronization demands inherent in data-parallel approaches create significant\nslowdowns, posing a critical challenge to further scaling. Recent work develops\nan approach (DiLoCo) that relaxes synchronization demands without compromising\nmodel quality. However, these works do not carefully analyze how DiLoCo's\nbehavior changes with model size. In this work, we study the scaling law\nbehavior of DiLoCo when training LLMs under a fixed compute budget. We focus on\nhow algorithmic factors, including number of model replicas, hyperparameters,\nand token budget affect training in ways that can be accurately predicted via\nscaling laws. We find that DiLoCo scales both predictably and robustly with\nmodel size. When well-tuned, DiLoCo scales better than data-parallel training\nwith model size, and can outperform data-parallel training even at small model\nsizes. Our results showcase a more general set of benefits of DiLoCo than\npreviously documented, including increased optimal batch sizes, improved\ndownstream generalization with scale, and improved evaluation loss for a fixed\ntoken budget.",
            "upvotes": 10,
            "discussionId": "67d456e3f624d5a396490fed",
            "ai_keywords": [
                "DiLoCo",
                "data-parallel approaches",
                "synchronization demands",
                "scaling law behavior",
                "LLMs (Large Language Models)",
                "compute budget",
                "model replicas",
                "hyperparameters",
                "token budget",
                "training",
                "algorithmic factors",
                "downsteam generalization"
            ]
        },
        "publishedAt": "2025-03-12T16:04:38.000Z",
        "title": "Communication-Efficient Language Model Training Scales Reliably and\n  Robustly: Scaling Laws for DiLoCo",
        "summary": "As we scale to more massive machine learning models, the frequent\nsynchronization demands inherent in data-parallel approaches create significant\nslowdowns, posing a critical challenge to further scaling. Recent work develops\nan approach (DiLoCo) that relaxes synchronization demands without compromising\nmodel quality. However, these works do not carefully analyze how DiLoCo's\nbehavior changes with model size. In this work, we study the scaling law\nbehavior of DiLoCo when training LLMs under a fixed compute budget. We focus on\nhow algorithmic factors, including number of model replicas, hyperparameters,\nand token budget affect training in ways that can be accurately predicted via\nscaling laws. We find that DiLoCo scales both predictably and robustly with\nmodel size. When well-tuned, DiLoCo scales better than data-parallel training\nwith model size, and can outperform data-parallel training even at small model\nsizes. Our results showcase a more general set of benefits of DiLoCo than\npreviously documented, including increased optimal batch sizes, improved\ndownstream generalization with scale, and improved evaluation loss for a fixed\ntoken budget.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09799.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622792366303bf1dc304f49f",
            "avatarUrl": "/avatars/975c1cc3eb2f97cf8e848162056d5bea.svg",
            "fullname": "Arthur Douillard",
            "name": "ArthurDouillard",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10637",
            "authors": [
                {
                    "_id": "67d3a2a6977358f62157977d",
                    "user": {
                        "_id": "636daf1b56c0762cfda074b5",
                        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
                        "isPro": false,
                        "fullname": "Rohit Gandikota",
                        "user": "RohitGandikota",
                        "type": "user"
                    },
                    "name": "Rohit Gandikota",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:16.089Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a2a6977358f62157977e",
                    "name": "David Bau",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:56.000Z",
            "submittedOnDailyAt": "2025-03-14T02:03:33.686Z",
            "title": "Distilling Diversity and Control in Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "636daf1b56c0762cfda074b5",
                "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
                "isPro": false,
                "fullname": "Rohit Gandikota",
                "user": "RohitGandikota",
                "type": "user"
            },
            "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
            "upvotes": 9,
            "discussionId": "67d3a2ab977358f6215798fc",
            "projectPage": "https://distillation.baulab.info",
            "githubRepo": "https://github.com/rohitgandikota/distillation",
            "ai_keywords": [
                "distilled diffusion models",
                "sample diversity",
                "base models",
                "Concept Sliders",
                "LoRAs",
                "control distillation",
                "representational structure",
                "diversity collapse",
                "Diffusion Target (DT) Visualization",
                "intermediate steps",
                "generation artifacts",
                "inconsistencies",
                "diffusion timesteps",
                "diversity distillation",
                "hybrid inference approach"
            ]
        },
        "publishedAt": "2025-03-13T13:59:56.000Z",
        "title": "Distilling Diversity and Control in Diffusion Models",
        "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10637.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636daf1b56c0762cfda074b5",
            "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
            "fullname": "Rohit Gandikota",
            "name": "RohitGandikota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10615",
            "authors": [
                {
                    "_id": "67d3aa3f2f42ed5552e8ea0c",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea0d",
                    "name": "Xiaoxuan He",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea0e",
                    "name": "Hongkun Pan",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea0f",
                    "user": {
                        "_id": "674972973dc92067bd606877",
                        "avatarUrl": "/avatars/8366448b45baf7d7f3d3d2b8793479ed.svg",
                        "isPro": false,
                        "fullname": "Jiang Xiyan",
                        "user": "Emilia515",
                        "type": "user"
                    },
                    "name": "Xiyan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:11:04.147Z",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea10",
                    "name": "Yan Deng",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea11",
                    "name": "Xingtao Yang",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea12",
                    "name": "Haoyu Lu",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea13",
                    "user": {
                        "_id": "650a932519f6f953cb5798ef",
                        "avatarUrl": "/avatars/c3b4bdc6d4dc13642b69664876532af9.svg",
                        "isPro": false,
                        "fullname": "Dacheng Yin",
                        "user": "dcyin",
                        "type": "user"
                    },
                    "name": "Dacheng Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:10:44.422Z",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea14",
                    "name": "Fengyun Rao",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea15",
                    "user": {
                        "_id": "66771ddf6a685a4ffb0fbf36",
                        "avatarUrl": "/avatars/55c9f32c710aadab0f71d53180a464a2.svg",
                        "isPro": false,
                        "fullname": "Minfeng Zhu",
                        "user": "twilightsnow",
                        "type": "user"
                    },
                    "name": "Minfeng Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:10:30.044Z",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea16",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d3aa3f2f42ed5552e8ea17",
                    "name": "Wei Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:56:05.000Z",
            "submittedOnDailyAt": "2025-03-14T02:32:20.499Z",
            "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.",
            "upvotes": 9,
            "discussionId": "67d3aa422f42ed5552e8eaee",
            "ai_keywords": [
                "multimodal reasoning",
                "cross-modal reasoning",
                "formal textural representations",
                "R1-Onevision dataset",
                "supervised fine-tuning",
                "reinforcement learning",
                "R1-Onevision-Bench",
                "GPT-4o",
                "Qwen2.5-VL"
            ]
        },
        "publishedAt": "2025-03-13T13:56:05.000Z",
        "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
        "summary": "Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10615.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6367
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09641",
            "authors": [
                {
                    "_id": "67d3b008e18f86384bd33fa1",
                    "name": "Junsong Chen",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa2",
                    "name": "Shuchen Xue",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa3",
                    "name": "Yuyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa4",
                    "name": "Jincheng Yu",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa5",
                    "user": {
                        "_id": "5f7fbd813e94f16a85448745",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
                        "isPro": false,
                        "fullname": "Sayak Paul",
                        "user": "sayakpaul",
                        "type": "user"
                    },
                    "name": "Sayak Paul",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:06.327Z",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa6",
                    "name": "Junyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa7",
                    "name": "Han Cai",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa8",
                    "user": {
                        "_id": "616961bdd3f656f79ad18ec1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616961bdd3f656f79ad18ec1/NYpxas378lDevRuUDepJJ.jpeg",
                        "isPro": false,
                        "fullname": "xieenze",
                        "user": "xieenze",
                        "type": "user"
                    },
                    "name": "Enze Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:03:11.565Z",
                    "hidden": false
                },
                {
                    "_id": "67d3b008e18f86384bd33fa9",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T04:53:07.000Z",
            "submittedOnDailyAt": "2025-03-14T02:57:11.544Z",
            "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "5f7fbd813e94f16a85448745",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
                "isPro": false,
                "fullname": "Sayak Paul",
                "user": "sayakpaul",
                "type": "user"
            },
            "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast\ntext-to-image (T2I) generation. SANA-Sprint is built on a pre-trained\nfoundation model and augmented with hybrid distillation, dramatically reducing\ninference steps from 20 to 1-4. We introduce three key innovations: (1) We\npropose a training-free approach that transforms a pre-trained flow-matching\nmodel for continuous-time consistency distillation (sCM), eliminating costly\ntraining from scratch and achieving high training efficiency. Our hybrid\ndistillation strategy combines sCM with latent adversarial distillation (LADD):\nsCM ensures alignment with the teacher model, while LADD enhances single-step\ngeneration fidelity. (2) SANA-Sprint is a unified step-adaptive model that\nachieves high-quality generation in 1-4 steps, eliminating step-specific\ntraining and improving efficiency. (3) We integrate ControlNet with SANA-Sprint\nfor real-time interactive image generation, enabling instant visual feedback\nfor user interaction. SANA-Sprint establishes a new Pareto frontier in\nspeed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID\nand 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71\nGenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s\n(T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s\n(T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for\nAI-powered consumer applications (AIPC). Code and pre-trained models will be\nopen-sourced.",
            "upvotes": 9,
            "discussionId": "67d3b00be18f86384bd3408f",
            "ai_keywords": [
                "diffusion model",
                "text-to-image (T2I)",
                "pre-trained foundation model",
                "hybrid distillation",
                "flow-matching model",
                "continuous-time consistency distillation (sCM)",
                "latent adversarial distillation (LADD)",
                "unified step-adaptive model",
                "ControlNet",
                "real-time interactive image generation",
                "FID",
                "GenEval",
                "FLUX-schnell",
                "H100",
                "RTX 4090",
                "AI-powered consumer applications (AIPC)"
            ]
        },
        "publishedAt": "2025-03-12T00:53:07.000Z",
        "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
        "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast\ntext-to-image (T2I) generation. SANA-Sprint is built on a pre-trained\nfoundation model and augmented with hybrid distillation, dramatically reducing\ninference steps from 20 to 1-4. We introduce three key innovations: (1) We\npropose a training-free approach that transforms a pre-trained flow-matching\nmodel for continuous-time consistency distillation (sCM), eliminating costly\ntraining from scratch and achieving high training efficiency. Our hybrid\ndistillation strategy combines sCM with latent adversarial distillation (LADD):\nsCM ensures alignment with the teacher model, while LADD enhances single-step\ngeneration fidelity. (2) SANA-Sprint is a unified step-adaptive model that\nachieves high-quality generation in 1-4 steps, eliminating step-specific\ntraining and improving efficiency. (3) We integrate ControlNet with SANA-Sprint\nfor real-time interactive image generation, enabling instant visual feedback\nfor user interaction. SANA-Sprint establishes a new Pareto frontier in\nspeed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID\nand 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71\nGenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s\n(T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s\n(T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for\nAI-powered consumer applications (AIPC). Code and pre-trained models will be\nopen-sourced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09641.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "fullname": "Sayak Paul",
            "name": "sayakpaul",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 588
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10391",
            "authors": [
                {
                    "_id": "67d39679ea264394acf948ad",
                    "user": {
                        "_id": "64210d1fd039a891a914986d",
                        "avatarUrl": "/avatars/b178a768657eb223bdbfbd9e0a2000ff.svg",
                        "isPro": false,
                        "fullname": "Yufan Deng",
                        "user": "dyf",
                        "type": "user"
                    },
                    "name": "Yufan Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:08:35.798Z",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948ae",
                    "name": "Xun Guo",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948af",
                    "name": "Yizhi Wang",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b0",
                    "name": "Jacob Zhiyuan Fang",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b1",
                    "user": {
                        "_id": "66a800a2c1454e2221e77473",
                        "avatarUrl": "/avatars/03cd7a4294f82168b4ca0541aadcb67b.svg",
                        "isPro": false,
                        "fullname": "Angtian Wang",
                        "user": "angtian",
                        "type": "user"
                    },
                    "name": "Angtian Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:09:20.695Z",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b2",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:29.782Z",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b3",
                    "user": {
                        "_id": "672ce28bb4215fd38821ed9a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ruFQmhLESJ4j9cxh6sW_J.png",
                        "isPro": false,
                        "fullname": "Yiding Yang",
                        "user": "u302117",
                        "type": "user"
                    },
                    "name": "Yiding Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:09:14.383Z",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b4",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b5",
                    "user": {
                        "_id": "66624861b01c3b590a8c289b",
                        "avatarUrl": "/avatars/afd02857eb1632680d0d53da4488c8bc.svg",
                        "isPro": false,
                        "fullname": "Haibin Huang",
                        "user": "brotherhuang",
                        "type": "user"
                    },
                    "name": "Haibin Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:09:43.105Z",
                    "hidden": false
                },
                {
                    "_id": "67d39679ea264394acf948b6",
                    "user": {
                        "_id": "66fedabfa77ec468ff320767",
                        "avatarUrl": "/avatars/5bd869c30df0f71733006081343b1bd0.svg",
                        "isPro": false,
                        "fullname": "Chongyang Ma",
                        "user": "chongyangma",
                        "type": "user"
                    },
                    "name": "Chongyang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:09:50.116Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T14:07:58.000Z",
            "submittedOnDailyAt": "2025-03-14T01:07:59.040Z",
            "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.",
            "upvotes": 6,
            "discussionId": "67d3967aea264394acf94915",
            "ai_keywords": [
                "diffusion models",
                "multimodal large language model (MLLM)"
            ]
        },
        "publishedAt": "2025-03-13T10:07:58.000Z",
        "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
        "summary": "Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10391.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 34
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10630",
            "authors": [
                {
                    "_id": "67d39f4828221b583a33be2c",
                    "user": {
                        "_id": "6651034c05fb627b10f8abe5",
                        "avatarUrl": "/avatars/4fc0835d5956744295e515f8ff83b3df.svg",
                        "isPro": false,
                        "fullname": "Hang Yin",
                        "user": "hangyin",
                        "type": "user"
                    },
                    "name": "Hang Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:05:49.754Z",
                    "hidden": false
                },
                {
                    "_id": "67d39f4828221b583a33be2d",
                    "user": {
                        "_id": "648ac65fd044b25978015634",
                        "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
                        "isPro": false,
                        "fullname": "Xiuwei Xu",
                        "user": "xuxw98",
                        "type": "user"
                    },
                    "name": "Xiuwei Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:05:43.448Z",
                    "hidden": false
                },
                {
                    "_id": "67d39f4828221b583a33be2e",
                    "name": "Lingqing Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d39f4828221b583a33be2f",
                    "name": "Ziwei Wang",
                    "hidden": false
                },
                {
                    "_id": "67d39f4828221b583a33be30",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d39f4828221b583a33be31",
                    "user": {
                        "_id": "66c44203ea476bea05e9fcd7",
                        "avatarUrl": "/avatars/b061eebec609446e669f5ad6365959f9.svg",
                        "isPro": false,
                        "fullname": "lu",
                        "user": "jiwenlu",
                        "type": "user"
                    },
                    "name": "Jiwen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:04:42.455Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:48.000Z",
            "submittedOnDailyAt": "2025-03-14T01:45:48.071Z",
            "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
            "submittedOnDailyBy": {
                "_id": "648ac65fd044b25978015634",
                "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
                "isPro": false,
                "fullname": "Xiuwei Xu",
                "user": "xuxw98",
                "type": "user"
            },
            "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
            "upvotes": 5,
            "discussionId": "67d39f4928221b583a33be7f",
            "ai_keywords": [
                "universal zero-shot goal-oriented navigation",
                "zero-shot methods",
                "large language models (LLM)",
                "uniform graph representation",
                "object category",
                "instance image",
                "text description",
                "scene graph",
                "explicit graph-based reasoning",
                "graph matching",
                "long-term goal of exploration",
                "subgraph search",
                "coordinate projection",
                "anchor pair alignment",
                "scene graph correction",
                "goal verification",
                "blacklist mechanism",
                "UniGoal"
            ]
        },
        "publishedAt": "2025-03-13T13:59:48.000Z",
        "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
        "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10630.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648ac65fd044b25978015634",
            "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
            "fullname": "Xiuwei Xu",
            "name": "xuxw98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10568",
            "authors": [
                {
                    "_id": "67d3a952687a7a8a4963a030",
                    "user": {
                        "_id": "64e86fbd0c2413c3571ef7a6",
                        "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
                        "isPro": false,
                        "fullname": "Haopeng Li",
                        "user": "hp-l33",
                        "type": "user"
                    },
                    "name": "Haopeng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:10.236Z",
                    "hidden": false
                },
                {
                    "_id": "67d3a952687a7a8a4963a031",
                    "name": "Jinyue Yang",
                    "hidden": false
                },
                {
                    "_id": "67d3a952687a7a8a4963a032",
                    "name": "Guoqi Li",
                    "hidden": false
                },
                {
                    "_id": "67d3a952687a7a8a4963a033",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:19:51.000Z",
            "submittedOnDailyAt": "2025-03-14T02:32:07.479Z",
            "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
            "submittedOnDailyBy": {
                "_id": "64e86fbd0c2413c3571ef7a6",
                "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
                "isPro": false,
                "fullname": "Haopeng Li",
                "user": "hp-l33",
                "type": "user"
            },
            "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
            "upvotes": 5,
            "discussionId": "67d3a957687a7a8a4963a179",
            "projectPage": "https://hp-l33.github.io/projects/arpg",
            "githubRepo": "https://github.com/hp-l33/ARPG",
            "ai_keywords": [
                "autoregressive model",
                "randomized parallel generation",
                "raster-order approaches",
                "inference efficiency",
                "zero-shot generalization",
                "sequential, predefined token generation order",
                "guided decoding framework",
                "positional guidance",
                "content representation",
                "queries",
                "key-value pairs",
                "causal attention mechanism",
                "fully random-order training",
                "bidirectional attention",
                "image inpainting",
                "outpainting",
                "resolution expansion",
                "parallel inference",
                "ImageNet-1K 256 benchmark",
                "FID",
                "sampling steps",
                "throughput",
                "memory consumption"
            ]
        },
        "publishedAt": "2025-03-13T13:19:51.000Z",
        "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
        "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10568.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e86fbd0c2413c3571ef7a6",
            "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
            "fullname": "Haopeng Li",
            "name": "hp-l33",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09905",
            "authors": [
                {
                    "_id": "67d393660d51cf27930a5e5d",
                    "user": {
                        "_id": "67d3930e4d3a41ed9f7ac71e",
                        "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
                        "isPro": false,
                        "fullname": "Allison Andreyev",
                        "user": "allisonandreyev",
                        "type": "user"
                    },
                    "name": "Allison Andreyev",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-14T02:25:50.445Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T23:50:35.000Z",
            "submittedOnDailyAt": "2025-03-14T00:57:22.699Z",
            "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
            "submittedOnDailyBy": {
                "_id": "67d3930e4d3a41ed9f7ac71e",
                "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
                "isPro": false,
                "fullname": "Allison Andreyev",
                "user": "allisonandreyev",
                "type": "user"
            },
            "summary": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git",
            "upvotes": 5,
            "discussionId": "67d393670d51cf27930a5e98",
            "githubRepo": "https://github.com/allisonandreyev/WhisperQuantization",
            "ai_keywords": [
                "Whisper",
                "live speech streaming",
                "offline transcription",
                "hallucinated content",
                "latency",
                "deployment",
                "quantization",
                "LibriSpeech dataset",
                "word error rate (WER)",
                "whispercpp",
                "INT4",
                "INT5",
                "INT8",
                "speech recognition (ASR)",
                "transcription accuracy",
                "edge deployment"
            ]
        },
        "publishedAt": "2025-03-12T19:50:35.000Z",
        "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
        "summary": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d3930e4d3a41ed9f7ac71e",
            "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
            "fullname": "Allison Andreyev",
            "name": "allisonandreyev",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09046",
            "authors": [
                {
                    "_id": "67d31012e19218b9780513e9",
                    "user": {
                        "_id": "67d30f6efcfb51018f9dd14c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9JDoVovUqeiPRAjHlWqYF.png",
                        "isPro": false,
                        "fullname": "Yifan Wang",
                        "user": "Zc0in",
                        "type": "user"
                    },
                    "name": "Yifan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:58.691Z",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513ea",
                    "name": "Yifei Liu",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513eb",
                    "name": "Yingdong Shi",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513ec",
                    "name": "Changming Li",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513ed",
                    "name": "Anqi Pang",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513ee",
                    "name": "Sibei Yang",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513ef",
                    "name": "Jingyi Yu",
                    "hidden": false
                },
                {
                    "_id": "67d31012e19218b9780513f0",
                    "name": "Kan Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T04:10:46.000Z",
            "submittedOnDailyAt": "2025-03-14T12:28:45.086Z",
            "title": "Discovering Influential Neuron Path in Vision Transformers",
            "submittedOnDailyBy": {
                "_id": "67d30f6efcfb51018f9dd14c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9JDoVovUqeiPRAjHlWqYF.png",
                "isPro": false,
                "fullname": "Yifan Wang",
                "user": "Zc0in",
                "type": "user"
            },
            "summary": "Vision Transformer models exhibit immense power yet remain opaque to human\nunderstanding, posing challenges and risks for practical applications. While\nprior research has attempted to demystify these models through input\nattribution and neuron role analysis, there's been a notable gap in considering\nlayer-level information and the holistic path of information flow across\nlayers. In this paper, we investigate the significance of influential neuron\npaths within vision Transformers, which is a path of neurons from the model\ninput to output that impacts the model inference most significantly. We first\npropose a joint influence measure to assess the contribution of a set of\nneurons to the model outcome. And we further provide a layer-progressive neuron\nlocating approach that efficiently selects the most influential neuron at each\nlayer trying to discover the crucial neuron path from input to output within\nthe target model. Our experiments demonstrate the superiority of our method\nfinding the most influential neuron path along which the information flows,\nover the existing baseline solutions. Additionally, the neuron paths have\nillustrated that vision Transformers exhibit some specific inner working\nmechanism for processing the visual information within the same image category.\nWe further analyze the key effects of these neurons on the image classification\ntask, showcasing that the found neuron paths have already preserved the model\ncapability on downstream tasks, which may also shed some lights on real-world\napplications like model pruning. The project website including implementation\ncode is available at https://foundation-model-research.github.io/NeuronPath/.",
            "upvotes": 5,
            "discussionId": "67d31016e19218b97805152c",
            "ai_keywords": [
                "Vision Transformer models",
                "input attribution",
                "neuron role analysis",
                "layer-level information",
                "information flow",
                "influential neuron paths",
                "joint influence measure",
                "layer-progressive neuron locating approach",
                "model inference",
                "neuron paths",
                "inner working mechanism",
                "image classification task",
                "model pruning"
            ]
        },
        "publishedAt": "2025-03-12T00:10:46.000Z",
        "title": "Discovering Influential Neuron Path in Vision Transformers",
        "summary": "Vision Transformer models exhibit immense power yet remain opaque to human\nunderstanding, posing challenges and risks for practical applications. While\nprior research has attempted to demystify these models through input\nattribution and neuron role analysis, there's been a notable gap in considering\nlayer-level information and the holistic path of information flow across\nlayers. In this paper, we investigate the significance of influential neuron\npaths within vision Transformers, which is a path of neurons from the model\ninput to output that impacts the model inference most significantly. We first\npropose a joint influence measure to assess the contribution of a set of\nneurons to the model outcome. And we further provide a layer-progressive neuron\nlocating approach that efficiently selects the most influential neuron at each\nlayer trying to discover the crucial neuron path from input to output within\nthe target model. Our experiments demonstrate the superiority of our method\nfinding the most influential neuron path along which the information flows,\nover the existing baseline solutions. Additionally, the neuron paths have\nillustrated that vision Transformers exhibit some specific inner working\nmechanism for processing the visual information within the same image category.\nWe further analyze the key effects of these neurons on the image classification\ntask, showcasing that the found neuron paths have already preserved the model\ncapability on downstream tasks, which may also shed some lights on real-world\napplications like model pruning. The project website including implementation\ncode is available at https://foundation-model-research.github.io/NeuronPath/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d30f6efcfb51018f9dd14c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9JDoVovUqeiPRAjHlWqYF.png",
            "fullname": "Yifan Wang",
            "name": "Zc0in",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10614",
            "authors": [
                {
                    "_id": "67d3d770336d57afb22fbe46",
                    "user": {
                        "_id": "66e508b7ad382452bb326cbd",
                        "avatarUrl": "/avatars/e541e312c4fe9919778d266339d38206.svg",
                        "isPro": false,
                        "fullname": "bolin",
                        "user": "chenblin26",
                        "type": "user"
                    },
                    "name": "Bolin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:55:56.615Z",
                    "hidden": false
                },
                {
                    "_id": "67d3d770336d57afb22fbe47",
                    "name": "Baoquan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d3d770336d57afb22fbe48",
                    "name": "Haoran Xie",
                    "hidden": false
                },
                {
                    "_id": "67d3d770336d57afb22fbe49",
                    "name": "Yi Cai",
                    "hidden": false
                },
                {
                    "_id": "67d3d770336d57afb22fbe4a",
                    "name": "Qing Li",
                    "hidden": false
                },
                {
                    "_id": "67d3d770336d57afb22fbe4b",
                    "name": "Xudong Mao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66e508b7ad382452bb326cbd/M0JMXtl5TkPxJH-7f0vod.png",
                "https://cdn-uploads.huggingface.co/production/uploads/66e508b7ad382452bb326cbd/6y8qxU2G5Y62OdipU8AyN.png"
            ],
            "publishedAt": "2025-03-13T17:55:58.000Z",
            "submittedOnDailyAt": "2025-03-14T11:54:54.482Z",
            "title": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer",
            "submittedOnDailyBy": {
                "_id": "66e508b7ad382452bb326cbd",
                "avatarUrl": "/avatars/e541e312c4fe9919778d266339d38206.svg",
                "isPro": false,
                "fullname": "bolin",
                "user": "chenblin26",
                "type": "user"
            },
            "summary": "Style transfer involves transferring the style from a reference image to the\ncontent of a target image. Recent advancements in LoRA-based (Low-Rank\nAdaptation) methods have shown promise in effectively capturing the style of a\nsingle image. However, these approaches still face significant challenges such\nas content inconsistency, style misalignment, and content leakage. In this\npaper, we comprehensively analyze the limitations of the standard diffusion\nparameterization, which learns to predict noise, in the context of style\ntransfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method\nthat enhances both content and style consistency by optimizing the LoRA weights\nto predict the original image rather than noise. We also propose a two-step\ntraining strategy that decouples the learning of content and style from the\nreference image. To effectively capture both the global structure and local\ndetails of the content image, we introduce a stepwise loss transition strategy.\nAdditionally, we present an inference guidance method that enables continuous\ncontrol over content and style strengths during inference. Through both\nqualitative and quantitative evaluations, our method demonstrates significant\nimprovements in content and style consistency while effectively reducing\ncontent leakage.",
            "upvotes": 4,
            "discussionId": "67d3d773336d57afb22fbf20",
            "projectPage": "https://consislora.github.io",
            "githubRepo": "https://github.com/000linlin/ConsisLoRA",
            "ai_keywords": [
                "LoRA-based methods",
                "Low-Rank Adaptation",
                "diffusion parameterization",
                "ConsisLoRA",
                "LoRA weights",
                "content consistency",
                "style consistency",
                "two-step training strategy",
                "stepwise loss transition strategy",
                "inference guidance method",
                "content leakage",
                "global structure",
                "local details"
            ]
        },
        "publishedAt": "2025-03-13T13:55:58.000Z",
        "title": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer",
        "summary": "Style transfer involves transferring the style from a reference image to the\ncontent of a target image. Recent advancements in LoRA-based (Low-Rank\nAdaptation) methods have shown promise in effectively capturing the style of a\nsingle image. However, these approaches still face significant challenges such\nas content inconsistency, style misalignment, and content leakage. In this\npaper, we comprehensively analyze the limitations of the standard diffusion\nparameterization, which learns to predict noise, in the context of style\ntransfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method\nthat enhances both content and style consistency by optimizing the LoRA weights\nto predict the original image rather than noise. We also propose a two-step\ntraining strategy that decouples the learning of content and style from the\nreference image. To effectively capture both the global structure and local\ndetails of the content image, we introduce a stepwise loss transition strategy.\nAdditionally, we present an inference guidance method that enables continuous\ncontrol over content and style strengths during inference. Through both\nqualitative and quantitative evaluations, our method demonstrates significant\nimprovements in content and style consistency while effectively reducing\ncontent leakage.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66e508b7ad382452bb326cbd/M0JMXtl5TkPxJH-7f0vod.png",
            "https://cdn-uploads.huggingface.co/production/uploads/66e508b7ad382452bb326cbd/6y8qxU2G5Y62OdipU8AyN.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10614.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e508b7ad382452bb326cbd",
            "avatarUrl": "/avatars/e541e312c4fe9919778d266339d38206.svg",
            "fullname": "bolin",
            "name": "chenblin26",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09837",
            "authors": [
                {
                    "_id": "67d4194fc9f149faff46913c",
                    "name": "Ahmad Mustafa Anis",
                    "hidden": false
                },
                {
                    "_id": "67d4194fc9f149faff46913d",
                    "name": "Hasnain Ali",
                    "hidden": false
                },
                {
                    "_id": "67d4194fc9f149faff46913e",
                    "name": "Saquib Sarfraz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T20:58:16.000Z",
            "submittedOnDailyAt": "2025-03-14T10:27:01.523Z",
            "title": "On the Limitations of Vision-Language Models in Understanding Image\n  Transforms",
            "submittedOnDailyBy": {
                "_id": "6246908d8031dcfa9ef6d80b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6246908d8031dcfa9ef6d80b/hVdURjUl1RS2MZf4qOhvI.jpeg",
                "isPro": true,
                "fullname": "Ahmad Mustafa Anis",
                "user": "AhmadMustafa",
                "type": "user"
            },
            "summary": "Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.",
            "upvotes": 4,
            "discussionId": "67d41950c9f149faff4691ad",
            "ai_keywords": [
                "Vision Language Models (VLMs)",
                "Image/Video Generation",
                "Visual Question Answering",
                "Multimodal Chatbots",
                "Video Understanding",
                "Image-level Understanding",
                "CLIP",
                "SigLIP",
                "Image-level augmentations",
                "Flickr8k dataset",
                "Image2Image models"
            ]
        },
        "publishedAt": "2025-03-12T16:58:16.000Z",
        "title": "On the Limitations of Vision-Language Models in Understanding Image\n  Transforms",
        "summary": "Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09837.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6246908d8031dcfa9ef6d80b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6246908d8031dcfa9ef6d80b/hVdURjUl1RS2MZf4qOhvI.jpeg",
            "fullname": "Ahmad Mustafa Anis",
            "name": "AhmadMustafa",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10636",
            "authors": [
                {
                    "_id": "67d39ae1faaad4ed2df1cc61",
                    "user": {
                        "_id": "63041b541dd5d3c62486c294",
                        "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
                        "isPro": false,
                        "fullname": "Ho Kei Cheng",
                        "user": "hkchengrex",
                        "type": "user"
                    },
                    "name": "Ho Kei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:19.361Z",
                    "hidden": false
                },
                {
                    "_id": "67d39ae1faaad4ed2df1cc62",
                    "name": "Alexander Schwing",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
            ],
            "publishedAt": "2025-03-13T17:59:56.000Z",
            "submittedOnDailyAt": "2025-03-14T01:27:30.007Z",
            "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
            "submittedOnDailyBy": {
                "_id": "63041b541dd5d3c62486c294",
                "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
                "isPro": false,
                "fullname": "Ho Kei Cheng",
                "user": "hkchengrex",
                "type": "user"
            },
            "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
            "upvotes": 3,
            "discussionId": "67d39ae4faaad4ed2df1cd42",
            "projectPage": "https://hkchengrex.com/C2OT/",
            "githubRepo": "https://github.com/hkchengrex/C2OT",
            "ai_keywords": [
                "minibatch optimal transport",
                "flow matching",
                "integration steps",
                "numerical solvers",
                "ordinary differential equation",
                "optimal transport mapping",
                "conditional optimal transport",
                "C^2OT",
                "cost matrix",
                "optimal transport assignment",
                "discrete conditions",
                "continuous conditions",
                "8gaussians-to-moons",
                "CIFAR-10",
                "ImageNet-32x32",
                "ImageNet-256x256",
                "function evaluation budgets"
            ]
        },
        "publishedAt": "2025-03-13T13:59:56.000Z",
        "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
        "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10636.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63041b541dd5d3c62486c294",
            "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
            "fullname": "Ho Kei Cheng",
            "name": "hkchengrex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10365",
            "authors": [
                {
                    "_id": "67d44cb6bb362707e0c548b9",
                    "name": "Elad Richardson",
                    "hidden": false
                },
                {
                    "_id": "67d44cb6bb362707e0c548ba",
                    "name": "Kfir Goldberg",
                    "hidden": false
                },
                {
                    "_id": "67d44cb6bb362707e0c548bb",
                    "name": "Yuval Alaluf",
                    "hidden": false
                },
                {
                    "_id": "67d44cb6bb362707e0c548bc",
                    "name": "Daniel Cohen-Or",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6381e8c72a85b6714b7eb5f7/6e_zZuRh4yJwyz60rNAzD.jpeg"
            ],
            "publishedAt": "2025-03-13T13:46:10.000Z",
            "submittedOnDailyAt": "2025-03-14T14:06:21.539Z",
            "title": "Piece it Together: Part-Based Concepting with IP-Priors",
            "submittedOnDailyBy": {
                "_id": "6381e8c72a85b6714b7eb5f7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381e8c72a85b6714b7eb5f7/GTqChXYVI1Q11btXn1X36.jpeg",
                "isPro": false,
                "fullname": "kfir goldberg",
                "user": "kfirgold99",
                "type": "user"
            },
            "summary": "Advanced generative models excel at synthesizing images but often rely on\ntext-based conditioning. Visual designers, however, often work beyond language,\ndirectly drawing inspiration from existing visual elements. In many cases,\nthese elements represent only fragments of a potential concept-such as an\nuniquely structured wing, or a specific hairstyle-serving as inspiration for\nthe artist to explore how they can come together creatively into a coherent\nwhole. Recognizing this need, we introduce a generative framework that\nseamlessly integrates a partial set of user-provided visual components into a\ncoherent composition while simultaneously sampling the missing parts needed to\ngenerate a plausible and complete concept. Our approach builds on a strong and\nunderexplored representation space, extracted from IP-Adapter+, on which we\ntrain IP-Prior, a lightweight flow-matching model that synthesizes coherent\ncompositions based on domain-specific priors, enabling diverse and\ncontext-aware generations. Additionally, we present a LoRA-based fine-tuning\nstrategy that significantly improves prompt adherence in IP-Adapter+ for a\ngiven task, addressing its common trade-off between reconstruction quality and\nprompt adherence.",
            "upvotes": 3,
            "discussionId": "67d44cbbbb362707e0c54a55",
            "ai_keywords": [
                "generative framework",
                "visual components",
                "coherent composition",
                "IP-Adapter+",
                "flow-matching model",
                "domain-specific priors",
                "LoRA-based fine-tuning"
            ]
        },
        "publishedAt": "2025-03-13T09:46:10.000Z",
        "title": "Piece it Together: Part-Based Concepting with IP-Priors",
        "summary": "Advanced generative models excel at synthesizing images but often rely on\ntext-based conditioning. Visual designers, however, often work beyond language,\ndirectly drawing inspiration from existing visual elements. In many cases,\nthese elements represent only fragments of a potential concept-such as an\nuniquely structured wing, or a specific hairstyle-serving as inspiration for\nthe artist to explore how they can come together creatively into a coherent\nwhole. Recognizing this need, we introduce a generative framework that\nseamlessly integrates a partial set of user-provided visual components into a\ncoherent composition while simultaneously sampling the missing parts needed to\ngenerate a plausible and complete concept. Our approach builds on a strong and\nunderexplored representation space, extracted from IP-Adapter+, on which we\ntrain IP-Prior, a lightweight flow-matching model that synthesizes coherent\ncompositions based on domain-specific priors, enabling diverse and\ncontext-aware generations. Additionally, we present a LoRA-based fine-tuning\nstrategy that significantly improves prompt adherence in IP-Adapter+ for a\ngiven task, addressing its common trade-off between reconstruction quality and\nprompt adherence.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6381e8c72a85b6714b7eb5f7/6e_zZuRh4yJwyz60rNAzD.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10365.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6381e8c72a85b6714b7eb5f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381e8c72a85b6714b7eb5f7/GTqChXYVI1Q11btXn1X36.jpeg",
            "fullname": "kfir goldberg",
            "name": "kfirgold99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10638",
            "authors": [
                {
                    "_id": "67d47010fa4c0750013998b6",
                    "name": "Xiaoming Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d47010fa4c0750013998b7",
                    "name": "Alexander G. Schwing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:59.000Z",
            "submittedOnDailyAt": "2025-03-14T16:40:36.609Z",
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "642006ed290342c5df8158cc",
                "avatarUrl": "/avatars/408d35ff2512eb93ed6570befa33aa03.svg",
                "isPro": false,
                "fullname": "Xiaoming Zhao",
                "user": "xzhao",
                "type": "user"
            },
            "summary": "Classifier-free guidance has become a staple for conditional generation with\ndenoising diffusion models. However, a comprehensive understanding of\nclassifier-free guidance is still missing. In this work, we carry out an\nempirical study to provide a fresh perspective on classifier-free guidance.\nConcretely, instead of solely focusing on classifier-free guidance, we trace\nback to the root, i.e., classifier guidance, pinpoint the key assumption for\nthe derivation, and conduct a systematic study to understand the role of the\nclassifier. We find that both classifier guidance and classifier-free guidance\nachieve conditional generation by pushing the denoising diffusion trajectories\naway from decision boundaries, i.e., areas where conditional information is\nusually entangled and is hard to learn. Based on this classifier-centric\nunderstanding, we propose a generic postprocessing step built upon\nflow-matching to shrink the gap between the learned distribution for a\npre-trained denoising diffusion model and the real data distribution, majorly\naround the decision boundaries. Experiments on various datasets verify the\neffectiveness of the proposed approach.",
            "upvotes": 2,
            "discussionId": "67d47015fa4c075001399a2f",
            "ai_keywords": [
                "classifier-free guidance",
                "denoising diffusion models",
                "classifier guidance",
                "decision boundaries",
                "conditional generation",
                "flow-matching",
                "learned distribution",
                "real data distribution"
            ]
        },
        "publishedAt": "2025-03-13T13:59:59.000Z",
        "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric\n  Perspective",
        "summary": "Classifier-free guidance has become a staple for conditional generation with\ndenoising diffusion models. However, a comprehensive understanding of\nclassifier-free guidance is still missing. In this work, we carry out an\nempirical study to provide a fresh perspective on classifier-free guidance.\nConcretely, instead of solely focusing on classifier-free guidance, we trace\nback to the root, i.e., classifier guidance, pinpoint the key assumption for\nthe derivation, and conduct a systematic study to understand the role of the\nclassifier. We find that both classifier guidance and classifier-free guidance\nachieve conditional generation by pushing the denoising diffusion trajectories\naway from decision boundaries, i.e., areas where conditional information is\nusually entangled and is hard to learn. Based on this classifier-centric\nunderstanding, we propose a generic postprocessing step built upon\nflow-matching to shrink the gap between the learned distribution for a\npre-trained denoising diffusion model and the real data distribution, majorly\naround the decision boundaries. Experiments on various datasets verify the\neffectiveness of the proposed approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10638.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642006ed290342c5df8158cc",
            "avatarUrl": "/avatars/408d35ff2512eb93ed6570befa33aa03.svg",
            "fullname": "Xiaoming Zhao",
            "name": "xzhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10242",
            "authors": [
                {
                    "_id": "67d3dd208d3b0ca8d5ca7f69",
                    "name": "Shaun Khoo",
                    "hidden": false
                },
                {
                    "_id": "67d3dd208d3b0ca8d5ca7f6a",
                    "user": {
                        "_id": "655f4ff710e5c5fbef30fd97",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
                        "isPro": true,
                        "fullname": "Gabriel C",
                        "user": "gabrielchua",
                        "type": "user"
                    },
                    "name": "Gabriel Chua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:55:54.401Z",
                    "hidden": false
                },
                {
                    "_id": "67d3dd208d3b0ca8d5ca7f6b",
                    "name": "Rachel Shong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T10:34:43.000Z",
            "submittedOnDailyAt": "2025-03-14T12:42:30.272Z",
            "title": "MinorBench: A hand-built benchmark for content-based risks for children",
            "submittedOnDailyBy": {
                "_id": "655f4ff710e5c5fbef30fd97",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
                "isPro": true,
                "fullname": "Gabriel C",
                "user": "gabrielchua",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.",
            "upvotes": 2,
            "discussionId": "67d3dd208d3b0ca8d5ca7fa4",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "chatbot",
                "content-based risks",
                "minor-focused safety mechanisms",
                "MinorBench",
                "open-source benchmark",
                "child-safety compliance"
            ]
        },
        "publishedAt": "2025-03-13T06:34:43.000Z",
        "title": "MinorBench: A hand-built benchmark for content-based risks for children",
        "summary": "Large Language Models (LLMs) are rapidly entering children's lives - through\nparent-driven adoption, schools, and peer networks - yet current AI ethics and\nsafety research do not adequately address content-related risks specific to\nminors. In this paper, we highlight these gaps with a real-world case study of\nan LLM-based chatbot deployed in a middle school setting, revealing how\nstudents used and sometimes misused the system. Building on these findings, we\npropose a new taxonomy of content-based risks for minors and introduce\nMinorBench, an open-source benchmark designed to evaluate LLMs on their ability\nto refuse unsafe or inappropriate queries from children. We evaluate six\nprominent LLMs under different system prompts, demonstrating substantial\nvariability in their child-safety compliance. Our results inform practical\nsteps for more robust, child-focused safety mechanisms and underscore the\nurgency of tailoring AI systems to safeguard young users.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10242.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655f4ff710e5c5fbef30fd97",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
            "fullname": "Gabriel C",
            "name": "gabrielchua",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 92
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10072",
            "authors": [
                {
                    "_id": "67d390de29a092bdbb0a2aeb",
                    "user": {
                        "_id": "6331c3f618711776b468e9ec",
                        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
                        "isPro": false,
                        "fullname": "Mia Mohammad Imran",
                        "user": "imranraad",
                        "type": "user"
                    },
                    "name": "Mia Mohammad Imran",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-14T02:14:33.331Z",
                    "hidden": false
                },
                {
                    "_id": "67d390de29a092bdbb0a2aec",
                    "name": "Jaydeb Sarker",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T05:39:29.000Z",
            "submittedOnDailyAt": "2025-03-14T00:44:11.345Z",
            "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
            "submittedOnDailyBy": {
                "_id": "6331c3f618711776b468e9ec",
                "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
                "isPro": false,
                "fullname": "Mia Mohammad Imran",
                "user": "imranraad",
                "type": "user"
            },
            "summary": "Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.",
            "upvotes": 2,
            "discussionId": "67d390df29a092bdbb0a2b2d",
            "projectPage": "https://zenodo.org/records/15015619"
        },
        "publishedAt": "2025-03-13T01:39:29.000Z",
        "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
        "summary": "Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10072.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "fullname": "Mia Mohammad Imran",
            "name": "imranraad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09368",
            "authors": [
                {
                    "_id": "67d2ca4be4696fda20bac029",
                    "user": {
                        "_id": "656c8721e8bf55919a9732c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
                        "isPro": false,
                        "fullname": "Nikolai",
                        "user": "Nikolai10",
                        "type": "user"
                    },
                    "name": "Nikolai Krber",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:57:39.634Z",
                    "hidden": false
                },
                {
                    "_id": "67d2ca4be4696fda20bac02a",
                    "user": {
                        "_id": "62f7f00647d782a6e28661df",
                        "avatarUrl": "/avatars/d2e5313995e14109c1dedc3a67d67707.svg",
                        "isPro": false,
                        "fullname": "Eduard Kromer",
                        "user": "edukrom",
                        "type": "user"
                    },
                    "name": "Eduard Kromer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:15:08.897Z",
                    "hidden": false
                },
                {
                    "_id": "67d2ca4be4696fda20bac02b",
                    "user": {
                        "_id": "64e48bd69ec28a2b5a016fbd",
                        "avatarUrl": "/avatars/c430012488f806579d4b5a4eab9c55b5.svg",
                        "isPro": false,
                        "fullname": "Andreas Siebert",
                        "user": "HerrSiebert",
                        "type": "user"
                    },
                    "name": "Andreas Siebert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-14T11:15:28.060Z",
                    "hidden": false
                },
                {
                    "_id": "67d2ca4be4696fda20bac02c",
                    "name": "Sascha Hauke",
                    "hidden": false
                },
                {
                    "_id": "67d2ca4be4696fda20bac02d",
                    "name": "Daniel Mueller-Gritschneder",
                    "hidden": false
                },
                {
                    "_id": "67d2ca4be4696fda20bac02e",
                    "name": "Bjrn Schuller",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T13:14:51.000Z",
            "submittedOnDailyAt": "2025-03-14T07:51:16.414Z",
            "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
            "submittedOnDailyBy": {
                "_id": "656c8721e8bf55919a9732c5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
                "isPro": false,
                "fullname": "Nikolai",
                "user": "Nikolai10",
                "type": "user"
            },
            "summary": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image\ncompression system designed for bandwidth- and storage-constrained\napplications. Building upon prior work by Careil et al., PerCoV2 extends the\noriginal formulation to the Stable Diffusion 3 ecosystem and enhances entropy\ncoding efficiency by explicitly modeling the discrete hyper-latent image\ndistribution. To this end, we conduct a comprehensive comparison of recent\nautoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our\napproach on the large-scale MSCOCO-30k benchmark. Compared to previous work,\nPerCoV2 (i) achieves higher image fidelity at even lower bit-rates while\nmaintaining competitive perceptual quality, (ii) features a hybrid generation\nmode for further bit-rate savings, and (iii) is built solely on public\ncomponents. Code and trained models will be released at\nhttps://github.com/Nikolai10/PerCoV2.",
            "upvotes": 1,
            "discussionId": "67d2ca50e4696fda20bac1a8",
            "githubRepo": "https://github.com/Nikolai10/PerCoV2",
            "ai_keywords": [
                "PerCoV2",
                "ultralow bit-rate",
                "perceptual image compression",
                "bandwidth-constrained applications",
                "Stable Diffusion 3",
                "entropy coding",
                "discrete hyper-latent image distribution",
                "autoregressive methods",
                "VAR",
                "MaskGIT",
                "MSCOCO-30k",
                "image fidelity",
                "perceptual quality",
                "hybrid generation mode",
                "public components"
            ]
        },
        "publishedAt": "2025-03-12T09:14:51.000Z",
        "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
        "summary": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image\ncompression system designed for bandwidth- and storage-constrained\napplications. Building upon prior work by Careil et al., PerCoV2 extends the\noriginal formulation to the Stable Diffusion 3 ecosystem and enhances entropy\ncoding efficiency by explicitly modeling the discrete hyper-latent image\ndistribution. To this end, we conduct a comprehensive comparison of recent\nautoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our\napproach on the large-scale MSCOCO-30k benchmark. Compared to previous work,\nPerCoV2 (i) achieves higher image fidelity at even lower bit-rates while\nmaintaining competitive perceptual quality, (ii) features a hybrid generation\nmode for further bit-rate savings, and (iii) is built solely on public\ncomponents. Code and trained models will be released at\nhttps://github.com/Nikolai10/PerCoV2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09368.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656c8721e8bf55919a9732c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
            "fullname": "Nikolai",
            "name": "Nikolai10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07111",
            "authors": [
                {
                    "_id": "67d4d78897767f49253a0739",
                    "name": "Alan Dao",
                    "hidden": false
                },
                {
                    "_id": "67d4d78897767f49253a073a",
                    "name": "Dinh Bach Vu",
                    "hidden": false
                },
                {
                    "_id": "67d4d78897767f49253a073b",
                    "name": "Tuan Le Duc Anh",
                    "hidden": false
                },
                {
                    "_id": "67d4d78897767f49253a073c",
                    "name": "Bui Quang Huy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T09:34:05.000Z",
            "submittedOnDailyAt": "2025-03-14T23:58:29.185Z",
            "title": "PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping\n  with VLM",
            "submittedOnDailyBy": {
                "_id": "62d7b2339b629105a5d6888a",
                "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                "isPro": false,
                "fullname": "Alan Dao",
                "user": "alandao",
                "type": "user"
            },
            "summary": "This paper introduces PoseLess, a novel framework for robot hand control that\neliminates the need for explicit pose estimation by directly mapping 2D images\nto joint angles using projected representations. Our approach leverages\nsynthetic training data generated through randomized joint configurations,\nenabling zero-shot generalization to real-world scenarios and cross-morphology\ntransfer from robotic to human hands. By projecting visual inputs and employing\na transformer-based decoder, PoseLess achieves robust, low-latency control\nwhile addressing challenges such as depth ambiguity and data scarcity.\nExperimental results demonstrate competitive performance in joint angle\nprediction accuracy without relying on any human-labelled dataset.",
            "upvotes": 1,
            "discussionId": "67d4d78997767f49253a0776",
            "githubRepo": "https://github.com/janhq/poseless",
            "ai_keywords": [
                "transformer-based decoder"
            ]
        },
        "publishedAt": "2025-03-10T05:34:05.000Z",
        "title": "PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping\n  with VLM",
        "summary": "This paper introduces PoseLess, a novel framework for robot hand control that\neliminates the need for explicit pose estimation by directly mapping 2D images\nto joint angles using projected representations. Our approach leverages\nsynthetic training data generated through randomized joint configurations,\nenabling zero-shot generalization to real-world scenarios and cross-morphology\ntransfer from robotic to human hands. By projecting visual inputs and employing\na transformer-based decoder, PoseLess achieves robust, low-latency control\nwhile addressing challenges such as depth ambiguity and data scarcity.\nExperimental results demonstrate competitive performance in joint angle\nprediction accuracy without relying on any human-labelled dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07111.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "fullname": "Alan Dao",
            "name": "alandao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10635",
            "authors": [
                {
                    "_id": "67d433de901d19119c275232",
                    "name": "Zhaoyi Li",
                    "hidden": false
                },
                {
                    "_id": "67d433de901d19119c275233",
                    "name": "Xiaohan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d433de901d19119c275234",
                    "name": "Dong-Dong Wu",
                    "hidden": false
                },
                {
                    "_id": "67d433de901d19119c275235",
                    "name": "Jiacheng Cui",
                    "hidden": false
                },
                {
                    "_id": "67d433de901d19119c275236",
                    "name": "Zhiqiang Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:55.000Z",
            "submittedOnDailyAt": "2025-03-14T19:12:38.236Z",
            "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
            "submittedOnDailyBy": {
                "_id": "649d5a3f4e08d59d20014652",
                "avatarUrl": "/avatars/017eccd6aec02137c1beebfe4c951720.svg",
                "isPro": false,
                "fullname": "Zhiqiang Shen",
                "user": "Jason0214",
                "type": "user"
            },
            "summary": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
            "upvotes": 0,
            "discussionId": "67d433e1901d19119c27534b",
            "ai_keywords": [
                "adversarial perturbations",
                "semantic details",
                "semantic information",
                "embedding space",
                "semantic clarity",
                "local regions",
                "semantically rich areas",
                "adversarial image",
                "optimization step",
                "local-aggregated perturbations",
                "commercial LVLMs",
                "GPT-4.5",
                "GPT-4o",
                "Gemini-2.0-flash",
                "Claude-3.5-sonnet",
                "Claude-3.7-sonnet",
                "reasoning models",
                "o1",
                "Claude-3.7-thinking",
                "Gemini-2.0-flash-thinking",
                "adversarial examples",
                "transferability"
            ]
        },
        "publishedAt": "2025-03-13T13:59:55.000Z",
        "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
        "summary": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10635.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649d5a3f4e08d59d20014652",
            "avatarUrl": "/avatars/017eccd6aec02137c1beebfe4c951720.svg",
            "fullname": "Zhiqiang Shen",
            "name": "Jason0214",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10602",
            "authors": [
                {
                    "_id": "67d42fbcf00a232c91a7a6c2",
                    "name": "Jinhao Duan",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c3",
                    "name": "Fei Kong",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c4",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c5",
                    "name": "James Diffenderfer",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c6",
                    "name": "Bhavya Kailkhura",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c7",
                    "name": "Lichao Sun",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c8",
                    "name": "Xiaofeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6c9",
                    "name": "Xiaoshuang Shi",
                    "hidden": false
                },
                {
                    "_id": "67d42fbcf00a232c91a7a6ca",
                    "name": "Kaidi Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:46:06.000Z",
            "submittedOnDailyAt": "2025-03-14T12:03:53.356Z",
            "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
            "submittedOnDailyBy": {
                "_id": "6423855c2d5965e7229ebe57",
                "avatarUrl": "/avatars/babb01f5f2329cc95e13e33b1ec5cf25.svg",
                "isPro": false,
                "fullname": "jinhaoduan",
                "user": "jhao",
                "type": "user"
            },
            "summary": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.",
            "upvotes": 0,
            "discussionId": "67d42fbef00a232c91a7a755",
            "ai_keywords": [
                "Object Hallucination (OH)",
                "Large Vision-Language Models (LVLMs)",
                "Large Language Models (LLMs)",
                "hidden states",
                "per-token",
                "hallucination indicators",
                "latent subspaces",
                "generic truthful directions",
                "Truthful-Guided Pre-Intervention (TruthPrInt)",
                "truthful direction",
                "inference-time intervention",
                "ComnHallu",
                "hallucination latent subspaces",
                "in-domain",
                "out-of-domain",
                "OH benchmarks"
            ]
        },
        "publishedAt": "2025-03-13T13:46:06.000Z",
        "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
        "summary": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10602.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6423855c2d5965e7229ebe57",
            "avatarUrl": "/avatars/babb01f5f2329cc95e13e33b1ec5cf25.svg",
            "fullname": "jinhaoduan",
            "name": "jhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]