[
    {
        "paper": {
            "id": "2503.11647",
            "authors": [
                {
                    "_id": "67d785fa473d4edd330edee1",
                    "user": {
                        "_id": "6530bf50f145530101ec03a2",
                        "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
                        "isPro": false,
                        "fullname": "Jianhong Bai",
                        "user": "jianhongbai",
                        "type": "user"
                    },
                    "name": "Jianhong Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:22.245Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee2",
                    "user": {
                        "_id": "63401c89f81b9d101361f712",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665146415483-63401c89f81b9d101361f712.png",
                        "isPro": false,
                        "fullname": "Richard",
                        "user": "menghanxia",
                        "type": "user"
                    },
                    "name": "Menghan Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:41.792Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee3",
                    "name": "Xiao Fu",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee4",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:51.145Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee5",
                    "user": {
                        "_id": "6672dd6d239ba86f129c5384",
                        "avatarUrl": "/avatars/6209afb551995b12d5e0d4d95e495694.svg",
                        "isPro": false,
                        "fullname": "Lianrui Mu",
                        "user": "Mu437",
                        "type": "user"
                    },
                    "name": "Lianrui Mu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:58.483Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee6",
                    "name": "Jinwen Cao",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee7",
                    "user": {
                        "_id": "6458b8d0990172cd1d703715",
                        "avatarUrl": "/avatars/55f0695e3cb9933c3903fde5a8f740d5.svg",
                        "isPro": false,
                        "fullname": "Zuozhu Liu",
                        "user": "Zuozhu",
                        "type": "user"
                    },
                    "name": "Zuozhu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:17.243Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee8",
                    "user": {
                        "_id": "66c46129d67297a9b93e03c5",
                        "avatarUrl": "/avatars/cffd8b07fa3655e240efc8e81f99d97d.svg",
                        "isPro": false,
                        "fullname": "Haoji Hu",
                        "user": "garland1979",
                        "type": "user"
                    },
                    "name": "Haoji Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:23.846Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edee9",
                    "user": {
                        "_id": "641790e2f1e86908935d82a0",
                        "avatarUrl": "/avatars/ced7a137c6344c74b7ac0d5c84833fc8.svg",
                        "isPro": false,
                        "fullname": "Xiang Bai",
                        "user": "baixianger",
                        "type": "user"
                    },
                    "name": "Xiang Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:29.804Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edeea",
                    "user": {
                        "_id": "662f93942510ef5735d7ad00",
                        "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
                        "isPro": false,
                        "fullname": "magicwpf",
                        "user": "magicwpf",
                        "type": "user"
                    },
                    "name": "Pengfei Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:32.134Z",
                    "hidden": false
                },
                {
                    "_id": "67d785fa473d4edd330edeeb",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:48:49.559Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
            ],
            "publishedAt": "2025-03-14T17:59:31.000Z",
            "submittedOnDailyAt": "2025-03-17T00:50:10.251Z",
            "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
            "submittedOnDailyBy": {
                "_id": "6530bf50f145530101ec03a2",
                "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
                "isPro": false,
                "fullname": "Jianhong Bai",
                "user": "jianhongbai",
                "type": "user"
            },
            "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
            "upvotes": 82,
            "discussionId": "67d785fb473d4edd330edf77",
            "ai_keywords": [
                "ReCamMaster",
                "text-to-video models",
                "video conditioning mechanism",
                "multi-camera synchronized video dataset",
                "Unreal Engine 5",
                "video stabilization",
                "super-resolution",
                "outpainting"
            ]
        },
        "publishedAt": "2025-03-14T13:59:31.000Z",
        "title": "ReCamMaster: Camera-Controlled Generative Rendering from A Single Video",
        "summary": "Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/RzQL-WqDDCxBy_j4rJuMl.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11647.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6530bf50f145530101ec03a2",
            "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg",
            "fullname": "Jianhong Bai",
            "name": "jianhongbai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07677",
            "authors": [
                {
                    "_id": "67d2ca0767366130cccad93d",
                    "user": {
                        "_id": "63973ee44e7b4959dc98028f",
                        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
                        "isPro": false,
                        "fullname": "Kwanyoung",
                        "user": "kwanyoung",
                        "type": "user"
                    },
                    "name": "Kwanyoung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:58:03.528Z",
                    "hidden": false
                },
                {
                    "_id": "67d2ca0767366130cccad93e",
                    "user": {
                        "_id": "668377232d89090894bea7b4",
                        "avatarUrl": "/avatars/1a74a08d645a352db4a460036b9fb6db.svg",
                        "isPro": false,
                        "fullname": "byeongsu sim",
                        "user": "byeongsus",
                        "type": "user"
                    },
                    "name": "Byeongsu Sim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:47:11.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T07:23:19.000Z",
            "submittedOnDailyAt": "2025-03-17T00:44:05.364Z",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
            "submittedOnDailyBy": {
                "_id": "63973ee44e7b4959dc98028f",
                "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
                "isPro": false,
                "fullname": "Kwanyoung",
                "user": "kwanyoung",
                "type": "user"
            },
            "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
            "upvotes": 68,
            "discussionId": "67d2ca0b67366130cccada34",
            "ai_keywords": [
                "diffusion models",
                "Classifier-Free Guidance (CFG)",
                "neural function evaluations (NFEs)",
                "guidance-distilled models",
                "PLADIS",
                "pre-trained models (U-Net/Transformer)",
                "sparse attention",
                "query-key correlations",
                "softmax",
                "cross-attention layer",
                "noise robustness",
                "text-to-image diffusion models",
                "text alignment",
                "human preference"
            ]
        },
        "publishedAt": "2025-03-10T03:23:19.000Z",
        "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity",
        "summary": "Diffusion models have shown impressive results in generating high-quality\nconditional samples using guidance techniques such as Classifier-Free Guidance\n(CFG). However, existing methods often require additional training or neural\nfunction evaluations (NFEs), making them incompatible with guidance-distilled\nmodels. Also, they rely on heuristic approaches that need identifying target\nlayers. In this work, we propose a novel and efficient method, termed PLADIS,\nwhich boosts pre-trained models (U-Net/Transformer) by leveraging sparse\nattention. Specifically, we extrapolate query-key correlations using softmax\nand its sparse counterpart in the cross-attention layer during inference,\nwithout requiring extra training or NFEs. By leveraging the noise robustness of\nsparse attention, our PLADIS unleashes the latent potential of text-to-image\ndiffusion models, enabling them to excel in areas where they once struggled\nwith newfound effectiveness. It integrates seamlessly with guidance techniques,\nincluding guidance-distilled models. Extensive experiments show notable\nimprovements in text alignment and human preference, offering a highly\nefficient and universally applicable solution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63973ee44e7b4959dc98028f",
            "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
            "fullname": "Kwanyoung",
            "name": "kwanyoung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11646",
            "authors": [
                {
                    "_id": "67d78c194fd0e3fa3a082f8d",
                    "user": {
                        "_id": "634e4120038b5879133552f5",
                        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                        "isPro": true,
                        "fullname": "Siyuan",
                        "user": "SiyuanH",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:21.620Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f8e",
                    "user": {
                        "_id": "670f827bb94a3734d270f707",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/D6qCPBMJAUgozfG7YTwky.png",
                        "isPro": false,
                        "fullname": "Yue Liao",
                        "user": "morninghaze",
                        "type": "user"
                    },
                    "name": "Yue Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:49:27.927Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f8f",
                    "user": {
                        "_id": "620326e962b2b0e46e79971b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620326e962b2b0e46e79971b/1FVPRpsWng5q3An4qbuYQ.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Feng",
                        "user": "Eralien",
                        "type": "user"
                    },
                    "name": "Siyuan Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:19.837Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f90",
                    "user": {
                        "_id": "666c463389e21df7d4a34d03",
                        "avatarUrl": "/avatars/8f73e78d740ace263961de1a2896fc09.svg",
                        "isPro": false,
                        "fullname": "姜姝",
                        "user": "jiang12345",
                        "type": "user"
                    },
                    "name": "Shu Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:21.098Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f91",
                    "name": "Si Liu",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f92",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:49:51.674Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f93",
                    "user": {
                        "_id": "67739bfa64e8b7438ae68eb4",
                        "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
                        "isPro": false,
                        "fullname": "Maoqing Yao",
                        "user": "AutobotZero",
                        "type": "user"
                    },
                    "name": "Maoqing Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:49:59.798Z",
                    "hidden": false
                },
                {
                    "_id": "67d78c194fd0e3fa3a082f94",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:05.432Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
            ],
            "publishedAt": "2025-03-14T17:59:07.000Z",
            "submittedOnDailyAt": "2025-03-17T01:30:24.394Z",
            "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
            "submittedOnDailyBy": {
                "_id": "634e4120038b5879133552f5",
                "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                "isPro": true,
                "fullname": "Siyuan",
                "user": "SiyuanH",
                "type": "user"
            },
            "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
            "upvotes": 31,
            "discussionId": "67d78c1b4fd0e3fa3a08301c",
            "projectPage": " https://sites.google.com/view/adc-robot",
            "ai_keywords": [
                "Adversarial Data Collection",
                "Human-in-the-Loop (HiL)",
                "real-time, bidirectional human-environment interactions",
                "collaborative perturbation paradigm",
                "adversarial operator",
                "tele-operator",
                "compositional generalization",
                "perceptual perturbations",
                "error recovery capabilities",
                "ADC-trained models",
                "ADC-Robotics dataset",
                "robotic imitation learning"
            ]
        },
        "publishedAt": "2025-03-14T13:59:07.000Z",
        "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
        "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/2Cb7g14KRbbgg6yotocsP.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11646.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634e4120038b5879133552f5",
            "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
            "fullname": "Siyuan",
            "name": "SiyuanH",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11224",
            "authors": [
                {
                    "_id": "67d788b6ba098a0651e1e235",
                    "user": {
                        "_id": "663f07d029be04778ba97871",
                        "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
                        "isPro": false,
                        "fullname": "Xingtai Lv",
                        "user": "XingtaiHF",
                        "type": "user"
                    },
                    "name": "Xingtai Lv",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:34.410Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e236",
                    "user": {
                        "_id": "679ce8c048ebd7903d76a832",
                        "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
                        "isPro": false,
                        "fullname": "Youbang Sun",
                        "user": "Youbang",
                        "type": "user"
                    },
                    "name": "Youbang Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:17.568Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e237",
                    "user": {
                        "_id": "60bc94cd85a3ab33829b6211",
                        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "iseesaw",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:26.057Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e238",
                    "user": {
                        "_id": "65597738deee83130a1301d5",
                        "avatarUrl": "/avatars/9bcc40aebe4db079927675d95c00463c.svg",
                        "isPro": false,
                        "fullname": "Shang (Lindsay) Qu",
                        "user": "lindsay-qu",
                        "type": "user"
                    },
                    "name": "Shang Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:23.487Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e239",
                    "user": {
                        "_id": "647ffddeb82adfa7cc1a10d9",
                        "avatarUrl": "/avatars/26aa168d6b2068298ebb16584aa52b6c.svg",
                        "isPro": false,
                        "fullname": "zhu",
                        "user": "xuekai",
                        "type": "user"
                    },
                    "name": "Xuekai Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:38.118Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23a",
                    "user": {
                        "_id": "672c2d7816766a76a747b7b5",
                        "avatarUrl": "/avatars/12c7b26d2b81721ccac3a5c71e32a1a1.svg",
                        "isPro": false,
                        "fullname": "Yuchen Fan",
                        "user": "yuchenFan",
                        "type": "user"
                    },
                    "name": "Yuchen Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:50:54.445Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23b",
                    "name": "Yi Wu",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23c",
                    "user": {
                        "_id": "6445fa2ffc22e309d78bef3e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
                        "isPro": false,
                        "fullname": "Messi Hua",
                        "user": "Messi-Hua",
                        "type": "user"
                    },
                    "name": "Ermo Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:30.639Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23d",
                    "user": {
                        "_id": "667e577139b49eba118d569f",
                        "avatarUrl": "/avatars/1a26dd96b4b352b8968561750ecae9a7.svg",
                        "isPro": false,
                        "fullname": "Xinwei Long",
                        "user": "xinwei666",
                        "type": "user"
                    },
                    "name": "Xinwei Long",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:02.068Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23e",
                    "user": {
                        "_id": "677b80e31ad30ab2c798e776",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/X8IFnIK3TDHOGKZCzLTe8.jpeg",
                        "isPro": false,
                        "fullname": "Ning Ding",
                        "user": "BradPitt2025",
                        "type": "user"
                    },
                    "name": "Ning Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:08.621Z",
                    "hidden": false
                },
                {
                    "_id": "67d788b6ba098a0651e1e23f",
                    "user": {
                        "_id": "669f614b59adf5b56e05bce3",
                        "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
                        "isPro": false,
                        "fullname": "BowenZhou",
                        "user": "bowenZhou",
                        "type": "user"
                    },
                    "name": "Bowen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:15.825Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T09:20:31.000Z",
            "submittedOnDailyAt": "2025-03-17T01:26:02.931Z",
            "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
            "submittedOnDailyBy": {
                "_id": "6445fa2ffc22e309d78bef3e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
                "isPro": false,
                "fullname": "Messi Hua",
                "user": "Messi-Hua",
                "type": "user"
            },
            "summary": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
            "upvotes": 22,
            "discussionId": "67d788b7ba098a0651e1e2a4",
            "ai_keywords": [
                "State Space Models (SSMs)",
                "transformer-based models",
                "sequential data",
                "theoretical motivations",
                "mathematical formulations",
                "comparison",
                "model classes",
                "original SSM",
                "structured SSM",
                "S4",
                "selective SSM",
                "Mamba",
                "effectiveness",
                "efficiency"
            ]
        },
        "publishedAt": "2025-03-14T05:20:31.000Z",
        "title": "Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models",
        "summary": "State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11224.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6445fa2ffc22e309d78bef3e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fa2ffc22e309d78bef3e/FQaINLd0PjgY9EnK_APRk.jpeg",
            "fullname": "Messi Hua",
            "name": "Messi-Hua",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11069",
            "authors": [
                {
                    "_id": "67d785458678eaf139e3c594",
                    "user": {
                        "_id": "654dbac9938fbf1e696be8aa",
                        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                        "isPro": false,
                        "fullname": "Chaoyun Zhang",
                        "user": "vyokky",
                        "type": "user"
                    },
                    "name": "Chaoyun Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:30.401Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c595",
                    "user": {
                        "_id": "62c6df026a092eda1f1ab6e5",
                        "avatarUrl": "/avatars/d58fff1a157b189ce2617889ef5f6e2f.svg",
                        "isPro": false,
                        "fullname": "Shilin He",
                        "user": "shilhe",
                        "type": "user"
                    },
                    "name": "Shilin He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:37.539Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c596",
                    "user": {
                        "_id": "666933c97bf97e24f7b5266e",
                        "avatarUrl": "/avatars/283961b37d463a386b08ad33dacca0f4.svg",
                        "isPro": false,
                        "fullname": "Liqun Li",
                        "user": "liqul",
                        "type": "user"
                    },
                    "name": "Liqun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:51:57.886Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c597",
                    "user": {
                        "_id": "67481846f47628abdd8c4397",
                        "avatarUrl": "/avatars/b43f2988ac17bd2bb2369133934ce75d.svg",
                        "isPro": false,
                        "fullname": "Si Qin",
                        "user": "SiQin88",
                        "type": "user"
                    },
                    "name": "Si Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:52:05.644Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c598",
                    "name": "Yu Kang",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c599",
                    "user": {
                        "_id": "652fc9f39bc50a6c0e435224",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc9f39bc50a6c0e435224/70OBVDHHBsxG2giJ-E3_1.jpeg",
                        "isPro": false,
                        "fullname": "Lin Qingwei",
                        "user": "Eliblo1969",
                        "type": "user"
                    },
                    "name": "Qingwei Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:52:17.826Z",
                    "hidden": false
                },
                {
                    "_id": "67d785458678eaf139e3c59a",
                    "user": {
                        "_id": "66473d2c7abe6ad66e81a3dd",
                        "avatarUrl": "/avatars/82f40244806c06ffeaa1c4265e9725ea.svg",
                        "isPro": false,
                        "fullname": "ZHANGDONGMEI",
                        "user": "ZDM6426",
                        "type": "user"
                    },
                    "name": "Dongmei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:52:31.623Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T04:26:21.000Z",
            "submittedOnDailyAt": "2025-03-17T00:43:33.225Z",
            "title": "API Agents vs. GUI Agents: Divergence and Convergence",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": false,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
            "upvotes": 20,
            "discussionId": "67d785468678eaf139e3c5ee"
        },
        "publishedAt": "2025-03-14T00:26:21.000Z",
        "title": "API Agents vs. GUI Agents: Divergence and Convergence",
        "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11069.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "fullname": "Chaoyun Zhang",
            "name": "vyokky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11576",
            "authors": [
                {
                    "_id": "67d7d1c38a15934c10b1576d",
                    "name": "Ahmed Nassar",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b1576e",
                    "user": {
                        "_id": "65d66b494bbd0d92b641cdbb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
                        "isPro": false,
                        "fullname": "Andres Marafioti",
                        "user": "andito",
                        "type": "user"
                    },
                    "name": "Andres Marafioti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:18.190Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b1576f",
                    "user": {
                        "_id": "663e1254887b6f5645a0399f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663e1254887b6f5645a0399f/CXtmipcpwK3LMIQNeinVg.jpeg",
                        "isPro": false,
                        "fullname": "Matteo Omenetti",
                        "user": "MatteoOmenetti",
                        "type": "user"
                    },
                    "name": "Matteo Omenetti",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:05:38.055Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15770",
                    "user": {
                        "_id": "665a2d536a8e042a0bf9b7b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665a2d536a8e042a0bf9b7b0/wA8QmKPW1BSRWNJyx_st0.jpeg",
                        "isPro": false,
                        "fullname": "Maksym Lysak",
                        "user": "MaxMnemonic",
                        "type": "user"
                    },
                    "name": "Maksym Lysak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T16:32:51.359Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15771",
                    "user": {
                        "_id": "67938030aa5e507f71eb0e44",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67938030aa5e507f71eb0e44/U_7TxvZFuTeJ-Gtfatbzj.jpeg",
                        "isPro": false,
                        "fullname": "Nikos Livathinos",
                        "user": "nlivathinos",
                        "type": "user"
                    },
                    "name": "Nikolaos Livathinos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T16:32:49.368Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15772",
                    "user": {
                        "_id": "67aa428095116c7b8e4204d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dT638jwk8CTQ6tGJ0ullR.png",
                        "isPro": false,
                        "fullname": "Christoph Auer",
                        "user": "ChristophAuer",
                        "type": "user"
                    },
                    "name": "Christoph Auer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:05:12.752Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15773",
                    "user": {
                        "_id": "6628967d99b11a1df0b1f256",
                        "avatarUrl": "/avatars/22d5a465df2b839e0f804521c732318b.svg",
                        "isPro": false,
                        "fullname": "Lucas Morin",
                        "user": "LucasMorin",
                        "type": "user"
                    },
                    "name": "Lucas Morin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:03:18.614Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15774",
                    "user": {
                        "_id": "65ce0dbef3bbc55ca4d0528f",
                        "avatarUrl": "/avatars/ae19424457dbec49579fe5a4ae7177b0.svg",
                        "isPro": false,
                        "fullname": "Rafael Teixeira de Lima",
                        "user": "rtdl",
                        "type": "user"
                    },
                    "name": "Rafael Teixeira de Lima",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:03:11.221Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15775",
                    "name": "Yusik Kim",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15776",
                    "user": {
                        "_id": "65d4dab8be26e8e084d29b98",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d4dab8be26e8e084d29b98/67m-1xtaONgP3sJCw5_k1.jpeg",
                        "isPro": false,
                        "fullname": "Said Gurbuz",
                        "user": "Saidgurbuz",
                        "type": "user"
                    },
                    "name": "A. Said Gurbuz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:03:01.577Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15777",
                    "user": {
                        "_id": "63c64dd877caf00391004e20",
                        "avatarUrl": "/avatars/6350443da6405ea7beccf47a80d8710e.svg",
                        "isPro": false,
                        "fullname": "Michele Dolfi",
                        "user": "dolfim-ibm",
                        "type": "user"
                    },
                    "name": "Michele Dolfi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:02:52.883Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15778",
                    "user": {
                        "_id": "61ed0ff29539bc0a3bbc89f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ed0ff29539bc0a3bbc89f4/iYWK7GParA7Ke5F6q132W.jpeg",
                        "isPro": false,
                        "fullname": "Miquel Farré",
                        "user": "mfarre",
                        "type": "user"
                    },
                    "name": "Miquel Farré",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:05:05.893Z",
                    "hidden": false
                },
                {
                    "_id": "67d7d1c38a15934c10b15779",
                    "user": {
                        "_id": "63c6522d8fddd19431f48bcc",
                        "avatarUrl": "/avatars/e2ec9c76656b7520472718595f9dafe1.svg",
                        "isPro": false,
                        "fullname": "Peter W. J. Staar",
                        "user": "PeterStaar",
                        "type": "user"
                    },
                    "name": "Peter W. J. Staar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:01:33.085Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T16:44:14.000Z",
            "submittedOnDailyAt": "2025-03-17T11:36:15.143Z",
            "title": "SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion",
            "submittedOnDailyBy": {
                "_id": "65d66b494bbd0d92b641cdbb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
                "isPro": false,
                "fullname": "Andres Marafioti",
                "user": "andito",
                "type": "user"
            },
            "summary": "We introduce SmolDocling, an ultra-compact vision-language model targeting\nend-to-end document conversion. Our model comprehensively processes entire\npages by generating DocTags, a new universal markup format that captures all\npage elements in their full context with location. Unlike existing approaches\nthat rely on large foundational models, or ensemble solutions that rely on\nhandcrafted pipelines of multiple specialized models, SmolDocling offers an\nend-to-end conversion for accurately capturing content, structure and spatial\nlocation of document elements in a 256M parameters vision-language model.\nSmolDocling exhibits robust performance in correctly reproducing document\nfeatures such as code listings, tables, equations, charts, lists, and more\nacross a diverse range of document types including business documents, academic\npapers, technical reports, patents, and forms -- significantly extending beyond\nthe commonly observed focus on scientific papers. Additionally, we contribute\nnovel publicly sourced datasets for charts, tables, equations, and code\nrecognition. Experimental results demonstrate that SmolDocling competes with\nother Vision Language Models that are up to 27 times larger in size, while\nreducing computational requirements substantially. The model is currently\navailable, datasets will be publicly available soon.",
            "upvotes": 16,
            "discussionId": "67d7d1c68a15934c10b15841",
            "ai_keywords": [
                "ultra-compact vision-language model",
                "end-to-end document conversion",
                "DocTags",
                "universal markup format",
                "page elements",
                "full context",
                "location",
                "foundational models",
                "ensemble solutions",
                "handcrafted pipelines",
                "specialized models",
                "vision-language model",
                "parameters",
                "document features",
                "code listings",
                "tables",
                "equations",
                "charts",
                "lists",
                "document types",
                "business documents",
                "academic papers",
                "technical reports",
                "patents",
                "forms",
                "publicly sourced datasets",
                "charts",
                "tables",
                "equations",
                "code recognition",
                "Vision Language Models"
            ]
        },
        "publishedAt": "2025-03-14T12:44:14.000Z",
        "title": "SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion",
        "summary": "We introduce SmolDocling, an ultra-compact vision-language model targeting\nend-to-end document conversion. Our model comprehensively processes entire\npages by generating DocTags, a new universal markup format that captures all\npage elements in their full context with location. Unlike existing approaches\nthat rely on large foundational models, or ensemble solutions that rely on\nhandcrafted pipelines of multiple specialized models, SmolDocling offers an\nend-to-end conversion for accurately capturing content, structure and spatial\nlocation of document elements in a 256M parameters vision-language model.\nSmolDocling exhibits robust performance in correctly reproducing document\nfeatures such as code listings, tables, equations, charts, lists, and more\nacross a diverse range of document types including business documents, academic\npapers, technical reports, patents, and forms -- significantly extending beyond\nthe commonly observed focus on scientific papers. Additionally, we contribute\nnovel publicly sourced datasets for charts, tables, equations, and code\nrecognition. Experimental results demonstrate that SmolDocling competes with\nother Vision Language Models that are up to 27 times larger in size, while\nreducing computational requirements substantially. The model is currently\navailable, datasets will be publicly available soon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11576.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "fullname": "Andres Marafioti",
            "name": "andito",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10772",
            "authors": [
                {
                    "_id": "67d78ce0b4d0fefa68385d7f",
                    "user": {
                        "_id": "661c9059bcd78151e5c06ea1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
                        "isPro": false,
                        "fullname": "Ju He",
                        "user": "turkeyju",
                        "type": "user"
                    },
                    "name": "Ju He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:17.453Z",
                    "hidden": false
                },
                {
                    "_id": "67d78ce0b4d0fefa68385d80",
                    "user": {
                        "_id": "677b60e17279b5c57354108b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677b60e17279b5c57354108b/YOwDhVf9DkeRjOCOLErb6.png",
                        "isPro": false,
                        "fullname": "QihangYu",
                        "user": "QihangYu",
                        "type": "user"
                    },
                    "name": "Qihang Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:55:33.453Z",
                    "hidden": false
                },
                {
                    "_id": "67d78ce0b4d0fefa68385d81",
                    "user": {
                        "_id": "639f1e519f1f2baab2f00d22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639f1e519f1f2baab2f00d22/pFjd51WZuVZ3A11rItvmk.jpeg",
                        "isPro": true,
                        "fullname": "Qihao Liu",
                        "user": "QHL067",
                        "type": "user"
                    },
                    "name": "Qihao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:55:40.463Z",
                    "hidden": false
                },
                {
                    "_id": "67d78ce0b4d0fefa68385d82",
                    "name": "Liang-Chieh Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T18:06:13.000Z",
            "submittedOnDailyAt": "2025-03-17T01:16:42.853Z",
            "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
            "submittedOnDailyBy": {
                "_id": "661c9059bcd78151e5c06ea1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
                "isPro": false,
                "fullname": "Ju He",
                "user": "turkeyju",
                "type": "user"
            },
            "summary": "Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https://github.com/bytedance/1d-tokenizer.",
            "upvotes": 13,
            "discussionId": "67d78ce1b4d0fefa68385dc8",
            "projectPage": "https://tacju.github.io/projects/flowtok.html",
            "githubRepo": "https://github.com/bytedance/1d-tokenizer/",
            "ai_keywords": [
                "cross-modality generation",
                "flow matching",
                "latent space",
                "denoising process",
                "Gaussian noise",
                "semantic",
                "1D tokens",
                "2D latent embeddings",
                "FlowTok",
                "compact 1D token representation",
                "image-to-text generation",
                "memory-efficient",
                "sampling speeds",
                "state-of-the-art models"
            ]
        },
        "publishedAt": "2025-03-13T14:06:13.000Z",
        "title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
        "summary": "Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https://github.com/bytedance/1d-tokenizer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10772.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661c9059bcd78151e5c06ea1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png",
            "fullname": "Ju He",
            "name": "turkeyju",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11514",
            "authors": [
                {
                    "_id": "67d778325121a10e6fc650b3",
                    "user": {
                        "_id": "668f440894dfc0ed1a7006ed",
                        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
                        "isPro": false,
                        "fullname": "Pengxin Guo",
                        "user": "gpx333",
                        "type": "user"
                    },
                    "name": "Pengxin Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:42.855Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650b4",
                    "user": {
                        "_id": "65a28d30c0e637bd9cbddc15",
                        "avatarUrl": "/avatars/50be3e38617a51f7f8c22fa219a4d10a.svg",
                        "isPro": false,
                        "fullname": "Runxi Wang",
                        "user": "Rx-Wang",
                        "type": "user"
                    },
                    "name": "Runxi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:39.377Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650b5",
                    "user": {
                        "_id": "66712ff09c609c2484ce4aa0",
                        "avatarUrl": "/avatars/717b96ddef8a4c19ce07ea1fd9e9fd66.svg",
                        "isPro": false,
                        "fullname": "Shuang Zeng",
                        "user": "stevezs",
                        "type": "user"
                    },
                    "name": "Shuang Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:53:09.089Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650b6",
                    "user": {
                        "_id": "6479ea5effe1b559f5408453",
                        "avatarUrl": "/avatars/6077dcc62fbd41dac92ee33b3133ceec.svg",
                        "isPro": false,
                        "fullname": "Zhu",
                        "user": "Jinjing08",
                        "type": "user"
                    },
                    "name": "Jinjing Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:53:22.627Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650b7",
                    "user": {
                        "_id": "66ff619fe48de0216cd43531",
                        "avatarUrl": "/avatars/e4642e02b6475cfbd677c6e28640b5b0.svg",
                        "isPro": false,
                        "fullname": "HaoningJiang",
                        "user": "haoning666",
                        "type": "user"
                    },
                    "name": "Haoning Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:36.769Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650b8",
                    "user": {
                        "_id": "656f698c80ff527c44e3c33b",
                        "avatarUrl": "/avatars/19ea552ed0bb36260ab0f6e41421f9b3.svg",
                        "isPro": false,
                        "fullname": "Yanran Wang",
                        "user": "yanranw1",
                        "type": "user"
                    },
                    "name": "Yanran Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:53:29.620Z",
                    "hidden": true
                },
                {
                    "_id": "67d778325121a10e6fc650b9",
                    "user": {
                        "_id": "66c7fb4ce2c92fe5b132f314",
                        "avatarUrl": "/avatars/22d915fa339a70803c5c748255250256.svg",
                        "isPro": false,
                        "fullname": "Yuyin Zhou",
                        "user": "RitaCoding",
                        "type": "user"
                    },
                    "name": "Yuyin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:53:35.758Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650ba",
                    "user": {
                        "_id": "653d6970885338b011d283d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d6970885338b011d283d8/FNViRXsMdrhrjOurBVBSf.jpeg",
                        "isPro": false,
                        "fullname": "Feifei Wang",
                        "user": "feifeiwang",
                        "type": "user"
                    },
                    "name": "Feifei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:53:43.569Z",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650bb",
                    "name": "Hui Xiong",
                    "hidden": false
                },
                {
                    "_id": "67d778325121a10e6fc650bc",
                    "user": {
                        "_id": "663058bc2653ec94f4a6235f",
                        "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
                        "isPro": false,
                        "fullname": "Liangqiong Qu",
                        "user": "Liangqiong-QU",
                        "type": "user"
                    },
                    "name": "Liangqiong Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:53:51.160Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T08:08:44.000Z",
            "submittedOnDailyAt": "2025-03-17T00:38:48.278Z",
            "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
            "submittedOnDailyBy": {
                "_id": "668f440894dfc0ed1a7006ed",
                "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
                "isPro": false,
                "fullname": "Pengxin Guo",
                "user": "gpx333",
                "type": "user"
            },
            "summary": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\noptimization-based GIA (OP-GIA), generation-based GIA\n(GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks.",
            "upvotes": 13,
            "discussionId": "67d778395121a10e6fc652eb",
            "ai_keywords": [
                "Gradient Inversion Attacks (GIA)",
                "optimization-based GIA (OP-GIA)",
                "generation-based GIA (GEN-GIA)",
                "analytics-based GIA (ANA-GIA)"
            ]
        },
        "publishedAt": "2025-03-13T04:08:44.000Z",
        "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks",
        "summary": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\noptimization-based GIA (OP-GIA), generation-based GIA\n(GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11514.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668f440894dfc0ed1a7006ed",
            "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
            "fullname": "Pengxin Guo",
            "name": "gpx333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10781",
            "authors": [
                {
                    "_id": "67d78ff6f789a7b68993ab6b",
                    "user": {
                        "_id": "62f38b19261bc5fb2e06652c",
                        "avatarUrl": "/avatars/0a7f7d63e1096f5d52bcb3be8e236c87.svg",
                        "isPro": false,
                        "fullname": "Evangelos Kazakos",
                        "user": "ekazakos",
                        "type": "user"
                    },
                    "name": "Evangelos Kazakos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:14.132Z",
                    "hidden": false
                },
                {
                    "_id": "67d78ff6f789a7b68993ab6c",
                    "name": "Cordelia Schmid",
                    "hidden": false
                },
                {
                    "_id": "67d78ff6f789a7b68993ab6d",
                    "name": "Josef Sivic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T18:21:07.000Z",
            "submittedOnDailyAt": "2025-03-17T07:19:07.091Z",
            "title": "Large-scale Pre-training for Grounded Video Caption Generation",
            "submittedOnDailyBy": {
                "_id": "62f38b19261bc5fb2e06652c",
                "avatarUrl": "/avatars/0a7f7d63e1096f5d52bcb3be8e236c87.svg",
                "isPro": false,
                "fullname": "Evangelos Kazakos",
                "user": "ekazakos",
                "type": "user"
            },
            "summary": "We propose a novel approach for captioning and object grounding in video,\nwhere the objects in the caption are grounded in the video via temporally dense\nbounding boxes. We introduce the following contributions. First, we present a\nlarge-scale automatic annotation method that aggregates captions grounded with\nbounding boxes across individual frames into temporally dense and consistent\nbounding box annotations. We apply this approach on the HowTo100M dataset to\nconstruct a large-scale pre-training dataset, named HowToGround1M. We also\nintroduce a Grounded Video Caption Generation model, dubbed GROVE, and\npre-train the model on HowToGround1M. Second, we introduce a new dataset,\ncalled iGround, of 3500 videos with manually annotated captions and dense\nspatio-temporally grounded bounding boxes. This allows us to measure progress\non this challenging problem, as well as to fine-tune our model on this\nsmall-scale but high-quality data. Third, we demonstrate that our approach\nachieves state-of-the-art results on the proposed iGround dataset compared to a\nnumber of baselines, as well as on the VidSTG and ActivityNet-Entities\ndatasets. We perform extensive ablations that demonstrate the importance of\npre-training using our automatically annotated HowToGround1M dataset followed\nby fine-tuning on the manually annotated iGround dataset and validate the key\ntechnical contributions of our model.",
            "upvotes": 11,
            "discussionId": "67d78ffaf789a7b68993ac8f",
            "projectPage": "https://ekazakos.github.io/grounded_video_caption_generation/",
            "githubRepo": "https://github.com/ekazakos/grove",
            "ai_keywords": [
                "temporally dense bounding boxes",
                "automatic annotation",
                "pre-training dataset",
                "Grounded Video Caption Generation",
                "spatio-temporally grounded bounding boxes",
                "fine-tuning",
                "state-of-the-art results",
                "VidSTG",
                "ActivityNet-Entities",
                "ablations"
            ]
        },
        "publishedAt": "2025-03-13T14:21:07.000Z",
        "title": "Large-scale Pre-training for Grounded Video Caption Generation",
        "summary": "We propose a novel approach for captioning and object grounding in video,\nwhere the objects in the caption are grounded in the video via temporally dense\nbounding boxes. We introduce the following contributions. First, we present a\nlarge-scale automatic annotation method that aggregates captions grounded with\nbounding boxes across individual frames into temporally dense and consistent\nbounding box annotations. We apply this approach on the HowTo100M dataset to\nconstruct a large-scale pre-training dataset, named HowToGround1M. We also\nintroduce a Grounded Video Caption Generation model, dubbed GROVE, and\npre-train the model on HowToGround1M. Second, we introduce a new dataset,\ncalled iGround, of 3500 videos with manually annotated captions and dense\nspatio-temporally grounded bounding boxes. This allows us to measure progress\non this challenging problem, as well as to fine-tune our model on this\nsmall-scale but high-quality data. Third, we demonstrate that our approach\nachieves state-of-the-art results on the proposed iGround dataset compared to a\nnumber of baselines, as well as on the VidSTG and ActivityNet-Entities\ndatasets. We perform extensive ablations that demonstrate the importance of\npre-training using our automatically annotated HowToGround1M dataset followed\nby fine-tuning on the manually annotated iGround dataset and validate the key\ntechnical contributions of our model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10781.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f38b19261bc5fb2e06652c",
            "avatarUrl": "/avatars/0a7f7d63e1096f5d52bcb3be8e236c87.svg",
            "fullname": "Evangelos Kazakos",
            "name": "ekazakos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11579",
            "authors": [
                {
                    "_id": "67d77743f918cc6703da2955",
                    "user": {
                        "_id": "64405a9d518271b0d1beea38",
                        "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
                        "isPro": false,
                        "fullname": "Weiming Ren",
                        "user": "wren93",
                        "type": "user"
                    },
                    "name": "Weiming Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:49:43.554Z",
                    "hidden": false
                },
                {
                    "_id": "67d77743f918cc6703da2956",
                    "user": {
                        "_id": "65c387c807a1445dfe1e9452",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c387c807a1445dfe1e9452/t0VnwQh2wRZ9W_UGTZ8zt.jpeg",
                        "isPro": false,
                        "fullname": "Wentao Ma",
                        "user": "tonymwt",
                        "type": "user"
                    },
                    "name": "Wentao Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T16:33:51.694Z",
                    "hidden": false
                },
                {
                    "_id": "67d77743f918cc6703da2957",
                    "name": "Huan Yang",
                    "hidden": false
                },
                {
                    "_id": "67d77743f918cc6703da2958",
                    "user": {
                        "_id": "6470cc0a7fd7ecdbd0ec1482",
                        "avatarUrl": "/avatars/1945ce9f933d199fc9ef17f12caebd2e.svg",
                        "isPro": false,
                        "fullname": "Cong Wei",
                        "user": "CongWei",
                        "type": "user"
                    },
                    "name": "Cong Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:50:04.030Z",
                    "hidden": false
                },
                {
                    "_id": "67d77743f918cc6703da2959",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:50:11.722Z",
                    "hidden": false
                },
                {
                    "_id": "67d77743f918cc6703da295a",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:50:18.571Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T16:45:23.000Z",
            "submittedOnDailyAt": "2025-03-17T11:14:16.941Z",
            "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640times360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks.",
            "upvotes": 10,
            "discussionId": "67d77747f918cc6703da2a4e",
            "ai_keywords": [
                "transformer-based large multimodal models (LMMs)",
                "causal self-attention",
                "token compression",
                "Mamba-Transformer model (VAMBA)",
                "Mamba-2 blocks",
                "linear complexity",
                "GPU memory usage",
                "LVBench",
                "video understanding",
                "hour-long video inputs"
            ]
        },
        "publishedAt": "2025-03-14T12:45:23.000Z",
        "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
        "summary": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640times360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11579.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 790
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10970",
            "authors": [
                {
                    "_id": "67d771335e9c4135a570f57f",
                    "user": {
                        "_id": "6350fc5ba8822aadf571304f",
                        "avatarUrl": "/avatars/19686add3cbdaef5772b913152333f9b.svg",
                        "isPro": false,
                        "fullname": "gasvn",
                        "user": "shgao",
                        "type": "user"
                    },
                    "name": "Shanghua Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:49.587Z",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f580",
                    "name": "Richard Zhu",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f581",
                    "name": "Zhenglun Kong",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f582",
                    "user": {
                        "_id": "643b2ce2c5f633a7fa82d507",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643b2ce2c5f633a7fa82d507/RFXzG5tiRqVYdF-bWbNl-.png",
                        "isPro": false,
                        "fullname": "Ayush",
                        "user": "ayushnoori",
                        "type": "user"
                    },
                    "name": "Ayush Noori",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:54:34.672Z",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f583",
                    "user": {
                        "_id": "660aef84362a1d713aea88ec",
                        "avatarUrl": "/avatars/7a16c54e1ee43d5366501d12e8087a7e.svg",
                        "isPro": false,
                        "fullname": "Xiaorui Su",
                        "user": "Blair1213",
                        "type": "user"
                    },
                    "name": "Xiaorui Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:54:42.342Z",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f584",
                    "name": "Curtis Ginder",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f585",
                    "name": "Theodoros Tsiligkaridis",
                    "hidden": false
                },
                {
                    "_id": "67d771335e9c4135a570f586",
                    "user": {
                        "_id": "636826f95bb06007ea0e911e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667770136112-636826f95bb06007ea0e911e.jpeg",
                        "isPro": false,
                        "fullname": "Marinka Zitnik",
                        "user": "marinkaz",
                        "type": "user"
                    },
                    "name": "Marinka Zitnik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:55:18.525Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T00:28:15.000Z",
            "submittedOnDailyAt": "2025-03-17T02:04:58.876Z",
            "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
            "upvotes": 10,
            "discussionId": "67d771345e9c4135a570f5d0",
            "ai_keywords": [
                "AI agent",
                "multi-step reasoning",
                "biomedical knowledge retrieval",
                "drug interactions",
                "contraindications",
                "patient-specific treatment strategies",
                "molecular levels",
                "pharmacokinetic levels",
                "clinical levels",
                "patient comorbidities",
                "concurrent medications",
                "ToolUniverse",
                "FDA-approved drugs",
                "Open Targets",
                "DrugPC",
                "BrandPC",
                "GenericPC",
                "TreatmentPC",
                "DescriptionPC",
                "drug reasoning tasks",
                "personalized treatment scenarios",
                "multi-step inference",
                "knowledge grounding",
                "tool-assisted decision-making",
                "clinical guidelines",
                "real-world evidence",
                "adverse events",
                "therapeutic decision-making"
            ]
        },
        "publishedAt": "2025-03-13T20:28:15.000Z",
        "title": "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
        "summary": "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10970.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6387
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11651",
            "authors": [
                {
                    "_id": "67d7f63e8c022f989bb5ecfe",
                    "user": {
                        "_id": "649bf403fd9cea8366d669ad",
                        "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
                        "isPro": false,
                        "fullname": "Jianyuan Wang",
                        "user": "JianyuanWang",
                        "type": "user"
                    },
                    "name": "Jianyuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:46:11.045Z",
                    "hidden": false
                },
                {
                    "_id": "67d7f63e8c022f989bb5ecff",
                    "user": {
                        "_id": "636a3d8bf8d9af4aea18553f",
                        "avatarUrl": "/avatars/028a86d088764fd66c36a2ddebf09f9a.svg",
                        "isPro": true,
                        "fullname": "MINGHAO CHEN",
                        "user": "silentchen",
                        "type": "user"
                    },
                    "name": "Minghao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:46:57.518Z",
                    "hidden": false
                },
                {
                    "_id": "67d7f63e8c022f989bb5ed00",
                    "user": {
                        "_id": "6393a73584c565d2c3416cb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6393a73584c565d2c3416cb9/OGtX-i-_MLg5--qA054ti.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Karaev",
                        "user": "nikkar",
                        "type": "user"
                    },
                    "name": "Nikita Karaev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:47:04.483Z",
                    "hidden": false
                },
                {
                    "_id": "67d7f63e8c022f989bb5ed01",
                    "name": "Andrea Vedaldi",
                    "hidden": false
                },
                {
                    "_id": "67d7f63e8c022f989bb5ed02",
                    "name": "Christian Rupprecht",
                    "hidden": false
                },
                {
                    "_id": "67d7f63e8c022f989bb5ed03",
                    "name": "David Novotny",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T17:59:47.000Z",
            "submittedOnDailyAt": "2025-03-17T10:42:25.637Z",
            "title": "VGGT: Visual Geometry Grounded Transformer",
            "submittedOnDailyBy": {
                "_id": "649bf403fd9cea8366d669ad",
                "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
                "isPro": false,
                "fullname": "Jianyuan Wang",
                "user": "JianyuanWang",
                "type": "user"
            },
            "summary": "We present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.",
            "upvotes": 9,
            "discussionId": "67d7f6418c022f989bb5edd1",
            "projectPage": "https://vgg-t.github.io/",
            "githubRepo": "https://github.com/facebookresearch/vggt",
            "ai_keywords": [
                "feed-forward neural network",
                "3D attributes",
                "camera parameters",
                "point maps",
                "depth maps",
                "3D point tracks",
                "3D computer vision",
                "single tasks",
                "image reconstruction",
                "state-of-the-art results",
                "camera parameter estimation",
                "multi-view depth estimation",
                "dense point cloud reconstruction",
                "3D point tracking",
                "pretrained VGGT",
                "feature backbone",
                "non-rigid point tracking",
                "feed-forward novel view synthesis"
            ]
        },
        "publishedAt": "2025-03-14T13:59:47.000Z",
        "title": "VGGT: Visual Geometry Grounded Transformer",
        "summary": "We present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11651.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649bf403fd9cea8366d669ad",
            "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
            "fullname": "Jianyuan Wang",
            "name": "JianyuanWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10632",
            "authors": [
                {
                    "_id": "67d4f1b1643653fd1cea5b5a",
                    "user": {
                        "_id": "66d5279130d7ea0b28d6d5d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
                        "isPro": false,
                        "fullname": "Subhajit Maity",
                        "user": "maitysubhajit",
                        "type": "user"
                    },
                    "name": "Subhajit Maity",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-15T03:19:40.103Z",
                    "hidden": false
                },
                {
                    "_id": "67d4f1b1643653fd1cea5b5b",
                    "name": "Killian Hitsman",
                    "hidden": false
                },
                {
                    "_id": "67d4f1b1643653fd1cea5b5c",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "67d4f1b1643653fd1cea5b5d",
                    "user": {
                        "_id": "67d58d156db0e6f0c33c0f60",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d58d156db0e6f0c33c0f60/9KiFw0iEMZQHcHnm1k0ED.jpeg",
                        "isPro": false,
                        "fullname": "Aritra Dutta",
                        "user": "aritradutta",
                        "type": "user"
                    },
                    "name": "Aritra Dutta",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-15T14:35:55.373Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:59:52.000Z",
            "submittedOnDailyAt": "2025-03-17T01:09:07.184Z",
            "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
            "submittedOnDailyBy": {
                "_id": "66d5279130d7ea0b28d6d5d2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
                "isPro": false,
                "fullname": "Subhajit Maity",
                "user": "maitysubhajit",
                "type": "user"
            },
            "summary": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
            "upvotes": 8,
            "discussionId": "67d4f1b6643653fd1cea5d20",
            "projectPage": "https://subhajitmaity.me/KArAt",
            "githubRepo": "https://github.com/MaitySubhajit/KArAt",
            "ai_keywords": [
                "Kolmogorov-Arnold networks (KANs)",
                "learnable activation functions",
                "multilayer perceptrons (MLPs)",
                "vision Transformers (ViTs)",
                "Kolmogorov-Arnold Attention (KArAt)",
                "Fourier-KArAt",
                "loss landscapes",
                "weight distributions",
                "optimizer path",
                "attention visualization",
                "spectral behavior"
            ]
        },
        "publishedAt": "2025-03-13T13:59:52.000Z",
        "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
        "summary": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10632.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d5279130d7ea0b28d6d5d2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d5279130d7ea0b28d6d5d2/oGFmux--lARBF4PKSkHAu.png",
            "fullname": "Subhajit Maity",
            "name": "maitysubhajit",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06542",
            "authors": [
                {
                    "_id": "67d7e4ec1414fcb6196e79ba",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79bb",
                    "user": {
                        "_id": "66d94f2a36aa5055694dfe04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/grAN83brH0E4_S0__yLdv.jpeg",
                        "isPro": false,
                        "fullname": "fengyukang",
                        "user": "finyorko",
                        "type": "user"
                    },
                    "name": "Yukang Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:45:01.498Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79bc",
                    "user": {
                        "_id": "6533f7ecb3852ed1ceb48e47",
                        "avatarUrl": "/avatars/5d767c093e73f06a89f625c3a5903902.svg",
                        "isPro": false,
                        "fullname": "Chuanhao Li",
                        "user": "cyrilli",
                        "type": "user"
                    },
                    "name": "Chuanhao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:45:07.745Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79bd",
                    "user": {
                        "_id": "665305eff0c8c891cae7fe01",
                        "avatarUrl": "/avatars/1f372e3bc6a4eb19ef702ec96a391c96.svg",
                        "isPro": false,
                        "fullname": "Fanrui Zhang",
                        "user": "fanrui00",
                        "type": "user"
                    },
                    "name": "Fanrui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:44:41.770Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79be",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79bf",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79c0",
                    "name": "Sizhuo Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79c1",
                    "name": "Yu Dai",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79c2",
                    "user": {
                        "_id": "6674d02914e2aebef893779e",
                        "avatarUrl": "/avatars/acdbe3820462b87126c8f1e14f0d1a60.svg",
                        "isPro": false,
                        "fullname": "ZhangShenglin",
                        "user": "ZhangShenglin",
                        "type": "user"
                    },
                    "name": "Shenglin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:45:56.321Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e4ec1414fcb6196e79c3",
                    "user": {
                        "_id": "65f1713552c38a91e0a445e8",
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:44:19.968Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T10:15:39.000Z",
            "submittedOnDailyAt": "2025-03-17T07:32:00.469Z",
            "title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Unified models (UniMs) for multimodal understanding and generation have\nrecently received much attention in the area of vision and language. Existing\nUniMs are designed to simultaneously learn both multimodal understanding and\ngeneration capabilities, demanding substantial computational resources, and\noften struggle to generate interleaved text-image. We present ARMOR, a\nresource-efficient and pure autoregressive framework that achieves both\nunderstanding and generation by fine-tuning existing multimodal large language\nmodels (MLLMs). Specifically, ARMOR extends existing MLLMs from three\nperspectives: (1) For model architecture, an asymmetric encoder-decoder\narchitecture with a forward-switching mechanism is introduced to unify\nembedding space integrating textual and visual modalities for enabling natural\ntext-image interleaved generation with minimal computational overhead. (2) For\ntraining data, a meticulously curated, high-quality interleaved dataset is\ncollected for fine-tuning MLLMs. (3) For the training algorithm, we propose a\n``what or how to generate\" algorithm to empower existing MLLMs with multimodal\ngeneration capabilities while preserving their multimodal understanding\ncapabilities, through three progressive training stages based on the collected\ndataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to\nUniMs with promising image generation capabilities, using limited training\nresources. Our code will be released soon at https://armor.github.io.",
            "upvotes": 7,
            "discussionId": "67d7e4ee1414fcb6196e7a43",
            "ai_keywords": [
                "asymmetric encoder-decoder architecture",
                "forward-switching mechanism",
                "embedding space",
                "textual modality",
                "visual modality",
                "natural text-image interleaved generation",
                "computational overhead",
                "high-quality interleaved dataset",
                "multimodal large language models",
                "``what or how to generate\" algorithm",
                "progressive training stages",
                "image generation capabilities",
                "multimodal understanding capabilities"
            ]
        },
        "publishedAt": "2025-03-09T06:15:39.000Z",
        "title": "ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy",
        "summary": "Unified models (UniMs) for multimodal understanding and generation have\nrecently received much attention in the area of vision and language. Existing\nUniMs are designed to simultaneously learn both multimodal understanding and\ngeneration capabilities, demanding substantial computational resources, and\noften struggle to generate interleaved text-image. We present ARMOR, a\nresource-efficient and pure autoregressive framework that achieves both\nunderstanding and generation by fine-tuning existing multimodal large language\nmodels (MLLMs). Specifically, ARMOR extends existing MLLMs from three\nperspectives: (1) For model architecture, an asymmetric encoder-decoder\narchitecture with a forward-switching mechanism is introduced to unify\nembedding space integrating textual and visual modalities for enabling natural\ntext-image interleaved generation with minimal computational overhead. (2) For\ntraining data, a meticulously curated, high-quality interleaved dataset is\ncollected for fine-tuning MLLMs. (3) For the training algorithm, we propose a\n``what or how to generate\" algorithm to empower existing MLLMs with multimodal\ngeneration capabilities while preserving their multimodal understanding\ncapabilities, through three progressive training stages based on the collected\ndataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to\nUniMs with promising image generation capabilities, using limited training\nresources. Our code will be released soon at https://armor.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06542.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09279",
            "authors": [
                {
                    "_id": "67d2bd340860f2d7ff10e3dc",
                    "user": {
                        "_id": "66a9b3533d417b0baa9220a6",
                        "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
                        "isPro": false,
                        "fullname": "Luozheng Qin",
                        "user": "Fr0zencr4nE",
                        "type": "user"
                    },
                    "name": "Luozheng Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:56:11.189Z",
                    "hidden": false
                },
                {
                    "_id": "67d2bd340860f2d7ff10e3dd",
                    "name": "Zhiyu Tan",
                    "hidden": false
                },
                {
                    "_id": "67d2bd340860f2d7ff10e3de",
                    "user": {
                        "_id": "6304d630dae2eb7d084148c7",
                        "avatarUrl": "/avatars/7d7a6ca99334bdae3ed1752ff40a8d94.svg",
                        "isPro": false,
                        "fullname": "mengping yang",
                        "user": "Kobeshegu",
                        "type": "user"
                    },
                    "name": "Mengping Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:56:25.200Z",
                    "hidden": false
                },
                {
                    "_id": "67d2bd340860f2d7ff10e3df",
                    "user": {
                        "_id": "658ea92268d0b7633176b4ed",
                        "avatarUrl": "/avatars/40173c9126dccfe78bc46b12c6ced8c8.svg",
                        "isPro": false,
                        "fullname": "xiaomeng yang",
                        "user": "xiaomengyang",
                        "type": "user"
                    },
                    "name": "Xiaomeng Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:56:33.077Z",
                    "hidden": false
                },
                {
                    "_id": "67d2bd340860f2d7ff10e3e0",
                    "name": "Hao Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T11:25:04.000Z",
            "submittedOnDailyAt": "2025-03-17T00:46:46.368Z",
            "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
            "submittedOnDailyBy": {
                "_id": "66a9b3533d417b0baa9220a6",
                "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
                "isPro": false,
                "fullname": "Luozheng Qin",
                "user": "Fr0zencr4nE",
                "type": "user"
            },
            "summary": "Video Detailed Captioning (VDC) is a crucial task for vision-language\nbridging, enabling fine-grained descriptions of complex video content. In this\npaper, we first comprehensively benchmark current state-of-the-art approaches\nand systematically identified two critical limitations: biased capability\ntowards specific captioning aspect and misalignment with human preferences. To\naddress these deficiencies, we propose Cockatiel, a novel three-stage training\npipeline that ensembles synthetic and human-aligned training for improving VDC\nperformance. In the first stage, we derive a scorer from a meticulously\nannotated dataset to select synthetic captions high-performing on certain\nfine-grained video-caption alignment and human-preferred while disregarding\nothers. Then, we train Cockatiel-13B, using this curated dataset to infuse it\nwith assembled model strengths and human preferences. Finally, we further\ndistill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive\nquantitative and qualitative experiments reflect the effectiveness of our\nmethod, as we not only set new state-of-the-art performance on VDCSCORE in a\ndimension-balanced way but also surpass leading alternatives on human\npreference by a large margin as depicted by the human evaluation results.",
            "upvotes": 5,
            "discussionId": "67d2bd370860f2d7ff10e4da",
            "ai_keywords": [
                "Cockatiel",
                "three-stage training pipeline",
                "synthetic and human-aligned training",
                "fine-grained video-caption alignment",
                "scorer",
                "meticulously annotated dataset",
                "curated dataset",
                "assembled model strengths",
                "human preferences",
                "VDCSCORE",
                "dimension-balanced way",
                "human evaluation results"
            ]
        },
        "publishedAt": "2025-03-12T07:25:04.000Z",
        "title": "Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption",
        "summary": "Video Detailed Captioning (VDC) is a crucial task for vision-language\nbridging, enabling fine-grained descriptions of complex video content. In this\npaper, we first comprehensively benchmark current state-of-the-art approaches\nand systematically identified two critical limitations: biased capability\ntowards specific captioning aspect and misalignment with human preferences. To\naddress these deficiencies, we propose Cockatiel, a novel three-stage training\npipeline that ensembles synthetic and human-aligned training for improving VDC\nperformance. In the first stage, we derive a scorer from a meticulously\nannotated dataset to select synthetic captions high-performing on certain\nfine-grained video-caption alignment and human-preferred while disregarding\nothers. Then, we train Cockatiel-13B, using this curated dataset to infuse it\nwith assembled model strengths and human preferences. Finally, we further\ndistill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive\nquantitative and qualitative experiments reflect the effectiveness of our\nmethod, as we not only set new state-of-the-art performance on VDCSCORE in a\ndimension-balanced way but also surpass leading alternatives on human\npreference by a large margin as depicted by the human evaluation results.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a9b3533d417b0baa9220a6",
            "avatarUrl": "/avatars/adc372bd24df1d3bf43258833411e8af.svg",
            "fullname": "Luozheng Qin",
            "name": "Fr0zencr4nE",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10696",
            "authors": [
                {
                    "_id": "67d7e9bd93b8599318993db2",
                    "user": {
                        "_id": "65a88c3d26598b995531fff1",
                        "avatarUrl": "/avatars/b1a524857d8572d0405476661b434160.svg",
                        "isPro": false,
                        "fullname": "Yefei He",
                        "user": "yefly",
                        "type": "user"
                    },
                    "name": "Yefei He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:54:40.739Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e9bd93b8599318993db3",
                    "user": {
                        "_id": "668a94d30b04331a0b6dc457",
                        "avatarUrl": "/avatars/66a311a7474e9d297231868dbbd6a18c.svg",
                        "isPro": false,
                        "fullname": "heyuanyu",
                        "user": "heyuanyu",
                        "type": "user"
                    },
                    "name": "Yuanyu He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:55:03.703Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e9bd93b8599318993db4",
                    "user": {
                        "_id": "66365ad3ea6f5788b32a2fb4",
                        "avatarUrl": "/avatars/99a7aebc1d022b52d7692494bdd2a1c7.svg",
                        "isPro": false,
                        "fullname": "Shaoxuan He",
                        "user": "shxhe",
                        "type": "user"
                    },
                    "name": "Shaoxuan He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:54:33.115Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e9bd93b8599318993db5",
                    "user": {
                        "_id": "64574380f3ef144c0e69d484",
                        "avatarUrl": "/avatars/a0a84757cb0bf09c24291803e1389b49.svg",
                        "isPro": false,
                        "fullname": "Feng Chen",
                        "user": "chenfeng1271",
                        "type": "user"
                    },
                    "name": "Feng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:15.701Z",
                    "hidden": false
                },
                {
                    "_id": "67d7e9bd93b8599318993db6",
                    "name": "Hong Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d7e9bd93b8599318993db7",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d7e9bd93b8599318993db8",
                    "user": {
                        "_id": "65a8a828ef14f9e6037dfad2",
                        "avatarUrl": "/avatars/55fdc0393b2e37ac998ddbdb1e1ff636.svg",
                        "isPro": false,
                        "fullname": "Bohan Zhuang",
                        "user": "BohanZ",
                        "type": "user"
                    },
                    "name": "Bohan Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:55:23.366Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T05:52:27.000Z",
            "submittedOnDailyAt": "2025-03-17T07:53:13.467Z",
            "title": "Neighboring Autoregressive Modeling for Efficient Visual Generation",
            "submittedOnDailyBy": {
                "_id": "65a88c3d26598b995531fff1",
                "avatarUrl": "/avatars/b1a524857d8572d0405476661b434160.svg",
                "isPro": false,
                "fullname": "Yefei He",
                "user": "yefly",
                "type": "user"
            },
            "summary": "Visual autoregressive models typically adhere to a raster-order ``next-token\nprediction\" paradigm, which overlooks the spatial and temporal locality\ninherent in visual content. Specifically, visual tokens exhibit significantly\nstronger correlations with their spatially or temporally adjacent tokens\ncompared to those that are distant. In this paper, we propose Neighboring\nAutoregressive Modeling (NAR), a novel paradigm that formulates autoregressive\nvisual generation as a progressive outpainting procedure, following a\nnear-to-far ``next-neighbor prediction\" mechanism. Starting from an initial\ntoken, the remaining tokens are decoded in ascending order of their Manhattan\ndistance from the initial token in the spatial-temporal space, progressively\nexpanding the boundary of the decoded region. To enable parallel prediction of\nmultiple adjacent tokens in the spatial-temporal space, we introduce a set of\ndimension-oriented decoding heads, each predicting the next token along a\nmutually orthogonal dimension. During inference, all tokens adjacent to the\ndecoded tokens are processed in parallel, substantially reducing the model\nforward steps for generation. Experiments on ImageNet256times 256 and UCF101\ndemonstrate that NAR achieves 2.4times and 8.6times higher throughput\nrespectively, while obtaining superior FID/FVD scores for both image and video\ngeneration tasks compared to the PAR-4X approach. When evaluating on\ntext-to-image generation benchmark GenEval, NAR with 0.8B parameters\noutperforms Chameleon-7B while using merely 0.4 of the training data. Code is\navailable at https://github.com/ThisisBillhe/NAR.",
            "upvotes": 5,
            "discussionId": "67d7e9c093b8599318993e93",
            "projectPage": "https://yuanyu0.github.io/nar/",
            "githubRepo": "https://github.com/ThisisBillhe/NAR",
            "ai_keywords": [
                "Neighboring Autoregressive Modeling (NAR)",
                "raster-order",
                "next-token prediction",
                "spatial and temporal locality",
                "visual tokens",
                "spatially or temporally adjacent tokens",
                "outpainting procedure",
                "near-to-far prediction",
                "Manhattan distance",
                "spatial-temporal space",
                "dimension-oriented decoding heads",
                "FID (Fréchet Inception Distance)",
                "FVD (Fréchet Video Distance)",
                "ImageNet$256\\times 256$",
                "UCF101",
                "text-to-image generation benchmark GenEval",
                "Chameleon-7B",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-03-12T01:52:27.000Z",
        "title": "Neighboring Autoregressive Modeling for Efficient Visual Generation",
        "summary": "Visual autoregressive models typically adhere to a raster-order ``next-token\nprediction\" paradigm, which overlooks the spatial and temporal locality\ninherent in visual content. Specifically, visual tokens exhibit significantly\nstronger correlations with their spatially or temporally adjacent tokens\ncompared to those that are distant. In this paper, we propose Neighboring\nAutoregressive Modeling (NAR), a novel paradigm that formulates autoregressive\nvisual generation as a progressive outpainting procedure, following a\nnear-to-far ``next-neighbor prediction\" mechanism. Starting from an initial\ntoken, the remaining tokens are decoded in ascending order of their Manhattan\ndistance from the initial token in the spatial-temporal space, progressively\nexpanding the boundary of the decoded region. To enable parallel prediction of\nmultiple adjacent tokens in the spatial-temporal space, we introduce a set of\ndimension-oriented decoding heads, each predicting the next token along a\nmutually orthogonal dimension. During inference, all tokens adjacent to the\ndecoded tokens are processed in parallel, substantially reducing the model\nforward steps for generation. Experiments on ImageNet256times 256 and UCF101\ndemonstrate that NAR achieves 2.4times and 8.6times higher throughput\nrespectively, while obtaining superior FID/FVD scores for both image and video\ngeneration tasks compared to the PAR-4X approach. When evaluating on\ntext-to-image generation benchmark GenEval, NAR with 0.8B parameters\noutperforms Chameleon-7B while using merely 0.4 of the training data. Code is\navailable at https://github.com/ThisisBillhe/NAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10696.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65a88c3d26598b995531fff1",
            "avatarUrl": "/avatars/b1a524857d8572d0405476661b434160.svg",
            "fullname": "Yefei He",
            "name": "yefly",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06674",
            "authors": [
                {
                    "_id": "67d6881cf997964e21f90598",
                    "user": {
                        "_id": "65f7e6856bd4bac5b6a4ecc3",
                        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
                        "isPro": false,
                        "fullname": "Yihong Luo",
                        "user": "Luo-Yihong",
                        "type": "user"
                    },
                    "name": "Yihong Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:52.452Z",
                    "hidden": false
                },
                {
                    "_id": "67d6881cf997964e21f90599",
                    "user": {
                        "_id": "636a40faa6f948c4f0c62ae5",
                        "avatarUrl": "/avatars/30c35b194ba84d6e274df30e91a8cc45.svg",
                        "isPro": false,
                        "fullname": "Tianyang Hu",
                        "user": "whatlegequ",
                        "type": "user"
                    },
                    "name": "Tianyang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:56:48.283Z",
                    "hidden": false
                },
                {
                    "_id": "67d6881cf997964e21f9059a",
                    "user": {
                        "_id": "67b91a3c186bc4f8d83c94cf",
                        "avatarUrl": "/avatars/a79538be4b5ed02cd54556458375e4af.svg",
                        "isPro": false,
                        "fullname": "Jiacheng Sun",
                        "user": "JIACSUN96",
                        "type": "user"
                    },
                    "name": "Jiacheng Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:56:55.114Z",
                    "hidden": false
                },
                {
                    "_id": "67d6881cf997964e21f9059b",
                    "name": "Yujun Cai",
                    "hidden": false
                },
                {
                    "_id": "67d6881cf997964e21f9059c",
                    "user": {
                        "_id": "636d660056c0762cfd9dc8d5",
                        "avatarUrl": "/avatars/50ea2100e00b67ef10adc57556477184.svg",
                        "isPro": false,
                        "fullname": "jing tang",
                        "user": "jingtang",
                        "type": "user"
                    },
                    "name": "Jing Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T08:57:09.362Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T15:53:49.000Z",
            "submittedOnDailyAt": "2025-03-17T02:34:51.976Z",
            "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
            "submittedOnDailyBy": {
                "_id": "65f7e6856bd4bac5b6a4ecc3",
                "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
                "isPro": false,
                "fullname": "Yihong Luo",
                "user": "Luo-Yihong",
                "type": "user"
            },
            "summary": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-alpha, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-alpha into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
            "upvotes": 5,
            "discussionId": "67d6881ef997964e21f90660",
            "projectPage": "https://tdm-t2x.github.io/",
            "githubRepo": "https://github.com/Luo-Yihong/TDM",
            "ai_keywords": [
                "diffusion model sampling",
                "diffusion distillation",
                "distribution matching",
                "trajectory matching",
                "few-step generation",
                "Trajectory Distribution Matching (TDM)",
                "data-free score distillation",
                "sampling-steps-aware objective",
                "deterministic sampling",
                "state-of-the-art performance",
                "SDXL",
                "PixArt-$\\alpha$",
                "TDM",
                "text-to-video diffusion",
                "CogVideoX-2B",
                "VBench",
                "NFE"
            ]
        },
        "publishedAt": "2025-03-09T11:53:49.000Z",
        "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
        "summary": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-alpha, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-alpha into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06674.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "fullname": "Yihong Luo",
            "name": "Luo-Yihong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06553",
            "authors": [
                {
                    "_id": "67cfcf664dac6ed12db8b10a",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b10b",
                    "user": {
                        "_id": "65df481e530333731ea24617",
                        "avatarUrl": "/avatars/3ed41f5d7d0489193807b5e6260f16c9.svg",
                        "isPro": false,
                        "fullname": "ZHOU PENGFEI",
                        "user": "lyuukuu",
                        "type": "user"
                    },
                    "name": "Pengfei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:54:16.357Z",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b10c",
                    "user": {
                        "_id": "646dee28174cc96d50951991",
                        "avatarUrl": "/avatars/17d88a24c905e9819268b27037015a35.svg",
                        "isPro": false,
                        "fullname": "xu",
                        "user": "xuzhaopan",
                        "type": "user"
                    },
                    "name": "Zhaopan Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:52:13.027Z",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b10d",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b10e",
                    "user": {
                        "_id": "665305eff0c8c891cae7fe01",
                        "avatarUrl": "/avatars/1f372e3bc6a4eb19ef702ec96a391c96.svg",
                        "isPro": false,
                        "fullname": "Fanrui Zhang",
                        "user": "fanrui00",
                        "type": "user"
                    },
                    "name": "Fanrui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:52:27.748Z",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b10f",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b110",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b111",
                    "user": {
                        "_id": "66d94f2a36aa5055694dfe04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/grAN83brH0E4_S0__yLdv.jpeg",
                        "isPro": false,
                        "fullname": "fengyukang",
                        "user": "finyorko",
                        "type": "user"
                    },
                    "name": "Yukang Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:51:23.539Z",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b112",
                    "name": "Baojin Huang",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b113",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67cfcf664dac6ed12db8b114",
                    "user": {
                        "_id": "65f1713552c38a91e0a445e8",
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:50:51.940Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-09T10:55:51.000Z",
            "submittedOnDailyAt": "2025-03-17T07:32:38.224Z",
            "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
            "upvotes": 5,
            "discussionId": "67cfcf684dac6ed12db8b185",
            "ai_keywords": [
                "ProJudgeBench",
                "multi-modal large language models (MLLMs)",
                "Reasoning processes",
                "Automated process judges",
                "Step-level labels",
                "Scientific disciplines",
                "Multi-modal content",
                "Error classification",
                "Dynamic Dual-Phase fine-tuning",
                "Instruction-tuning dataset",
                "Problem-solving reasoning"
            ]
        },
        "publishedAt": "2025-03-09T06:55:51.000Z",
        "title": "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
        "summary": "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06553.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10624",
            "authors": [
                {
                    "_id": "67d5b604f58a6a411a5bb598",
                    "user": {
                        "_id": "65987383bf533e3c0dd1914b",
                        "avatarUrl": "/avatars/4b3c0e791ef24a439a43371c4d92bc4b.svg",
                        "isPro": false,
                        "fullname": "Boqian Li",
                        "user": "Boqian-Li",
                        "type": "user"
                    },
                    "name": "Boqian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-16T21:12:23.978Z",
                    "hidden": false
                },
                {
                    "_id": "67d5b604f58a6a411a5bb599",
                    "name": "Haiwen Feng",
                    "hidden": false
                },
                {
                    "_id": "67d5b604f58a6a411a5bb59a",
                    "name": "Zeyu Cai",
                    "hidden": false
                },
                {
                    "_id": "67d5b604f58a6a411a5bb59b",
                    "name": "Michael J. Black",
                    "hidden": false
                },
                {
                    "_id": "67d5b604f58a6a411a5bb59c",
                    "name": "Yuliang Xiu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/IP3jsn2w6NAUnlzaLby5W.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/YFQHw4OpnXHm7SDXX14x3.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/vWNGuO2-4RWYzKS3X5kip.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/VPNP0CWFQW0VbiSXgf39v.mp4"
            ],
            "publishedAt": "2025-03-13T17:59:14.000Z",
            "submittedOnDailyAt": "2025-03-17T07:50:58.923Z",
            "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness",
            "submittedOnDailyBy": {
                "_id": "65987383bf533e3c0dd1914b",
                "avatarUrl": "/avatars/4b3c0e791ef24a439a43371c4d92bc4b.svg",
                "isPro": false,
                "fullname": "Boqian Li",
                "user": "Boqian-Li",
                "type": "user"
            },
            "summary": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
            "upvotes": 4,
            "discussionId": "67d5b607f58a6a411a5bb680",
            "projectPage": "https://boqian-li.github.io/ETCH/",
            "githubRepo": "https://github.com/boqian-li/ETCH",
            "ai_keywords": [
                "equivariant",
                "tightness fitting",
                "SE(3) equivariance",
                "displacement vectors",
                "pose-invariant body features",
                "sparse body markers",
                "inner-body marker fitting task",
                "CAPE",
                "4D-Dress",
                "tightness-agnostic",
                "tightness-aware",
                "body fitting accuracy",
                "shape accuracy",
                "directional errors",
                "one-shot",
                "out-of-distribution",
                "generalization"
            ]
        },
        "publishedAt": "2025-03-13T13:59:14.000Z",
        "title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness",
        "summary": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/IP3jsn2w6NAUnlzaLby5W.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/YFQHw4OpnXHm7SDXX14x3.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/vWNGuO2-4RWYzKS3X5kip.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/65987383bf533e3c0dd1914b/VPNP0CWFQW0VbiSXgf39v.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10624.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65987383bf533e3c0dd1914b",
            "avatarUrl": "/avatars/4b3c0e791ef24a439a43371c4d92bc4b.svg",
            "fullname": "Boqian Li",
            "name": "Boqian-Li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11629",
            "authors": [
                {
                    "_id": "67d816adc51e889cd00ad162",
                    "user": {
                        "_id": "63bfb9a54ed1b8fd6883524c",
                        "avatarUrl": "/avatars/5462264a8cf58cb15fcc5194336d45ac.svg",
                        "isPro": false,
                        "fullname": "Stefan Lionar",
                        "user": "slionar",
                        "type": "user"
                    },
                    "name": "Stefan Lionar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:03.888Z",
                    "hidden": false
                },
                {
                    "_id": "67d816adc51e889cd00ad163",
                    "name": "Jiabin Liang",
                    "hidden": false
                },
                {
                    "_id": "67d816adc51e889cd00ad164",
                    "name": "Gim Hee Lee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63bfb9a54ed1b8fd6883524c/MswsxAZ-x1j0vLsWXCb26.png"
            ],
            "publishedAt": "2025-03-14T17:48:06.000Z",
            "submittedOnDailyAt": "2025-03-17T11:13:55.209Z",
            "title": "TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing",
            "submittedOnDailyBy": {
                "_id": "63bfb9a54ed1b8fd6883524c",
                "avatarUrl": "/avatars/5462264a8cf58cb15fcc5194336d45ac.svg",
                "isPro": false,
                "fullname": "Stefan Lionar",
                "user": "slionar",
                "type": "user"
            },
            "summary": "We introduce TreeMeshGPT, an autoregressive Transformer designed to generate\nhigh-quality artistic meshes aligned with input point clouds. Instead of the\nconventional next-token prediction in autoregressive Transformer, we propose a\nnovel Autoregressive Tree Sequencing where the next input token is retrieved\nfrom a dynamically growing tree structure that is built upon the triangle\nadjacency of faces within the mesh. Our sequencing enables the mesh to extend\nlocally from the last generated triangular face at each step, and therefore\nreduces training difficulty and improves mesh quality. Our approach represents\neach triangular face with two tokens, achieving a compression rate of\napproximately 22% compared to the naive face tokenization. This efficient\ntokenization enables our model to generate highly detailed artistic meshes with\nstrong point cloud conditioning, surpassing previous methods in both capacity\nand fidelity. Furthermore, our method generates mesh with strong normal\norientation constraints, minimizing flipped normals commonly encountered in\nprevious methods. Our experiments show that TreeMeshGPT enhances the mesh\ngeneration quality with refined details and normal orientation consistency.",
            "upvotes": 2,
            "discussionId": "67d816b0c51e889cd00ad241",
            "ai_keywords": [
                "TreeMeshGPT",
                "autoregressive Transformer",
                "Autoregressive Tree Sequencing",
                "triangle adjacency",
                "triangular face",
                "tokenization",
                "normal orientation constraints"
            ]
        },
        "publishedAt": "2025-03-14T13:48:06.000Z",
        "title": "TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing",
        "summary": "We introduce TreeMeshGPT, an autoregressive Transformer designed to generate\nhigh-quality artistic meshes aligned with input point clouds. Instead of the\nconventional next-token prediction in autoregressive Transformer, we propose a\nnovel Autoregressive Tree Sequencing where the next input token is retrieved\nfrom a dynamically growing tree structure that is built upon the triangle\nadjacency of faces within the mesh. Our sequencing enables the mesh to extend\nlocally from the last generated triangular face at each step, and therefore\nreduces training difficulty and improves mesh quality. Our approach represents\neach triangular face with two tokens, achieving a compression rate of\napproximately 22% compared to the naive face tokenization. This efficient\ntokenization enables our model to generate highly detailed artistic meshes with\nstrong point cloud conditioning, surpassing previous methods in both capacity\nand fidelity. Furthermore, our method generates mesh with strong normal\norientation constraints, minimizing flipped normals commonly encountered in\nprevious methods. Our experiments show that TreeMeshGPT enhances the mesh\ngeneration quality with refined details and normal orientation consistency.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63bfb9a54ed1b8fd6883524c/MswsxAZ-x1j0vLsWXCb26.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11629.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bfb9a54ed1b8fd6883524c",
            "avatarUrl": "/avatars/5462264a8cf58cb15fcc5194336d45ac.svg",
            "fullname": "Stefan Lionar",
            "name": "slionar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11207",
            "authors": [
                {
                    "_id": "67d82c6e6d6baf84fba32ea1",
                    "user": {
                        "_id": "6395beb3b5af054482d87456",
                        "avatarUrl": "/avatars/b24ae0d2e3fea8ad716d1b7225d9e722.svg",
                        "isPro": false,
                        "fullname": "Giacomo Camposampiero",
                        "user": "gcamposampie",
                        "type": "user"
                    },
                    "name": "Giacomo Camposampiero",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T15:07:03.399Z",
                    "hidden": false
                },
                {
                    "_id": "67d82c6e6d6baf84fba32ea2",
                    "user": {
                        "_id": "668f96ca35cddb688ad9f2cf",
                        "avatarUrl": "/avatars/eb4febcf47b424732ee0a73a81c3b6dd.svg",
                        "isPro": false,
                        "fullname": "Michael Hersche",
                        "user": "mihoesche",
                        "type": "user"
                    },
                    "name": "Michael Hersche",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-17T14:06:39.558Z",
                    "hidden": false
                },
                {
                    "_id": "67d82c6e6d6baf84fba32ea3",
                    "user": {
                        "_id": "64afd0c109d6573c1d89dbf7",
                        "avatarUrl": "/avatars/ed6bd7133517590f46847a0930d7a049.svg",
                        "isPro": false,
                        "fullname": "Roger Wattenhofer",
                        "user": "Rogrrr",
                        "type": "user"
                    },
                    "name": "Roger Wattenhofer",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-17T14:06:39.558Z",
                    "hidden": false
                },
                {
                    "_id": "67d82c6e6d6baf84fba32ea4",
                    "name": "Abu Sebastian",
                    "hidden": false
                },
                {
                    "_id": "67d82c6e6d6baf84fba32ea5",
                    "name": "Abbas Rahimi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/668f96ca35cddb688ad9f2cf/U8Mu7oyXVKH0Hgo6Ly_7D.png"
            ],
            "publishedAt": "2025-03-14T08:52:25.000Z",
            "submittedOnDailyAt": "2025-03-17T12:41:29.242Z",
            "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?",
            "submittedOnDailyBy": {
                "_id": "668f96ca35cddb688ad9f2cf",
                "avatarUrl": "/avatars/eb4febcf47b424732ee0a73a81c3b6dd.svg",
                "isPro": false,
                "fullname": "Michael Hersche",
                "user": "mihoesche",
                "type": "user"
            },
            "summary": "This work presents a first evaluation of two state-of-the-art Large Reasoning\nModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,\nfocusing on well-established nonverbal human IQ tests based on Raven's\nprogressive matrices. We benchmark with the I-RAVEN dataset and its more\ndifficult extension, I-RAVEN-X, which tests the ability to generalize to longer\nreasoning rules and ranges of the attribute values. To assess the influence of\nvisual uncertainties on these nonverbal analogical reasoning tests, we extend\nthe I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a\ntwo-fold strategy to simulate this imperfect visual perception: 1) we introduce\nconfounding attributes which, being sampled at random, do not contribute to the\nprediction of the correct answer of the puzzles and 2) smoothen the\ndistributions of the input attributes' values. We observe a sharp decline in\nOpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to\njust 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X,\nwhich increases input length and range and emulates perceptual uncertainty.\nThis drop occurred despite spending 3.4x more reasoning tokens. A similar trend\nis also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a\nneuro-symbolic probabilistic abductive model, ARLC, that achieves\nstate-of-the-art performances on I-RAVEN, can robustly reason under all these\nout-of-distribution tests, maintaining strong accuracy with only a modest\nreduction from 98.6% to 88.0%. Our code is available at\nhttps://github.com/IBM/raven-large-language-models.",
            "upvotes": 2,
            "discussionId": "67d82c6f6d6baf84fba32f04",
            "githubRepo": "https://github.com/IBM/raven-large-language-models",
            "ai_keywords": [
                "Large Reasoning Models (LRMs)",
                "OpenAI's o3-mini",
                "DeepSeek R1",
                "analogical reasoning",
                "Raven's progressive matrices",
                "I-RAVEN dataset",
                "I-RAVEN-X",
                "visual uncertainties",
                "oracle perception",
                "confounding attributes",
                "input attributes",
                "reasoning tokens",
                "neuro-symbolic probabilistic abductive model",
                "ARLC",
                "state-of-the-art performances",
                "out-of-distribution tests"
            ]
        },
        "publishedAt": "2025-03-14T04:52:25.000Z",
        "title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?",
        "summary": "This work presents a first evaluation of two state-of-the-art Large Reasoning\nModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,\nfocusing on well-established nonverbal human IQ tests based on Raven's\nprogressive matrices. We benchmark with the I-RAVEN dataset and its more\ndifficult extension, I-RAVEN-X, which tests the ability to generalize to longer\nreasoning rules and ranges of the attribute values. To assess the influence of\nvisual uncertainties on these nonverbal analogical reasoning tests, we extend\nthe I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a\ntwo-fold strategy to simulate this imperfect visual perception: 1) we introduce\nconfounding attributes which, being sampled at random, do not contribute to the\nprediction of the correct answer of the puzzles and 2) smoothen the\ndistributions of the input attributes' values. We observe a sharp decline in\nOpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to\njust 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X,\nwhich increases input length and range and emulates perceptual uncertainty.\nThis drop occurred despite spending 3.4x more reasoning tokens. A similar trend\nis also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a\nneuro-symbolic probabilistic abductive model, ARLC, that achieves\nstate-of-the-art performances on I-RAVEN, can robustly reason under all these\nout-of-distribution tests, maintaining strong accuracy with only a modest\nreduction from 98.6% to 88.0%. Our code is available at\nhttps://github.com/IBM/raven-large-language-models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/668f96ca35cddb688ad9f2cf/U8Mu7oyXVKH0Hgo6Ly_7D.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11207.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668f96ca35cddb688ad9f2cf",
            "avatarUrl": "/avatars/eb4febcf47b424732ee0a73a81c3b6dd.svg",
            "fullname": "Michael Hersche",
            "name": "mihoesche",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10620",
            "authors": [
                {
                    "_id": "67d46c53f624d5a39650024d",
                    "user": {
                        "_id": "65bce8265604c1d38309e3d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7_JdljgybU6AWglzwMr93.png",
                        "isPro": false,
                        "fullname": "Kshitij Ambilduke",
                        "user": "KshitijAmbilduke",
                        "type": "user"
                    },
                    "name": "Kshitij Ambilduke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:56:05.985Z",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a39650024e",
                    "user": {
                        "_id": "6568a0efdd4a892a1491605c",
                        "avatarUrl": "/avatars/0ec70d96c0e78538fc8dd9153bb4220f.svg",
                        "isPro": false,
                        "fullname": "Ben Peters",
                        "user": "bpop",
                        "type": "user"
                    },
                    "name": "Ben Peters",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-16T21:13:33.209Z",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a39650024f",
                    "user": {
                        "_id": "65c37598f7296c2feeddc531",
                        "avatarUrl": "/avatars/cc2242f8822b422556449227cac9762c.svg",
                        "isPro": false,
                        "fullname": "Sonal Sannigrahi",
                        "user": "sonalsannigrahi",
                        "type": "user"
                    },
                    "name": "Sonal Sannigrahi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:56:12.084Z",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a396500250",
                    "user": {
                        "_id": "637cfe296231c36c63711f3c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637cfe296231c36c63711f3c/IVT-o57LMsOpNE_ddT_tW.jpeg",
                        "isPro": false,
                        "fullname": "Anil Keshwani",
                        "user": "anilkeshwani",
                        "type": "user"
                    },
                    "name": "Anil Keshwani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:56:18.745Z",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a396500251",
                    "user": {
                        "_id": "65d629f8f67b1764166e2ab0",
                        "avatarUrl": "/avatars/492da03e63a3a647b14132f1ea39cca7.svg",
                        "isPro": false,
                        "fullname": "Tsz Kin Lam",
                        "user": "edinkin",
                        "type": "user"
                    },
                    "name": "Tsz Kin Lam",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:56:24.816Z",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a396500252",
                    "name": "Bruno Martins",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a396500253",
                    "user": {
                        "_id": "62262e19d36494a6f743a28d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667383417385-62262e19d36494a6f743a28d.jpeg",
                        "isPro": false,
                        "fullname": "Marcely Zanon Boito",
                        "user": "mzboito",
                        "type": "user"
                    },
                    "name": "Marcely Zanon Boito",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:57:03.534Z",
                    "hidden": false
                },
                {
                    "_id": "67d46c53f624d5a396500254",
                    "name": "André F. T. Martins",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T17:57:32.000Z",
            "submittedOnDailyAt": "2025-03-17T09:39:01.583Z",
            "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
            "submittedOnDailyBy": {
                "_id": "6568a0efdd4a892a1491605c",
                "avatarUrl": "/avatars/0ec70d96c0e78538fc8dd9153bb4220f.svg",
                "isPro": false,
                "fullname": "Ben Peters",
                "user": "bpop",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community.",
            "upvotes": 2,
            "discussionId": "67d46c54f624d5a3965002a2",
            "ai_keywords": [
                "LLMs",
                "speech discretization",
                "continued pre-training",
                "multilingual LLMs",
                "TOWER",
                "SPIRE",
                "transcription",
                "translation-related tasks"
            ]
        },
        "publishedAt": "2025-03-13T13:57:32.000Z",
        "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
        "summary": "Large language models (LLMs) have shown remarkable performance and\ngeneralization capabilities across multiple languages and tasks, making them\nvery attractive targets for multi-modality integration (e.g., images or\nspeech). In this work, we extend an existing LLM to the speech modality via\nspeech discretization and continued pre-training. In particular, we are\ninterested in multilingual LLMs, such as TOWER, as their pre-training setting\nallows us to treat discretized speech input as an additional translation\nlanguage. The resulting open-source model, SPIRE, is able to transcribe and\ntranslate English speech input while maintaining TOWER's original performance\non translation-related tasks, showcasing that discretized speech input\nintegration as an additional language is feasible during LLM adaptation. We\nmake our code and models available to the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10620.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6568a0efdd4a892a1491605c",
            "avatarUrl": "/avatars/0ec70d96c0e78538fc8dd9153bb4220f.svg",
            "fullname": "Ben Peters",
            "name": "bpop",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10684",
            "authors": [
                {
                    "_id": "67d8030568c586e2c8fcac8f",
                    "user": {
                        "_id": "66962a99a51ccecc90a662ae",
                        "avatarUrl": "/avatars/b4e200cc42eef5990678c168e6e51ab2.svg",
                        "isPro": false,
                        "fullname": "DJW",
                        "user": "fatty-belly",
                        "type": "user"
                    },
                    "name": "Jingwen Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:10.438Z",
                    "hidden": false
                },
                {
                    "_id": "67d8030568c586e2c8fcac90",
                    "user": {
                        "_id": "642e8c99c1b0f8e4e76bcaab",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
                        "isPro": false,
                        "fullname": "Zihao Wang",
                        "user": "zhwang4ai",
                        "type": "user"
                    },
                    "name": "Zihao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T14:42:12.462Z",
                    "hidden": false
                },
                {
                    "_id": "67d8030568c586e2c8fcac91",
                    "name": "Shaofei Cai",
                    "hidden": false
                },
                {
                    "_id": "67d8030568c586e2c8fcac92",
                    "name": "Anji Liu",
                    "hidden": false
                },
                {
                    "_id": "67d8030568c586e2c8fcac93",
                    "name": "Yitao Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T18:51:40.000Z",
            "submittedOnDailyAt": "2025-03-17T09:41:33.974Z",
            "title": "Open-World Skill Discovery from Unsegmented Demonstrations",
            "submittedOnDailyBy": {
                "_id": "642e8c99c1b0f8e4e76bcaab",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
                "isPro": false,
                "fullname": "Zihao Wang",
                "user": "zhwang4ai",
                "type": "user"
            },
            "summary": "Learning skills in open-world environments is essential for developing agents\ncapable of handling a variety of tasks by combining basic skills. Online\ndemonstration videos are typically long but unsegmented, making them difficult\nto segment and label with skill identifiers. Unlike existing methods that rely\non sequence sampling or human labeling, we have developed a self-supervised\nlearning-based approach to segment these long videos into a series of\nsemantic-aware and skill-consistent segments. Drawing inspiration from human\ncognitive event segmentation theory, we introduce Skill Boundary Detection\n(SBD), an annotation-free temporal video segmentation algorithm. SBD detects\nskill boundaries in a video by leveraging prediction errors from a pretrained\nunconditional action-prediction model. This approach is based on the assumption\nthat a significant increase in prediction error indicates a shift in the skill\nbeing executed. We evaluated our method in Minecraft, a rich open-world\nsimulator with extensive gameplay videos available online. Our SBD-generated\nsegments improved the average performance of conditioned policies by 63.7% and\n52.1% on short-term atomic skill tasks, and their corresponding hierarchical\nagents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the\ndiverse YouTube videos to train instruction-following agents. The project page\ncan be found in https://craftjarvis.github.io/SkillDiscovery.",
            "upvotes": 2,
            "discussionId": "67d8030f68c586e2c8fcb00a",
            "projectPage": "https://craftjarvis.github.io/SkillDiscovery",
            "githubRepo": "https://github.com/CraftJarvis/SkillDiscovery",
            "ai_keywords": [
                "self-supervised learning",
                "semantic-aware",
                "skill-consistent",
                "Skill Boundary Detection (SBD)",
                "prediction error",
                "pretrained unconditional action-prediction model",
                "Minecraft",
                "conditioned policies",
                "hierarchical agents",
                "short-term atomic skill tasks",
                "long-horizon tasks",
                "instruction-following agents"
            ]
        },
        "publishedAt": "2025-03-11T14:51:40.000Z",
        "title": "Open-World Skill Discovery from Unsegmented Demonstrations",
        "summary": "Learning skills in open-world environments is essential for developing agents\ncapable of handling a variety of tasks by combining basic skills. Online\ndemonstration videos are typically long but unsegmented, making them difficult\nto segment and label with skill identifiers. Unlike existing methods that rely\non sequence sampling or human labeling, we have developed a self-supervised\nlearning-based approach to segment these long videos into a series of\nsemantic-aware and skill-consistent segments. Drawing inspiration from human\ncognitive event segmentation theory, we introduce Skill Boundary Detection\n(SBD), an annotation-free temporal video segmentation algorithm. SBD detects\nskill boundaries in a video by leveraging prediction errors from a pretrained\nunconditional action-prediction model. This approach is based on the assumption\nthat a significant increase in prediction error indicates a shift in the skill\nbeing executed. We evaluated our method in Minecraft, a rich open-world\nsimulator with extensive gameplay videos available online. Our SBD-generated\nsegments improved the average performance of conditioned policies by 63.7% and\n52.1% on short-term atomic skill tasks, and their corresponding hierarchical\nagents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the\ndiverse YouTube videos to train instruction-following agents. The project page\ncan be found in https://craftjarvis.github.io/SkillDiscovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10684.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "642e8c99c1b0f8e4e76bcaab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
            "fullname": "Zihao Wang",
            "name": "zhwang4ai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.05689",
            "authors": [
                {
                    "_id": "67d37c5c3b54e330517a545d",
                    "user": {
                        "_id": "665b2ac6e0e2374ca24ba000",
                        "avatarUrl": "/avatars/d5218c9fa3dceae7b91df2e1d396bcf3.svg",
                        "isPro": false,
                        "fullname": "Zebin Xing",
                        "user": "XXXXing",
                        "type": "user"
                    },
                    "name": "Zebin Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-14T08:56:46.875Z",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a545e",
                    "name": "Xingyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a545f",
                    "name": "Yang Hu",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a5460",
                    "name": "Bo Jiang",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a5461",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a5462",
                    "name": "Qian Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a5463",
                    "name": "Xiaoxiao Long",
                    "hidden": false
                },
                {
                    "_id": "67d37c5c3b54e330517a5464",
                    "user": {
                        "_id": "654a2b1a83e7bfc4313a5cc7",
                        "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
                        "isPro": false,
                        "fullname": "Wei Yin",
                        "user": "WonderingWorld",
                        "type": "user"
                    },
                    "name": "Wei Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:44:54.732Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-07T18:52:08.000Z",
            "submittedOnDailyAt": "2025-03-17T01:05:31.649Z",
            "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "654a2b1a83e7bfc4313a5cc7",
                "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
                "isPro": false,
                "fullname": "Wei Yin",
                "user": "WonderingWorld",
                "type": "user"
            },
            "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the NavsimDauner2024_navsim,\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
            "upvotes": 2,
            "discussionId": "67d37c5d3b54e330517a54c7",
            "ai_keywords": [
                "GoalFlow",
                "multimodal trajectories",
                "trajectory selection complexity",
                "trajectory divergence",
                "diffusion-based methods",
                "goal point",
                "scoring mechanism",
                "Flow Matching",
                "Navsim",
                "PDMS",
                "diffusion-policy-based methods",
                "denoising step"
            ]
        },
        "publishedAt": "2025-03-07T13:52:08.000Z",
        "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving",
        "summary": "We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the NavsimDauner2024_navsim,\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654a2b1a83e7bfc4313a5cc7",
            "avatarUrl": "/avatars/dc3dfc3fcd26bb7350a9db0d075c5ea0.svg",
            "fullname": "Wei Yin",
            "name": "WonderingWorld",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11958",
            "authors": [
                {
                    "_id": "67d8c02c924be985c26ff727",
                    "name": "Chong Su",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff728",
                    "name": "Yingbin Fu",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff729",
                    "name": "Zheyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff72a",
                    "name": "Jing Yang",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff72b",
                    "name": "Param Hanji",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff72c",
                    "name": "Shaojun Wang",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff72d",
                    "name": "Xuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff72e",
                    "name": "Cengiz Öztireli",
                    "hidden": false
                },
                {
                    "_id": "67d8c02c924be985c26ff72f",
                    "name": "Fangcheng Zhong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-15T02:05:10.000Z",
            "submittedOnDailyAt": "2025-03-17T23:09:54.260Z",
            "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital\n  Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts",
            "submittedOnDailyBy": {
                "_id": "653053c6657ae56cdb5c490b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
                "isPro": false,
                "fullname": "Peter Hu",
                "user": "Peter2023HuggingFace",
                "type": "user"
            },
            "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.",
            "upvotes": 1,
            "discussionId": "67d8c031924be985c26ff88e",
            "ai_keywords": [
                "scene graph",
                "object list",
                "2D image-based intermediate layout representation",
                "out-of-distribution (OOD) scenarios",
                "complex floor plans",
                "multi-modal controls",
                "scene layouts",
                "household items",
                "room configurations",
                "data quality",
                "spatially coherent indoor scene synthesis",
                "photorealistic synthesis"
            ]
        },
        "publishedAt": "2025-03-14T22:05:10.000Z",
        "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital\n  Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts",
        "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11958.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653053c6657ae56cdb5c490b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
            "fullname": "Peter Hu",
            "name": "Peter2023HuggingFace",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.08111",
            "authors": [
                {
                    "_id": "67d69afb060a3df28c886b2b",
                    "user": {
                        "_id": "6716120136c0c14b5b18a16b",
                        "avatarUrl": "/avatars/9d759fb7c0849aa9f5283a514b4ff2e7.svg",
                        "isPro": false,
                        "fullname": "Jianhui Wang",
                        "user": "xchrysalis",
                        "type": "user"
                    },
                    "name": "Jianhui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:58:11.013Z",
                    "hidden": false
                },
                {
                    "_id": "67d69afb060a3df28c886b2c",
                    "user": {
                        "_id": "6464c4ef92773d5eeb588525",
                        "avatarUrl": "/avatars/65fc839453d892016640249cc1acf277.svg",
                        "isPro": false,
                        "fullname": "Zhifei Yang",
                        "user": "yangzhifei",
                        "type": "user"
                    },
                    "name": "Zhifei Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-16T21:12:12.522Z",
                    "hidden": true
                },
                {
                    "_id": "67d69afb060a3df28c886b2d",
                    "name": "Yangfan He",
                    "hidden": false
                },
                {
                    "_id": "67d69afb060a3df28c886b2e",
                    "name": "Huixiong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d69afb060a3df28c886b2f",
                    "name": "Yuxuan Chen",
                    "hidden": false
                },
                {
                    "_id": "67d69afb060a3df28c886b30",
                    "name": "Jingwei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T07:23:11.000Z",
            "submittedOnDailyAt": "2025-03-17T07:52:48.263Z",
            "title": "MaRI: Material Retrieval Integration across Domains",
            "submittedOnDailyBy": {
                "_id": "6464c4ef92773d5eeb588525",
                "avatarUrl": "/avatars/65fc839453d892016640249cc1acf277.svg",
                "isPro": false,
                "fullname": "Zhifei Yang",
                "user": "yangzhifei",
                "type": "user"
            },
            "summary": "Accurate material retrieval is critical for creating realistic 3D assets.\nExisting methods rely on datasets that capture shape-invariant and\nlighting-varied representations of materials, which are scarce and face\nchallenges due to limited diversity and inadequate real-world generalization.\nMost current approaches adopt traditional image search techniques. They fall\nshort in capturing the unique properties of material spaces, leading to\nsuboptimal performance in retrieval tasks. Addressing these challenges, we\nintroduce MaRI, a framework designed to bridge the feature space gap between\nsynthetic and real-world materials. MaRI constructs a shared embedding space\nthat harmonizes visual and material attributes through a contrastive learning\nstrategy by jointly training an image and a material encoder, bringing similar\nmaterials and images closer while separating dissimilar pairs within the\nfeature space. To support this, we construct a comprehensive dataset comprising\nhigh-quality synthetic materials rendered with controlled shape variations and\ndiverse lighting conditions, along with real-world materials processed and\nstandardized using material transfer techniques. Extensive experiments\ndemonstrate the superior performance, accuracy, and generalization capabilities\nof MaRI across diverse and complex material retrieval tasks, outperforming\nexisting methods.",
            "upvotes": 1,
            "discussionId": "67d69afd060a3df28c886c24",
            "projectPage": "https://jianhuiwemi.github.io/MaRI/",
            "ai_keywords": [
                "contrastive learning",
                "embedding space",
                "feature space",
                "image encoder",
                "material encoder",
                "material transfer techniques",
                "synthetic materials",
                "real-world materials",
                "shape variations",
                "lighting conditions",
                "material retrieval tasks"
            ]
        },
        "publishedAt": "2025-03-11T03:23:11.000Z",
        "title": "MaRI: Material Retrieval Integration across Domains",
        "summary": "Accurate material retrieval is critical for creating realistic 3D assets.\nExisting methods rely on datasets that capture shape-invariant and\nlighting-varied representations of materials, which are scarce and face\nchallenges due to limited diversity and inadequate real-world generalization.\nMost current approaches adopt traditional image search techniques. They fall\nshort in capturing the unique properties of material spaces, leading to\nsuboptimal performance in retrieval tasks. Addressing these challenges, we\nintroduce MaRI, a framework designed to bridge the feature space gap between\nsynthetic and real-world materials. MaRI constructs a shared embedding space\nthat harmonizes visual and material attributes through a contrastive learning\nstrategy by jointly training an image and a material encoder, bringing similar\nmaterials and images closer while separating dissimilar pairs within the\nfeature space. To support this, we construct a comprehensive dataset comprising\nhigh-quality synthetic materials rendered with controlled shape variations and\ndiverse lighting conditions, along with real-world materials processed and\nstandardized using material transfer techniques. Extensive experiments\ndemonstrate the superior performance, accuracy, and generalization capabilities\nof MaRI across diverse and complex material retrieval tasks, outperforming\nexisting methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08111.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6464c4ef92773d5eeb588525",
            "avatarUrl": "/avatars/65fc839453d892016640249cc1acf277.svg",
            "fullname": "Zhifei Yang",
            "name": "yangzhifei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09330",
            "authors": [
                {
                    "_id": "67d7fdac83b2126604707a84",
                    "user": {
                        "_id": "62f79e5a92950415b637d010",
                        "avatarUrl": "/avatars/f86cd0447c827eef9b8b25f248341455.svg",
                        "isPro": false,
                        "fullname": "Thomas De Min",
                        "user": "tdemin16",
                        "type": "user"
                    },
                    "name": "Thomas De Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:48:36.167Z",
                    "hidden": false
                },
                {
                    "_id": "67d7fdac83b2126604707a85",
                    "name": "Subhankar Roy",
                    "hidden": false
                },
                {
                    "_id": "67d7fdac83b2126604707a86",
                    "user": {
                        "_id": "64b6b19811676010a443b8f9",
                        "avatarUrl": "/avatars/23320beef2c77a7d3a6736bc57530f3b.svg",
                        "isPro": false,
                        "fullname": "Stephane Lathuilière",
                        "user": "stelat",
                        "type": "user"
                    },
                    "name": "Stéphane Lathuilière",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:49:07.526Z",
                    "hidden": false
                },
                {
                    "_id": "67d7fdac83b2126604707a87",
                    "name": "Elisa Ricci",
                    "hidden": false
                },
                {
                    "_id": "67d7fdac83b2126604707a88",
                    "user": {
                        "_id": "62cf293d3200bfd438e81f1f",
                        "avatarUrl": "/avatars/608c19ee375ef091ca77d7cfbc40e76e.svg",
                        "isPro": false,
                        "fullname": "Massimiliano Mancini",
                        "user": "massimilianom",
                        "type": "user"
                    },
                    "name": "Massimiliano Mancini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-17T14:49:30.879Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62f79e5a92950415b637d010/MKw9jpns6stDCx980fBMc.png"
            ],
            "publishedAt": "2025-03-12T12:24:05.000Z",
            "submittedOnDailyAt": "2025-03-17T09:23:33.733Z",
            "title": "Group-robust Machine Unlearning",
            "submittedOnDailyBy": {
                "_id": "62f79e5a92950415b637d010",
                "avatarUrl": "/avatars/f86cd0447c827eef9b8b25f248341455.svg",
                "isPro": false,
                "fullname": "Thomas De Min",
                "user": "tdemin16",
                "type": "user"
            },
            "summary": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
            "upvotes": 0,
            "discussionId": "67d7fdad83b2126604707afc",
            "ai_keywords": [
                "group-robust machine unlearning",
                "sample distribution reweighting",
                "MIU (Mutual Information-aware Machine Unlearning)",
                "mutual information",
                "performance degradation",
                "group robustness",
                "mutual information calibration",
                "model features",
                "group information"
            ]
        },
        "publishedAt": "2025-03-12T08:24:05.000Z",
        "title": "Group-robust Machine Unlearning",
        "summary": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f79e5a92950415b637d010/MKw9jpns6stDCx980fBMc.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09330.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f79e5a92950415b637d010",
            "avatarUrl": "/avatars/f86cd0447c827eef9b8b25f248341455.svg",
            "fullname": "Thomas De Min",
            "name": "tdemin16",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]