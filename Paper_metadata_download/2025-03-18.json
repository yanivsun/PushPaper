[
    {
        "paper": {
            "id": "2503.06053",
            "authors": [
                {
                    "_id": "67cfd2d7bc539099da9ebecb",
                    "name": "Runze Zhang",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebecc",
                    "user": {
                        "_id": "6474a63f7d131daf633d10f2",
                        "avatarUrl": "/avatars/5e5d1ce5731987a810448835a1a69c91.svg",
                        "isPro": false,
                        "fullname": "GeorgeDu",
                        "user": "georgedu",
                        "type": "user"
                    },
                    "name": "Guoguang Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:45:36.478Z",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebecd",
                    "user": {
                        "_id": "66b01dc4e48856bb718f2ba8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
                        "isPro": false,
                        "fullname": "Xiaochuan Li",
                        "user": "lixiaochuan",
                        "type": "user"
                    },
                    "name": "Xiaochuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:45:39.724Z",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebece",
                    "name": "Qi Jia",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebecf",
                    "name": "Liang Jin",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed0",
                    "user": {
                        "_id": "66f67725cdcb9a4eaef04027",
                        "avatarUrl": "/avatars/fb5f4b467cc4d73e129fa9aa60ef344d.svg",
                        "isPro": false,
                        "fullname": "Ellen Liu",
                        "user": "EllenAP",
                        "type": "user"
                    },
                    "name": "Lu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-17T08:45:32.476Z",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed1",
                    "name": "Jingjing Wang",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed2",
                    "user": {
                        "_id": "6297889a64501abb8d002c6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-tfSq05d4nkLkU_E-N75e.png",
                        "isPro": false,
                        "fullname": "Cong Xu",
                        "user": "NeilXu",
                        "type": "user"
                    },
                    "name": "Cong Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T09:16:48.232Z",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed3",
                    "name": "Zhenhua Guo",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed4",
                    "name": "Yaqian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed5",
                    "name": "Xiaoli Gong",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed6",
                    "name": "Rengang Li",
                    "hidden": false
                },
                {
                    "_id": "67cfd2d7bc539099da9ebed7",
                    "name": "Baoyu Fan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
            ],
            "publishedAt": "2025-03-08T04:37:38.000Z",
            "submittedOnDailyAt": "2025-03-18T05:40:31.378Z",
            "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
            "submittedOnDailyBy": {
                "_id": "66b01dc4e48856bb718f2ba8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
                "isPro": false,
                "fullname": "Xiaochuan Li",
                "user": "lixiaochuan",
                "type": "user"
            },
            "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
            "upvotes": 73,
            "discussionId": "67cfd2debc539099da9ec061",
            "ai_keywords": [
                "spatio-temporal consistency",
                "video generation",
                "plot plausibility",
                "visual consistency",
                "objects",
                "scenes",
                "viewpoints",
                "camera movement",
                "prompt",
                "narrative",
                "plot progression",
                "camera techniques",
                "long-term impact",
                "dataset construction",
                "DropletVideo-10M dataset",
                "dynamic camera motion",
                "object actions",
                "caption",
                "DropletVideo model",
                "spatio-temporal coherence"
            ]
        },
        "publishedAt": "2025-03-07T23:37:38.000Z",
        "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
        "summary": "Spatio-temporal consistency is a critical research topic in video generation.\nA qualified generated video segment must ensure plot plausibility and coherence\nwhile maintaining visual consistency of objects and scenes across varying\nviewpoints. Prior research, especially in open-source projects, primarily\nfocuses on either temporal or spatial consistency, or their basic combination,\nsuch as appending a description of a camera movement after a prompt without\nconstraining the outcomes of this movement. However, camera movement may\nintroduce new objects to the scene or eliminate existing ones, thereby\noverlaying and affecting the preceding narrative. Especially in videos with\nnumerous camera movements, the interplay between multiple plots becomes\nincreasingly complex. This paper introduces and examines integral\nspatio-temporal consistency, considering the synergy between plot progression\nand camera techniques, and the long-term impact of prior content on subsequent\ngeneration. Our research encompasses dataset construction through to the\ndevelopment of the model. Initially, we constructed a DropletVideo-10M dataset,\nwhich comprises 10 million videos featuring dynamic camera motion and object\nactions. Each video is annotated with an average caption of 206 words,\ndetailing various camera movements and plot developments. Following this, we\ndeveloped and trained the DropletVideo model, which excels in preserving\nspatio-temporal coherence during video generation. The DropletVideo dataset and\nmodel are accessible at https://dropletx.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66b01dc4e48856bb718f2ba8/_R9279HPwaWKB34BDJmAv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06053.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b01dc4e48856bb718f2ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
            "fullname": "Xiaochuan Li",
            "name": "lixiaochuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12533",
            "authors": [
                {
                    "_id": "67d8eadc045f869fea1ce3f2",
                    "user": {
                        "_id": "644560657a7b94ddc2d445a3",
                        "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg",
                        "isPro": false,
                        "fullname": "Haoqi Yuan",
                        "user": "Yaya041",
                        "type": "user"
                    },
                    "name": "Haoqi Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T11:33:52.255Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f3",
                    "name": "Yu Bai",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f4",
                    "user": {
                        "_id": "67d92b2218de6ef86c60f7d4",
                        "avatarUrl": "/avatars/9758522c99bc38bc7b60845eff8bf8d7.svg",
                        "isPro": false,
                        "fullname": "Yuhui Fu",
                        "user": "fuyh",
                        "type": "user"
                    },
                    "name": "Yuhui Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:17:46.443Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f5",
                    "name": "Bohan Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f6",
                    "user": {
                        "_id": "6655b86e607894ea80d74910",
                        "avatarUrl": "/avatars/663c0135c903c9c127fe1b8d8aaf279c.svg",
                        "isPro": false,
                        "fullname": "yicheng feng",
                        "user": "takenpeanut",
                        "type": "user"
                    },
                    "name": "Yicheng Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:17:31.771Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f7",
                    "user": {
                        "_id": "653238fdcd5377e9adee0c41",
                        "avatarUrl": "/avatars/78aea70cde6ab0050c7e18b5e148075c.svg",
                        "isPro": false,
                        "fullname": "Xinrun Xu",
                        "user": "SherryXu",
                        "type": "user"
                    },
                    "name": "Xinrun Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:17:05.200Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f8",
                    "name": "Yi Zhan",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3f9",
                    "user": {
                        "_id": "61e52be53d6dbb1da842316a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
                        "isPro": false,
                        "fullname": "Börje Karlsson",
                        "user": "tellarin",
                        "type": "user"
                    },
                    "name": "Börje F. Karlsson",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:34.005Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eadc045f869fea1ce3fa",
                    "user": {
                        "_id": "67d905c0e27ba28109384f5c",
                        "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg",
                        "isPro": false,
                        "fullname": "Zongqing Lu",
                        "user": "chungtsing",
                        "type": "user"
                    },
                    "name": "Zongqing Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:16:58.409Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T14:53:53.000Z",
            "submittedOnDailyAt": "2025-03-18T02:11:08.263Z",
            "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
            "submittedOnDailyBy": {
                "_id": "61e52be53d6dbb1da842316a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
                "isPro": false,
                "fullname": "Börje Karlsson",
                "user": "tellarin",
                "type": "user"
            },
            "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
            "upvotes": 51,
            "discussionId": "67d8eadd045f869fea1ce44a",
            "projectPage": "https://beingbeyond.github.io/Being-0/",
            "ai_keywords": [
                "Foundation Models (FMs)",
                "modular skill library",
                "high-level cognitive tasks",
                "instruction understanding",
                "task planning",
                "reasoning",
                "stable locomotion",
                "dexterous manipulation",
                "low-level control",
                "Connector module",
                "lightweight vision-language model (VLM",
                "embodied capabilities",
                "language-based plans",
                "actionable skill commands",
                "dynamic coordination",
                "full-sized humanoid robot",
                "dexterous hands",
                "active vision",
                "complex, long-horizon tasks",
                "challenging navigation",
                "manipulation subtasks"
            ]
        },
        "publishedAt": "2025-03-16T10:53:53.000Z",
        "title": "Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills",
        "summary": "Building autonomous robotic agents capable of achieving human-level\nperformance in real-world embodied tasks is an ultimate goal in humanoid robot\nresearch. Recent advances have made significant progress in high-level\ncognition with Foundation Models (FMs) and low-level skill development for\nhumanoid robots. However, directly combining these components often results in\npoor robustness and efficiency due to compounding errors in long-horizon tasks\nand the varied latency of different modules. We introduce Being-0, a\nhierarchical agent framework that integrates an FM with a modular skill\nlibrary. The FM handles high-level cognitive tasks such as instruction\nunderstanding, task planning, and reasoning, while the skill library provides\nstable locomotion and dexterous manipulation for low-level control. To bridge\nthe gap between these levels, we propose a novel Connector module, powered by a\nlightweight vision-language model (VLM). The Connector enhances the FM's\nembodied capabilities by translating language-based plans into actionable skill\ncommands and dynamically coordinating locomotion and manipulation to improve\ntask success. With all components, except the FM, deployable on low-cost\nonboard computation devices, Being-0 achieves efficient, real-time performance\non a full-sized humanoid robot equipped with dexterous hands and active vision.\nExtensive experiments in large indoor environments demonstrate Being-0's\neffectiveness in solving complex, long-horizon tasks that require challenging\nnavigation and manipulation subtasks. For further details and videos, visit\nhttps://beingbeyond.github.io/being-0.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12533.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "fullname": "Börje Karlsson",
            "name": "tellarin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12885",
            "authors": [
                {
                    "_id": "67d8e23afa59a8b15a9057e8",
                    "user": {
                        "_id": "65eaa1e2b11eeb516a973508",
                        "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
                        "isPro": false,
                        "fullname": "Dewei Zhou",
                        "user": "limuloo1999",
                        "type": "user"
                    },
                    "name": "Dewei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:20:39.038Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e23afa59a8b15a9057e9",
                    "user": {
                        "_id": "64551bc2c9c0dcc8c2484cf6",
                        "avatarUrl": "/avatars/0d1ed4f4502f6f54ac6ba071e4c9a220.svg",
                        "isPro": false,
                        "fullname": "Mingwei Li",
                        "user": "aiJojosh",
                        "type": "user"
                    },
                    "name": "Mingwei Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:20:46.810Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e23afa59a8b15a9057ea",
                    "user": {
                        "_id": "619bf9b3cbedb87e1a92fb3b",
                        "avatarUrl": "/avatars/ee280db0232e21416c948ab9a9a2344e.svg",
                        "isPro": false,
                        "fullname": "Zongxin Yang",
                        "user": "z-x-yang",
                        "type": "user"
                    },
                    "name": "Zongxin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:20:52.869Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e23afa59a8b15a9057eb",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T07:30:16.000Z",
            "submittedOnDailyAt": "2025-03-18T01:33:30.593Z",
            "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
            "submittedOnDailyBy": {
                "_id": "65eaa1e2b11eeb516a973508",
                "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
                "isPro": false,
                "fullname": "Dewei Zhou",
                "user": "limuloo1999",
                "type": "user"
            },
            "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
            "upvotes": 35,
            "discussionId": "67d8e23cfa59a8b15a9058ba",
            "projectPage": "https://limuloo.github.io/DreamRenderer/",
            "githubRepo": "https://github.com/limuloo/DreamRenderer",
            "ai_keywords": [
                "Bridge Image Tokens",
                "Hard Text Attribute Binding",
                "Replicated image tokens",
                "T5 text embeddings",
                "Joint Attention",
                "Hard Image Attribute Binding",
                "Vital layers",
                "Soft binding",
                "Image Success Ratio",
                "COCO-POS",
                "COCO-MIG",
                "layout-to-image models",
                "GLIGEN",
                "3DIS"
            ]
        },
        "publishedAt": "2025-03-17T03:30:16.000Z",
        "title": "DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models",
        "summary": "Image-conditioned generation methods, such as depth- and canny-conditioned\napproaches, have demonstrated remarkable abilities for precise image synthesis.\nHowever, existing models still struggle to accurately control the content of\nmultiple instances (or regions). Even state-of-the-art models like FLUX and\n3DIS face challenges, such as attribute leakage between instances, which limits\nuser control. To address these issues, we introduce DreamRenderer, a\ntraining-free approach built upon the FLUX model. DreamRenderer enables users\nto control the content of each instance via bounding boxes or masks, while\nensuring overall visual harmony. We propose two key innovations: 1) Bridge\nImage Tokens for Hard Text Attribute Binding, which uses replicated image\ntokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely\non text data, bind the correct visual attributes for each instance during Joint\nAttention; 2) Hard Image Attribute Binding applied only to vital layers.\nThrough our analysis of FLUX, we identify the critical layers responsible for\ninstance attribute rendering and apply Hard Image Attribute Binding only in\nthese layers, using soft binding in the others. This approach ensures precise\ncontrol while preserving image quality. Evaluations on the COCO-POS and\nCOCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success\nRatio by 17.7% over FLUX and enhances the performance of layout-to-image models\nlike GLIGEN and 3DIS by up to 26.8%. Project Page:\nhttps://limuloo.github.io/DreamRenderer/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12885.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65eaa1e2b11eeb516a973508",
            "avatarUrl": "/avatars/beecd135bb940fdc02406f9063b3fa67.svg",
            "fullname": "Dewei Zhou",
            "name": "limuloo1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12590",
            "authors": [
                {
                    "_id": "67d8de85f7809eea577c4805",
                    "name": "Haoran Feng",
                    "hidden": false
                },
                {
                    "_id": "67d8de85f7809eea577c4806",
                    "user": {
                        "_id": "6375d136dee28348a9c63cbf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
                        "isPro": false,
                        "fullname": "zehuan-huang",
                        "user": "huanngzh",
                        "type": "user"
                    },
                    "name": "Zehuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:06.131Z",
                    "hidden": false
                },
                {
                    "_id": "67d8de85f7809eea577c4807",
                    "name": "Lin Li",
                    "hidden": false
                },
                {
                    "_id": "67d8de85f7809eea577c4808",
                    "user": {
                        "_id": "674ded8ee50d988a4b9e108b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8oQITwlb7AB8LeIJjooYc.png",
                        "isPro": false,
                        "fullname": "Hairong Lv",
                        "user": "lvhairong",
                        "type": "user"
                    },
                    "name": "Hairong Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:22:23.294Z",
                    "hidden": false
                },
                {
                    "_id": "67d8de85f7809eea577c4809",
                    "name": "Lu Sheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T17:51:16.000Z",
            "submittedOnDailyAt": "2025-03-18T01:18:31.307Z",
            "title": "Personalize Anything for Free with Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "6375d136dee28348a9c63cbf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
                "isPro": false,
                "fullname": "zehuan-huang",
                "user": "huanngzh",
                "type": "user"
            },
            "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
            "upvotes": 23,
            "discussionId": "67d8de89f7809eea577c4930",
            "projectPage": "https://fenghora.github.io/Personalize-Anything-Page/",
            "githubRepo": "https://github.com/fenghora/personalize-anything",
            "ai_keywords": [
                "diffusion transformers (DiTs)",
                "denoising tokens",
                "zero-shot subject reconstruction",
                "timestep-adaptive token replacement",
                "early-stage injection",
                "late-stage regularization",
                "patch perturbation strategies",
                "layout-guided generation",
                "multi-subject personalization",
                "mask-controlled editing",
                "identity preservation",
                "versatility"
            ]
        },
        "publishedAt": "2025-03-16T13:51:16.000Z",
        "title": "Personalize Anything for Free with Diffusion Transformer",
        "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose Personalize\nAnything, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12590.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "fullname": "zehuan-huang",
            "name": "huanngzh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13327",
            "authors": [
                {
                    "_id": "67d8e00f0922c3dc8866520c",
                    "user": {
                        "_id": "640d704c8036cc2142299c19",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
                        "isPro": false,
                        "fullname": "Lan Chen",
                        "user": "Orannue",
                        "type": "user"
                    },
                    "name": "Lan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:21:18.982Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e00f0922c3dc8866520d",
                    "user": {
                        "_id": "6388a7e98a5dbe2f3dc61faa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg",
                        "isPro": false,
                        "fullname": "Qi Mao",
                        "user": "HelenMao",
                        "type": "user"
                    },
                    "name": "Qi Mao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:58:03.938Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e00f0922c3dc8866520e",
                    "user": {
                        "_id": "63021630a35b21bd8a53305a",
                        "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
                        "isPro": true,
                        "fullname": "Gu Yuchao",
                        "user": "guyuchao",
                        "type": "user"
                    },
                    "name": "Yuchao Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:21:44.702Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e00f0922c3dc8866520f",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:21:29.754Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T16:04:44.000Z",
            "submittedOnDailyAt": "2025-03-18T01:26:49.605Z",
            "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
            "submittedOnDailyBy": {
                "_id": "640d704c8036cc2142299c19",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
                "isPro": false,
                "fullname": "Lan Chen",
                "user": "Orannue",
                "type": "user"
            },
            "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
            "upvotes": 22,
            "discussionId": "67d8e0100922c3dc88665285",
            "projectPage": "https://cuc-mipg.github.io/EditTransfer.github.io",
            "githubRepo": "https://github.com/CUC-MIPG/Edit-Transfer",
            "ai_keywords": [
                "Edit Transfer",
                "visual relation in-context learning",
                "DiT-based text-to-image model",
                "four-panel composite",
                "LoRA fine-tuning",
                "few-shot visual relation learning",
                "non-rigid transformations",
                "TIE methods",
                "RIE methods"
            ]
        },
        "publishedAt": "2025-03-17T12:04:44.000Z",
        "title": "Edit Transfer: Learning Image Editing via Vision In-Context Relations",
        "summary": "We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13327.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "640d704c8036cc2142299c19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d704c8036cc2142299c19/Wt9AslcVxWOSSc11epk8l.jpeg",
            "fullname": "Lan Chen",
            "name": "Orannue",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12349",
            "authors": [
                {
                    "_id": "67d8e49df8b0e148f609c3a6",
                    "user": {
                        "_id": "660dc90fab11c96e148f95f1",
                        "avatarUrl": "/avatars/b59fe4b7c16602ea33887a7517ebacb0.svg",
                        "isPro": false,
                        "fullname": "Albert",
                        "user": "yyyyyyjjjjzzz",
                        "type": "user"
                    },
                    "name": "Jianzhu Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:54.531Z",
                    "hidden": true
                },
                {
                    "_id": "67d8e49df8b0e148f609c3a7",
                    "user": {
                        "_id": "66b979d9088299999ab714b7",
                        "avatarUrl": "/avatars/28f941da791e51788a4d8e2321c00ded.svg",
                        "isPro": false,
                        "fullname": "Kevin Wang",
                        "user": "Huanyu123",
                        "type": "user"
                    },
                    "name": "Kevin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:58:01.093Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e49df8b0e148f609c3a8",
                    "name": "Ryan Hsieh",
                    "hidden": false
                },
                {
                    "_id": "67d8e49df8b0e148f609c3a9",
                    "user": {
                        "_id": "672a86150957c590b21e49a2",
                        "avatarUrl": "/avatars/38b12f09af1ad994615b119cb3928404.svg",
                        "isPro": false,
                        "fullname": "Monica Zhou",
                        "user": "monicazhou1568",
                        "type": "user"
                    },
                    "name": "Haisu Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:43.752Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e49df8b0e148f609c3aa",
                    "name": "Tianqing Zou",
                    "hidden": false
                },
                {
                    "_id": "67d8e49df8b0e148f609c3ab",
                    "user": {
                        "_id": "67d8e5f041fc44bb16e5468c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jUCe0rf7Z-cF51OC5Iczn.png",
                        "isPro": false,
                        "fullname": "Zerui Cheng",
                        "user": "ZeruiCheng",
                        "type": "user"
                    },
                    "name": "Zerui Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:56.447Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e49df8b0e148f609c3ac",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "67d8e49df8b0e148f609c3ad",
                    "name": "Pramod Viswanath",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T04:10:53.000Z",
            "submittedOnDailyAt": "2025-03-18T12:21:01.652Z",
            "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
            "submittedOnDailyBy": {
                "_id": "660dc90fab11c96e148f95f1",
                "avatarUrl": "/avatars/b59fe4b7c16602ea33887a7517ebacb0.svg",
                "isPro": false,
                "fullname": "Albert",
                "user": "yyyyyyjjjjzzz",
                "type": "user"
            },
            "summary": "Reasoning and strategic behavior in social interactions is a hallmark\nof intelligence. This form of reasoning is significantly more sophisticated\nthan isolated planning or reasoning tasks in static settings (e.g., math\nproblem solving). In this paper, we present Strategic Planning,\nInteraction, and Negotiation (SPIN-Bench), a new multi-domain\nevaluation designed to measure the intelligence of strategic planning\nand social reasoning. While many existing benchmarks focus on narrow\nplanning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks,\ncompetitive board games, cooperative card games, and multi-agent negotiation\nscenarios in one unified framework. The framework includes both a benchmark as\nwell as an arena to simulate and evaluate the variety of social settings to\ntest reasoning and strategic behavior of AI agents. We formulate the benchmark\nSPIN-Bench by systematically varying action spaces, state complexity, and the\nnumber of interacting agents to simulate a variety of social settings where\nsuccess depends on not only methodical and step-wise decision making, but also\nconceptual inference of other (adversarial or cooperative) participants.\nOur experiments reveal that while contemporary LLMs handle basic fact\nretrieval and short-range planning reasonably well, they encounter\nsignificant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming.",
            "upvotes": 22,
            "discussionId": "67d8e49ff8b0e148f609c448",
            "projectPage": "https://spinbench.github.io/",
            "ai_keywords": [
                "Strategic Planning",
                "Interaction",
                "Negotiation",
                "SPIN-Bench",
                "multi-domain evaluation",
                "strategic planning",
                "social reasoning",
                "PDDL tasks",
                "competitive board games",
                "cooperative card games",
                "multi-agent negotiation scenarios",
                "action spaces",
                "state complexity",
                "conceptual inference",
                "basic fact retrieval",
                "short-range planning",
                "deep multi-hop reasoning",
                "socially adept coordination",
                "multi-agent planning",
                "human--AI teaming"
            ]
        },
        "publishedAt": "2025-03-16T00:10:53.000Z",
        "title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
        "summary": "Reasoning and strategic behavior in social interactions is a hallmark\nof intelligence. This form of reasoning is significantly more sophisticated\nthan isolated planning or reasoning tasks in static settings (e.g., math\nproblem solving). In this paper, we present Strategic Planning,\nInteraction, and Negotiation (SPIN-Bench), a new multi-domain\nevaluation designed to measure the intelligence of strategic planning\nand social reasoning. While many existing benchmarks focus on narrow\nplanning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks,\ncompetitive board games, cooperative card games, and multi-agent negotiation\nscenarios in one unified framework. The framework includes both a benchmark as\nwell as an arena to simulate and evaluate the variety of social settings to\ntest reasoning and strategic behavior of AI agents. We formulate the benchmark\nSPIN-Bench by systematically varying action spaces, state complexity, and the\nnumber of interacting agents to simulate a variety of social settings where\nsuccess depends on not only methodical and step-wise decision making, but also\nconceptual inference of other (adversarial or cooperative) participants.\nOur experiments reveal that while contemporary LLMs handle basic fact\nretrieval and short-range planning reasonably well, they encounter\nsignificant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12349.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "660dc90fab11c96e148f95f1",
            "avatarUrl": "/avatars/b59fe4b7c16602ea33887a7517ebacb0.svg",
            "fullname": "Albert",
            "name": "yyyyyyjjjjzzz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13434",
            "authors": [
                {
                    "_id": "67d916b500030726e0df2a67",
                    "user": {
                        "_id": "6362801380c1a705a6ea54ac",
                        "avatarUrl": "/avatars/041ad5abf9be42e336938f51ebb8746c.svg",
                        "isPro": false,
                        "fullname": "Yaowei Li",
                        "user": "Yw22",
                        "type": "user"
                    },
                    "name": "Yaowei Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:23:48.215Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a68",
                    "user": {
                        "_id": "66837d3c48edefb453b0640a",
                        "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
                        "isPro": false,
                        "fullname": "Lingen Li",
                        "user": "l-li",
                        "type": "user"
                    },
                    "name": "Lingen Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:23:54.498Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a69",
                    "user": {
                        "_id": "658409ceca19ccf6d9989add",
                        "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Zhang",
                        "user": "ZyZcuhk",
                        "type": "user"
                    },
                    "name": "Zhaoyang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:22.583Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a6a",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a6b",
                    "user": {
                        "_id": "6422b973ef9e8971003cdd22",
                        "avatarUrl": "/avatars/8564a2e984e2e79e46d90cc9c35e5773.svg",
                        "isPro": false,
                        "fullname": "Guangzhi Wang",
                        "user": "daoyuan98",
                        "type": "user"
                    },
                    "name": "Guangzhi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:24:18.787Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a6c",
                    "user": {
                        "_id": "653eb3bd4a52f10eaf72fbaf",
                        "avatarUrl": "/avatars/b525482b61c6f6054bf44bbc3113c29f.svg",
                        "isPro": false,
                        "fullname": "Hongxiang Li",
                        "user": "HongxiangLi",
                        "type": "user"
                    },
                    "name": "Hongxiang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:24:25.700Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a6d",
                    "user": {
                        "_id": "63184c517ca1b876d99b7e0e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
                        "isPro": false,
                        "fullname": "Xiaodong Cun",
                        "user": "vinthony",
                        "type": "user"
                    },
                    "name": "Xiaodong Cun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:24:32.118Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a6e",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:24:39.725Z",
                    "hidden": false
                },
                {
                    "_id": "67d916b500030726e0df2a6f",
                    "name": "Yuexian Zou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
            ],
            "publishedAt": "2025-03-17T17:58:05.000Z",
            "submittedOnDailyAt": "2025-03-18T05:20:30.708Z",
            "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
            "submittedOnDailyBy": {
                "_id": "658409ceca19ccf6d9989add",
                "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
                "isPro": false,
                "fullname": "Zhaoyang Zhang",
                "user": "ZyZcuhk",
                "type": "user"
            },
            "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
            "upvotes": 19,
            "discussionId": "67d916bc00030726e0df2c3e",
            "projectPage": "https://liyaowei-stu.github.io/project/BlobCtrl/",
            "githubRepo": "https://github.com/TencentARC/BlobCtrl",
            "ai_keywords": [
                "probabilistic blob-based representation",
                "dual-branch diffusion architecture",
                "hierarchical feature fusion",
                "self-supervised training paradigm",
                "tailored data augmentation",
                "score functions",
                "controllable dropout strategies",
                "BlobData",
                "BlobBench"
            ]
        },
        "publishedAt": "2025-03-17T13:58:05.000Z",
        "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing",
        "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658409ceca19ccf6d9989add/fhyGe83BiDhDf9a71kUyP.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13434.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "658409ceca19ccf6d9989add",
            "avatarUrl": "/avatars/3ac1dbd25e52435185babdeb3da28875.svg",
            "fullname": "Zhaoyang Zhang",
            "name": "ZyZcuhk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12937",
            "authors": [
                {
                    "_id": "67d8eb0c18de6ef86c4eb457",
                    "name": "Jingyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d8eb0c18de6ef86c4eb458",
                    "user": {
                        "_id": "65237910b80dc49ba03a96d9",
                        "avatarUrl": "/avatars/9d81c4c8fb2d597079e8dd9d9b79a8d8.svg",
                        "isPro": false,
                        "fullname": "jiaxing",
                        "user": "huangjiaxing",
                        "type": "user"
                    },
                    "name": "Jiaxing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:31:01.731Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eb0c18de6ef86c4eb459",
                    "user": {
                        "_id": "6590e03454f8826173ed5ee6",
                        "avatarUrl": "/avatars/b2fbaaf444e1e53c5e914cd42a41389a.svg",
                        "isPro": false,
                        "fullname": "Huanjin Yao",
                        "user": "HuanjinYao",
                        "type": "user"
                    },
                    "name": "Huanjin Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:31:08.069Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eb0c18de6ef86c4eb45a",
                    "user": {
                        "_id": "6713afea187a20dc579e121b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
                        "isPro": false,
                        "fullname": "Shunyu Liu",
                        "user": "liushunyu",
                        "type": "user"
                    },
                    "name": "Shunyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:31:15.587Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eb0c18de6ef86c4eb45b",
                    "user": {
                        "_id": "6274a9620d11b4f675085fbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651812606924-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xikun Zhang",
                        "user": "Xikun",
                        "type": "user"
                    },
                    "name": "Xikun Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:31:21.950Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eb0c18de6ef86c4eb45c",
                    "name": "Shijian Lu",
                    "hidden": false
                },
                {
                    "_id": "67d8eb0c18de6ef86c4eb45d",
                    "name": "Dacheng Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T08:51:44.000Z",
            "submittedOnDailyAt": "2025-03-18T02:10:10.429Z",
            "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
            "upvotes": 17,
            "discussionId": "67d8eb0d18de6ef86c4eb4aa",
            "ai_keywords": [
                "Step-wise Group Relative Policy Optimization (StepGRPO)",
                "online reinforcement learning",
                "Step-wise Reasoning Accuracy Reward (StepRAR)",
                "Step-wise Reasoning Validity Reward (StepRVR)",
                "soft key-step matching",
                "reasoning completeness",
                "logic evaluation",
                "R1-VL",
                "step-by-step reasoning"
            ]
        },
        "publishedAt": "2025-03-17T04:51:44.000Z",
        "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization",
        "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12937.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6392
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.12605",
            "authors": [
                {
                    "_id": "67d939f6fa59a8b15aa931a8",
                    "user": {
                        "_id": "64ff369d9abcc85a5519b33e",
                        "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
                        "isPro": false,
                        "fullname": "Yaoting Wang",
                        "user": "Gh0stAR",
                        "type": "user"
                    },
                    "name": "Yaoting Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:29:33.845Z",
                    "hidden": false
                },
                {
                    "_id": "67d939f6fa59a8b15aa931a9",
                    "user": {
                        "_id": "64c139d867eff857ea51caa8",
                        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                        "isPro": false,
                        "fullname": "Shengqiong Wu",
                        "user": "ChocoWu",
                        "type": "user"
                    },
                    "name": "Shengqiong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:29:41.386Z",
                    "hidden": false
                },
                {
                    "_id": "67d939f6fa59a8b15aa931aa",
                    "user": {
                        "_id": "6418554a0956be7233a1023e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
                        "isPro": false,
                        "fullname": "zhang yuechen",
                        "user": "julianjuaner",
                        "type": "user"
                    },
                    "name": "Yuecheng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:58:19.876Z",
                    "hidden": false
                },
                {
                    "_id": "67d939f6fa59a8b15aa931ab",
                    "name": "William Wang",
                    "hidden": false
                },
                {
                    "_id": "67d939f6fa59a8b15aa931ac",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:30:17.710Z",
                    "hidden": false
                },
                {
                    "_id": "67d939f6fa59a8b15aa931ad",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "67d939f6fa59a8b15aa931ae",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T09:30:49.867Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
            ],
            "publishedAt": "2025-03-16T18:39:13.000Z",
            "submittedOnDailyAt": "2025-03-18T07:53:47.429Z",
            "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
            "submittedOnDailyBy": {
                "_id": "64ff369d9abcc85a5519b33e",
                "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
                "isPro": false,
                "fullname": "Yaoting Wang",
                "user": "Gh0stAR",
                "type": "user"
            },
            "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI.",
            "upvotes": 17,
            "discussionId": "67d939f7fa59a8b15aa9322a",
            "projectPage": "https://github.com/yaotingwangofficial/Awesome-MCoT",
            "githubRepo": "https://github.com/yaotingwangofficial/Awesome-MCoT",
            "ai_keywords": [
                "chain-of-thought (CoT) reasoning",
                "multimodal CoT (MCoT) reasoning",
                "multimodal large language models (MLLMs)",
                "image",
                "video",
                "speech",
                "audio",
                "3D",
                "structured data",
                "robotics",
                "healthcare",
                "autonomous driving",
                "multimodal generation",
                "multimodal AGI"
            ]
        },
        "publishedAt": "2025-03-16T14:39:13.000Z",
        "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
        "summary": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like\nstep-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning\nhas recently garnered significant research attention, especially in the\nintegration with multimodal large language models (MLLMs). Existing MCoT\nstudies design various methodologies and innovative reasoning paradigms to\naddress the unique challenges of image, video, speech, audio, 3D, and\nstructured data across different modalities, achieving extensive success in\napplications such as robotics, healthcare, autonomous driving, and multimodal\ngeneration. However, MCoT still presents distinct challenges and opportunities\nthat require further focus to ensure consistent thriving in this field, where,\nunfortunately, an up-to-date review of this domain is lacking. To bridge this\ngap, we present the first systematic survey of MCoT reasoning, elucidating the\nrelevant foundational concepts and definitions. We offer a comprehensive\ntaxonomy and an in-depth analysis of current methodologies from diverse\nperspectives across various application scenarios. Furthermore, we provide\ninsights into existing challenges and future research directions, aiming to\nfoster innovation toward multimodal AGI.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/YhPHIT3BwUWkXcOT1r1LJ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/7SvBogepNT-uNYcY5dAgv.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64ff369d9abcc85a5519b33e/VqwQO1NsLKrjaKEBZxXUb.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12605.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "fullname": "Yaoting Wang",
            "name": "Gh0stAR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13435",
            "authors": [
                {
                    "_id": "67d8dd0b924be985c277c8f6",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:22:51.251Z",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8f7",
                    "user": {
                        "_id": "6708920aeae29d1cd41a703b",
                        "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg",
                        "isPro": false,
                        "fullname": "kaixin zhu",
                        "user": "czkk566",
                        "type": "user"
                    },
                    "name": "Kaixin Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T09:16:44.192Z",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8f8",
                    "user": {
                        "_id": "670880950e79a8b46f7ff9dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
                        "isPro": false,
                        "fullname": "Juanxi Tian",
                        "user": "Juanxi",
                        "type": "user"
                    },
                    "name": "Juanxi Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:12.128Z",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8f9",
                    "user": {
                        "_id": "6671214c92412fd4640714eb",
                        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                        "isPro": false,
                        "fullname": "bohan zeng",
                        "user": "zbhpku",
                        "type": "user"
                    },
                    "name": "Bohan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:15.654Z",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8fa",
                    "user": {
                        "_id": "64a2a8127adb12be606ec33e",
                        "avatarUrl": "/avatars/f85dc39a23727a4d50f8a5f5a3865b0d.svg",
                        "isPro": false,
                        "fullname": "Mingbao Lin",
                        "user": "mingbao",
                        "type": "user"
                    },
                    "name": "Mingbao Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:22:58.991Z",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8fb",
                    "name": "Hongjuan Pei",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8fc",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d8dd0b924be985c277c8fd",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T17:58:18.000Z",
            "submittedOnDailyAt": "2025-03-18T01:44:02.731Z",
            "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
            "upvotes": 16,
            "discussionId": "67d8dd0d924be985c277c998",
            "projectPage": "https://huggingface.co/datasets/Gen-Verse/WideRange4D",
            "githubRepo": "https://github.com/Gen-Verse/WideRange4D",
            "ai_keywords": [
                "deformation fields",
                "4D reconstruction",
                "scene reconstruction",
                "multi-view video",
                "spatial movements",
                "3D objects",
                "4D scene data",
                "generation capabilities",
                "4D generation methods",
                "WideRange4D",
                "Progress4D"
            ]
        },
        "publishedAt": "2025-03-17T13:58:18.000Z",
        "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
        "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13435.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13399",
            "authors": [
                {
                    "_id": "67d8d99a0983992037cdf33f",
                    "user": {
                        "_id": "650871aeb44445e9b3625c7b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
                        "isPro": false,
                        "fullname": "James Burgess",
                        "user": "jmhb",
                        "type": "user"
                    },
                    "name": "James Burgess",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:18.287Z",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf340",
                    "name": "Jeffrey J Nirschl",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf341",
                    "name": "Laura Bravo-Sánchez",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf342",
                    "name": "Alejandro Lozano",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf343",
                    "name": "Sanket Rajan Gupte",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf344",
                    "name": "Jesus G. Galaz-Montoya",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf345",
                    "name": "Yuhui Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf346",
                    "user": {
                        "_id": "666aa5183263a8feca6b7003",
                        "avatarUrl": "/avatars/6ac4d52e8abea0df9f83da408502c076.svg",
                        "isPro": false,
                        "fullname": "Yuchang Su",
                        "user": "suyc21",
                        "type": "user"
                    },
                    "name": "Yuchang Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:27:03.753Z",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf347",
                    "name": "Disha Bhowmik",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf348",
                    "name": "Zachary Coman",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf349",
                    "name": "Sarina M. Hasan",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf34a",
                    "name": "Alexandra Johannesson",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf34b",
                    "name": "William D. Leineweber",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf34c",
                    "name": "Malvika G Nair",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf34d",
                    "name": "Ridhi Yarlagadda",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf34e",
                    "name": "Connor Zuraski",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf34f",
                    "name": "Wah Chiu",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf350",
                    "user": {
                        "_id": "655b8dbb83186f133f7f8a98",
                        "avatarUrl": "/avatars/15f41d0efb3a59a2e389cdb5338e0c1e.svg",
                        "isPro": false,
                        "fullname": "Sarah Cohen",
                        "user": "shcohen",
                        "type": "user"
                    },
                    "name": "Sarah Cohen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:26:24.760Z",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf351",
                    "name": "Jan N. Hansen",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf352",
                    "name": "Manuel D Leonetti",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf353",
                    "user": {
                        "_id": "64fc5c1cc45dd732acc2ec48",
                        "avatarUrl": "/avatars/b1f072cbfec014f1a054d4a433cff93c.svg",
                        "isPro": false,
                        "fullname": "Chad Liu",
                        "user": "chadliu",
                        "type": "user"
                    },
                    "name": "Chad Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:25:42.249Z",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf354",
                    "user": {
                        "_id": "678af263320331c7e008f842",
                        "avatarUrl": "/avatars/e2cde80f018f3dd278000270fdbc104d.svg",
                        "isPro": false,
                        "fullname": "emma lundberg",
                        "user": "lundbergemma",
                        "type": "user"
                    },
                    "name": "Emma Lundberg",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:25:33.946Z",
                    "hidden": false
                },
                {
                    "_id": "67d8d99a0983992037cdf355",
                    "name": "Serena Yeung-Levy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T17:33:10.000Z",
            "submittedOnDailyAt": "2025-03-18T01:06:54.667Z",
            "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
            "submittedOnDailyBy": {
                "_id": "650871aeb44445e9b3625c7b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
                "isPro": false,
                "fullname": "James Burgess",
                "user": "jmhb",
                "type": "user"
            },
            "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
            "upvotes": 16,
            "discussionId": "67d8d99f0983992037cdf47e",
            "projectPage": "https://jmhb0.github.io/microvqa/",
            "githubRepo": "https://github.com/jmhb0/microvqa",
            "ai_keywords": [
                "multimodal large language models",
                "visual-question answering (VQA)",
                "multiple-choice questions (MCQs)",
                "biology experts",
                "microscopy modalities",
                "standard MCQ generation methods",
                "optimized LLM prompt",
                "agent-based `RefineBot'",
                "chain-of-thought responses",
                "perception errors",
                "knowledge errors",
                "overgeneralization errors"
            ]
        },
        "publishedAt": "2025-03-17T13:33:10.000Z",
        "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
        "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13399.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
            "fullname": "James Burgess",
            "name": "jmhb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11751",
            "authors": [
                {
                    "_id": "67d8e861fa59a8b15a921052",
                    "user": {
                        "_id": "6351712b40dffad651f128c7",
                        "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
                        "isPro": false,
                        "fullname": "Zhaofeng Wu",
                        "user": "ZhaofengWu",
                        "type": "user"
                    },
                    "name": "Zhaofeng Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:27:29.372Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e861fa59a8b15a921053",
                    "user": {
                        "_id": "621e9388345a1d9ab65391c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e9388345a1d9ab65391c3/RxurNzyAWJOUdgeSHQi1R.jpeg",
                        "isPro": false,
                        "fullname": "Michihiro Yasunaga",
                        "user": "michiyasunaga",
                        "type": "user"
                    },
                    "name": "Michihiro Yasunaga",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:27:35.318Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e861fa59a8b15a921054",
                    "name": "Andrew Cohen",
                    "hidden": false
                },
                {
                    "_id": "67d8e861fa59a8b15a921055",
                    "name": "Yoon Kim",
                    "hidden": false
                },
                {
                    "_id": "67d8e861fa59a8b15a921056",
                    "name": "Asli Celikyilmaz",
                    "hidden": false
                },
                {
                    "_id": "67d8e861fa59a8b15a921057",
                    "user": {
                        "_id": "660f0fd377a1e2509aa5a679",
                        "avatarUrl": "/avatars/e04ef05bed0bf6cefdc7e3e39674e2f9.svg",
                        "isPro": false,
                        "fullname": "Marjan Ghazvininejad",
                        "user": "mghazvininejad",
                        "type": "user"
                    },
                    "name": "Marjan Ghazvininejad",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:28:15.386Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T17:59:41.000Z",
            "submittedOnDailyAt": "2025-03-18T01:59:35.073Z",
            "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
            "submittedOnDailyBy": {
                "_id": "6351712b40dffad651f128c7",
                "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
                "isPro": false,
                "fullname": "Zhaofeng Wu",
                "user": "ZhaofengWu",
                "type": "user"
            },
            "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
            "upvotes": 14,
            "discussionId": "67d8e866fa59a8b15a92117c",
            "ai_keywords": [
                "reward models",
                "NLP",
                "text evaluator",
                "alignment",
                "inference-time algorithms",
                "overfitting",
                "reWordBench",
                "meaning-preserving",
                "ranking-preserving",
                "state-of-the-art",
                "performance degradation",
                "below-random accuracy",
                "brittleness",
                "paraphrases",
                "robust reward model",
                "Chat Hard subset",
                "RewardBench",
                "utility",
                "higher-quality outputs"
            ]
        },
        "publishedAt": "2025-03-14T13:59:41.000Z",
        "title": "reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs",
        "summary": "Reward models have become a staple in modern NLP, serving as not only a\nscalable text evaluator, but also an indispensable component in many alignment\nrecipes and inference-time algorithms. However, while recent reward models\nincrease performance on standard benchmarks, this may partly be due to\noverfitting effects, which would confound an understanding of their true\ncapability. In this work, we scrutinize the robustness of reward models and the\nextent of such overfitting. We build **reWordBench**, which systematically\ntransforms reward model inputs in meaning- or ranking-preserving ways. We show\nthat state-of-the-art reward models suffer from substantial performance\ndegradation even with minor input transformations, sometimes dropping to\nsignificantly below-random accuracy, suggesting brittleness. To improve reward\nmodel robustness, we propose to explicitly train them to assign similar scores\nto paraphrases, and find that this approach also improves robustness to other\ndistinct kinds of transformations. For example, our robust reward model reduces\nsuch degradation by roughly half for the Chat Hard subset in RewardBench.\nFurthermore, when used in alignment, our robust reward models demonstrate\nbetter utility and lead to higher-quality outputs, winning in up to 59% of\ninstances against a standardly trained RM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6351712b40dffad651f128c7",
            "avatarUrl": "/avatars/87708c86c1baef548ef556f5d32dca71.svg",
            "fullname": "Zhaofeng Wu",
            "name": "ZhaofengWu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11495",
            "authors": [
                {
                    "_id": "67d8ca56e94f1237cb3ba3ca",
                    "user": {
                        "_id": "667ee096b0fad0fdee319ed4",
                        "avatarUrl": "/avatars/d9df687e8522d47f7fcefe40fd9b575b.svg",
                        "isPro": false,
                        "fullname": "Zixu Cheng",
                        "user": "Cade921",
                        "type": "user"
                    },
                    "name": "Zixu Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:31:58.589Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ca56e94f1237cb3ba3cb",
                    "user": {
                        "_id": "65e1b6e9501590df0173cbd3",
                        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
                        "isPro": false,
                        "fullname": "Jian Hu",
                        "user": "lwpyh",
                        "type": "user"
                    },
                    "name": "Jian Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:20.643Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ca56e94f1237cb3ba3cc",
                    "name": "Ziquan Liu",
                    "hidden": false
                },
                {
                    "_id": "67d8ca56e94f1237cb3ba3cd",
                    "user": {
                        "_id": "635f8ed47c05eb9f59963d3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
                        "isPro": false,
                        "fullname": "ChenyangSi",
                        "user": "ChenyangSi",
                        "type": "user"
                    },
                    "name": "Chenyang Si",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:32:21.924Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ca56e94f1237cb3ba3ce",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67d8ca56e94f1237cb3ba3cf",
                    "name": "Shaogang Gong",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
            ],
            "publishedAt": "2025-03-14T15:21:44.000Z",
            "submittedOnDailyAt": "2025-03-18T01:11:08.161Z",
            "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
            "submittedOnDailyBy": {
                "_id": "65e1b6e9501590df0173cbd3",
                "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
                "isPro": false,
                "fullname": "Jian Hu",
                "user": "lwpyh",
                "type": "user"
            },
            "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
            "upvotes": 10,
            "discussionId": "67d8ca59e94f1237cb3ba47c",
            "ai_keywords": [
                "Video Large Language Models (Video-LLMs)",
                "Reverse Spatio-Temporal Reasoning (RSTR)",
                "Chain-of-thought (CoT)",
                "GPT-4",
                "Video Spatio-Temporal Reasoning (V-STaR)",
                "CoT questions",
                "CoT logic",
                "human cognition"
            ]
        },
        "publishedAt": "2025-03-14T11:21:44.000Z",
        "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
        "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65e1b6e9501590df0173cbd3/A2JrOm6FBAq5keCRRdVMK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11495.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e1b6e9501590df0173cbd3",
            "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
            "fullname": "Jian Hu",
            "name": "lwpyh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13082",
            "authors": [
                {
                    "_id": "67d948f741d31cc626fb0adb",
                    "name": "Runyu Jiao",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0adc",
                    "name": "Alice Fasoli",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0add",
                    "user": {
                        "_id": "6331d79818711776b469b4ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6331d79818711776b469b4ca/g8o6s_o3_YpO5wOS7vvOC.jpeg",
                        "isPro": false,
                        "fullname": "Francesco Giuliari",
                        "user": "FGiuliari",
                        "type": "user"
                    },
                    "name": "Francesco Giuliari",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T11:33:47.178Z",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0ade",
                    "name": "Matteo Bortolon",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0adf",
                    "user": {
                        "_id": "64febddf57a40de19101c302",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64febddf57a40de19101c302/_XWwNUV5QBL2PzRhOLaKJ.jpeg",
                        "isPro": false,
                        "fullname": "Sergio Povoli",
                        "user": "SPovoli",
                        "type": "user"
                    },
                    "name": "Sergio Povoli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T11:33:44.457Z",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0ae0",
                    "name": "Guofeng Mei",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0ae1",
                    "name": "Yiming Wang",
                    "hidden": false
                },
                {
                    "_id": "67d948f741d31cc626fb0ae2",
                    "user": {
                        "_id": "65762418769f3ee9bd5f223f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65762418769f3ee9bd5f223f/pk1g2ne31RMDVsPhmF-UQ.jpeg",
                        "isPro": false,
                        "fullname": "fabio poiesi",
                        "user": "fpoiesi",
                        "type": "user"
                    },
                    "name": "Fabio Poiesi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:57:58.696Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T11:41:16.000Z",
            "submittedOnDailyAt": "2025-03-18T08:59:41.278Z",
            "title": "Free-form language-based robotic reasoning and grasping",
            "submittedOnDailyBy": {
                "_id": "65762418769f3ee9bd5f223f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65762418769f3ee9bd5f223f/pk1g2ne31RMDVsPhmF-UQ.jpeg",
                "isPro": false,
                "fullname": "fabio poiesi",
                "user": "fpoiesi",
                "type": "user"
            },
            "summary": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.",
            "upvotes": 9,
            "discussionId": "67d9490441d31cc626fb0ecf",
            "projectPage": "https://tev-fbk.github.io/FreeGrasp/",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "GPT-4o",
                "free-form language",
                "zero-shot setting",
                "spatial reasoning",
                "keypoints",
                "synthetic dataset",
                "FreeGraspData",
                "MetaGraspNetV2",
                "human-annotated instructions",
                "ground-truth grasping sequences"
            ]
        },
        "publishedAt": "2025-03-17T07:41:16.000Z",
        "title": "Free-form language-based robotic reasoning and grasping",
        "summary": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13082.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65762418769f3ee9bd5f223f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65762418769f3ee9bd5f223f/pk1g2ne31RMDVsPhmF-UQ.jpeg",
            "fullname": "fabio poiesi",
            "name": "fpoiesi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13444",
            "authors": [
                {
                    "_id": "67d8eeb17e184aa2954d19f4",
                    "name": "Ye Liu",
                    "hidden": false
                },
                {
                    "_id": "67d8eeb17e184aa2954d19f5",
                    "user": {
                        "_id": "64440be5af034cdfd69ca3a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                        "isPro": true,
                        "fullname": "Qinghong (Kevin) Lin",
                        "user": "KevinQHLin",
                        "type": "user"
                    },
                    "name": "Kevin Qinghong Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:28:47.081Z",
                    "hidden": false
                },
                {
                    "_id": "67d8eeb17e184aa2954d19f6",
                    "name": "Chang Wen Chen",
                    "hidden": false
                },
                {
                    "_id": "67d8eeb17e184aa2954d19f7",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:29:07.309Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T17:59:33.000Z",
            "submittedOnDailyAt": "2025-03-18T02:25:58.731Z",
            "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
            "submittedOnDailyBy": {
                "_id": "64440be5af034cdfd69ca3a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
                "isPro": true,
                "fullname": "Qinghong (Kevin) Lin",
                "user": "KevinQHLin",
                "type": "user"
            },
            "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
            "upvotes": 8,
            "discussionId": "67d8eeb37e184aa2954d1a39",
            "ai_keywords": [
                "VideoMind",
                "temporal-grounded video understanding",
                "role-based agentic workflow",
                "planner",
                "grounder",
                "temporale localization",
                "verifier",
                "temporal interval accuracy",
                "answerer",
                "question-answering",
                "Chain-of-LoRA",
                "LoRA adaptors",
                "grounded video question-answering",
                "video temporal grounding",
                "general video question-answering",
                "temporal reasoning"
            ]
        },
        "publishedAt": "2025-03-17T13:59:33.000Z",
        "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
        "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13444.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "fullname": "Qinghong (Kevin) Lin",
            "name": "KevinQHLin",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11412",
            "authors": [
                {
                    "_id": "67d8f8b77f61dda9ea6512b7",
                    "name": "Shiyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67d8f8b77f61dda9ea6512b8",
                    "user": {
                        "_id": "678f49878af7a399877b87c0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3IHnM4AKbIW_wmL15wdZf.png",
                        "isPro": false,
                        "fullname": "GuZheng",
                        "user": "GuZheng",
                        "type": "user"
                    },
                    "name": "Zheng Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:33:49.096Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f8b77f61dda9ea6512b9",
                    "user": {
                        "_id": "64560a2aaaaf85a98fa9a4b9",
                        "avatarUrl": "/avatars/e81e21f353baf48f0d91bf29ad200eea.svg",
                        "isPro": false,
                        "fullname": "Liang Hou",
                        "user": "lianghou",
                        "type": "user"
                    },
                    "name": "Liang Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:33:57.501Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f8b77f61dda9ea6512ba",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "67d8f8b77f61dda9ea6512bb",
                    "user": {
                        "_id": "662f93942510ef5735d7ad00",
                        "avatarUrl": "/avatars/dc9486db75869ce902d0a638eea126bd.svg",
                        "isPro": false,
                        "fullname": "magicwpf",
                        "user": "magicwpf",
                        "type": "user"
                    },
                    "name": "Pengfei Wan",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-18T04:42:06.437Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f8b77f61dda9ea6512bc",
                    "user": {
                        "_id": "67be7cd21616162fc336cb44",
                        "avatarUrl": "/avatars/e58cc3c2d1484419222a5ccfc11f5c48.svg",
                        "isPro": false,
                        "fullname": "Xiaodong Chen",
                        "user": "XiaodongChen",
                        "type": "user"
                    },
                    "name": "Xiaodong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:34:15.858Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f8b77f61dda9ea6512bd",
                    "user": {
                        "_id": "65e77726767bfc7d109c45bf",
                        "avatarUrl": "/avatars/24e68c86e06055ea1209598ba49ce8b9.svg",
                        "isPro": false,
                        "fullname": "Jing Liao",
                        "user": "CeciliaJL",
                        "type": "user"
                    },
                    "name": "Jing Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:34:23.653Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
            ],
            "publishedAt": "2025-03-14T13:54:10.000Z",
            "submittedOnDailyAt": "2025-03-18T03:20:01.050Z",
            "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
            "submittedOnDailyBy": {
                "_id": "63316d499e3604f3f17f5d89",
                "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
                "isPro": false,
                "fullname": "catfood",
                "user": "ysy31415926",
                "type": "user"
            },
            "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
            "upvotes": 7,
            "discussionId": "67d8f8bf7f61dda9ea6514a7",
            "projectPage": "https://mtv-inpaint.github.io/",
            "ai_keywords": [
                "text-to-video (T2V) diffusion models",
                "text-guided video inpainting",
                "dual-branch spatial attention mechanism",
                "T2V diffusion U-Net",
                "multimodal control",
                "image-to-video (I2V) inpainting mode",
                "keyframe inpainting",
                "in-between frame propagation",
                "multi-modal inpainting",
                "object editing",
                "object removal",
                "image object brush"
            ]
        },
        "publishedAt": "2025-03-14T09:54:10.000Z",
        "title": "MTV-Inpaint: Multi-Task Long Video Inpainting",
        "summary": "Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63316d499e3604f3f17f5d89/jV8ETjZh0NTDoFBhSNv8I.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11412.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63316d499e3604f3f17f5d89",
            "avatarUrl": "/avatars/c7e0c8bdd7e598276cfd133d5222c5e8.svg",
            "fullname": "catfood",
            "name": "ysy31415926",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.13070",
            "authors": [
                {
                    "_id": "67d8fbf641d31cc626e4d7b9",
                    "user": {
                        "_id": "65f7e6856bd4bac5b6a4ecc3",
                        "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
                        "isPro": false,
                        "fullname": "Yihong Luo",
                        "user": "Luo-Yihong",
                        "type": "user"
                    },
                    "name": "Yihong Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:30.236Z",
                    "hidden": false
                },
                {
                    "_id": "67d8fbf641d31cc626e4d7ba",
                    "user": {
                        "_id": "636a40faa6f948c4f0c62ae5",
                        "avatarUrl": "/avatars/30c35b194ba84d6e274df30e91a8cc45.svg",
                        "isPro": false,
                        "fullname": "Tianyang Hu",
                        "user": "whatlegequ",
                        "type": "user"
                    },
                    "name": "Tianyang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:33:01.489Z",
                    "hidden": false
                },
                {
                    "_id": "67d8fbf641d31cc626e4d7bb",
                    "name": "Weijian Luo",
                    "hidden": false
                },
                {
                    "_id": "67d8fbf641d31cc626e4d7bc",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                },
                {
                    "_id": "67d8fbf641d31cc626e4d7bd",
                    "name": "Jing Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T11:21:43.000Z",
            "submittedOnDailyAt": "2025-03-18T03:24:24.806Z",
            "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
            "submittedOnDailyBy": {
                "_id": "65f7e6856bd4bac5b6a4ecc3",
                "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
                "isPro": false,
                "fullname": "Yihong Luo",
                "user": "Luo-Yihong",
                "type": "user"
            },
            "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
            "upvotes": 6,
            "discussionId": "67d8fbf841d31cc626e4d812",
            "githubRepo": "https://github.com/Luo-Yihong/R0",
            "ai_keywords": [
                "reward-enhanced diffusion distillation",
                "diffusion losses",
                "R0",
                "regularized reward maximization",
                "optimization problem in data space",
                "compositional rewards",
                "generator parameterization",
                "state-of-the-art few-step text-to-image generative models",
                "diffusion post-training",
                "human-centric generation",
                "reward-centric generation paradigms"
            ]
        },
        "publishedAt": "2025-03-17T07:21:43.000Z",
        "title": "Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation",
        "summary": "Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13070.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "fullname": "Yihong Luo",
            "name": "Luo-Yihong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10719",
            "authors": [
                {
                    "_id": "67d91dadb533888991ade4e1",
                    "user": {
                        "_id": "672c6f3d4c1e2de12c6f174e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
                        "isPro": false,
                        "fullname": "Yehang Zhang",
                        "user": "Buzz-lightyear",
                        "type": "user"
                    },
                    "name": "Yehang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:07:19.937Z",
                    "hidden": false
                },
                {
                    "_id": "67d91dadb533888991ade4e2",
                    "user": {
                        "_id": "64b4ab62eec33e27dcd733b5",
                        "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
                        "isPro": false,
                        "fullname": "Xinli XU",
                        "user": "Xxlbigbrother",
                        "type": "user"
                    },
                    "name": "Xinli Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:35:50.412Z",
                    "hidden": false
                },
                {
                    "_id": "67d91dadb533888991ade4e3",
                    "name": "Xiaojie Xu",
                    "hidden": false
                },
                {
                    "_id": "67d91dadb533888991ade4e4",
                    "name": "Li Liu",
                    "hidden": false
                },
                {
                    "_id": "67d91dadb533888991ade4e5",
                    "user": {
                        "_id": "655cba1d87b67834000590e8",
                        "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
                        "isPro": false,
                        "fullname": "Yingcong Chen",
                        "user": "yingcongchen",
                        "type": "user"
                    },
                    "name": "Yingcong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:36:19.820Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T07:58:23.000Z",
            "submittedOnDailyAt": "2025-03-18T06:41:21.358Z",
            "title": "Long-Video Audio Synthesis with Multi-Agent Collaboration",
            "submittedOnDailyBy": {
                "_id": "672c6f3d4c1e2de12c6f174e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
                "isPro": false,
                "fullname": "Yehang Zhang",
                "user": "Buzz-lightyear",
                "type": "user"
            },
            "summary": "Video-to-audio synthesis, which generates synchronized audio for visual\ncontent, critically enhances viewer immersion and narrative coherence in film\nand interactive media. However, video-to-audio dubbing for long-form content\nremains an unsolved challenge due to dynamic semantic shifts, temporal\nmisalignment, and the absence of dedicated datasets. While existing methods\nexcel in short videos, they falter in long scenarios (e.g., movies) due to\nfragmented synthesis and inadequate cross-scene consistency. We propose\nLVAS-Agent, a novel multi-agent framework that emulates professional dubbing\nworkflows through collaborative role specialization. Our approach decomposes\nlong-video synthesis into four steps including scene segmentation, script\ngeneration, sound design and audio synthesis. Central innovations include a\ndiscussion-correction mechanism for scene/script refinement and a\ngeneration-retrieval loop for temporal-semantic alignment. To enable systematic\nevaluation, we introduce LVAS-Bench, the first benchmark with 207\nprofessionally curated long videos spanning diverse scenarios. Experiments\ndemonstrate superior audio-visual alignment over baseline methods. Project\npage: https://lvas-agent.github.io",
            "upvotes": 6,
            "discussionId": "67d91dafb533888991ade557",
            "projectPage": "https://lvas-agent.github.io/",
            "githubRepo": "https://github.com/ZYH-Lightyear/LVAS",
            "ai_keywords": [
                "scene segmentation",
                "script generation",
                "sound design",
                "audio synthesis",
                "discussion-correction mechanism",
                "generation-retrieval loop",
                "temporal-semantic alignment",
                "LVAS-Agent",
                "LVAS-Bench",
                "audio-visual alignment"
            ]
        },
        "publishedAt": "2025-03-13T03:58:23.000Z",
        "title": "Long-Video Audio Synthesis with Multi-Agent Collaboration",
        "summary": "Video-to-audio synthesis, which generates synchronized audio for visual\ncontent, critically enhances viewer immersion and narrative coherence in film\nand interactive media. However, video-to-audio dubbing for long-form content\nremains an unsolved challenge due to dynamic semantic shifts, temporal\nmisalignment, and the absence of dedicated datasets. While existing methods\nexcel in short videos, they falter in long scenarios (e.g., movies) due to\nfragmented synthesis and inadequate cross-scene consistency. We propose\nLVAS-Agent, a novel multi-agent framework that emulates professional dubbing\nworkflows through collaborative role specialization. Our approach decomposes\nlong-video synthesis into four steps including scene segmentation, script\ngeneration, sound design and audio synthesis. Central innovations include a\ndiscussion-correction mechanism for scene/script refinement and a\ngeneration-retrieval loop for temporal-semantic alignment. To enable systematic\nevaluation, we introduce LVAS-Bench, the first benchmark with 207\nprofessionally curated long videos spanning diverse scenarios. Experiments\ndemonstrate superior audio-visual alignment over baseline methods. Project\npage: https://lvas-agent.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10719.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "672c6f3d4c1e2de12c6f174e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mv-8GX6OBBpJLzVvyPYbz.png",
            "fullname": "Yehang Zhang",
            "name": "Buzz-lightyear",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10704",
            "authors": [
                {
                    "_id": "67d7eba831dd5b46c3e6fdcb",
                    "user": {
                        "_id": "633c2310c0fb6fd232f0accf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
                        "isPro": false,
                        "fullname": "Wang Jing",
                        "user": "k-nick",
                        "type": "user"
                    },
                    "name": "Jing Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:09:01.169Z",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdcc",
                    "user": {
                        "_id": "64b8c1a995bd42c7707f7918",
                        "avatarUrl": "/avatars/08c2929f8f150ecd6f8e5a06c4cb9034.svg",
                        "isPro": false,
                        "fullname": "Fengzhuo Zhang",
                        "user": "Fengzhuo",
                        "type": "user"
                    },
                    "name": "Fengzhuo Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:34:37.712Z",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdcd",
                    "user": {
                        "_id": "67aa01782183876b1ec5760f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hZd1iBn_2yjHVoavcXPQo.png",
                        "isPro": false,
                        "fullname": "xiaolili",
                        "user": "xiaolili",
                        "type": "user"
                    },
                    "name": "Xiaoli Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:34:47.006Z",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdce",
                    "name": "Vincent Y. F. Tan",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdcf",
                    "user": {
                        "_id": "661a4a556fb488fa078c60aa",
                        "avatarUrl": "/avatars/c77401fa9c6d2db896b4a337bb3f8add.svg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "TIanyupang",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:35:17.732Z",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdd0",
                    "user": {
                        "_id": "632407c892e07e3ca20aca28",
                        "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
                        "isPro": false,
                        "fullname": "Chao Du",
                        "user": "duchao",
                        "type": "user"
                    },
                    "name": "Chao Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:35:29.239Z",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdd1",
                    "user": {
                        "_id": "664aab898fa42b4fe70ebf52",
                        "avatarUrl": "/avatars/a38455fd17bbc74ce3111f2c3da9aa59.svg",
                        "isPro": false,
                        "fullname": "Aixin Sun",
                        "user": "aixinsun",
                        "type": "user"
                    },
                    "name": "Aixin Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:35:35.567Z",
                    "hidden": false
                },
                {
                    "_id": "67d7eba831dd5b46c3e6fdd2",
                    "user": {
                        "_id": "6397873ec0b27f432db8693f",
                        "avatarUrl": "/avatars/1db65fe55002ad5c137c4a59bbcd239d.svg",
                        "isPro": false,
                        "fullname": "Zhuoran Yang",
                        "user": "zhuoran",
                        "type": "user"
                    },
                    "name": "Zhuoran Yang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-17T09:30:18.764Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T15:32:44.000Z",
            "submittedOnDailyAt": "2025-03-18T03:10:46.254Z",
            "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
            "submittedOnDailyBy": {
                "_id": "633c2310c0fb6fd232f0accf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
                "isPro": false,
                "fullname": "Wang Jing",
                "user": "k-nick",
                "type": "user"
            },
            "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
            "upvotes": 5,
            "discussionId": "67d7ebaa31dd5b46c3e6fe5a",
            "projectPage": "https://sail-sg.github.io/AR-Video-Diffusion",
            "ai_keywords": [
                "Auto-Regressive Video Diffusion Models",
                "Meta-ARVDM",
                "KL-divergence",
                "error accumulation",
                "memory bottleneck",
                "information-theoretic impossibility result",
                "network structures",
                "frame compression",
                "DMLab",
                "Minecraft",
                "Pareto-frontier"
            ]
        },
        "publishedAt": "2025-03-12T11:32:44.000Z",
        "title": "Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework",
        "summary": "A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved\nremarkable successes in generating realistic long-form videos. However,\ntheoretical analyses of these models remain scant. In this work, we develop\ntheoretical underpinnings for these models and use our insights to improve the\nperformance of existing models. We first develop Meta-ARVDM, a unified\nframework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we\nanalyze the KL-divergence between the videos generated by Meta-ARVDM and the\ntrue videos. Our analysis uncovers two important phenomena inherent to ARVDM --\nerror accumulation and memory bottleneck. By deriving an information-theoretic\nimpossibility result, we show that the memory bottleneck phenomenon cannot be\navoided. To mitigate the memory bottleneck, we design various network\nstructures to explicitly use more past frames. We also achieve a significantly\nimproved trade-off between the mitigation of the memory bottleneck and the\ninference efficiency by compressing the frames. Experimental results on DMLab\nand Minecraft validate the efficacy of our methods. Our experiments also\ndemonstrate a Pareto-frontier between the error accumulation and memory\nbottleneck across different methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10704.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633c2310c0fb6fd232f0accf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633c2310c0fb6fd232f0accf/AnvEvxMtd-BpSZbaoA4ha.jpeg",
            "fullname": "Wang Jing",
            "name": "k-nick",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13369",
            "authors": [
                {
                    "_id": "67d8e38e5fde3b1be16874e4",
                    "user": {
                        "_id": "64b214c4f4361a032002cdcf",
                        "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
                        "isPro": false,
                        "fullname": "Andrew Wan Ju Kang",
                        "user": "soarhigh",
                        "type": "user"
                    },
                    "name": "Wan Ju Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:01.967Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e38e5fde3b1be16874e5",
                    "user": {
                        "_id": "628e3b87a2cb9819d4391ba6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653488512816-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Eunki Kim",
                        "user": "eunkey",
                        "type": "user"
                    },
                    "name": "Eunki Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:36:36.532Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e38e5fde3b1be16874e6",
                    "user": {
                        "_id": "65cccd8c80d3c4b865d3b262",
                        "avatarUrl": "/avatars/e6f6d8f06dd54e1e7b6d686835a9c075.svg",
                        "isPro": false,
                        "fullname": "Na Min An",
                        "user": "namin0202",
                        "type": "user"
                    },
                    "name": "Na Min An",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:36:43.821Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e38e5fde3b1be16874e7",
                    "user": {
                        "_id": "62f2638d04674e28535d40f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672818467177-62f2638d04674e28535d40f8.png",
                        "isPro": false,
                        "fullname": "Sangryul Kim",
                        "user": "sangryul",
                        "type": "user"
                    },
                    "name": "Sangryul Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:36:49.867Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e38e5fde3b1be16874e8",
                    "user": {
                        "_id": "67a86f66c6f66e2fa5888b41",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a86f66c6f66e2fa5888b41/P2d1vJ_iGn5QWvOYxn7pc.jpeg",
                        "isPro": false,
                        "fullname": "Haemin Choi",
                        "user": "Jaime-Choi",
                        "type": "user"
                    },
                    "name": "Haemin Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T11:34:30.468Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e38e5fde3b1be16874e9",
                    "name": "Ki Hoon Kwak",
                    "hidden": false
                },
                {
                    "_id": "67d8e38e5fde3b1be16874ea",
                    "name": "James Thorne",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T16:52:46.000Z",
            "submittedOnDailyAt": "2025-03-18T06:53:48.028Z",
            "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
            "submittedOnDailyBy": {
                "_id": "64b214c4f4361a032002cdcf",
                "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
                "isPro": false,
                "fullname": "Andrew Wan Ju Kang",
                "user": "soarhigh",
                "type": "user"
            },
            "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
            "upvotes": 3,
            "discussionId": "67d8e3905fde3b1be168759f",
            "projectPage": "https://huggingface.co/Sightation",
            "ai_keywords": [
                "vision-language models (VLM)",
                "latent supervision",
                "multi-pass inference",
                "diagram description datasets",
                "fine-tuning"
            ]
        },
        "publishedAt": "2025-03-17T12:52:46.000Z",
        "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
        "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13369.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b214c4f4361a032002cdcf",
            "avatarUrl": "/avatars/3188a4edec4d6c945a0ed9f36050a03c.svg",
            "fullname": "Andrew Wan Ju Kang",
            "name": "soarhigh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12964",
            "authors": [
                {
                    "_id": "67d8d29fbe55b912664641d7",
                    "name": "Zeeshan Patel",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641d8",
                    "name": "Ethan He",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641d9",
                    "name": "Parth Mannan",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641da",
                    "name": "Xiaowei Ren",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641db",
                    "name": "Ryan Wolf",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641dc",
                    "name": "Niket Agarwal",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641dd",
                    "name": "Jacob Huffman",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641de",
                    "name": "Zhuoyao Wang",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641df",
                    "name": "Carl Wang",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e0",
                    "name": "Jack Chang",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e1",
                    "name": "Yan Bai",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e2",
                    "name": "Tommy Huang",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e3",
                    "name": "Linnan Wang",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e4",
                    "name": "Sahil Jain",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e5",
                    "name": "Shanmugam Ramasamy",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e6",
                    "name": "Joseph Jennings",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e7",
                    "name": "Ekaterina Sirazitdinova",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e8",
                    "name": "Oleg Sudakov",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641e9",
                    "name": "Mingyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641ea",
                    "name": "Bobby Chen",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641eb",
                    "name": "Forrest Lin",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641ec",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641ed",
                    "name": "Vasanth Rao Naik Sabavat",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641ee",
                    "name": "Sriharsha Niverty",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641ef",
                    "name": "Rong Ou",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641f0",
                    "name": "Pallab Bhattacharya",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641f1",
                    "name": "David Page",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641f2",
                    "name": "Nima Tajbakhsh",
                    "hidden": false
                },
                {
                    "_id": "67d8d29fbe55b912664641f3",
                    "name": "Ashwath Aithal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T09:19:12.000Z",
            "submittedOnDailyAt": "2025-03-18T18:27:57.760Z",
            "title": "Training Video Foundation Models with NVIDIA NeMo",
            "submittedOnDailyBy": {
                "_id": "638ee4df6251c8bd7abd246e",
                "avatarUrl": "/avatars/8d8c4dac7d82cba6dcf884460bdc5328.svg",
                "isPro": false,
                "fullname": "Zeeshan Patel",
                "user": "zeeshanp",
                "type": "user"
            },
            "summary": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.",
            "upvotes": 2,
            "discussionId": "67d8d2a3be55b912664642f0",
            "githubRepo": "https://github.com/NVIDIA/NeMo/tree/main/nemo/collections/diffusion",
            "ai_keywords": [
                "Video Foundation Models (VFMs)",
                "video diffusion model",
                "NVIDIA NeMo",
                "accelerated video dataset curation",
                "multimodal data loading",
                "parallelized video diffusion model training",
                "inference",
                "performance analysis"
            ]
        },
        "publishedAt": "2025-03-17T05:19:12.000Z",
        "title": "Training Video Foundation Models with NVIDIA NeMo",
        "summary": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12964.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638ee4df6251c8bd7abd246e",
            "avatarUrl": "/avatars/8d8c4dac7d82cba6dcf884460bdc5328.svg",
            "fullname": "Zeeshan Patel",
            "name": "zeeshanp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.12530",
            "authors": [
                {
                    "_id": "67d8f0fa53f713733d6c6b1c",
                    "user": {
                        "_id": "6737d99b728a96aa64a2b00a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p3vMX0xkvL_4IpWwyURLM.png",
                        "isPro": false,
                        "fullname": "Hunter Sawyer",
                        "user": "HTSawyer",
                        "type": "user"
                    },
                    "name": "Hunter Sawyer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:37:16.399Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f0fa53f713733d6c6b1d",
                    "user": {
                        "_id": "63c19eb3a0ffa3857eae2efa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c19eb3a0ffa3857eae2efa/pB-CbPa1QFTImiHanNCxU.jpeg",
                        "isPro": false,
                        "fullname": "Jesse Roberts",
                        "user": "JesseTNRoberts",
                        "type": "user"
                    },
                    "name": "Jesse Roberts",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-18T04:05:16.739Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f0fa53f713733d6c6b1e",
                    "user": {
                        "_id": "675ec603e4d6d0e820ad9d3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ruPDRCZlcbDhZ6zp4xi_P.png",
                        "isPro": false,
                        "fullname": "Olzhas",
                        "user": "KyleMoore",
                        "type": "user"
                    },
                    "name": "Kyle Moore",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-18T09:37:25.148Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T14:50:54.000Z",
            "submittedOnDailyAt": "2025-03-18T02:37:23.781Z",
            "title": "Basic Category Usage in Vision Language Models",
            "submittedOnDailyBy": {
                "_id": "63c19eb3a0ffa3857eae2efa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c19eb3a0ffa3857eae2efa/pB-CbPa1QFTImiHanNCxU.jpeg",
                "isPro": false,
                "fullname": "Jesse Roberts",
                "user": "JesseTNRoberts",
                "type": "user"
            },
            "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
            "upvotes": 2,
            "discussionId": "67d8f0fc53f713733d6c6b89",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "basic level categorization",
                "biological versus non-biological basic level effects",
                "expert basic level shift",
                "cognitive categorization behaviors"
            ]
        },
        "publishedAt": "2025-03-16T10:50:54.000Z",
        "title": "Basic Category Usage in Vision Language Models",
        "summary": "The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12530.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c19eb3a0ffa3857eae2efa/pB-CbPa1QFTImiHanNCxU.jpeg",
            "fullname": "Jesse Roberts",
            "name": "JesseTNRoberts",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12528",
            "authors": [
                {
                    "_id": "67d8f044f8b0e148f60cef0d",
                    "name": "Kyle Moore",
                    "hidden": false
                },
                {
                    "_id": "67d8f044f8b0e148f60cef0e",
                    "user": {
                        "_id": "63c19eb3a0ffa3857eae2efa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c19eb3a0ffa3857eae2efa/pB-CbPa1QFTImiHanNCxU.jpeg",
                        "isPro": false,
                        "fullname": "Jesse Roberts",
                        "user": "JesseTNRoberts",
                        "type": "user"
                    },
                    "name": "Jesse Roberts",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-18T04:02:14.260Z",
                    "hidden": false
                },
                {
                    "_id": "67d8f044f8b0e148f60cef0f",
                    "name": "Daryl Watson",
                    "hidden": false
                },
                {
                    "_id": "67d8f044f8b0e148f60cef10",
                    "name": "Pamela Wisniewski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T14:45:43.000Z",
            "submittedOnDailyAt": "2025-03-18T02:34:14.582Z",
            "title": "Investigating Human-Aligned Large Language Model Uncertainty",
            "submittedOnDailyBy": {
                "_id": "63c19eb3a0ffa3857eae2efa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c19eb3a0ffa3857eae2efa/pB-CbPa1QFTImiHanNCxU.jpeg",
                "isPro": false,
                "fullname": "Jesse Roberts",
                "user": "JesseTNRoberts",
                "type": "user"
            },
            "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
            "upvotes": 2,
            "discussionId": "67d8f046f8b0e148f60cef95",
            "ai_keywords": [
                "Bayesian measures",
                "entropy measures",
                "top-k entropy",
                "human-similarity",
                "human-alignment",
                "multiple linear regression"
            ]
        },
        "publishedAt": "2025-03-16T10:45:43.000Z",
        "title": "Investigating Human-Aligned Large Language Model Uncertainty",
        "summary": "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12528.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c19eb3a0ffa3857eae2efa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c19eb3a0ffa3857eae2efa/pB-CbPa1QFTImiHanNCxU.jpeg",
            "fullname": "Jesse Roberts",
            "name": "JesseTNRoberts",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12720",
            "authors": [
                {
                    "_id": "67d985207fadfdd9d2a8fb93",
                    "user": {
                        "_id": "6487e839341b4c7ec11013e5",
                        "avatarUrl": "/avatars/e53a310f40c79bc8ba8f12027adf6a28.svg",
                        "isPro": true,
                        "fullname": "Feng Qiao",
                        "user": "FQiao",
                        "type": "user"
                    },
                    "name": "Feng Qiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:57:40.079Z",
                    "hidden": false
                },
                {
                    "_id": "67d985207fadfdd9d2a8fb94",
                    "name": "Zhexiao Xiong",
                    "hidden": false
                },
                {
                    "_id": "67d985207fadfdd9d2a8fb95",
                    "name": "Eric Xing",
                    "hidden": false
                },
                {
                    "_id": "67d985207fadfdd9d2a8fb96",
                    "name": "Nathan Jacobs",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T01:19:28.000Z",
            "submittedOnDailyAt": "2025-03-18T13:32:11.285Z",
            "title": "GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching",
            "submittedOnDailyBy": {
                "_id": "6487e839341b4c7ec11013e5",
                "avatarUrl": "/avatars/e53a310f40c79bc8ba8f12027adf6a28.svg",
                "isPro": true,
                "fullname": "Feng Qiao",
                "user": "FQiao",
                "type": "user"
            },
            "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https://qjizhi.github.io/genstereo",
            "upvotes": 1,
            "discussionId": "67d9852d7fadfdd9d2a900c9",
            "projectPage": "https://qjizhi.github.io/genstereo/",
            "githubRepo": "https://github.com/Qjizhi/GenStereo",
            "ai_keywords": [
                "diffusion-based approach",
                "disparity-aware coordinate embedding",
                "warped input image",
                "adaptive fusion mechanism",
                "generalized ability",
                "stereo image generation",
                "unsupervised stereo matching",
                "GenStereo"
            ]
        },
        "publishedAt": "2025-03-16T21:19:28.000Z",
        "title": "GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching",
        "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https://qjizhi.github.io/genstereo",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12720.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6487e839341b4c7ec11013e5",
            "avatarUrl": "/avatars/e53a310f40c79bc8ba8f12027adf6a28.svg",
            "fullname": "Feng Qiao",
            "name": "FQiao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.08153",
            "authors": [
                {
                    "_id": "67d98394f333b86d4b17fc17",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc18",
                    "name": "Ao Ma",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc19",
                    "name": "Ke Cao",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc1a",
                    "user": {
                        "_id": "6381847a471a4550ff298c63",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381847a471a4550ff298c63/RTKepvX67R6pLiiUidpUO.png",
                        "isPro": false,
                        "fullname": "Jun",
                        "user": "zxbsmk",
                        "type": "user"
                    },
                    "name": "Jun Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:57:42.218Z",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc1b",
                    "name": "Zhanjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc1c",
                    "name": "Jiasong Feng",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc1d",
                    "name": "Shanyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc1e",
                    "name": "Yuhang Ma",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc1f",
                    "name": "Bo Cheng",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc20",
                    "user": {
                        "_id": "649935abbe8fd92c27ab1ed8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
                        "isPro": false,
                        "fullname": "David Leon",
                        "user": "DavidLeon",
                        "type": "user"
                    },
                    "name": "Dawei Leng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-19T01:32:34.905Z",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc21",
                    "name": "Yuhui Yin",
                    "hidden": false
                },
                {
                    "_id": "67d98394f333b86d4b17fc22",
                    "name": "Xiaodan Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T08:10:03.000Z",
            "submittedOnDailyAt": "2025-03-18T13:01:36.017Z",
            "title": "WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6381847a471a4550ff298c63",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381847a471a4550ff298c63/RTKepvX67R6pLiiUidpUO.png",
                "isPro": false,
                "fullname": "Jun",
                "user": "zxbsmk",
                "type": "user"
            },
            "summary": "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and\nKling, have shown great potential for building world simulators. However,\ncurrent T2V models struggle to grasp abstract physical principles and generate\nvideos that adhere to physical laws. This challenge arises primarily from a\nlack of clear guidance on physical information due to a significant gap between\nabstract physical principles and generation models. To this end, we introduce\nthe World Simulator Assistant (WISA), an effective framework for decomposing\nand incorporating physical principles into T2V models. Specifically, WISA\ndecomposes physical principles into textual physical descriptions, qualitative\nphysical categories, and quantitative physical properties. To effectively embed\nthese physical attributes into the generation process, WISA incorporates\nseveral key designs, including Mixture-of-Physical-Experts Attention (MoPA) and\na Physical Classifier, enhancing the model's physics awareness. Furthermore,\nmost existing datasets feature videos where physical phenomena are either\nweakly represented or entangled with multiple co-occurring processes, limiting\ntheir suitability as dedicated resources for learning explicit physical\nprinciples. We propose a novel video dataset, WISA-32K, collected based on\nqualitative physical categories. It consists of 32,000 videos, representing 17\nphysical laws across three domains of physics: dynamics, thermodynamics, and\noptics. Experimental results demonstrate that WISA can effectively enhance the\ncompatibility of T2V models with real-world physical laws, achieving a\nconsiderable improvement on the VideoPhy benchmark. The visual exhibitions of\nWISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/.",
            "upvotes": 0,
            "discussionId": "67d98396f333b86d4b17fcb5",
            "ai_keywords": [
                "Mixture-of-Physical-Experts Attention (MoPA)",
                "Physical Classifier"
            ]
        },
        "publishedAt": "2025-03-11T04:10:03.000Z",
        "title": "WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation",
        "summary": "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and\nKling, have shown great potential for building world simulators. However,\ncurrent T2V models struggle to grasp abstract physical principles and generate\nvideos that adhere to physical laws. This challenge arises primarily from a\nlack of clear guidance on physical information due to a significant gap between\nabstract physical principles and generation models. To this end, we introduce\nthe World Simulator Assistant (WISA), an effective framework for decomposing\nand incorporating physical principles into T2V models. Specifically, WISA\ndecomposes physical principles into textual physical descriptions, qualitative\nphysical categories, and quantitative physical properties. To effectively embed\nthese physical attributes into the generation process, WISA incorporates\nseveral key designs, including Mixture-of-Physical-Experts Attention (MoPA) and\na Physical Classifier, enhancing the model's physics awareness. Furthermore,\nmost existing datasets feature videos where physical phenomena are either\nweakly represented or entangled with multiple co-occurring processes, limiting\ntheir suitability as dedicated resources for learning explicit physical\nprinciples. We propose a novel video dataset, WISA-32K, collected based on\nqualitative physical categories. It consists of 32,000 videos, representing 17\nphysical laws across three domains of physics: dynamics, thermodynamics, and\noptics. Experimental results demonstrate that WISA can effectively enhance the\ncompatibility of T2V models with real-world physical laws, achieving a\nconsiderable improvement on the VideoPhy benchmark. The visual exhibitions of\nWISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08153.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6381847a471a4550ff298c63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6381847a471a4550ff298c63/RTKepvX67R6pLiiUidpUO.png",
            "fullname": "Jun",
            "name": "zxbsmk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 32
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.06269",
            "authors": [
                {
                    "_id": "67d0a39d9d994f341c4e6d83",
                    "user": {
                        "_id": "66a8fa93751e65b10a141039",
                        "avatarUrl": "/avatars/b4cd67c5441ba2c8add576cf13e1f916.svg",
                        "isPro": false,
                        "fullname": "Thomas Winninger",
                        "user": "Sckathach",
                        "type": "user"
                    },
                    "name": "Thomas Winninger",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-12T08:41:49.453Z",
                    "hidden": false
                },
                {
                    "_id": "67d0a39d9d994f341c4e6d84",
                    "name": "Boussad Addad",
                    "hidden": false
                },
                {
                    "_id": "67d0a39d9d994f341c4e6d85",
                    "name": "Katarzyna Kapusta",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-08T16:29:45.000Z",
            "submittedOnDailyAt": "2025-03-18T19:24:59.021Z",
            "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "66a8fa93751e65b10a141039",
                "avatarUrl": "/avatars/b4cd67c5441ba2c8add576cf13e1f916.svg",
                "isPro": false,
                "fullname": "Thomas Winninger",
                "user": "Sckathach",
                "type": "user"
            },
            "summary": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.",
            "upvotes": 0,
            "discussionId": "67d0a39e9d994f341c4e6db9",
            "projectPage": "https://sckathach.github.io/mech-interp/subspace-rerouting/",
            "githubRepo": "https://github.com/Sckathach/subspace-rerouting",
            "ai_keywords": [
                "adversarial perturbations",
                "LLMs (Large Language Models)",
                "gradient computation",
                "refusal mechanisms",
                "mechanistic interpretability",
                "acceptance subspaces",
                "feature vectors",
                "gradient-based optimization",
                "jailbreaks",
                "attack success rates",
                "Gemma2",
                "Llama3.2",
                "Qwen2.5",
                "computation cost",
                "runtime interventions"
            ]
        },
        "publishedAt": "2025-03-08T11:29:45.000Z",
        "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models",
        "summary": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06269.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a8fa93751e65b10a141039",
            "avatarUrl": "/avatars/b4cd67c5441ba2c8add576cf13e1f916.svg",
            "fullname": "Thomas Winninger",
            "name": "Sckathach",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]