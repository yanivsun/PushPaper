[
    {
        "paper": {
            "id": "2503.14456",
            "authors": [
                {
                    "_id": "67da21ed78c08b432f9fee0c",
                    "user": {
                        "_id": "62b3d8d651b07307bd12b7f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655953609090-noauth.jpeg",
                        "isPro": false,
                        "fullname": "BlinkDL",
                        "user": "BlinkDL",
                        "type": "user"
                    },
                    "name": "Bo Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:45.587Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee0d",
                    "user": {
                        "_id": "6418629fd13ffa408128d7ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
                        "isPro": false,
                        "fullname": "Zhang Ruichong",
                        "user": "ZhangRC",
                        "type": "user"
                    },
                    "name": "Ruichong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:54.749Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee0e",
                    "user": {
                        "_id": "647f4bac45baf21ad709fcd0",
                        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
                        "isPro": false,
                        "fullname": "Dan Goldstein",
                        "user": "SmerkyG",
                        "type": "user"
                    },
                    "name": "Daniel Goldstein",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:57.333Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee0f",
                    "name": "Eric Alcaide",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee10",
                    "name": "Haowen Hou",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee11",
                    "name": "Janna Lu",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee12",
                    "name": "William Merrill",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee13",
                    "user": {
                        "_id": "622c062645261ac5cc0bda94",
                        "avatarUrl": "/avatars/ce544b74110f7fe1ad11a3939526f5da.svg",
                        "isPro": false,
                        "fullname": "Guangyu Song",
                        "user": "Guangyu",
                        "type": "user"
                    },
                    "name": "Guangyu Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T09:50:42.957Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee14",
                    "name": "Kaifeng Tan",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee15",
                    "user": {
                        "_id": "638f1fd8c4444c6ca86ff823",
                        "avatarUrl": "/avatars/405807c3868663246cfe371a2034f351.svg",
                        "isPro": false,
                        "fullname": "saitejautpala",
                        "user": "saitejautpala",
                        "type": "user"
                    },
                    "name": "Saiteja Utpala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:48:50.276Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee16",
                    "user": {
                        "_id": "63cac1a50932c72f13995d6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cac1a50932c72f13995d6f/Bd9jEsCL9yWXdyAQCx61J.jpeg",
                        "isPro": false,
                        "fullname": "Nathan Wilce",
                        "user": "m8than",
                        "type": "user"
                    },
                    "name": "Nathan Wilce",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T09:50:59.151Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee17",
                    "name": "Johan S. Wind",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee18",
                    "name": "Tianyi Wu",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee19",
                    "user": {
                        "_id": "63f6706a9cbd6730302b80dc",
                        "avatarUrl": "/avatars/dcbbeeeb6b12641b5884df38c5e13766.svg",
                        "isPro": false,
                        "fullname": "Dr. Daniel Wuttke",
                        "user": "hevok",
                        "type": "user"
                    },
                    "name": "Daniel Wuttke",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T13:15:03.098Z",
                    "hidden": false
                },
                {
                    "_id": "67da21ed78c08b432f9fee1a",
                    "user": {
                        "_id": "6584f042b378d311dccea501",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
                        "isPro": false,
                        "fullname": "Christian Zhou-Zheng",
                        "user": "ChristianAzinn",
                        "type": "user"
                    },
                    "name": "Christian Zhou-Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T09:51:20.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:31:05.000Z",
            "submittedOnDailyAt": "2025-03-19T00:29:42.147Z",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "submittedOnDailyBy": {
                "_id": "6418629fd13ffa408128d7ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
                "isPro": false,
                "fullname": "Zhang Ruichong",
                "user": "ZhangRC",
                "type": "user"
            },
            "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
            "upvotes": 93,
            "discussionId": "67da21ee78c08b432f9fee71",
            "projectPage": "https://rwkv.cn",
            "githubRepo": "https://github.com/RWKV/RWKV-LM",
            "ai_keywords": [
                "sequence modeling architecture",
                "pre-trained language models",
                "downstream performance",
                "multilingual tasks",
                "in-context learning rates",
                "delta rule",
                "vector-valued gating",
                "value replacement rule",
                "state tracking",
                "regular languages",
                "parallelizability of training",
                "Transformers",
                "$\\mathsf{TC}^0$"
            ]
        },
        "publishedAt": "2025-03-18T13:31:05.000Z",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "summary": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14456.png",
        "numComments": 9,
        "submittedBy": {
            "_id": "6418629fd13ffa408128d7ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679319546731-noauth.png",
            "fullname": "Zhang Ruichong",
            "name": "ZhangRC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14378",
            "authors": [
                {
                    "_id": "67da1ee1f1a4a52e8a1e0241",
                    "user": {
                        "_id": "64b7833aa5018e3c7c9b50d8",
                        "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
                        "isPro": false,
                        "fullname": "Zechen Bai",
                        "user": "ZechenBai",
                        "type": "user"
                    },
                    "name": "Zechen Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:45:09.659Z",
                    "hidden": false
                },
                {
                    "_id": "67da1ee1f1a4a52e8a1e0242",
                    "user": {
                        "_id": "647352516a972f252de1fd58",
                        "avatarUrl": "/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg",
                        "isPro": false,
                        "fullname": "Hai Ci",
                        "user": "HaiCi",
                        "type": "user"
                    },
                    "name": "Hai Ci",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:14:19.533Z",
                    "hidden": false
                },
                {
                    "_id": "67da1ee1f1a4a52e8a1e0243",
                    "user": {
                        "_id": "63a55320ce5763e06f78519c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Mike Shou",
                        "user": "mikeshou",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T01:33:23.736Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
            ],
            "publishedAt": "2025-03-18T16:10:24.000Z",
            "submittedOnDailyAt": "2025-03-19T00:11:44.927Z",
            "title": "Impossible Videos",
            "submittedOnDailyBy": {
                "_id": "64b7833aa5018e3c7c9b50d8",
                "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
                "isPro": false,
                "fullname": "Zechen Bai",
                "user": "ZechenBai",
                "type": "user"
            },
            "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
            "upvotes": 47,
            "discussionId": "67da1ee3f1a4a52e8a1e02df",
            "projectPage": "https://showlab.github.io/Impossible-Videos/",
            "githubRepo": "https://github.com/showlab/Impossible-Videos",
            "ai_keywords": [
                "IPV-Bench",
                "taxonomy",
                "prompt suite",
                "video generation models",
                "prompt following",
                "creativity capabilities",
                "video benchmark",
                "Video-LLMs",
                "temporal dynamics",
                "world knowledge"
            ]
        },
        "publishedAt": "2025-03-18T12:10:24.000Z",
        "title": "Impossible Videos",
        "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b7833aa5018e3c7c9b50d8/1jxSsiEAyMr5fSng7GOB3.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14378.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7833aa5018e3c7c9b50d8",
            "avatarUrl": "/avatars/782415605ed786b73f484fcc86a6384f.svg",
            "fullname": "Zechen Bai",
            "name": "ZechenBai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14476",
            "authors": [
                {
                    "_id": "67da2b54e5335651349e262c",
                    "user": {
                        "_id": "6341375be5071b0c5cc946a3",
                        "avatarUrl": "/avatars/c26a32b54a3f7731854059d247b5c523.svg",
                        "isPro": false,
                        "fullname": "Qiying Yu",
                        "user": "qiying",
                        "type": "user"
                    },
                    "name": "Qiying Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:19:18.079Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e262d",
                    "user": {
                        "_id": "648e9244e6598457da62a180",
                        "avatarUrl": "/avatars/60329251f08e93260196ff67a842c188.svg",
                        "isPro": false,
                        "fullname": "Zheng Zhang",
                        "user": "zhengzhang",
                        "type": "user"
                    },
                    "name": "Zheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:19:31.824Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e262e",
                    "name": "Ruofei Zhu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e262f",
                    "user": {
                        "_id": "64b755e500bac1088cea02f1",
                        "avatarUrl": "/avatars/f4adf70175e84e945436d1f091ba338b.svg",
                        "isPro": false,
                        "fullname": "Yufeng Yuan",
                        "user": "yufeng10",
                        "type": "user"
                    },
                    "name": "Yufeng Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:19:54.321Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2630",
                    "name": "Xiaochen Zuo",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2631",
                    "name": "Yu Yue",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2632",
                    "name": "Tiantian Fan",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2633",
                    "user": {
                        "_id": "61512f1b3e89795099b6fced",
                        "avatarUrl": "/avatars/8f19253df7343e9d95ad7d0b3d429c06.svg",
                        "isPro": false,
                        "fullname": "Gaohong Liu",
                        "user": "Cheimu",
                        "type": "user"
                    },
                    "name": "Gaohong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:20:22.957Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2634",
                    "user": {
                        "_id": "6626274011772517e5469d48",
                        "avatarUrl": "/avatars/5f2606614740da04a7fb9d80fa628fb1.svg",
                        "isPro": false,
                        "fullname": "Lingjun Liu",
                        "user": "EEchollj",
                        "type": "user"
                    },
                    "name": "Lingjun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:20:29.992Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2635",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2636",
                    "user": {
                        "_id": "65e809f67c1853c8c180baff",
                        "avatarUrl": "/avatars/e489eaebbbf86f8826fe0e9d5423be01.svg",
                        "isPro": false,
                        "fullname": "haibin",
                        "user": "haibinlin",
                        "type": "user"
                    },
                    "name": "Haibin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:20:36.744Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2637",
                    "name": "Zhiqi Lin",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2638",
                    "name": "Bole Ma",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2639",
                    "name": "Guangming Sheng",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263a",
                    "user": {
                        "_id": "6448e1fbe988635a3d6aa97d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eG4R9-3hgrimttP7ep3dN.jpeg",
                        "isPro": false,
                        "fullname": "Shawn/Yuxuan Tong",
                        "user": "tongyx361",
                        "type": "user"
                    },
                    "name": "Yuxuan Tong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:21:29.044Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263b",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263c",
                    "user": {
                        "_id": "64f33235bbf2fd1b4816632b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sVN-M1Hj-aB4QqytdpUsn.jpeg",
                        "isPro": false,
                        "fullname": "Mofan Zhang",
                        "user": "NewMomo",
                        "type": "user"
                    },
                    "name": "Mofan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:21:51.391Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263d",
                    "name": "Wang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263e",
                    "name": "Hang Zhu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e263f",
                    "user": {
                        "_id": "650b0009bddcb62e458ebadf",
                        "avatarUrl": "/avatars/0431a38b9c9ca0e11ec505ce81183be8.svg",
                        "isPro": false,
                        "fullname": "Jinhua Zhu",
                        "user": "teslazhu",
                        "type": "user"
                    },
                    "name": "Jinhua Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:22:23.684Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2640",
                    "name": "Jiaze Chen",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2641",
                    "name": "Jiangjie Chen",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2642",
                    "name": "Chengyi Wang",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2643",
                    "name": "Hongli Yu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2644",
                    "name": "Weinan Dai",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2645",
                    "name": "Yuxuan Song",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2646",
                    "name": "Xiangpeng Wei",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2647",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2648",
                    "name": "Jingjing Liu",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e2649",
                    "name": "Wei-Ying Ma",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264a",
                    "user": {
                        "_id": "656c1600665d15428a8bdc60",
                        "avatarUrl": "/avatars/dfdf5f8712c769896b4ab655c4ce32e4.svg",
                        "isPro": false,
                        "fullname": "Ya-Qin Zhang",
                        "user": "Ya-Qin",
                        "type": "user"
                    },
                    "name": "Ya-Qin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:22:33.159Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264b",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264c",
                    "name": "Mu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264d",
                    "user": {
                        "_id": "647a13ba5f3450e1ded4ca85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647a13ba5f3450e1ded4ca85/0ryiqG2gpFlffOFNq3jpj.jpeg",
                        "isPro": false,
                        "fullname": "Yonghui Wu",
                        "user": "yonghuiwu",
                        "type": "user"
                    },
                    "name": "Yonghui Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:22:16.717Z",
                    "hidden": false
                },
                {
                    "_id": "67da2b54e5335651349e264e",
                    "name": "Mingxuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:49:06.000Z",
            "submittedOnDailyAt": "2025-03-19T00:56:39.773Z",
            "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
            "upvotes": 46,
            "discussionId": "67da2b55e5335651349e26c7",
            "ai_keywords": [
                "inference scaling",
                "LLMs (Large Language Models)",
                "reinforcement learning (RL)",
                "Decoupled Clip and Dynamic Sampling Action Policy Optimization (DAPO)",
                "Qwen2.5-32B",
                "AIME 2024",
                "verl framework"
            ]
        },
        "publishedAt": "2025-03-18T13:49:06.000Z",
        "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
        "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe Decoupled Clip and Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14476.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6402
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14478",
            "authors": [
                {
                    "_id": "67da34a648348387ebac36ff",
                    "user": {
                        "_id": "64f5f8dd9b17cd59c453c57f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                        "isPro": false,
                        "fullname": "Xinyu Fang",
                        "user": "nebulae09",
                        "type": "user"
                    },
                    "name": "Xinyu Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:17:40.816Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3700",
                    "name": "Zhijian Chen",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3701",
                    "name": "Kai Lan",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3702",
                    "user": {
                        "_id": "646cd947da8e99940b6e55cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
                        "isPro": false,
                        "fullname": "Shengyuan Ding",
                        "user": "ChrisDing1105",
                        "type": "user"
                    },
                    "name": "Shengyuan Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:18:02.611Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3703",
                    "name": "Yingji Liang",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3704",
                    "user": {
                        "_id": "6530e62f536dbca918e71c3e",
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T13:14:48.768Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3705",
                    "name": "Farong Wen",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3706",
                    "name": "Zicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3707",
                    "name": "Guofeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3708",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:18:37.741Z",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac3709",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "67da34a648348387ebac370a",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:18:43.705Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:51:34.000Z",
            "submittedOnDailyAt": "2025-03-19T01:40:40.617Z",
            "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
            "submittedOnDailyBy": {
                "_id": "64f5f8dd9b17cd59c453c57f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
                "isPro": false,
                "fullname": "Xinyu Fang",
                "user": "nebulae09",
                "type": "user"
            },
            "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
            "upvotes": 37,
            "discussionId": "67da34ad48348387ebac3926",
            "projectPage": "https://open-compass.github.io/Creation-MMBench/",
            "githubRepo": "https://github.com/open-compass/Creation-MMBench",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "Creation-MMBench",
                "image-based tasks",
                "instance-specific evaluation criteria",
                "visual fine-tuning",
                "multimodal generative intelligence"
            ]
        },
        "publishedAt": "2025-03-18T13:51:34.000Z",
        "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
        "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14478.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f5f8dd9b17cd59c453c57f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg",
            "fullname": "Xinyu Fang",
            "name": "nebulae09",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12797",
            "authors": [
                {
                    "_id": "67da2c83aa2c34f7d95e46ff",
                    "user": {
                        "_id": "63c3b67ec7d7f4c63a4eea3a",
                        "avatarUrl": "/avatars/4a5f98cb6b0c1e37a2c09af72f7a9946.svg",
                        "isPro": false,
                        "fullname": "Xinyu Ma",
                        "user": "MaxyLee",
                        "type": "user"
                    },
                    "name": "Xinyu Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:31.865Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4700",
                    "user": {
                        "_id": "65903c4aa78a277803bde77b",
                        "avatarUrl": "/avatars/061388320ccf1a66e1e99519dd426a60.svg",
                        "isPro": false,
                        "fullname": "Ziyang Ding",
                        "user": "sdudzy",
                        "type": "user"
                    },
                    "name": "Ziyang Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:35.757Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4701",
                    "name": "Zhicong Luo",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4702",
                    "user": {
                        "_id": "642086ed290342c5df85662d",
                        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                        "isPro": false,
                        "fullname": "Chi Chen",
                        "user": "carboncoo",
                        "type": "user"
                    },
                    "name": "Chi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:39.356Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4703",
                    "user": {
                        "_id": "6491af36c1741666238f3bff",
                        "avatarUrl": "/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg",
                        "isPro": false,
                        "fullname": "Zonghao Guo",
                        "user": "guozonghao96",
                        "type": "user"
                    },
                    "name": "Zonghao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:23:57.046Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4704",
                    "user": {
                        "_id": "648bd523805e4bcc541ec320",
                        "avatarUrl": "/avatars/443ca0c7cbeda3a08eb4af6a0e2da8bc.svg",
                        "isPro": false,
                        "fullname": "Derek Wong",
                        "user": "derekfw",
                        "type": "user"
                    },
                    "name": "Derek F. Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:23:48.590Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4705",
                    "user": {
                        "_id": "6502c59b8670cc10e6b6b039",
                        "avatarUrl": "/avatars/ae96d1fa2bed83cd427d18ca25d69efa.svg",
                        "isPro": false,
                        "fullname": "Xiaoyi Feng",
                        "user": "LecterF",
                        "type": "user"
                    },
                    "name": "Xiaoyi Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:23:24.786Z",
                    "hidden": false
                },
                {
                    "_id": "67da2c83aa2c34f7d95e4706",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T04:06:34.000Z",
            "submittedOnDailyAt": "2025-03-19T01:06:07.094Z",
            "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
            "submittedOnDailyBy": {
                "_id": "642086ed290342c5df85662d",
                "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                "isPro": false,
                "fullname": "Chi Chen",
                "user": "carboncoo",
                "type": "user"
            },
            "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
            "upvotes": 24,
            "discussionId": "67da2c85aa2c34f7d95e4796",
            "projectPage": "https://deepperception-kvg.github.io",
            "githubRepo": "https://github.com/thunlp/DeepPerception",
            "ai_keywords": [
                "knowledge-intensive visual grounding (KVG)",
                "DeepPerception",
                "automated data synthesis pipeline",
                "supervised fine-tuning",
                "reinforcement learning",
                "perception-cognition synergy",
                "KVG-Bench",
                "cognitive reasoning scaffolding",
                "cross-domain generalization",
                "cognitive processes",
                "multimodal reasoning"
            ]
        },
        "publishedAt": "2025-03-17T00:06:34.000Z",
        "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
        "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12797.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "fullname": "Chi Chen",
            "name": "carboncoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12329",
            "authors": [
                {
                    "_id": "67d8e115f55b855ae6d8f29b",
                    "user": {
                        "_id": "63340dbbd92c5842ae71d1e9",
                        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
                        "isPro": false,
                        "fullname": "Kanzhi Cheng",
                        "user": "cckevinn",
                        "type": "user"
                    },
                    "name": "Kanzhi Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T11:34:32.729Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f29c",
                    "user": {
                        "_id": "653f8cc9d4d0924ad2404d86",
                        "avatarUrl": "/avatars/83defb31d669393447ee04fe9989b96b.svg",
                        "isPro": false,
                        "fullname": "Wenpo Song",
                        "user": "songwp",
                        "type": "user"
                    },
                    "name": "Wenpo Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:46:11.039Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f29d",
                    "user": {
                        "_id": "65d1d15371ca8e594f494c4f",
                        "avatarUrl": "/avatars/80b324be013f1717f625fd7f690fed35.svg",
                        "isPro": false,
                        "fullname": "Jiaxin Fan",
                        "user": "DerekFan",
                        "type": "user"
                    },
                    "name": "Jiaxin Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:24:11.342Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f29e",
                    "name": "Zheng Ma",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f29f",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:25:26.087Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f2a0",
                    "user": {
                        "_id": "64e6cf78ecce34cb442dc889",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                        "isPro": false,
                        "fullname": "Fangzhi Xu",
                        "user": "xufangzhi",
                        "type": "user"
                    },
                    "name": "Fangzhi Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:25:14.904Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f2a1",
                    "name": "Chenyang Yan",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f2a2",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f2a3",
                    "name": "Jianbing Zhang",
                    "hidden": false
                },
                {
                    "_id": "67d8e115f55b855ae6d8f2a4",
                    "name": "Jiajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T02:56:09.000Z",
            "submittedOnDailyAt": "2025-03-19T02:04:01.010Z",
            "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
            "submittedOnDailyBy": {
                "_id": "63340dbbd92c5842ae71d1e9",
                "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
                "isPro": false,
                "fullname": "Kanzhi Cheng",
                "user": "cckevinn",
                "type": "user"
            },
            "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.",
            "upvotes": 19,
            "discussionId": "67d8e118f55b855ae6d8f34e",
            "projectPage": "https://caparena.github.io/",
            "githubRepo": "https://github.com/njucckevin/CapArena",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "GPT-4o",
                "CapArena",
                "pairwise caption battles",
                "high-quality human preference votes",
                "VLM-as-a-Judge",
                "METEOR",
                "CapArena-Auto"
            ]
        },
        "publishedAt": "2025-03-15T22:56:09.000Z",
        "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
        "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12329.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63340dbbd92c5842ae71d1e9",
            "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
            "fullname": "Kanzhi Cheng",
            "name": "cckevinn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13424",
            "authors": [
                {
                    "_id": "67da22b75fe852c86d3c419b",
                    "user": {
                        "_id": "663b78429b8151660fbb66d4",
                        "avatarUrl": "/avatars/6664af07c38a1ce34fc6b2f3d41ca719.svg",
                        "isPro": false,
                        "fullname": "Lian Xinyu",
                        "user": "Lianxy",
                        "type": "user"
                    },
                    "name": "Xinyu Lian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:29:18.619Z",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c419c",
                    "name": "Zichao Yu",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c419d",
                    "user": {
                        "_id": "66f4dde070850ff358a098e8",
                        "avatarUrl": "/avatars/d52973933e33379977c32c775477fc97.svg",
                        "isPro": false,
                        "fullname": "Ruiming Liang",
                        "user": "lrmbbj",
                        "type": "user"
                    },
                    "name": "Ruiming Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:29:34.205Z",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c419e",
                    "name": "Yitong Wang",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c419f",
                    "name": "Li Ray Luo",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a0",
                    "user": {
                        "_id": "66b1dce83158b1fadffb0d9d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RbWKIxCjawVaSidwZLZA1.png",
                        "isPro": false,
                        "fullname": "Kaixu Chen",
                        "user": "kaixuChen",
                        "type": "user"
                    },
                    "name": "Kaixu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:30:14.753Z",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a1",
                    "user": {
                        "_id": "642e4debb42737f9e1e2a6cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e4debb42737f9e1e2a6cb/yw_sfzSK7dxYe0Xym4Uea.png",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "zhouyuanzhen",
                        "type": "user"
                    },
                    "name": "Yuanzhen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:30:45.060Z",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a2",
                    "user": {
                        "_id": "673dbb45df154a60e3aed0f1",
                        "avatarUrl": "/avatars/5945d2907e2a673b04afacdf443e73d4.svg",
                        "isPro": false,
                        "fullname": "Qihong Tang",
                        "user": "tangqh",
                        "type": "user"
                    },
                    "name": "Qihong Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:30:51.629Z",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a3",
                    "name": "Xudong Xu",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a4",
                    "user": {
                        "_id": "63f2ec797ddf724fbcc75aee",
                        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Lyu",
                        "user": "ZhaoyangLyu",
                        "type": "user"
                    },
                    "name": "Zhaoyang Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:29:05.257Z",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a5",
                    "name": "Bo Dai",
                    "hidden": false
                },
                {
                    "_id": "67da22b75fe852c86d3c41a6",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:31:33.584Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
            ],
            "publishedAt": "2025-03-17T17:53:56.000Z",
            "submittedOnDailyAt": "2025-03-19T00:24:14.520Z",
            "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
            "submittedOnDailyBy": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
            },
            "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility",
            "upvotes": 18,
            "discussionId": "67da22bb5fe852c86d3c4304",
            "projectPage": "https://infinite-mobility.github.io/",
            "githubRepo": "https://github.com/Intern-Nexus/Infinite-Mobility",
            "ai_keywords": [
                "articulated objects",
                "high-fidelity",
                "embodied AI",
                "data-driven",
                "simulation-based",
                "procedural generation",
                "physics property",
                "mesh quality",
                "generative models"
            ]
        },
        "publishedAt": "2025-03-17T13:53:56.000Z",
        "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
        "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/7Is4N1AFDor-EuzkZEiui.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13424.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f2ec797ddf724fbcc75aee",
            "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
            "fullname": "Zhaoyang Lyu",
            "name": "ZhaoyangLyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14125",
            "authors": [
                {
                    "_id": "67da200db41738a058666623",
                    "user": {
                        "_id": "667505f4361b960c79e35486",
                        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
                        "isPro": false,
                        "fullname": "Defa Zhu",
                        "user": "mathfinder",
                        "type": "user"
                    },
                    "name": "Defa Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:32:01.715Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a058666624",
                    "user": {
                        "_id": "66defe949fda73ea123ede1c",
                        "avatarUrl": "/avatars/52bda14a533fdd13436bcf7c65a38fb3.svg",
                        "isPro": false,
                        "fullname": "huang",
                        "user": "hongzhihuang",
                        "type": "user"
                    },
                    "name": "Hongzhi Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:32:07.284Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a058666625",
                    "user": {
                        "_id": "642ae9f4d651bae3c11ebad1",
                        "avatarUrl": "/avatars/d0f9253da54e148c152710923d8d3804.svg",
                        "isPro": false,
                        "fullname": "Zhou Jundong",
                        "user": "ulrical",
                        "type": "user"
                    },
                    "name": "Jundong Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:32:22.854Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a058666626",
                    "user": {
                        "_id": "65a62085576772f531e13856",
                        "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg",
                        "isPro": false,
                        "fullname": "Huang Zihao",
                        "user": "FetchFortune",
                        "type": "user"
                    },
                    "name": "Zihao Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:45:07.454Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a058666627",
                    "user": {
                        "_id": "6371128eafbe42caa5a5222b",
                        "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
                        "isPro": false,
                        "fullname": "Yutao Zeng",
                        "user": "Taoer",
                        "type": "user"
                    },
                    "name": "Yutao Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:45:05.382Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a058666628",
                    "user": {
                        "_id": "64a3b0fa565496b629879293",
                        "avatarUrl": "/avatars/9d5de6cc4a01052ea971701f72bd3489.svg",
                        "isPro": false,
                        "fullname": "wubanggu",
                        "user": "banggu",
                        "type": "user"
                    },
                    "name": "Banggu Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:32:34.670Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a058666629",
                    "user": {
                        "_id": "645507b89d37c3fb33279fe3",
                        "avatarUrl": "/avatars/ec1122137c49204ab968182d1f726c35.svg",
                        "isPro": false,
                        "fullname": "min",
                        "user": "qiyang-attn",
                        "type": "user"
                    },
                    "name": "Qiyang Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:32:46.730Z",
                    "hidden": false
                },
                {
                    "_id": "67da200db41738a05866662a",
                    "name": "Xun Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T10:37:50.000Z",
            "submittedOnDailyAt": "2025-03-19T00:09:57.233Z",
            "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
            "submittedOnDailyBy": {
                "_id": "667505f4361b960c79e35486",
                "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
                "isPro": false,
                "fullname": "Defa Zhu",
                "user": "mathfinder",
                "type": "user"
            },
            "summary": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
            "upvotes": 13,
            "discussionId": "67da200eb41738a058666690",
            "ai_keywords": [
                "residual connections",
                "gradient vanishing",
                "Hyper-Connections",
                "multiple connection strengths",
                "seesaw effect",
                "representation collapse",
                "Frac-Connections",
                "hidden states",
                "language tasks",
                "MoE model"
            ]
        },
        "publishedAt": "2025-03-18T06:37:50.000Z",
        "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
        "summary": "Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14125.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "667505f4361b960c79e35486",
            "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
            "fullname": "Defa Zhu",
            "name": "mathfinder",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14492",
            "authors": [
                {
                    "_id": "67da2cbde5335651349e98c8",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ca",
                    "name": "Hassan Abu Alhaija",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98cb",
                    "name": "Jose Alvarez",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98cc",
                    "user": {
                        "_id": "675304737c4876b7a1475695",
                        "avatarUrl": "/avatars/7ee3c443f6a143b4a79d679fb7f60fe5.svg",
                        "isPro": false,
                        "fullname": "Maciej Bala",
                        "user": "mbalaNV",
                        "type": "user"
                    },
                    "name": "Maciej Bala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:33:45.678Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98cd",
                    "name": "Tiffany Cai",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ce",
                    "user": {
                        "_id": "63e07dfaf0c75dfb876d9d71",
                        "avatarUrl": "/avatars/8dab667735111e070cc6d44609dbdaf3.svg",
                        "isPro": false,
                        "fullname": "Tianshi Cao",
                        "user": "jeanlancel",
                        "type": "user"
                    },
                    "name": "Tianshi Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:33:56.311Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98cf",
                    "user": {
                        "_id": "67a113431a84e2c54d77ca7c",
                        "avatarUrl": "/avatars/b61e487b14eec1b29a4ccb61e13bb484.svg",
                        "isPro": false,
                        "fullname": "Liz Cha",
                        "user": "lizcha",
                        "type": "user"
                    },
                    "name": "Liz Cha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:03.422Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d0",
                    "user": {
                        "_id": "640f46a506c3b5ca88404360",
                        "avatarUrl": "/avatars/26c97b51d245bda62a0c3808aaba96a7.svg",
                        "isPro": false,
                        "fullname": "Joshua Chen",
                        "user": "JoshuaChen",
                        "type": "user"
                    },
                    "name": "Joshua Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:11.200Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d1",
                    "user": {
                        "_id": "650874a5f36bb51c50f16294",
                        "avatarUrl": "/avatars/112b7beb6e8c9d834349f99fefaca742.svg",
                        "isPro": false,
                        "fullname": "Mike Chen",
                        "user": "mikechen",
                        "type": "user"
                    },
                    "name": "Mike Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:18.080Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d2",
                    "user": {
                        "_id": "6353d3e95eac2d2efa7501f9",
                        "avatarUrl": "/avatars/4b063f54000bed4bfb1bfcc3cde1a09e.svg",
                        "isPro": false,
                        "fullname": "Francesco Ferroni",
                        "user": "fferroni",
                        "type": "user"
                    },
                    "name": "Francesco Ferroni",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:24.297Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d3",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d4",
                    "name": "Dieter Fox",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d5",
                    "user": {
                        "_id": "64370de87cbe1e781fc5c7a2",
                        "avatarUrl": "/avatars/98101739726ed797958e14f3d666933b.svg",
                        "isPro": false,
                        "fullname": "Yunhao Ge",
                        "user": "andyge",
                        "type": "user"
                    },
                    "name": "Yunhao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:33:19.980Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d6",
                    "user": {
                        "_id": "647e8118770c299e56fc2bc8",
                        "avatarUrl": "/avatars/adf80f3473dda42450148789ae5c208f.svg",
                        "isPro": false,
                        "fullname": "Jinwei Gu",
                        "user": "jwgu",
                        "type": "user"
                    },
                    "name": "Jinwei Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:33:27.182Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d7",
                    "name": "Ali Hassani",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d8",
                    "user": {
                        "_id": "60d4c4616178079fbd2a0d1f",
                        "avatarUrl": "/avatars/d3c2221fe1122fb04b383bc47c14e80d.svg",
                        "isPro": false,
                        "fullname": "Michael Isaev",
                        "user": "michael-isaev",
                        "type": "user"
                    },
                    "name": "Michael Isaev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:43.605Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98d9",
                    "user": {
                        "_id": "6771ca7396f52ab8156372c4",
                        "avatarUrl": "/avatars/02adad495c9bd869e90db78d7baf41e0.svg",
                        "isPro": false,
                        "fullname": "Pooya Jannaty",
                        "user": "pjannaty",
                        "type": "user"
                    },
                    "name": "Pooya Jannaty",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:49.164Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98da",
                    "name": "Shiyi Lan",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98db",
                    "user": {
                        "_id": "66ec2efa1cd3794eb428806b",
                        "avatarUrl": "/avatars/f00032293c1a2eb2f784f7881b7a744b.svg",
                        "isPro": false,
                        "fullname": "Tobias Lasser",
                        "user": "tlasser",
                        "type": "user"
                    },
                    "name": "Tobias Lasser",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:34:58.285Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98dc",
                    "name": "Huan Ling",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98dd",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98de",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98df",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e0",
                    "name": "Alice Luo",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e1",
                    "name": "Qianli Ma",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e2",
                    "name": "Hanzi Mao",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e3",
                    "name": "Fabio Ramos",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e4",
                    "name": "Xuanchi Ren",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e5",
                    "name": "Tianchang Shen",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e6",
                    "name": "Shitao Tang",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e7",
                    "name": "Ting-Chun Wang",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e8",
                    "name": "Jay Wu",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98e9",
                    "name": "Jiashu Xu",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ea",
                    "user": {
                        "_id": "645b420308d9a18f91e9748d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5YOYvR2jgpKegopbMjVUF.jpeg",
                        "isPro": false,
                        "fullname": "Stella Xu",
                        "user": "sqx",
                        "type": "user"
                    },
                    "name": "Stella Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:35:18.303Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98eb",
                    "name": "Kevin Xie",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ec",
                    "name": "Yuchong Ye",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ed",
                    "name": "Xiaodong Yang",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ee",
                    "user": {
                        "_id": "64d9cf178767727dff1da0c7",
                        "avatarUrl": "/avatars/c2ec61b511754a8d312e2db3229215ec.svg",
                        "isPro": false,
                        "fullname": "Xiaohui  ZENG",
                        "user": "eccony",
                        "type": "user"
                    },
                    "name": "Xiaohui Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:35:10.244Z",
                    "hidden": false
                },
                {
                    "_id": "67da2cbde5335651349e98ef",
                    "name": "Yu Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:57:54.000Z",
            "submittedOnDailyAt": "2025-03-19T01:03:48.943Z",
            "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
            "upvotes": 12,
            "discussionId": "67da2cc1e5335651349e9a3e",
            "ai_keywords": [
                "conditional world generation model",
                "world simulations",
                "spatial control inputs",
                "segmentation",
                "depth",
                "edge",
                "spatial conditional scheme",
                "adaptive",
                "customizable",
                "high controllable world generation",
                "world-to-world transfer",
                "Sim2Real",
                "Physical AI",
                "robotics Sim2Real",
                "autonomous vehicle data enrichment",
                "inference scaling strategy",
                "real-time world generation",
                "NVIDIA GB200 NVL72 rack"
            ]
        },
        "publishedAt": "2025-03-18T13:57:54.000Z",
        "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
        "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14492.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6402
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14504",
            "authors": [
                {
                    "_id": "67da436711b6db6920802e9e",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802e9f",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "Yi-Fan Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T04:09:15.055Z",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea0",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea1",
                    "name": "Junkang Wu",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea2",
                    "name": "Jinda Lu",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea3",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea4",
                    "name": "Xingyu Lu",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea5",
                    "user": {
                        "_id": "6483143902f98c3f05aff915",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
                        "isPro": true,
                        "fullname": " Yunhang Shen",
                        "user": "shenyunhang",
                        "type": "user"
                    },
                    "name": "Yunhang Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:38:55.153Z",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea6",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea7",
                    "user": {
                        "_id": "619f01b8cc04eadf54fa5d5d",
                        "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
                        "isPro": false,
                        "fullname": "Song Dingjie",
                        "user": "songdj",
                        "type": "user"
                    },
                    "name": "Dingjie Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:38:06.473Z",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea8",
                    "name": "Yibo Yan",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ea9",
                    "user": {
                        "_id": "66b65be6308cd3b74061c71a",
                        "avatarUrl": "/avatars/e8e9120a25044fde0a9483b4553ab8fa.svg",
                        "isPro": false,
                        "fullname": "Tianlong Xu",
                        "user": "xtl090994",
                        "type": "user"
                    },
                    "name": "Tianlong Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:37:52.755Z",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802eaa",
                    "name": "Qingsong Wen",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802eab",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802eac",
                    "name": "Yan Huang",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802ead",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "67da436711b6db6920802eae",
                    "name": "Tieniu Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:59:56.000Z",
            "submittedOnDailyAt": "2025-03-19T02:39:30.454Z",
            "title": "Aligning Multimodal LLM with Human Preference: A Survey",
            "submittedOnDailyBy": {
                "_id": "623d8ca4c29adf5ef6175615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                "isPro": false,
                "fullname": "Yi-Fan Zhang",
                "user": "yifanzhang114",
                "type": "user"
            },
            "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
            "upvotes": 11,
            "discussionId": "67da436b11b6db6920803040",
            "githubRepo": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment",
            "ai_keywords": [
                "Large language models (LLMs)",
                "Multimodal Large Language Models (MLLMs)",
                "Truthfulness",
                "Safety",
                "o1-like reasoning",
                "Alignment with human preference",
                "Alignment algorithms",
                "General image understanding",
                "Multi-image",
                "Video",
                "Audio",
                "Extended multimodal applications",
                "Alignment datasets",
                "Data sources",
                "Model responses",
                "Preference annotations",
                "Benchmarks"
            ]
        },
        "publishedAt": "2025-03-18T13:59:56.000Z",
        "title": "Aligning Multimodal LLM with Human Preference: A Survey",
        "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14504.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10522",
            "authors": [
                {
                    "_id": "67d6736ca8b6228408aa2a5c",
                    "user": {
                        "_id": "6504409b52ca06fef957266d",
                        "avatarUrl": "/avatars/ac96eab7318cd8d472601cf7e7c50c97.svg",
                        "isPro": false,
                        "fullname": "Zeyue",
                        "user": "Zeyue7",
                        "type": "user"
                    },
                    "name": "Zeyue Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-16T21:12:14.592Z",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a5d",
                    "name": "Yizhu Jin",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a5e",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a5f",
                    "name": "Ruibin Yuan",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a60",
                    "name": "Xu Tan",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a61",
                    "name": "Qifeng Chen",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a62",
                    "name": "Wei Xue",
                    "hidden": false
                },
                {
                    "_id": "67d6736ca8b6228408aa2a63",
                    "name": "Yike Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T16:30:59.000Z",
            "submittedOnDailyAt": "2025-03-19T15:18:23.664Z",
            "title": "AudioX: Diffusion Transformer for Anything-to-Audio Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Audio and music generation have emerged as crucial tasks in many\napplications, yet existing approaches face significant limitations: they\noperate in isolation without unified capabilities across modalities, suffer\nfrom scarce high-quality, multi-modal training data, and struggle to\neffectively integrate diverse inputs. In this work, we propose AudioX, a\nunified Diffusion Transformer model for Anything-to-Audio and Music Generation.\nUnlike previous domain-specific models, AudioX can generate both general audio\nand music with high quality, while offering flexible natural language control\nand seamless processing of various modalities including text, video, image,\nmusic, and audio. Its key innovation is a multi-modal masked training strategy\nthat masks inputs across modalities and forces the model to learn from masked\ninputs, yielding robust and unified cross-modal representations. To address\ndata scarcity, we curate two comprehensive datasets: vggsound-caps with 190K\naudio captions based on the VGGSound dataset, and V2M-caps with 6 million music\ncaptions derived from the V2M dataset. Extensive experiments demonstrate that\nAudioX not only matches or outperforms state-of-the-art specialized models, but\nalso offers remarkable versatility in handling diverse input modalities and\ngeneration tasks within a unified architecture. The code and datasets will be\navailable at https://zeyuet.github.io/AudioX/",
            "upvotes": 10,
            "discussionId": "67d67370a8b6228408aa2b28",
            "projectPage": "https://zeyuet.github.io/AudioX/",
            "ai_keywords": [
                "Diffusion Transformer",
                "Anything-to-Audio",
                "Music Generation",
                "multi-modal masked training strategy",
                "cross-modal representations"
            ]
        },
        "publishedAt": "2025-03-13T12:30:59.000Z",
        "title": "AudioX: Diffusion Transformer for Anything-to-Audio Generation",
        "summary": "Audio and music generation have emerged as crucial tasks in many\napplications, yet existing approaches face significant limitations: they\noperate in isolation without unified capabilities across modalities, suffer\nfrom scarce high-quality, multi-modal training data, and struggle to\neffectively integrate diverse inputs. In this work, we propose AudioX, a\nunified Diffusion Transformer model for Anything-to-Audio and Music Generation.\nUnlike previous domain-specific models, AudioX can generate both general audio\nand music with high quality, while offering flexible natural language control\nand seamless processing of various modalities including text, video, image,\nmusic, and audio. Its key innovation is a multi-modal masked training strategy\nthat masks inputs across modalities and forces the model to learn from masked\ninputs, yielding robust and unified cross-modal representations. To address\ndata scarcity, we curate two comprehensive datasets: vggsound-caps with 190K\naudio captions based on the VGGSound dataset, and V2M-caps with 6 million music\ncaptions derived from the V2M dataset. Extensive experiments demonstrate that\nAudioX not only matches or outperforms state-of-the-art specialized models, but\nalso offers remarkable versatility in handling diverse input modalities and\ngeneration tasks within a unified architecture. The code and datasets will be\navailable at https://zeyuet.github.io/AudioX/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10522.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6402
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14499",
            "authors": [
                {
                    "_id": "67da2e831bba0f73374fd5a0",
                    "name": "Thomas Kwa",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a1",
                    "name": "Ben West",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a2",
                    "user": {
                        "_id": "664fe38dcd6b0c097982f9dc",
                        "avatarUrl": "/avatars/00180e8352ea76c922bd86599af159f6.svg",
                        "isPro": false,
                        "fullname": "Joel Becker",
                        "user": "voxoid",
                        "type": "user"
                    },
                    "name": "Joel Becker",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:40:48.474Z",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a3",
                    "name": "Amy Deng",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a4",
                    "name": "Katharyn Garcia",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a5",
                    "name": "Max Hasin",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a6",
                    "user": {
                        "_id": "663bc1a595085055e96afdce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MK8s0IC7r93Tm4oQoPaqO.jpeg",
                        "isPro": false,
                        "fullname": "Sami Jawhar",
                        "user": "sjawhar",
                        "type": "user"
                    },
                    "name": "Sami Jawhar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:41:05.417Z",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a7",
                    "user": {
                        "_id": "65fe059b2c739e076189fbb9",
                        "avatarUrl": "/avatars/758063db87fbae4262ac12d41335990f.svg",
                        "isPro": false,
                        "fullname": "Megan Kinniment",
                        "user": "MeganKW",
                        "type": "user"
                    },
                    "name": "Megan Kinniment",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:41:11.957Z",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a8",
                    "user": {
                        "_id": "643458d8a4c9c55871a99894",
                        "avatarUrl": "/avatars/df86144dafe543a973754235fa643e28.svg",
                        "isPro": false,
                        "fullname": "Nate Rush",
                        "user": "naterush",
                        "type": "user"
                    },
                    "name": "Nate Rush",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:41:18.725Z",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5a9",
                    "name": "Sydney Von Arx",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5aa",
                    "name": "Ryan Bloom",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5ab",
                    "user": {
                        "_id": "664246cf273e54872591fde4",
                        "avatarUrl": "/avatars/1f0e66467aa854de9d4fc938de4fac20.svg",
                        "isPro": false,
                        "fullname": "Thomas Broadley",
                        "user": "tbroadley",
                        "type": "user"
                    },
                    "name": "Thomas Broadley",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:41:33.643Z",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5ac",
                    "name": "Haoxing Du",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5ad",
                    "name": "Brian Goodrich",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5ae",
                    "name": "Nikola Jurkovic",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5af",
                    "name": "Luke Harold Miles",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b0",
                    "name": "Seraphina Nix",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b1",
                    "name": "Tao Lin",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b2",
                    "user": {
                        "_id": "653f71b92273676554d8cb14",
                        "avatarUrl": "/avatars/64ef4c04aa3db8a3a5913598577c10bb.svg",
                        "isPro": false,
                        "fullname": "Neev Parikh",
                        "user": "neevparikh",
                        "type": "user"
                    },
                    "name": "Neev Parikh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:42:09.842Z",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b3",
                    "name": "David Rein",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b4",
                    "name": "Lucas Jun Koba Sato",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b5",
                    "name": "Hjalmar Wijk",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b6",
                    "name": "Daniel M. Ziegler",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b7",
                    "name": "Elizabeth Barnes",
                    "hidden": false
                },
                {
                    "_id": "67da2e831bba0f73374fd5b8",
                    "name": "Lawrence Chan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:59:31.000Z",
            "submittedOnDailyAt": "2025-03-19T01:10:23.636Z",
            "title": "Measuring AI Ability to Complete Long Tasks",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
            "upvotes": 9,
            "discussionId": "67da2e8a1bba0f73374fd89e",
            "ai_keywords": [
                "RE-Bench",
                "HCAST",
                "Claude 3.7 Sonnet",
                "50%-task-completion time horizon",
                "reliability",
                "ability to adapt to mistakes",
                "logical reasoning",
                "tool use capabilities"
            ]
        },
        "publishedAt": "2025-03-18T13:59:31.000Z",
        "title": "Measuring AI Ability to Complete Long Tasks",
        "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 6402
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.12355",
            "authors": [
                {
                    "_id": "67daf5dff34884a28b17bb53",
                    "user": {
                        "_id": "6254623af74165c7d2df0c18",
                        "avatarUrl": "/avatars/4ddd1aa858723dd9348fc51966f700c3.svg",
                        "isPro": false,
                        "fullname": "Kumar Krishna Agrawal",
                        "user": "kumarkrishna",
                        "type": "user"
                    },
                    "name": "Kumar Krishna Agrawal",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T16:50:44.848Z",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb54",
                    "name": "Long Lian",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb55",
                    "name": "Longchao Liu",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb56",
                    "user": {
                        "_id": "6333a9195a032dcd095dda13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Adam Yala",
                        "user": "yala",
                        "type": "user"
                    },
                    "name": "Natalia Harguindeguy",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T16:50:44.848Z",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb57",
                    "name": "Boyi Li",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb58",
                    "name": "Alexander Bick",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb59",
                    "name": "Maggie Chung",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb5a",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "67daf5dff34884a28b17bb5b",
                    "name": "Adam Yala",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/Rff3u2uP4fPRO-4OHcR_Y.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/7WFEZ_zDr9zYLsZkGb34U.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/sWNex7CarElcXriLARI5q.png"
            ],
            "publishedAt": "2025-03-16T04:52:13.000Z",
            "submittedOnDailyAt": "2025-03-19T15:23:52.707Z",
            "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling",
            "submittedOnDailyBy": {
                "_id": "6254623af74165c7d2df0c18",
                "avatarUrl": "/avatars/4ddd1aa858723dd9348fc51966f700c3.svg",
                "isPro": false,
                "fullname": "Kumar Krishna Agrawal",
                "user": "kumarkrishna",
                "type": "user"
            },
            "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.",
            "upvotes": 9,
            "discussionId": "67daf5e4f34884a28b17bc5e",
            "githubRepo": "https://github.com/YalaLab/atlas",
            "ai_keywords": [
                "Multi-Scale Attention",
                "multi-scale representations",
                "bi-directional cross-scale communication",
                "cross-attention",
                "Atlas",
                "neural network architecture",
                "compute-performance tradeoff",
                "long-context image modeling",
                "high-resolution variant",
                "ImageNet 100",
                "ConvNext-B",
                "FasterViT",
                "LongViT",
                "MambaVision-S"
            ]
        },
        "publishedAt": "2025-03-16T00:52:13.000Z",
        "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling",
        "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/Rff3u2uP4fPRO-4OHcR_Y.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/7WFEZ_zDr9zYLsZkGb34U.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/sWNex7CarElcXriLARI5q.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12355.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6254623af74165c7d2df0c18",
            "avatarUrl": "/avatars/4ddd1aa858723dd9348fc51966f700c3.svg",
            "fullname": "Kumar Krishna Agrawal",
            "name": "kumarkrishna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13265",
            "authors": [
                {
                    "_id": "67da8c561a87aaeb4ca06067",
                    "user": {
                        "_id": "67a2c1eb07690f2a570edb15",
                        "avatarUrl": "/avatars/b4a56782774a590ef01b1a1ec96464b2.svg",
                        "isPro": false,
                        "fullname": "luxi chen",
                        "user": "lucydetrack",
                        "type": "user"
                    },
                    "name": "Luxi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:39:20.898Z",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca06068",
                    "user": {
                        "_id": "64586db4bb7bdc9655afb5ce",
                        "avatarUrl": "/avatars/0756344c48927a3b92e583fe6a43868c.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "ZihanZhou",
                        "type": "user"
                    },
                    "name": "Zihan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:39:28.536Z",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca06069",
                    "name": "Min Zhao",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca0606a",
                    "name": "Yikai Wang",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca0606b",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:40:15.137Z",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca0606c",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca0606d",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca0606e",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:39:41.490Z",
                    "hidden": false
                },
                {
                    "_id": "67da8c561a87aaeb4ca0606f",
                    "user": {
                        "_id": "64c07b488e2612254361153b",
                        "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg",
                        "isPro": false,
                        "fullname": "chongxuan li",
                        "user": "zhenxuan00",
                        "type": "user"
                    },
                    "name": "Chongxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:39:35.810Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T15:18:38.000Z",
            "submittedOnDailyAt": "2025-03-19T09:09:30.790Z",
            "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming,\nfrom single images is challenging due to a lack of 3D data. To this end, we\nintroduce FlexWorld, a novel framework consisting of two key components: (1) a\nstrong video-to-video (V2V) diffusion model to generate high-quality novel view\nimages from incomplete input rendered from a coarse scene, and (2) a\nprogressive expansion process to construct a complete 3D scene. In particular,\nleveraging an advanced pre-trained video model and accurate depth-estimated\ntraining pairs, our V2V model can generate novel views under large camera pose\nvariations. Building upon it, FlexWorld progressively generates new 3D content\nand integrates it into the global scene through geometry-aware scene fusion.\nExtensive experiments demonstrate the effectiveness of FlexWorld in generating\nhigh-quality novel view videos and flexible-view 3D scenes from single images,\nachieving superior visual quality under multiple popular metrics and datasets\ncompared to existing state-of-the-art methods. Qualitatively, we highlight that\nFlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg}\nrotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.",
            "upvotes": 8,
            "discussionId": "67da8c601a87aaeb4ca06337",
            "ai_keywords": [
                "video-to-video (V2V) diffusion model",
                "novel view images",
                "coarse scene",
                "progressive expansion process",
                "advanced pre-trained video model",
                "depth-estimated training pairs",
                "geometry-aware scene fusion",
                "novel view videos",
                "flexible-view 3D scenes"
            ]
        },
        "publishedAt": "2025-03-17T11:18:38.000Z",
        "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis",
        "summary": "Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming,\nfrom single images is challenging due to a lack of 3D data. To this end, we\nintroduce FlexWorld, a novel framework consisting of two key components: (1) a\nstrong video-to-video (V2V) diffusion model to generate high-quality novel view\nimages from incomplete input rendered from a coarse scene, and (2) a\nprogressive expansion process to construct a complete 3D scene. In particular,\nleveraging an advanced pre-trained video model and accurate depth-estimated\ntraining pairs, our V2V model can generate novel views under large camera pose\nvariations. Building upon it, FlexWorld progressively generates new 3D content\nand integrates it into the global scene through geometry-aware scene fusion.\nExtensive experiments demonstrate the effectiveness of FlexWorld in generating\nhigh-quality novel view videos and flexible-view 3D scenes from single images,\nachieving superior visual quality under multiple popular metrics and datasets\ncompared to existing state-of-the-art methods. Qualitatively, we highlight that\nFlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg}\nrotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13265.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12505",
            "authors": [
                {
                    "_id": "67d9442ec37d05ff0ab28e44",
                    "user": {
                        "_id": "646dee28174cc96d50951991",
                        "avatarUrl": "/avatars/17d88a24c905e9819268b27037015a35.svg",
                        "isPro": false,
                        "fullname": "xu",
                        "user": "xuzhaopan",
                        "type": "user"
                    },
                    "name": "Zhaopan Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:36:05.632Z",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e45",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e46",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e47",
                    "user": {
                        "_id": "65d8b116f5d21ab1939efe03",
                        "avatarUrl": "/avatars/54d84f74faa2cc6fb8d6b58573dfde6d.svg",
                        "isPro": false,
                        "fullname": "Wangbo Zhao",
                        "user": "heisejiasuo",
                        "type": "user"
                    },
                    "name": "Wangbo Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:36:30.319Z",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e48",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e49",
                    "user": {
                        "_id": "64acc5b3f5d40283c620d625",
                        "avatarUrl": "/avatars/1e7f488db184f445f7d93a37f6470590.svg",
                        "isPro": false,
                        "fullname": "Xiaojiang Peng",
                        "user": "xpeng-sztu",
                        "type": "user"
                    },
                    "name": "Xiaojiang Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:36:37.040Z",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e4a",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:36:43.155Z",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e4b",
                    "name": "Hongxun Yao",
                    "hidden": false
                },
                {
                    "_id": "67d9442ec37d05ff0ab28e4c",
                    "user": {
                        "_id": "65f1713552c38a91e0a445e8",
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:37:10.793Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T13:50:38.000Z",
            "submittedOnDailyAt": "2025-03-19T00:16:20.299Z",
            "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
            "upvotes": 8,
            "discussionId": "67d94430c37d05ff0ab28eb3",
            "projectPage": "https://mpbench.github.io/",
            "ai_keywords": [
                "process-level reward models (PRMs)",
                "reinforcement learning",
                "step-wise rewards",
                "error detection",
                "reasoning search",
                "MPBench",
                "multi-task",
                "multimodal benchmark",
                "Step Correctness",
                "Answer Aggregation",
                "Reasoning Process Search"
            ]
        },
        "publishedAt": "2025-03-16T09:50:38.000Z",
        "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
        "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12505.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14495",
            "authors": [
                {
                    "_id": "67da454fd5132b0eebd066ff",
                    "user": {
                        "_id": "66b42f6717f9cb4e8d70afd2",
                        "avatarUrl": "/avatars/3829e8c7044fac861405d1a48c02aab5.svg",
                        "isPro": false,
                        "fullname": "Jiacheng Guo",
                        "user": "gjcjcg",
                        "type": "user"
                    },
                    "name": "Jiacheng Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:43:08.518Z",
                    "hidden": false
                },
                {
                    "_id": "67da454fd5132b0eebd06700",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "67da454fd5132b0eebd06701",
                    "name": "Jiahao Qiu",
                    "hidden": false
                },
                {
                    "_id": "67da454fd5132b0eebd06702",
                    "user": {
                        "_id": "64a02d1625af3c141c2653da",
                        "avatarUrl": "/avatars/d9a79815188020f795ab7aa64707dfeb.svg",
                        "isPro": false,
                        "fullname": "huangkaixuan",
                        "user": "huangkaixuan",
                        "type": "user"
                    },
                    "name": "Kaixuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:43:51.256Z",
                    "hidden": false
                },
                {
                    "_id": "67da454fd5132b0eebd06703",
                    "user": {
                        "_id": "674500b57a76d46e9141af8b",
                        "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
                        "isPro": false,
                        "fullname": "Xinzhe Juan",
                        "user": "ChrisJuan",
                        "type": "user"
                    },
                    "name": "Xinzhe Juan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:43:39.557Z",
                    "hidden": false
                },
                {
                    "_id": "67da454fd5132b0eebd06704",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67da454fd5132b0eebd06705",
                    "user": {
                        "_id": "6599415e8c8ac79295e0b5e3",
                        "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
                        "isPro": false,
                        "fullname": "Mengdi Wang",
                        "user": "Edify-Kd2024",
                        "type": "user"
                    },
                    "name": "Mengdi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:43:31.791Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:58:28.000Z",
            "submittedOnDailyAt": "2025-03-19T02:48:17.494Z",
            "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
            "upvotes": 6,
            "discussionId": "67da4550d5132b0eebd0673c",
            "ai_keywords": [
                "temporal consistency",
                "verifiers",
                "iterative refinement",
                "self-reflection actions",
                "verification accuracy",
                "Mathcheck",
                "ProcessBench",
                "PRM800K",
                "DeepSeek R1",
                "distilled models",
                "GPT-4o",
                "performance comparable"
            ]
        },
        "publishedAt": "2025-03-18T13:58:28.000Z",
        "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
        "summary": "Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14495.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14151",
            "authors": [
                {
                    "_id": "67da71b1c26b43885226a72d",
                    "name": "Yong Zhong",
                    "hidden": false
                },
                {
                    "_id": "67da71b1c26b43885226a72e",
                    "name": "Zhuoyi Yang",
                    "hidden": false
                },
                {
                    "_id": "67da71b1c26b43885226a72f",
                    "user": {
                        "_id": "65228733377bffdc59a10117",
                        "avatarUrl": "/avatars/6eec07553658ab22f8058caa0bfbed49.svg",
                        "isPro": false,
                        "fullname": "tengjiayan",
                        "user": "tengjiayan",
                        "type": "user"
                    },
                    "name": "Jiayan Teng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:42:47.456Z",
                    "hidden": false
                },
                {
                    "_id": "67da71b1c26b43885226a730",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "67da71b1c26b43885226a731",
                    "user": {
                        "_id": "64c07b488e2612254361153b",
                        "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg",
                        "isPro": false,
                        "fullname": "chongxuan li",
                        "user": "zhenxuan00",
                        "type": "user"
                    },
                    "name": "Chongxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:42:26.051Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T11:17:32.000Z",
            "submittedOnDailyAt": "2025-03-19T05:57:05.443Z",
            "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
            "upvotes": 6,
            "discussionId": "67da71bdc26b43885226ab4e",
            "projectPage": "https://ml-gsai.github.io/Concat-ID-demo/",
            "githubRepo": "https://github.com/ML-GSAI/Concat-ID",
            "ai_keywords": [
                "Variational Autoencoders",
                "3D self-attention mechanisms",
                "cross-video pairing strategy",
                "multi-stage training regimen",
                "identity consistency",
                "facial editability",
                "video naturalness",
                "identity-preserving video synthesis"
            ]
        },
        "publishedAt": "2025-03-18T07:17:32.000Z",
        "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
        "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14151.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 35
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.13111",
            "authors": [
                {
                    "_id": "67da9cfc42f0b1995666d81d",
                    "user": {
                        "_id": "66150e514476535462a7166c",
                        "avatarUrl": "/avatars/75a0b6546dde9853a06c9536f067005d.svg",
                        "isPro": false,
                        "fullname": "Erik Daxberger",
                        "user": "edaxberger",
                        "type": "user"
                    },
                    "name": "Erik Daxberger",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:48:58.293Z",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d81e",
                    "user": {
                        "_id": "65f9a3ebf8c8ab9966fccffc",
                        "avatarUrl": "/avatars/07e466225e1142d2fd7ebc2f23fe5062.svg",
                        "isPro": false,
                        "fullname": "Nina Wenzel",
                        "user": "nm-w",
                        "type": "user"
                    },
                    "name": "Nina Wenzel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:49:04.691Z",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d81f",
                    "name": "David Griffiths",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d820",
                    "name": "Haiming Gang",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d821",
                    "user": {
                        "_id": "66a13bb1c7a8aaa80e69ff00",
                        "avatarUrl": "/avatars/b09aff14b97dcbcfb7601e26bdc0dda2.svg",
                        "isPro": false,
                        "fullname": "Justin Lazarow",
                        "user": "jlazarow",
                        "type": "user"
                    },
                    "name": "Justin Lazarow",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:49:38.680Z",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d822",
                    "user": {
                        "_id": "6729c5cf10c844285570ccd8",
                        "avatarUrl": "/avatars/162a86583e71dcb005bf79d8e182da85.svg",
                        "isPro": false,
                        "fullname": "Gefen Kohavi",
                        "user": "gekoh",
                        "type": "user"
                    },
                    "name": "Gefen Kohavi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:49:44.261Z",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d823",
                    "name": "Kai Kang",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d824",
                    "name": "Marcin Eichner",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d825",
                    "user": {
                        "_id": "64b762568c632fbca942a405",
                        "avatarUrl": "/avatars/1eb737ec169967872f1ebf5ff29f1e6b.svg",
                        "isPro": false,
                        "fullname": "Yinfei Yang",
                        "user": "yinfeiy",
                        "type": "user"
                    },
                    "name": "Yinfei Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:49:54.995Z",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d826",
                    "user": {
                        "_id": "66fc2377516eaf950d4b8209",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mcUxUxXy18Gv9KvCW23s0.png",
                        "isPro": false,
                        "fullname": "Afshin Dehghan",
                        "user": "afshindn",
                        "type": "user"
                    },
                    "name": "Afshin Dehghan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:50:01.179Z",
                    "hidden": false
                },
                {
                    "_id": "67da9cfc42f0b1995666d827",
                    "user": {
                        "_id": "6649e0b6f50d471119285ab8",
                        "avatarUrl": "/avatars/1c93d0e15f2b68281a1a2ea052450a45.svg",
                        "isPro": false,
                        "fullname": "Peter Grasch",
                        "user": "petergrasch",
                        "type": "user"
                    },
                    "name": "Peter Grasch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:50:07.186Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66150e514476535462a7166c/aGVXY1PdRCuAr3XuvPI21.png"
            ],
            "publishedAt": "2025-03-17T12:34:22.000Z",
            "submittedOnDailyAt": "2025-03-19T09:09:25.984Z",
            "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "66150e514476535462a7166c",
                "avatarUrl": "/avatars/75a0b6546dde9853a06c9536f067005d.svg",
                "isPro": false,
                "fullname": "Erik Daxberger",
                "user": "edaxberger",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.",
            "upvotes": 6,
            "discussionId": "67da9cff42f0b1995666d8f6",
            "ai_keywords": [
                "Multimodal large language models (MLLMs)",
                "3D scene data",
                "open-set annotations",
                "supervised fine-tuning dataset",
                "evaluation benchmark",
                "indoor scenes",
                "Cubify Anything VQA (CA-VQA)",
                "spatial relationship prediction",
                "metric size and distance estimation",
                "3D grounding",
                "MM-Spatial",
                "state-of-the-art performance",
                "3D spatial understanding",
                "metric depth",
                "multi-view inputs",
                "depth perception",
                "dedicated monocular depth estimation models"
            ]
        },
        "publishedAt": "2025-03-17T08:34:22.000Z",
        "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
        "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66150e514476535462a7166c/aGVXY1PdRCuAr3XuvPI21.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13111.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66150e514476535462a7166c",
            "avatarUrl": "/avatars/75a0b6546dde9853a06c9536f067005d.svg",
            "fullname": "Erik Daxberger",
            "name": "edaxberger",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12271",
            "authors": [
                {
                    "_id": "67d926523acb37a1cfa74cf8",
                    "user": {
                        "_id": "6310531914aa81e1044363ed",
                        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
                        "isPro": false,
                        "fullname": "Shufan Li",
                        "user": "jacklishufan",
                        "type": "user"
                    },
                    "name": "Shufan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:47:36.651Z",
                    "hidden": false
                },
                {
                    "_id": "67d926523acb37a1cfa74cf9",
                    "name": "Konstantinos Kallidromitis",
                    "hidden": false
                },
                {
                    "_id": "67d926523acb37a1cfa74cfa",
                    "user": {
                        "_id": "63093856b105f8675bd1c783",
                        "avatarUrl": "/avatars/505a7b99893330a36a136d13fa99e05e.svg",
                        "isPro": false,
                        "fullname": "Akash Gokul",
                        "user": "akashgokul",
                        "type": "user"
                    },
                    "name": "Akash Gokul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:47:47.689Z",
                    "hidden": false
                },
                {
                    "_id": "67d926523acb37a1cfa74cfb",
                    "user": {
                        "_id": "65cdc744b4a3a21113eed76a",
                        "avatarUrl": "/avatars/f1dceb512382cafbde81e3b8e28b10cf.svg",
                        "isPro": false,
                        "fullname": "Arsh Koneru",
                        "user": "arshkon",
                        "type": "user"
                    },
                    "name": "Arsh Koneru",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:47:55.709Z",
                    "hidden": false
                },
                {
                    "_id": "67d926523acb37a1cfa74cfc",
                    "name": "Yusuke Kato",
                    "hidden": false
                },
                {
                    "_id": "67d926523acb37a1cfa74cfd",
                    "name": "Kazuki Kozuka",
                    "hidden": false
                },
                {
                    "_id": "67d926523acb37a1cfa74cfe",
                    "user": {
                        "_id": "65d38c1e5d5749637491679b",
                        "avatarUrl": "/avatars/010a6588f382d6d4394216549a2385e3.svg",
                        "isPro": false,
                        "fullname": "Aditya Grover",
                        "user": "adityagrover",
                        "type": "user"
                    },
                    "name": "Aditya Grover",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:48:22.431Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-15T21:58:12.000Z",
            "submittedOnDailyAt": "2025-03-19T02:08:42.869Z",
            "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
            "submittedOnDailyBy": {
                "_id": "6310531914aa81e1044363ed",
                "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
                "isPro": false,
                "fullname": "Shufan Li",
                "user": "jacklishufan",
                "type": "user"
            },
            "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.",
            "upvotes": 6,
            "discussionId": "67d926543acb37a1cfa74d9f",
            "ai_keywords": [
                "diffusion models",
                "text-to-image diffusion models",
                "best-of-N sampling",
                "in-context reflection",
                "Diffusion Transformers",
                "Reflect-DiT",
                "GenEval benchmark",
                "in-context examples",
                "textual feedback",
                "performance improvement",
                "state-of-the-art"
            ]
        },
        "publishedAt": "2025-03-15T17:58:12.000Z",
        "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
        "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12271.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6310531914aa81e1044363ed",
            "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
            "fullname": "Shufan Li",
            "name": "jacklishufan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12545",
            "authors": [
                {
                    "_id": "67d943d272843a36b74ab41c",
                    "user": {
                        "_id": "646dee28174cc96d50951991",
                        "avatarUrl": "/avatars/17d88a24c905e9819268b27037015a35.svg",
                        "isPro": false,
                        "fullname": "xu",
                        "user": "xuzhaopan",
                        "type": "user"
                    },
                    "name": "Zhaopan Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:50:26.175Z",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab41d",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab41e",
                    "user": {
                        "_id": "67933ffb4fa1841697c9a77d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zYVbcrtZvcIC0BKXLQUBE.png",
                        "isPro": false,
                        "fullname": "weidong.tang",
                        "user": "weidongTang",
                        "type": "user"
                    },
                    "name": "Weidong Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:50:48.159Z",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab41f",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab420",
                    "user": {
                        "_id": "65d8b116f5d21ab1939efe03",
                        "avatarUrl": "/avatars/54d84f74faa2cc6fb8d6b58573dfde6d.svg",
                        "isPro": false,
                        "fullname": "Wangbo Zhao",
                        "user": "heisejiasuo",
                        "type": "user"
                    },
                    "name": "Wangbo Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:50:50.721Z",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab421",
                    "user": {
                        "_id": "64acc5b3f5d40283c620d625",
                        "avatarUrl": "/avatars/1e7f488db184f445f7d93a37f6470590.svg",
                        "isPro": false,
                        "fullname": "Xiaojiang Peng",
                        "user": "xpeng-sztu",
                        "type": "user"
                    },
                    "name": "Xiaojiang Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:50:56.476Z",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab422",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab423",
                    "name": "Yang You",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab424",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:51:03.971Z",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab425",
                    "name": "Hongxun Yao",
                    "hidden": false
                },
                {
                    "_id": "67d943d272843a36b74ab426",
                    "user": {
                        "_id": "65f1713552c38a91e0a445e8",
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:51:29.357Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T15:26:20.000Z",
            "submittedOnDailyAt": "2025-03-19T00:14:50.269Z",
            "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.",
            "upvotes": 4,
            "discussionId": "67d943db72843a36b74ab652",
            "projectPage": "https://pebench.github.io/",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "visual question answering",
                "visual understanding",
                "reasoning",
                "machine unlearning",
                "benchmark",
                "PEBench",
                "dataset",
                "personal entities",
                "general event scenes",
                "secure",
                "privacy-preserving",
                "multimodal models"
            ]
        },
        "publishedAt": "2025-03-16T11:26:20.000Z",
        "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
        "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12545.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12303",
            "authors": [
                {
                    "_id": "67da52cd11b6db69208527b9",
                    "name": "Xiaoying Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527ba",
                    "user": {
                        "_id": "665b276ea132595fafc4a431",
                        "avatarUrl": "/avatars/f238582a506bc24a822360b4d4cbd625.svg",
                        "isPro": false,
                        "fullname": "PengDa",
                        "user": "PengDa02",
                        "type": "user"
                    },
                    "name": "Da Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:20.846Z",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527bb",
                    "user": {
                        "_id": "666e91b1623133f1ce35acc5",
                        "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
                        "isPro": false,
                        "fullname": "YipengZhang",
                        "user": "YipengZhang",
                        "type": "user"
                    },
                    "name": "Yipeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:16.169Z",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527bc",
                    "user": {
                        "_id": "6491af36c1741666238f3bff",
                        "avatarUrl": "/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg",
                        "isPro": false,
                        "fullname": "Zonghao Guo",
                        "user": "guozonghao96",
                        "type": "user"
                    },
                    "name": "Zonghao Guo",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T05:14:57.514Z",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527bd",
                    "name": "Chengyue Wu",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527be",
                    "name": "Chi Chen",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527bf",
                    "name": "Wei Ke",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527c0",
                    "name": "Helen Meng",
                    "hidden": false
                },
                {
                    "_id": "67da52cd11b6db69208527c1",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T00:25:13.000Z",
            "submittedOnDailyAt": "2025-03-19T11:48:17.192Z",
            "title": "Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs",
            "submittedOnDailyBy": {
                "_id": "665b276ea132595fafc4a431",
                "avatarUrl": "/avatars/f238582a506bc24a822360b4d4cbd625.svg",
                "isPro": false,
                "fullname": "PengDa",
                "user": "PengDa02",
                "type": "user"
            },
            "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent multimodal pre-training approaches focus on enhancing perception by\ntraining on high-quality image captions due to the extremely high cost of\ncollecting chain-of-thought (CoT) reasoning data for improving reasoning. While\nleveraging advanced MLLMs for caption generation enhances scalability, the\noutputs often lack comprehensiveness and accuracy. In this paper, we introduce\nSelf-Improving cognition (SIcog), a self-learning framework designed to\nconstruct next-generation foundation MLLMs by enhancing their systematic\ncognitive capabilities through multimodal pre-training with self-generated\ndata. Specifically, we propose Chain-of-Description, an approach that improves\nan MLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used for\nmultimodal pre-training to develop next-generation foundation models. Extensive\nexperiments on both low- and high-resolution MLLMs across diverse benchmarks\ndemonstrate that, with merely 213K self-generated pre-training samples, SIcog\nproduces next-generation foundation MLLMs with significantly improved\ncognition, achieving benchmark-leading performance compared to prevalent\npre-training approaches.",
            "upvotes": 4,
            "discussionId": "67da52d111b6db6920852912",
            "githubRepo": "https://github.com/thunlp/SICOG",
            "ai_keywords": [
                "Self-Improving cognition (SIcog)",
                "Chain-of-Description",
                "systematic perception",
                "systematic reasoning",
                "step-by-step visual understanding",
                "structured CoT reasoning technique",
                "self-generated data",
                "self-consistency",
                "multimodal pre-training",
                "next-generation foundation MLLMs",
                "benchmark-leading performance"
            ]
        },
        "publishedAt": "2025-03-15T20:25:13.000Z",
        "title": "Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs",
        "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent multimodal pre-training approaches focus on enhancing perception by\ntraining on high-quality image captions due to the extremely high cost of\ncollecting chain-of-thought (CoT) reasoning data for improving reasoning. While\nleveraging advanced MLLMs for caption generation enhances scalability, the\noutputs often lack comprehensiveness and accuracy. In this paper, we introduce\nSelf-Improving cognition (SIcog), a self-learning framework designed to\nconstruct next-generation foundation MLLMs by enhancing their systematic\ncognitive capabilities through multimodal pre-training with self-generated\ndata. Specifically, we propose Chain-of-Description, an approach that improves\nan MLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used for\nmultimodal pre-training to develop next-generation foundation models. Extensive\nexperiments on both low- and high-resolution MLLMs across diverse benchmarks\ndemonstrate that, with merely 213K self-generated pre-training samples, SIcog\nproduces next-generation foundation MLLMs with significantly improved\ncognition, achieving benchmark-leading performance compared to prevalent\npre-training approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12303.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "665b276ea132595fafc4a431",
            "avatarUrl": "/avatars/f238582a506bc24a822360b4d4cbd625.svg",
            "fullname": "PengDa",
            "name": "PengDa02",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.09443",
            "authors": [
                {
                    "_id": "67da8b794e1138ddc328de09",
                    "user": {
                        "_id": "630a4aaa9df54451d91cc6fa",
                        "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
                        "isPro": false,
                        "fullname": "Julian Spravil",
                        "user": "Spravil",
                        "type": "user"
                    },
                    "name": "Julian Spravil",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:43:48.893Z",
                    "hidden": false
                },
                {
                    "_id": "67da8b794e1138ddc328de0a",
                    "user": {
                        "_id": "654892f5c6dadd513f5a7143",
                        "avatarUrl": "/avatars/c167b45776391e02709246670e4843d1.svg",
                        "isPro": false,
                        "fullname": "Sebastian Houben",
                        "user": "EthylS2",
                        "type": "user"
                    },
                    "name": "Sebastian Houben",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:47:14.517Z",
                    "hidden": false
                },
                {
                    "_id": "67da8b794e1138ddc328de0b",
                    "user": {
                        "_id": "65df1a033e759810359ad7c8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df1a033e759810359ad7c8/51dnHqNxFuOT6pStpLYyX.jpeg",
                        "isPro": false,
                        "fullname": "Sven Behnke",
                        "user": "sven-behnke",
                        "type": "user"
                    },
                    "name": "Sven Behnke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:47:23.428Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-12T14:41:10.000Z",
            "submittedOnDailyAt": "2025-03-19T07:58:17.987Z",
            "title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "630a4aaa9df54451d91cc6fa",
                "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
                "isPro": false,
                "fullname": "Julian Spravil",
                "user": "Spravil",
                "type": "user"
            },
            "summary": "Cross-lingual transfer enables vision-language models (VLMs) to perform\nvision tasks in various languages with training data only in one language.\nCurrent approaches rely on large pre-trained multilingual language models.\nHowever, they face the curse of multilinguality, sacrificing downstream task\nperformance for multilingual capabilities, struggling with lexical ambiguities,\nand falling behind recent advances. In this work, we study the scaling laws of\nsystematic generalization with monolingual VLMs for multilingual tasks,\nfocusing on the impact of model size and seen training samples. We propose\nFlorenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters\ncombining the pre-trained VLM Florence-2 and the large language model Gemma-2.\nFlorenz is trained with varying compute budgets on a synthetic dataset that\nfeatures intentionally incomplete language coverage for image captioning, thus,\ntesting generalization from the fully covered translation task. We show that\nnot only does indirectly learning unseen task-language pairs adhere to a\nscaling law, but also that with our data generation pipeline and the proposed\nFlorenz model family, image captioning abilities can emerge in a specific\nlanguage even when only data for the translation task is available. Fine-tuning\non a mix of downstream datasets yields competitive performance and demonstrates\npromising scaling trends in multimodal machine translation (Multi30K, CoMMuTE),\nlexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO\nKarpathy).",
            "upvotes": 4,
            "discussionId": "67da8b7a4e1138ddc328de44",
            "ai_keywords": [
                "vision-language models",
                "cross-lingual transfer",
                "multilingual language models",
                "curse of multilinguality",
                "lexical ambiguities",
                "systematic generalization",
                "monolingual encoder-decoder",
                "model size",
                "seen training samples",
                "image captioning",
                "synthetic dataset",
                "translation task",
                "scaling law",
                "downstream datasets",
                "multimodal machine translation",
                "lexical disambiguation"
            ]
        },
        "publishedAt": "2025-03-12T10:41:10.000Z",
        "title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models",
        "summary": "Cross-lingual transfer enables vision-language models (VLMs) to perform\nvision tasks in various languages with training data only in one language.\nCurrent approaches rely on large pre-trained multilingual language models.\nHowever, they face the curse of multilinguality, sacrificing downstream task\nperformance for multilingual capabilities, struggling with lexical ambiguities,\nand falling behind recent advances. In this work, we study the scaling laws of\nsystematic generalization with monolingual VLMs for multilingual tasks,\nfocusing on the impact of model size and seen training samples. We propose\nFlorenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters\ncombining the pre-trained VLM Florence-2 and the large language model Gemma-2.\nFlorenz is trained with varying compute budgets on a synthetic dataset that\nfeatures intentionally incomplete language coverage for image captioning, thus,\ntesting generalization from the fully covered translation task. We show that\nnot only does indirectly learning unseen task-language pairs adhere to a\nscaling law, but also that with our data generation pipeline and the proposed\nFlorenz model family, image captioning abilities can emerge in a specific\nlanguage even when only data for the translation task is available. Fine-tuning\non a mix of downstream datasets yields competitive performance and demonstrates\npromising scaling trends in multimodal machine translation (Multi30K, CoMMuTE),\nlexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO\nKarpathy).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09443.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630a4aaa9df54451d91cc6fa",
            "avatarUrl": "/avatars/10898c7886e25c64d58334083e3cf2d5.svg",
            "fullname": "Julian Spravil",
            "name": "Spravil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10905",
            "authors": [
                {
                    "_id": "67daf18da3b9c06eb0727795",
                    "name": "Zhuoyan Xu",
                    "hidden": false
                },
                {
                    "_id": "67daf18da3b9c06eb0727796",
                    "name": "Khoi Duc Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67daf18da3b9c06eb0727797",
                    "name": "Preeti Mukherjee",
                    "hidden": false
                },
                {
                    "_id": "67daf18da3b9c06eb0727798",
                    "name": "Saurabh Bagchi",
                    "hidden": false
                },
                {
                    "_id": "67daf18da3b9c06eb0727799",
                    "name": "Somali Chaterji",
                    "hidden": false
                },
                {
                    "_id": "67daf18da3b9c06eb072779a",
                    "name": "Yingyu Liang",
                    "hidden": false
                },
                {
                    "_id": "67daf18da3b9c06eb072779b",
                    "name": "Yin Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64be0648b567ae97c3493af6/kgD71NUu0g_Yp7y44v-ft.png"
            ],
            "publishedAt": "2025-03-13T21:39:38.000Z",
            "submittedOnDailyAt": "2025-03-19T15:07:35.837Z",
            "title": "Learning to Inference Adaptively for Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64be0648b567ae97c3493af6",
                "avatarUrl": "/avatars/c932a4d49e2c9d72898d190de6545f92.svg",
                "isPro": false,
                "fullname": "Zhuoyan Xu",
                "user": "zhuoyanxu",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities\nin reasoning, yet come with substantial computational cost, limiting their\ndeployment in resource-constrained settings. Despite recent efforts on\nimproving the efficiency of MLLMs, prior solutions fall short in responding to\nvarying runtime conditions, in particular changing resource availability (e.g.,\ncontention due to the execution of other programs on the device). To bridge\nthis gap, we introduce AdaLLaVA, an adaptive inference framework that learns to\ndynamically reconfigure operations in an MLLM during inference, accounting for\nthe input data and a latency budget. We conduct extensive experiments across\nbenchmarks involving question-answering, reasoning, and hallucination. Our\nresults show that AdaLLaVA effectively adheres to input latency budget,\nachieving varying accuracy and latency tradeoffs at runtime. Further, we\ndemonstrate that AdaLLaVA adapts to both input latency and content, can be\nintegrated with token selection for enhanced efficiency, and generalizes across\nMLLMs. Our project webpage with code release is at\nhttps://zhuoyan-xu.github.io/ada-llava/.",
            "upvotes": 2,
            "discussionId": "67daf18ea3b9c06eb07277d1",
            "projectPage": "https://zhuoyan-xu.github.io/ada-llava/",
            "githubRepo": "https://github.com/zhuoyan-xu/AdaLLaVA",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "reasoning",
                "computational cost",
                "resource-constrained settings",
                "adaptive inference framework",
                "reconfigure operations",
                "latency budget",
                "question-answering",
                "hallucination",
                "accuracy and latency tradeoffs",
                "token selection"
            ]
        },
        "publishedAt": "2025-03-13T17:39:38.000Z",
        "title": "Learning to Inference Adaptively for Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities\nin reasoning, yet come with substantial computational cost, limiting their\ndeployment in resource-constrained settings. Despite recent efforts on\nimproving the efficiency of MLLMs, prior solutions fall short in responding to\nvarying runtime conditions, in particular changing resource availability (e.g.,\ncontention due to the execution of other programs on the device). To bridge\nthis gap, we introduce AdaLLaVA, an adaptive inference framework that learns to\ndynamically reconfigure operations in an MLLM during inference, accounting for\nthe input data and a latency budget. We conduct extensive experiments across\nbenchmarks involving question-answering, reasoning, and hallucination. Our\nresults show that AdaLLaVA effectively adheres to input latency budget,\nachieving varying accuracy and latency tradeoffs at runtime. Further, we\ndemonstrate that AdaLLaVA adapts to both input latency and content, can be\nintegrated with token selection for enhanced efficiency, and generalizes across\nMLLMs. Our project webpage with code release is at\nhttps://zhuoyan-xu.github.io/ada-llava/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64be0648b567ae97c3493af6/kgD71NUu0g_Yp7y44v-ft.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64be0648b567ae97c3493af6",
            "avatarUrl": "/avatars/c932a4d49e2c9d72898d190de6545f92.svg",
            "fullname": "Zhuoyan Xu",
            "name": "zhuoyanxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.10546",
            "authors": [
                {
                    "_id": "67da141d6b2857e3ec1412a7",
                    "name": "Zixian Liu",
                    "hidden": false
                },
                {
                    "_id": "67da141d6b2857e3ec1412a8",
                    "user": {
                        "_id": "671c6a3e255aa50ebb504fc5",
                        "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
                        "isPro": false,
                        "fullname": "Mingtong Zhang",
                        "user": "Mingtongz",
                        "type": "user"
                    },
                    "name": "Mingtong Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:46:28.495Z",
                    "hidden": false
                },
                {
                    "_id": "67da141d6b2857e3ec1412a9",
                    "user": {
                        "_id": "64a46562c641afb468a3dc04",
                        "avatarUrl": "/avatars/bc54e5c13f657251b05bf2725086e56d.svg",
                        "isPro": false,
                        "fullname": "Yunzhu Li",
                        "user": "yunzhuli",
                        "type": "user"
                    },
                    "name": "Yunzhu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:46:19.486Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T16:59:17.000Z",
            "submittedOnDailyAt": "2025-03-19T00:00:05.763Z",
            "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
            "submittedOnDailyBy": {
                "_id": "671c6a3e255aa50ebb504fc5",
                "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
                "isPro": false,
                "fullname": "Mingtong Zhang",
                "user": "Mingtongz",
                "type": "user"
            },
            "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
            "upvotes": 2,
            "discussionId": "67da141e6b2857e3ec141301",
            "projectPage": "https://kuda-dynamics.github.io",
            "githubRepo": "https://github.com/StoreBlank/KUDA",
            "ai_keywords": [
                "LLMs",
                "VLMs",
                "open-vocabulary robotic manipulation systems",
                "object dynamics",
                "KUDA",
                "dynamics learning",
                "visual prompting",
                "keypoints",
                "learning-based neural dynamics models",
                "keypoint-based target specification",
                "cost functions",
                "model-based planning",
                "robotic trajectories",
                "free-form language instructions",
                "multi-object interactions",
                "deformable objects",
                "granular objects"
            ]
        },
        "publishedAt": "2025-03-13T12:59:17.000Z",
        "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
        "summary": "With the rapid advancement of large language models (LLMs) and\nvision-language models (VLMs), significant progress has been made in developing\nopen-vocabulary robotic manipulation systems. However, many existing approaches\noverlook the importance of object dynamics, limiting their applicability to\nmore complex, dynamic tasks. In this work, we introduce KUDA, an\nopen-vocabulary manipulation system that integrates dynamics learning and\nvisual prompting through keypoints, leveraging both VLMs and learning-based\nneural dynamics models. Our key insight is that a keypoint-based target\nspecification is simultaneously interpretable by VLMs and can be efficiently\ntranslated into cost functions for model-based planning. Given language\ninstructions and visual observations, KUDA first assigns keypoints to the RGB\nimage and queries the VLM to generate target specifications. These abstract\nkeypoint-based representations are then converted into cost functions, which\nare optimized using a learned dynamics model to produce robotic trajectories.\nWe evaluate KUDA on a range of manipulation tasks, including free-form language\ninstructions across diverse object categories, multi-object interactions, and\ndeformable or granular objects, demonstrating the effectiveness of our\nframework. The project page is available at http://kuda-dynamics.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10546.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "671c6a3e255aa50ebb504fc5",
            "avatarUrl": "/avatars/b549bb19990ae21690472799817f951e.svg",
            "fullname": "Mingtong Zhang",
            "name": "Mingtongz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10410",
            "authors": [
                {
                    "_id": "67d9638d92e48ed07860ecee",
                    "user": {
                        "_id": "67934b85c67af4a116b5594b",
                        "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
                        "isPro": false,
                        "fullname": "yuwendu",
                        "user": "yuwendu",
                        "type": "user"
                    },
                    "name": "Yuwen Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:57:49.908Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecef",
                    "user": {
                        "_id": "6622a19fabae16393f24a3ff",
                        "avatarUrl": "/avatars/5578f13a000252d906455fc6361a4fa0.svg",
                        "isPro": false,
                        "fullname": "Anning Hu",
                        "user": "PathfinderToT",
                        "type": "user"
                    },
                    "name": "Anning Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:44:46.862Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf0",
                    "name": "Zichen Chao",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf1",
                    "user": {
                        "_id": "6434b8aeea46c009904ce8cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b8aeea46c009904ce8cb/AiXL-BJ4KFSPbaorKPHNL.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Lu",
                        "user": "yifanlu",
                        "type": "user"
                    },
                    "name": "Yifan Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:45:07.177Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf2",
                    "user": {
                        "_id": "664aebb7cf5fd472d7c37b2f",
                        "avatarUrl": "/avatars/59b44b608a27269ab680cafa29a50f18.svg",
                        "isPro": false,
                        "fullname": "Junhao Ge",
                        "user": "Cancaries",
                        "type": "user"
                    },
                    "name": "Junhao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:45:18.493Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf3",
                    "user": {
                        "_id": "634187b38d8089ebaef3dc18",
                        "avatarUrl": "/avatars/a225077ee00d34cd4c084e3e347ce047.svg",
                        "isPro": false,
                        "fullname": "Genjia Liu",
                        "user": "gjliu",
                        "type": "user"
                    },
                    "name": "Genjia Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:45:24.545Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf4",
                    "user": {
                        "_id": "65bc8587262a04f94c38808f",
                        "avatarUrl": "/avatars/9478bb68e931edc9b0eba86807de5579.svg",
                        "isPro": false,
                        "fullname": "Wuweitao",
                        "user": "VictorWuweitao",
                        "type": "user"
                    },
                    "name": "Weitao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:45:39.488Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf5",
                    "user": {
                        "_id": "646ec1c65d05113e4327d900",
                        "avatarUrl": "/avatars/02b02a885cac821430bf4916e91e626e.svg",
                        "isPro": false,
                        "fullname": "Lanjun Wang",
                        "user": "lanzerwang",
                        "type": "user"
                    },
                    "name": "Lanjun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:45:46.738Z",
                    "hidden": false
                },
                {
                    "_id": "67d9638d92e48ed07860ecf6",
                    "user": {
                        "_id": "65257545b017be1fc1915364",
                        "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg",
                        "isPro": false,
                        "fullname": "Siheng Chen",
                        "user": "sihengchen",
                        "type": "user"
                    },
                    "name": "Siheng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:45:52.526Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
            ],
            "publishedAt": "2025-03-13T14:33:42.000Z",
            "submittedOnDailyAt": "2025-03-19T00:00:36.900Z",
            "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
            "submittedOnDailyBy": {
                "_id": "67934b85c67af4a116b5594b",
                "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
                "isPro": false,
                "fullname": "yuwendu",
                "user": "yuwendu",
                "type": "user"
            },
            "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim",
            "upvotes": 2,
            "discussionId": "67d9639192e48ed07860ee1f",
            "ai_keywords": [
                "Multi-View Occlusion-Aware Sampler",
                "DepthSAM",
                "Scalable Post-Processing Toolkit",
                "3D object detection"
            ]
        },
        "publishedAt": "2025-03-13T10:33:42.000Z",
        "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
        "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/rDLvz9Jma7IWsaGubueis.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67934b85c67af4a116b5594b/1PULsx8sLyy9CqDRMuzo1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10410.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67934b85c67af4a116b5594b",
            "avatarUrl": "/avatars/076cf0803b50e1ab54e5ba4f8f2a8e44.svg",
            "fullname": "yuwendu",
            "name": "yuwendu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.08893",
            "authors": [
                {
                    "_id": "67dade8168f86d8f53993ed8",
                    "name": "Zhiyuan Zeng",
                    "hidden": false
                },
                {
                    "_id": "67dade8168f86d8f53993ed9",
                    "name": "Yizhong Wang",
                    "hidden": false
                },
                {
                    "_id": "67dade8168f86d8f53993eda",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                },
                {
                    "_id": "67dade8168f86d8f53993edb",
                    "name": "Pang Wei Koh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T21:12:48.000Z",
            "submittedOnDailyAt": "2025-03-19T13:45:17.001Z",
            "title": "EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees",
            "submittedOnDailyBy": {
                "_id": "64a85e23b6512b8328f9d9e2",
                "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
                "isPro": false,
                "fullname": "Zhiyuan Zeng",
                "user": "ZhiyuanZeng",
                "type": "user"
            },
            "summary": "An ideal model evaluation should achieve two goals: identifying where the\nmodel fails and providing actionable improvement guidance. Toward these goals\nfor Language Model (LM) evaluations, we formulate the problem of generating a\nweakness profile, a set of weaknesses expressed in natural language, given an\nLM's performance on every individual instance in a benchmark. We introduce a\nsuite of quantitative assessments to compare different weakness profiling\nmethods. We also propose a weakness profiling method EvalTree. It constructs a\ncapability tree where each node represents a capability described in natural\nlanguage and is linked to a subset of benchmark instances that specifically\nevaluate this capability; it then extracts nodes where the LM performs poorly\nto generate a weakness profile. On the MATH and WildChat benchmarks, we show\nthat EvalTree outperforms baseline weakness profiling methods by identifying\nweaknesses more precisely and comprehensively. Weakness profiling further\nenables weakness-guided data collection, and training data collection guided by\nEvalTree-identified weaknesses improves LM performance more than other data\ncollection strategies. We also show how EvalTree exposes flaws in Chatbot\nArena's human-voter-based evaluation practice. To facilitate future work, we\nrelease our code and an interface that allows practitioners to interactively\nexplore the capability trees built by EvalTree.",
            "upvotes": 2,
            "discussionId": "67dade8368f86d8f53993f4f",
            "projectPage": "https://zhiyuan-zeng.github.io/EvalTree/",
            "githubRepo": "https://github.com/Zhiyuan-Zeng/EvalTree",
            "ai_keywords": [
                "Language Model (LM)",
                "weakness profile",
                "capability tree",
                "benchmark instances",
                "weakness profiling",
                "EvalTree",
                "MATH",
                "WildChat",
                "human-voter-based evaluation practice"
            ]
        },
        "publishedAt": "2025-03-11T17:12:48.000Z",
        "title": "EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees",
        "summary": "An ideal model evaluation should achieve two goals: identifying where the\nmodel fails and providing actionable improvement guidance. Toward these goals\nfor Language Model (LM) evaluations, we formulate the problem of generating a\nweakness profile, a set of weaknesses expressed in natural language, given an\nLM's performance on every individual instance in a benchmark. We introduce a\nsuite of quantitative assessments to compare different weakness profiling\nmethods. We also propose a weakness profiling method EvalTree. It constructs a\ncapability tree where each node represents a capability described in natural\nlanguage and is linked to a subset of benchmark instances that specifically\nevaluate this capability; it then extracts nodes where the LM performs poorly\nto generate a weakness profile. On the MATH and WildChat benchmarks, we show\nthat EvalTree outperforms baseline weakness profiling methods by identifying\nweaknesses more precisely and comprehensively. Weakness profiling further\nenables weakness-guided data collection, and training data collection guided by\nEvalTree-identified weaknesses improves LM performance more than other data\ncollection strategies. We also show how EvalTree exposes flaws in Chatbot\nArena's human-voter-based evaluation practice. To facilitate future work, we\nrelease our code and an interface that allows practitioners to interactively\nexplore the capability trees built by EvalTree.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08893.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a85e23b6512b8328f9d9e2",
            "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
            "fullname": "Zhiyuan Zeng",
            "name": "ZhiyuanZeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14002",
            "authors": [
                {
                    "_id": "67dacbe75f7f898d5eb2d0ad",
                    "user": {
                        "_id": "64a6a1d2d09682887da149ea",
                        "avatarUrl": "/avatars/eab8441d681616fe44521f6a3c0cf60b.svg",
                        "isPro": true,
                        "fullname": "Damian Boborzi",
                        "user": "DamianBoborzi",
                        "type": "user"
                    },
                    "name": "Damian Boborzi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T14:25:21.075Z",
                    "hidden": false
                },
                {
                    "_id": "67dacbe75f7f898d5eb2d0ae",
                    "name": "Phillip Mueller",
                    "hidden": false
                },
                {
                    "_id": "67dacbe75f7f898d5eb2d0af",
                    "name": "Jonas Emrich",
                    "hidden": false
                },
                {
                    "_id": "67dacbe75f7f898d5eb2d0b0",
                    "name": "Dominik Schmid",
                    "hidden": false
                },
                {
                    "_id": "67dacbe75f7f898d5eb2d0b1",
                    "name": "Sebastian Mueller",
                    "hidden": false
                },
                {
                    "_id": "67dacbe75f7f898d5eb2d0b2",
                    "name": "Lars Mikelsons",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a6a1d2d09682887da149ea/nBQj_aejZ97Lhu52tzyRI.png"
            ],
            "publishedAt": "2025-03-18T08:09:24.000Z",
            "submittedOnDailyAt": "2025-03-19T13:04:20.532Z",
            "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling",
            "submittedOnDailyBy": {
                "_id": "64a6a1d2d09682887da149ea",
                "avatarUrl": "/avatars/eab8441d681616fe44521f6a3c0cf60b.svg",
                "isPro": true,
                "fullname": "Damian Boborzi",
                "user": "DamianBoborzi",
                "type": "user"
            },
            "summary": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.",
            "upvotes": 1,
            "discussionId": "67dacbec5f7f898d5eb2d22f",
            "githubRepo": "https://github.com/FeMa42/MeshFleet",
            "ai_keywords": [
                "generative models",
                "3D objects",
                "fine-tuning",
                "3D datasets",
                "data filtering",
                "annotation",
                "MeshFleet",
                "Objaverse-XL",
                "quality classifier",
                "DINOv2",
                "SigLIP embeddings",
                "caption-based analysis",
                "uncertainty estimation",
                "SV3D",
                "domain-specific 3D generative modeling"
            ]
        },
        "publishedAt": "2025-03-18T04:09:24.000Z",
        "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling",
        "summary": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a6a1d2d09682887da149ea/nBQj_aejZ97Lhu52tzyRI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14002.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a6a1d2d09682887da149ea",
            "avatarUrl": "/avatars/eab8441d681616fe44521f6a3c0cf60b.svg",
            "fullname": "Damian Boborzi",
            "name": "DamianBoborzi",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13661",
            "authors": [
                {
                    "_id": "67da8a9fdab8cc723c349fb0",
                    "user": {
                        "_id": "630a5ef0e81e1dea2cedcec0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
                        "isPro": false,
                        "fullname": "H Huy Hong",
                        "user": "HoangHa",
                        "type": "user"
                    },
                    "name": "Huy Hoang Ha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:43:52.541Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/630a5ef0e81e1dea2cedcec0/9DxLIM8Ftd4OVGfuGqAEU.png"
            ],
            "publishedAt": "2025-03-17T19:09:11.000Z",
            "submittedOnDailyAt": "2025-03-19T07:43:37.853Z",
            "title": "Pensez: Less Data, Better Reasoning -- Rethinking French LLM",
            "submittedOnDailyBy": {
                "_id": "630a5ef0e81e1dea2cedcec0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
                "isPro": false,
                "fullname": "H Huy Hong",
                "user": "HoangHa",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, achieving strong\nperformance in specialized domains like mathematical reasoning and non-English\nlanguages often requires extensive training on massive datasets. This paper\ninvestigates a contrasting approach: strategic fine-tuning on a small,\nhigh-quality, bilingual (English-French) dataset to enhance both the reasoning\ncapabilities and French language proficiency of a large language model. Rather\nthan relying on scale, we explore the hypothesis that targeted data curation\nand optimized training can achieve competitive, or even superior, performance.\nWe demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000\ncarefully selected samples, significant improvements in mathematical reasoning.\nSpecifically, Pensez 7B exhibits an increase in accuracy of the base model up\nto 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.\nThese results challenge the prevailing assumption that massive datasets are\naprerequisite for strong reasoning performance in LLMs, highlighting the\npotential of strategic data curation and optimized fine-tuning for enhancing\nboth specialized skills and multilingual capabilities. Our findings have\nimplications for the efficient development of high-performing, multilingual\nLLMs, especially in resource-constrained scenarios.",
            "upvotes": 1,
            "discussionId": "67da8aa1dab8cc723c34a039",
            "ai_keywords": [
                "large language models (LLMs)",
                "natural language processing tasks",
                "specialized domains",
                "mathematical reasoning",
                "non-English languages",
                "extensive training",
                "massive datasets",
                "strategic fine-tuning",
                "high-quality",
                "bilingual (English-French) dataset",
                "targeted data curation",
                "optimized training",
                "supervised fine-tuning (SFT)",
                "AIME25",
                "French MATH level 5 benchmark",
                "resource-constrained scenarios"
            ]
        },
        "publishedAt": "2025-03-17T15:09:11.000Z",
        "title": "Pensez: Less Data, Better Reasoning -- Rethinking French LLM",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, achieving strong\nperformance in specialized domains like mathematical reasoning and non-English\nlanguages often requires extensive training on massive datasets. This paper\ninvestigates a contrasting approach: strategic fine-tuning on a small,\nhigh-quality, bilingual (English-French) dataset to enhance both the reasoning\ncapabilities and French language proficiency of a large language model. Rather\nthan relying on scale, we explore the hypothesis that targeted data curation\nand optimized training can achieve competitive, or even superior, performance.\nWe demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000\ncarefully selected samples, significant improvements in mathematical reasoning.\nSpecifically, Pensez 7B exhibits an increase in accuracy of the base model up\nto 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.\nThese results challenge the prevailing assumption that massive datasets are\naprerequisite for strong reasoning performance in LLMs, highlighting the\npotential of strategic data curation and optimized fine-tuning for enhancing\nboth specialized skills and multilingual capabilities. Our findings have\nimplications for the efficient development of high-performing, multilingual\nLLMs, especially in resource-constrained scenarios.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630a5ef0e81e1dea2cedcec0/9DxLIM8Ftd4OVGfuGqAEU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13661.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630a5ef0e81e1dea2cedcec0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630a5ef0e81e1dea2cedcec0/ATtyCvYoX4z7uxsm2sJU2.png",
            "fullname": "H Huy Hong",
            "name": "HoangHa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 26
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12127",
            "authors": [
                {
                    "_id": "67d95fa8fb17ef1c744db2db",
                    "user": {
                        "_id": "64ee11c125d2bb76c06e243d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
                        "isPro": false,
                        "fullname": "tobia poppi",
                        "user": "tobi1modna",
                        "type": "user"
                    },
                    "name": "Tobia Poppi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T14:57:56.046Z",
                    "hidden": false
                },
                {
                    "_id": "67d95fa8fb17ef1c744db2dc",
                    "user": {
                        "_id": "67a4de92a41dcedc1632eb43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rKpXABi8RqSeRh2RXBlZU.png",
                        "isPro": false,
                        "fullname": "Tejaswi Kasarla",
                        "user": "tkasarla",
                        "type": "user"
                    },
                    "name": "Tejaswi Kasarla",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-19T13:46:42.945Z",
                    "hidden": false
                },
                {
                    "_id": "67d95fa8fb17ef1c744db2dd",
                    "name": "Pascal Mettes",
                    "hidden": false
                },
                {
                    "_id": "67d95fa8fb17ef1c744db2de",
                    "name": "Lorenzo Baraldi",
                    "hidden": false
                },
                {
                    "_id": "67d95fa8fb17ef1c744db2df",
                    "name": "Rita Cucchiara",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-15T13:18:04.000Z",
            "submittedOnDailyAt": "2025-03-19T07:49:10.270Z",
            "title": "Hyperbolic Safety-Aware Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "64ee11c125d2bb76c06e243d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
                "isPro": false,
                "fullname": "tobia poppi",
                "user": "tobi1modna",
                "type": "user"
            },
            "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.",
            "upvotes": 1,
            "discussionId": "67d95fabfb17ef1c744db411",
            "ai_keywords": [
                "hyperbolic space",
                "entailment hierarchy",
                "entailment loss functions",
                "Euclidean embeddings",
                "multimodal unsafe classifier",
                "content retriever",
                "hyperbolic Safety-Aware CLIP",
                "HySAC"
            ]
        },
        "publishedAt": "2025-03-15T09:18:04.000Z",
        "title": "Hyperbolic Safety-Aware Vision-Language Models",
        "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12127.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ee11c125d2bb76c06e243d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ee11c125d2bb76c06e243d/DR2NV4NYINtpRoMd0NupA.png",
            "fullname": "tobia poppi",
            "name": "tobi1modna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10284",
            "authors": [
                {
                    "_id": "67d6471368b6209d9ad9f7f2",
                    "user": {
                        "_id": "67d645ef9b5e492b00e79af9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AW_0U1377safcLfIIwgA5.png",
                        "isPro": false,
                        "fullname": "Zhen Zhang",
                        "user": "zhenzhangcs",
                        "type": "user"
                    },
                    "name": "Zhen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-16T21:12:18.394Z",
                    "hidden": false
                },
                {
                    "_id": "67d6471368b6209d9ad9f7f3",
                    "name": "Meihan Liu",
                    "hidden": false
                },
                {
                    "_id": "67d6471368b6209d9ad9f7f4",
                    "name": "Bingsheng He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67d645ef9b5e492b00e79af9/Xw7IR-5EKHDKO9LCAxgCx.png"
            ],
            "publishedAt": "2025-03-13T11:52:23.000Z",
            "submittedOnDailyAt": "2025-03-19T23:44:40.963Z",
            "title": "PyGDA: A Python Library for Graph Domain Adaptation",
            "submittedOnDailyBy": {
                "_id": "67d645ef9b5e492b00e79af9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AW_0U1377safcLfIIwgA5.png",
                "isPro": false,
                "fullname": "Zhen Zhang",
                "user": "zhenzhangcs",
                "type": "user"
            },
            "summary": "Graph domain adaptation has emerged as a promising approach to facilitate\nknowledge transfer across different domains. Recently, numerous models have\nbeen proposed to enhance their generalization capabilities in this field.\nHowever, there is still no unified library that brings together existing\ntechniques and simplifies their implementation. To fill this gap, we introduce\nPyGDA, an open-source Python library tailored for graph domain adaptation. As\nthe first comprehensive library in this area, PyGDA covers more than 20 widely\nused graph domain adaptation methods together with different types of graph\ndatasets. Specifically, PyGDA offers modular components, enabling users to\nseamlessly build custom models with a variety of commonly used utility\nfunctions. To handle large-scale graphs, PyGDA includes support for features\nsuch as sampling and mini-batch processing, ensuring efficient computation. In\naddition, PyGDA also includes comprehensive performance benchmarks and\nwell-documented user-friendly API for both researchers and practitioners. To\nfoster convenient accessibility, PyGDA is released under the MIT license at\nhttps://github.com/pygda-team/pygda, and the API documentation is\nhttps://pygda.readthedocs.io/en/stable/.",
            "upvotes": 1,
            "discussionId": "67d6471468b6209d9ad9f824"
        },
        "publishedAt": "2025-03-13T07:52:23.000Z",
        "title": "PyGDA: A Python Library for Graph Domain Adaptation",
        "summary": "Graph domain adaptation has emerged as a promising approach to facilitate\nknowledge transfer across different domains. Recently, numerous models have\nbeen proposed to enhance their generalization capabilities in this field.\nHowever, there is still no unified library that brings together existing\ntechniques and simplifies their implementation. To fill this gap, we introduce\nPyGDA, an open-source Python library tailored for graph domain adaptation. As\nthe first comprehensive library in this area, PyGDA covers more than 20 widely\nused graph domain adaptation methods together with different types of graph\ndatasets. Specifically, PyGDA offers modular components, enabling users to\nseamlessly build custom models with a variety of commonly used utility\nfunctions. To handle large-scale graphs, PyGDA includes support for features\nsuch as sampling and mini-batch processing, ensuring efficient computation. In\naddition, PyGDA also includes comprehensive performance benchmarks and\nwell-documented user-friendly API for both researchers and practitioners. To\nfoster convenient accessibility, PyGDA is released under the MIT license at\nhttps://github.com/pygda-team/pygda, and the API documentation is\nhttps://pygda.readthedocs.io/en/stable/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67d645ef9b5e492b00e79af9/Xw7IR-5EKHDKO9LCAxgCx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10284.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d645ef9b5e492b00e79af9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AW_0U1377safcLfIIwgA5.png",
            "fullname": "Zhen Zhang",
            "name": "zhenzhangcs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.08683",
            "authors": [
                {
                    "_id": "67da5a69f182e9e349a04ca5",
                    "user": {
                        "_id": "66484213e744424922b2edd0",
                        "avatarUrl": "/avatars/5318ba5edd7cab27649fdc012ab9118e.svg",
                        "isPro": false,
                        "fullname": "Changxing Liu",
                        "user": "cxliu0314",
                        "type": "user"
                    },
                    "name": "Changxing Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:07.149Z",
                    "hidden": false
                },
                {
                    "_id": "67da5a69f182e9e349a04ca6",
                    "name": "Genjia Liu",
                    "hidden": false
                },
                {
                    "_id": "67da5a69f182e9e349a04ca7",
                    "name": "Zijun Wang",
                    "hidden": false
                },
                {
                    "_id": "67da5a69f182e9e349a04ca8",
                    "name": "Jinchang Yang",
                    "hidden": false
                },
                {
                    "_id": "67da5a69f182e9e349a04ca9",
                    "name": "Siheng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-11T17:58:42.000Z",
            "submittedOnDailyAt": "2025-03-19T14:05:14.923Z",
            "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
            "submittedOnDailyBy": {
                "_id": "66484213e744424922b2edd0",
                "avatarUrl": "/avatars/5318ba5edd7cab27649fdc012ab9118e.svg",
                "isPro": false,
                "fullname": "Changxing Liu",
                "user": "cxliu0314",
                "type": "user"
            },
            "summary": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
            "upvotes": 1,
            "discussionId": "67da5a6df182e9e349a04e27",
            "ai_keywords": [
                "actor-critic paradigm",
                "negotiation module",
                "intention-guided waypoint generator",
                "CARLA-based simulation benchmark"
            ]
        },
        "publishedAt": "2025-03-11T13:58:42.000Z",
        "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
        "summary": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08683.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66484213e744424922b2edd0",
            "avatarUrl": "/avatars/5318ba5edd7cab27649fdc012ab9118e.svg",
            "fullname": "Changxing Liu",
            "name": "cxliu0314",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]