[
    {
        "paper": {
            "id": "2503.13288",
            "authors": [
                {
                    "_id": "67dbc49d85eacb364e913c38",
                    "user": {
                        "_id": "64e6cf78ecce34cb442dc889",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                        "isPro": false,
                        "fullname": "Fangzhi Xu",
                        "user": "xufangzhi",
                        "type": "user"
                    },
                    "name": "Fangzhi Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:44:57.855Z",
                    "hidden": false
                },
                {
                    "_id": "67dbc49d85eacb364e913c39",
                    "user": {
                        "_id": "67dbe3d969655e406fda64b8",
                        "avatarUrl": "/avatars/6053c84e32d0e46dd1e490c493f766ed.svg",
                        "isPro": false,
                        "fullname": "Mei Tuan",
                        "user": "Meituannnnnn",
                        "type": "user"
                    },
                    "name": "Hang Yan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-20T09:48:32.179Z",
                    "hidden": false
                },
                {
                    "_id": "67dbc49d85eacb364e913c3a",
                    "user": {
                        "_id": "637f22fd932a61b89aeeea37",
                        "avatarUrl": "/avatars/342957f8242d4edaf1d58e1274313afe.svg",
                        "isPro": false,
                        "fullname": "Chang Ma",
                        "user": "changma",
                        "type": "user"
                    },
                    "name": "Chang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:05:18.661Z",
                    "hidden": false
                },
                {
                    "_id": "67dbc49d85eacb364e913c3b",
                    "user": {
                        "_id": "64a7c6e223622f7f189bcbe1",
                        "avatarUrl": "/avatars/4f13a7ed0d2b8d8dfff7dc650e46450a.svg",
                        "isPro": false,
                        "fullname": "haiteng zhao",
                        "user": "haitengzhao",
                        "type": "user"
                    },
                    "name": "Haiteng Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:05:24.460Z",
                    "hidden": false
                },
                {
                    "_id": "67dbc49d85eacb364e913c3c",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "67dbc49d85eacb364e913c3d",
                    "user": {
                        "_id": "66ac77011cfb12c087605acb",
                        "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
                        "isPro": false,
                        "fullname": "Lin",
                        "user": "Qika",
                        "type": "user"
                    },
                    "name": "Qika Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:05:34.255Z",
                    "hidden": false
                },
                {
                    "_id": "67dbc49d85eacb364e913c3e",
                    "user": {
                        "_id": "6280e830e99dccaac4bbfde5",
                        "avatarUrl": "/avatars/9242b8d2826ce2f79af9bb794bba2b61.svg",
                        "isPro": false,
                        "fullname": "Zhiyong Wu",
                        "user": "zy001",
                        "type": "user"
                    },
                    "name": "Zhiyong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:05:41.288Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T15:38:33.000Z",
            "submittedOnDailyAt": "2025-03-20T06:08:48.330Z",
            "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
            "submittedOnDailyBy": {
                "_id": "64e6cf78ecce34cb442dc889",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                "isPro": false,
                "fullname": "Fangzhi Xu",
                "user": "xufangzhi",
                "type": "user"
            },
            "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
            "upvotes": 40,
            "discussionId": "67dbc49f85eacb364e913d20",
            "githubRepo": "https://github.com/xufangzhi/phi-Decoding",
            "ai_keywords": [
                "inference-time optimization",
                "auto-regressive generation",
                "foresight sampling",
                "$\\phi$-Decoding",
                "joint distribution",
                "in-width and in-depth pruning",
                "LLMs (Large Language Models)"
            ]
        },
        "publishedAt": "2025-03-17T11:38:33.000Z",
        "title": "φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
        "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed phi-Decoding. To provide a precise and expressive estimation of step\nvalue, phi-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show phi-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13288.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "fullname": "Fangzhi Xu",
            "name": "xufangzhi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15265",
            "authors": [
                {
                    "_id": "67db8c4c9e4f93ee46411c1d",
                    "user": {
                        "_id": "6522e4fbd89bc7773ddc4b58",
                        "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
                        "isPro": false,
                        "fullname": "Ruowen Zhao",
                        "user": "zzzrw",
                        "type": "user"
                    },
                    "name": "Ruowen Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:12.321Z",
                    "hidden": false
                },
                {
                    "_id": "67db8c4c9e4f93ee46411c1e",
                    "user": {
                        "_id": "65a420cd90e65dc39a6abe9e",
                        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
                        "isPro": false,
                        "fullname": "yejunliang",
                        "user": "yejunliang23",
                        "type": "user"
                    },
                    "name": "Junliang Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:14.849Z",
                    "hidden": false
                },
                {
                    "_id": "67db8c4c9e4f93ee46411c1f",
                    "user": {
                        "_id": "634e15aec1ce28f1de91c470",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634e15aec1ce28f1de91c470/MdtJKgE_fbHydhqESr-DH.png",
                        "isPro": false,
                        "fullname": "Zhengyi Wang",
                        "user": "Zhengyi",
                        "type": "user"
                    },
                    "name": "Zhengyi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T19:17:06.252Z",
                    "hidden": false
                },
                {
                    "_id": "67db8c4c9e4f93ee46411c20",
                    "user": {
                        "_id": "674fcc541dbfdd4dee12d8e1",
                        "avatarUrl": "/avatars/f4adbfce8f7611fa5fce5f0f03d61a46.svg",
                        "isPro": false,
                        "fullname": "Guangce Liu",
                        "user": "guangce",
                        "type": "user"
                    },
                    "name": "Guangce Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:06:05.371Z",
                    "hidden": false
                },
                {
                    "_id": "67db8c4c9e4f93ee46411c21",
                    "user": {
                        "_id": "6422a9669a00fbe9dfe665a1",
                        "avatarUrl": "/avatars/288bf5ea18205bcb0bbea29a304ffbbb.svg",
                        "isPro": false,
                        "fullname": "Yiwen Chen",
                        "user": "NTU-yiwen",
                        "type": "user"
                    },
                    "name": "Yiwen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:06:35.743Z",
                    "hidden": false
                },
                {
                    "_id": "67db8c4c9e4f93ee46411c22",
                    "user": {
                        "_id": "63463bc4547c70e4b7d3009f",
                        "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                        "isPro": false,
                        "fullname": "Yikai Wang",
                        "user": "yikaiwang",
                        "type": "user"
                    },
                    "name": "Yikai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:06:45.552Z",
                    "hidden": false
                },
                {
                    "_id": "67db8c4c9e4f93ee46411c23",
                    "name": "Jun Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T14:39:30.000Z",
            "submittedOnDailyAt": "2025-03-20T03:22:40.364Z",
            "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6522e4fbd89bc7773ddc4b58",
                "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
                "isPro": false,
                "fullname": "Ruowen Zhao",
                "user": "zzzrw",
                "type": "user"
            },
            "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
            "upvotes": 35,
            "discussionId": "67db8c519e4f93ee46411d60",
            "projectPage": "https://zhaorw02.github.io/DeepMesh/",
            "githubRepo": "https://github.com/zhaorw02/DeepMesh",
            "ai_keywords": [
                "triangle meshes",
                "auto-regressive methods",
                "discrete vertex tokens",
                "face counts",
                "mesh incompleteness",
                "DeepMesh",
                "tokenization algorithm",
                "data curation",
                "data processing",
                "Reinforcement Learning (RL)",
                "Direct Preference Optimization (DPO)",
                "human evaluation",
                "3D metrics",
                "point clouds",
                "intricate details",
                "precise topology",
                "state-of-the-art methods",
                "precision",
                "quality"
            ]
        },
        "publishedAt": "2025-03-19T10:39:30.000Z",
        "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
        "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15265.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6522e4fbd89bc7773ddc4b58",
            "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
            "fullname": "Ruowen Zhao",
            "name": "zzzrw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15485",
            "authors": [
                {
                    "_id": "67db7dd224fe67fe45b21e63",
                    "user": {
                        "_id": "6326e20bf0e99f96e024a164",
                        "avatarUrl": "/avatars/9be111af274cad965f72ed48c71d6122.svg",
                        "isPro": false,
                        "fullname": "Zineng Tang",
                        "user": "ZinengTang",
                        "type": "user"
                    },
                    "name": "Zineng Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T11:06:57.434Z",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e64",
                    "name": "Long Lian",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e65",
                    "name": "Seun Eisape",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e66",
                    "name": "XuDong Wang",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e67",
                    "user": {
                        "_id": "667c5764186b27ef806636d3",
                        "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
                        "isPro": false,
                        "fullname": "Roei Herzig",
                        "user": "roeiherz",
                        "type": "user"
                    },
                    "name": "Roei Herzig",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:19:56.648Z",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e68",
                    "user": {
                        "_id": "6333a9195a032dcd095dda13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Adam Yala",
                        "user": "yala",
                        "type": "user"
                    },
                    "name": "Adam Yala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:44.806Z",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e69",
                    "user": {
                        "_id": "6611e6e1188ff298b0dd0b79",
                        "avatarUrl": "/avatars/3a495283955ec9e06e1829c7eb2cd9a4.svg",
                        "isPro": false,
                        "fullname": "Alane Suhr",
                        "user": "alsuhr",
                        "type": "user"
                    },
                    "name": "Alane Suhr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:20:09.062Z",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e6a",
                    "user": {
                        "_id": "64cbdf02f103036e23d1c7f3",
                        "avatarUrl": "/avatars/496069463900dea20929b57381182d39.svg",
                        "isPro": false,
                        "fullname": "Trevor Darrell",
                        "user": "trevordarrell",
                        "type": "user"
                    },
                    "name": "Trevor Darrell",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:20:14.935Z",
                    "hidden": false
                },
                {
                    "_id": "67db7dd224fe67fe45b21e6b",
                    "user": {
                        "_id": "6388f68c43d8b0797a09ff84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
                        "isPro": false,
                        "fullname": "David Chan",
                        "user": "davidchan",
                        "type": "user"
                    },
                    "name": "David M. Chan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:20:24.455Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T17:58:57.000Z",
            "submittedOnDailyAt": "2025-03-20T01:01:18.127Z",
            "title": "TULIP: Towards Unified Language-Image Pretraining",
            "submittedOnDailyBy": {
                "_id": "6388f68c43d8b0797a09ff84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
                "isPro": false,
                "fullname": "David Chan",
                "user": "davidchan",
                "type": "user"
            },
            "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
            "upvotes": 34,
            "discussionId": "67db7dd424fe67fe45b21ee1",
            "projectPage": "https://tulip-berkeley.github.io/",
            "githubRepo": "https://github.com/tulip-berkeley/open_clip",
            "ai_keywords": [
                "generative data augmentation",
                "enhanced image-image and text-text contrastive learning",
                "image/text reconstruction regularization",
                "fine-grained visual features",
                "global semantic alignment",
                "zero-shot performance",
                "few-shot classification",
                "vision-language models"
            ]
        },
        "publishedAt": "2025-03-19T13:58:57.000Z",
        "title": "TULIP: Towards Unified Language-Image Pretraining",
        "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over 3times\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15485.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6388f68c43d8b0797a09ff84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
            "fullname": "David Chan",
            "name": "davidchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15475",
            "authors": [
                {
                    "_id": "67db729fa720e711cff4d205",
                    "name": "Foundation AI Team",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d206",
                    "name": "Kiran Bhat",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d207",
                    "user": {
                        "_id": "6439e28a43fab7b650dad215",
                        "avatarUrl": "/avatars/f0ac35d4a32750e02461fe9bbd61e6ff.svg",
                        "isPro": false,
                        "fullname": "Nishchaie Khanna",
                        "user": "nishchaie-roblox",
                        "type": "user"
                    },
                    "name": "Nishchaie Khanna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:20:51.138Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d208",
                    "name": "Karun Channa",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d209",
                    "user": {
                        "_id": "63899707ec1f539adc0f9206",
                        "avatarUrl": "/avatars/b758a7fc9f3125bac23d689856120607.svg",
                        "isPro": false,
                        "fullname": "Tinghui Zhou",
                        "user": "tinghuiz",
                        "type": "user"
                    },
                    "name": "Tinghui Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:21:01.173Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d20a",
                    "user": {
                        "_id": "6569260115c7a64e88892510",
                        "avatarUrl": "/avatars/45105544f8e681e15a19cc45b4988144.svg",
                        "isPro": false,
                        "fullname": "Yiheng Zhu",
                        "user": "yzhu-roblox",
                        "type": "user"
                    },
                    "name": "Yiheng Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T19:17:12.819Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d20b",
                    "user": {
                        "_id": "635c26accb0f36a40bba6b7f",
                        "avatarUrl": "/avatars/f16603da02a2f262a2e6e42b7b69d801.svg",
                        "isPro": false,
                        "fullname": "Xiaoxia Sun",
                        "user": "xiaoxiaroblox",
                        "type": "user"
                    },
                    "name": "Xiaoxia Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:21:25.849Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d20c",
                    "name": "Charles Shang",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d20d",
                    "user": {
                        "_id": "65c2a2460aa2d5313589e89f",
                        "avatarUrl": "/avatars/705395e754dbf44d26e9457114772df2.svg",
                        "isPro": false,
                        "fullname": "Anirudh Sudarshan",
                        "user": "animan123",
                        "type": "user"
                    },
                    "name": "Anirudh Sudarshan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:21:37.263Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d20e",
                    "name": "Maurice Chu",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d20f",
                    "user": {
                        "_id": "633f44efc11d723b1809958b",
                        "avatarUrl": "/avatars/d70b067deabddae984c3637290b7de99.svg",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "Daiqing",
                        "type": "user"
                    },
                    "name": "Daiqing Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:05.411Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d210",
                    "user": {
                        "_id": "645d34ecce72244df7b29317",
                        "avatarUrl": "/avatars/1248933d9f89a15e67086325a8322d5e.svg",
                        "isPro": false,
                        "fullname": "Kangle Deng",
                        "user": "kangled",
                        "type": "user"
                    },
                    "name": "Kangle Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:11.753Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d211",
                    "user": {
                        "_id": "62cd5c43299c0c2e0e437842",
                        "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
                        "isPro": false,
                        "fullname": "Jean-Philippe Fauconnier",
                        "user": "j4kn",
                        "type": "user"
                    },
                    "name": "Jean-Philippe Fauconnier",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:18.262Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d212",
                    "user": {
                        "_id": "6671bf698171db46e7b6e00f",
                        "avatarUrl": "/avatars/16d20f61bcbce540704575cd26a30ecb.svg",
                        "isPro": false,
                        "fullname": "Tijmen Verhulsdonck",
                        "user": "F1shcalledwanda",
                        "type": "user"
                    },
                    "name": "Tijmen Verhulsdonck",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:24.222Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d213",
                    "user": {
                        "_id": "642bb3e44f379cdc3deb26cd",
                        "avatarUrl": "/avatars/556e792f013bcbe3748d7c9c719070d8.svg",
                        "isPro": false,
                        "fullname": "Maneesh Agrawala",
                        "user": "magrawala",
                        "type": "user"
                    },
                    "name": "Maneesh Agrawala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:30.095Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d214",
                    "user": {
                        "_id": "6501ceb6d6583ade3dff5399",
                        "avatarUrl": "/avatars/07fff5e56b9969642060acc43dd238fe.svg",
                        "isPro": false,
                        "fullname": "Kayvon Fatahalian",
                        "user": "kayvonf",
                        "type": "user"
                    },
                    "name": "Kayvon Fatahalian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:36.801Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d215",
                    "user": {
                        "_id": "6303eafb3926de1f7ec3d748",
                        "avatarUrl": "/avatars/5d23469a5005b00cc110cb0064bf6cee.svg",
                        "isPro": false,
                        "fullname": "Alexander Weiss",
                        "user": "abweiss",
                        "type": "user"
                    },
                    "name": "Alexander Weiss",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:43.610Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d216",
                    "user": {
                        "_id": "6776731df76978b99ec9e5c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/DZ22vXYR8ArefQUJt52Mz.png",
                        "isPro": false,
                        "fullname": "Christian Reiser",
                        "user": "creiser42",
                        "type": "user"
                    },
                    "name": "Christian Reiser",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:50.653Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d217",
                    "user": {
                        "_id": "648b7b123d7ab530fb8fe898",
                        "avatarUrl": "/avatars/bf6647dbd06b7d20690d676c061cbd10.svg",
                        "isPro": false,
                        "fullname": "Ravi Kiran Chirravuri",
                        "user": "coders1122",
                        "type": "user"
                    },
                    "name": "Ravi Kiran Chirravuri",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:22:57.065Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d218",
                    "user": {
                        "_id": "6752101a2f16c25ac9338292",
                        "avatarUrl": "/avatars/15ec542f5202905bf2f113a5dd736582.svg",
                        "isPro": false,
                        "fullname": "Ravali Kandur",
                        "user": "ravali607",
                        "type": "user"
                    },
                    "name": "Ravali Kandur",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:23:03.896Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d219",
                    "name": "Alejandro Pelaez",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d21a",
                    "name": "Akash Garg",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d21b",
                    "name": "Michael Palleschi",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d21c",
                    "name": "Jessica Wang",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d21d",
                    "name": "Skylar Litz",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d21e",
                    "name": "Leon Liu",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d21f",
                    "name": "Anying Li",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d220",
                    "name": "David Harmon",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d221",
                    "name": "Derek Liu",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d222",
                    "name": "Liangjun Feng",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d223",
                    "name": "Denis Goupil",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d224",
                    "name": "Lukas Kuczynski",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d225",
                    "name": "Jihyun Yoon",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d226",
                    "name": "Naveen Marri",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d227",
                    "name": "Peiye Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d228",
                    "name": "Yinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d229",
                    "name": "Brian Yin",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d22a",
                    "name": "Haomiao Jiang",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d22b",
                    "user": {
                        "_id": "664eca30e24897b66ca0b54b",
                        "avatarUrl": "/avatars/16b5e031a36b0d19bd55dae6524e124f.svg",
                        "isPro": false,
                        "fullname": "Marcel van Workum",
                        "user": "marcelvanworkum",
                        "type": "user"
                    },
                    "name": "Marcel van Workum",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:51.777Z",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d22c",
                    "name": "Thomas Lane",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d22d",
                    "name": "Bryce Erickson",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d22e",
                    "name": "Salil Pathare",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d22f",
                    "name": "Kyle Price",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d230",
                    "name": "Anupam Singh",
                    "hidden": false
                },
                {
                    "_id": "67db729fa720e711cff4d231",
                    "name": "David Baszucki",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T17:52:17.000Z",
            "submittedOnDailyAt": "2025-03-20T00:57:52.833Z",
            "title": "Cube: A Roblox View of 3D Intelligence",
            "submittedOnDailyBy": {
                "_id": "62cd5c43299c0c2e0e437842",
                "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
                "isPro": false,
                "fullname": "Jean-Philippe Fauconnier",
                "user": "j4kn",
                "type": "user"
            },
            "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
            "upvotes": 21,
            "discussionId": "67db72a1a720e711cff4d292",
            "githubRepo": "https://github.com/Roblox/cube",
            "ai_keywords": [
                "3D foundation model",
                "3D geometric shapes",
                "3D shape tokenizer",
                "text-to-shape generation",
                "shape-to-text generation",
                "text-to-scene generation",
                "large language models (LLMs)",
                "scene analysis",
                "reasoning",
                "unified foundation model"
            ]
        },
        "publishedAt": "2025-03-19T13:52:17.000Z",
        "title": "Cube: A Roblox View of 3D Intelligence",
        "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15475.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cd5c43299c0c2e0e437842",
            "avatarUrl": "/avatars/8fa8511baf2d9bd95d3ba4535a5b3d69.svg",
            "fullname": "Jean-Philippe Fauconnier",
            "name": "j4kn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15417",
            "authors": [
                {
                    "_id": "67db8e05842d8b6642a135d0",
                    "user": {
                        "_id": "6570450a78d7aca0c361a177",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
                        "isPro": false,
                        "fullname": "Harold Chen",
                        "user": "Harold328",
                        "type": "user"
                    },
                    "name": "Harold Haodong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:09.768Z",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d1",
                    "user": {
                        "_id": "657a776153e2fa36fed33259",
                        "avatarUrl": "/avatars/f60777abe61bc6743e4cef0ede301295.svg",
                        "isPro": false,
                        "fullname": "Haojian Huang",
                        "user": "Jethro37",
                        "type": "user"
                    },
                    "name": "Haojian Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:24:10.070Z",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d2",
                    "user": {
                        "_id": "6581a9e2e4bcbca0322e3608",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IxvVmIg2YQaqa0ZEV6JPa.png",
                        "isPro": false,
                        "fullname": "Xianfeng Wu",
                        "user": "Beckham808",
                        "type": "user"
                    },
                    "name": "Xianfeng Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:24:16.210Z",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d3",
                    "user": {
                        "_id": "6497ff395b5d43c1c77d2a11",
                        "avatarUrl": "/avatars/e3704701ed3ea0c1c165001521f40086.svg",
                        "isPro": false,
                        "fullname": "Yexin Liu",
                        "user": "AI4VR",
                        "type": "user"
                    },
                    "name": "Yexin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:24:22.201Z",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d4",
                    "user": {
                        "_id": "66e15e6a9cbd604971999bf2",
                        "avatarUrl": "/avatars/dc02874bbe33e33c9f0dad4b40e55e01.svg",
                        "isPro": false,
                        "fullname": "Yajing Bai",
                        "user": "YajingB",
                        "type": "user"
                    },
                    "name": "Yajing Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:24:28.817Z",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d5",
                    "name": "Wen-Jie Shu",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d6",
                    "name": "Harry Yang",
                    "hidden": false
                },
                {
                    "_id": "67db8e05842d8b6642a135d7",
                    "user": {
                        "_id": "65884498ce38d143c435d279",
                        "avatarUrl": "/avatars/96a917c40680a79a37dcb8fa75014c21.svg",
                        "isPro": false,
                        "fullname": "Ser Nam Lim",
                        "user": "sernam",
                        "type": "user"
                    },
                    "name": "Ser-Nam Lim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:24:48.864Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T16:59:32.000Z",
            "submittedOnDailyAt": "2025-03-20T02:10:52.068Z",
            "title": "Temporal Regularization Makes Your Video Generator Stronger",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
            "upvotes": 18,
            "discussionId": "67db8e07842d8b6642a1365f",
            "projectPage": "https://haroldchen19.github.io/FluxFlow/",
            "ai_keywords": [
                "temporal augmentation",
                "FluxFlow",
                "temporal perturbations",
                "temporal quality",
                "temporal coherence",
                "UCF-101",
                "VBench",
                "U-Net",
                "DiT",
                "AR-based architectures",
                "spatial fidelity"
            ]
        },
        "publishedAt": "2025-03-19T12:59:32.000Z",
        "title": "Temporal Regularization Makes Your Video Generator Stronger",
        "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15417.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6412
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14868",
            "authors": [
                {
                    "_id": "67db9f06842d8b6642a5eeaf",
                    "user": {
                        "_id": "633e6f07309a99325095dd42",
                        "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
                        "isPro": false,
                        "fullname": "Hoigi Seo",
                        "user": "Agorium",
                        "type": "user"
                    },
                    "name": "Hoigi Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:07.668Z",
                    "hidden": false
                },
                {
                    "_id": "67db9f06842d8b6642a5eeb0",
                    "name": "Wongi Jeong",
                    "hidden": false
                },
                {
                    "_id": "67db9f06842d8b6642a5eeb1",
                    "user": {
                        "_id": "67076244679d36bc0e3eda5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0yE8cHzBuz9QA7sE8rQkf.png",
                        "isPro": false,
                        "fullname": "Kyungryeol Lee",
                        "user": "hirussell",
                        "type": "user"
                    },
                    "name": "Kyungryeol Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:23:58.734Z",
                    "hidden": false
                },
                {
                    "_id": "67db9f06842d8b6642a5eeb2",
                    "name": "Se Young Chun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T03:45:37.000Z",
            "submittedOnDailyAt": "2025-03-20T03:25:51.779Z",
            "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
            "submittedOnDailyBy": {
                "_id": "633e6f07309a99325095dd42",
                "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
                "isPro": false,
                "fullname": "Hoigi Seo",
                "user": "Agorium",
                "type": "user"
            },
            "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
            "upvotes": 18,
            "discussionId": "67db9f11842d8b6642a5f165",
            "projectPage": "https://ignoww.github.io/ZOODiP_project/",
            "githubRepo": "https://github.com/ignoww/ZOODiP",
            "ai_keywords": [
                "diffusion models",
                "image synthesis",
                "quantization techniques",
                "dequantization",
                "gradient-based algorithms",
                "memory-efficient fine-tuning",
                "Textual Inversion",
                "zeroth-order optimization",
                "personalization tokens",
                "gradient estimation",
                "Subspace Gradient",
                "subspace projection",
                "text embedding",
                "Partial Uniform Timestep Sampling",
                "diffusion timesteps",
                "Stable Diffusion",
                "image and text alignment scores"
            ]
        },
        "publishedAt": "2025-03-18T23:45:37.000Z",
        "title": "Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation",
        "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to 8.2times.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14868.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633e6f07309a99325095dd42",
            "avatarUrl": "/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg",
            "fullname": "Hoigi Seo",
            "name": "Agorium",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11557",
            "authors": [
                {
                    "_id": "67d8889acf12ecfbbb2f109c",
                    "user": {
                        "_id": "675cae2c127c72c5682a4df6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/675cae2c127c72c5682a4df6/Fpkm1ilmoIpjPR1j8ipSD.png",
                        "isPro": false,
                        "fullname": "jing bi",
                        "user": "jing-bi",
                        "type": "user"
                    },
                    "name": "Jing Bi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-18T08:08:56.144Z",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f109d",
                    "name": "Junjia Guo",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f109e",
                    "user": {
                        "_id": "65763434a4ee9a4fe7cfb156",
                        "avatarUrl": "/avatars/2c4a23ff309f750dd9c0d67bd9fd7abc.svg",
                        "isPro": false,
                        "fullname": "Susan Liang",
                        "user": "susanliang",
                        "type": "user"
                    },
                    "name": "Susan Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T19:17:23.273Z",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f109f",
                    "user": {
                        "_id": "65970f61719cf10fc5c28d85",
                        "avatarUrl": "/avatars/1cad14d28e9a53f5283a139c856690df.svg",
                        "isPro": false,
                        "fullname": "Guangyu Sun",
                        "user": "imguangyu",
                        "type": "user"
                    },
                    "name": "Guangyu Sun",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-17T20:42:24.004Z",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a0",
                    "name": "Luchuan Song",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a1",
                    "user": {
                        "_id": "6344c87f0f69ad8aa61dfcf6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/RJfMH9KaBIBubyk1TN5OM.jpeg",
                        "isPro": false,
                        "fullname": "Yunlong Tang",
                        "user": "yunlong10",
                        "type": "user"
                    },
                    "name": "Yunlong Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T19:17:25.910Z",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a2",
                    "name": "Jinxi He",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a3",
                    "name": "Jiarui Wu",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a4",
                    "name": "Ali Vosoughi",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a5",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "67d8889acf12ecfbbb2f10a6",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/EqgBdRXowuWEve4uYBK4u.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/JQHAG-UHUMDX9hHHM9_Jk.png"
            ],
            "publishedAt": "2025-03-14T16:26:11.000Z",
            "submittedOnDailyAt": "2025-03-20T13:11:14.839Z",
            "title": "VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity",
            "submittedOnDailyBy": {
                "_id": "675cae2c127c72c5682a4df6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/675cae2c127c72c5682a4df6/Fpkm1ilmoIpjPR1j8ipSD.png",
                "isPro": false,
                "fullname": "jing bi",
                "user": "jing-bi",
                "type": "user"
            },
            "summary": "Visual reasoning is central to human cognition, enabling individuals to\ninterpret and abstractly understand their environment. Although recent\nMultimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance across language and vision-language tasks, existing benchmarks\nprimarily measure recognition-based skills and inadequately assess true visual\nreasoning capabilities. To bridge this critical gap, we introduce VERIFY, a\nbenchmark explicitly designed to isolate and rigorously evaluate the visual\nreasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to\nreason primarily from visual information, providing minimal textual context to\nreduce reliance on domain-specific knowledge and linguistic biases. Each\nproblem is accompanied by a human-annotated reasoning path, making it the first\nto provide in-depth evaluation of model decision-making processes.\nAdditionally, we propose novel metrics that assess visual reasoning fidelity\nbeyond mere accuracy, highlighting critical imbalances in current model\nreasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers\nsignificant limitations, underscoring the need for a balanced and holistic\napproach to both perception and reasoning. For more teaser and testing, visit\nour project page (https://verify-eqh.pages.dev/).",
            "upvotes": 18,
            "discussionId": "67d8889dcf12ecfbbb2f1192",
            "projectPage": "https://proj-verify.jing.vision/",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "visual reasoning capabilities",
                "recognition-based skills",
                "benchmark",
                "reasoning path",
                "visual reasoning fidelity"
            ]
        },
        "publishedAt": "2025-03-14T12:26:11.000Z",
        "title": "VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity",
        "summary": "Visual reasoning is central to human cognition, enabling individuals to\ninterpret and abstractly understand their environment. Although recent\nMultimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance across language and vision-language tasks, existing benchmarks\nprimarily measure recognition-based skills and inadequately assess true visual\nreasoning capabilities. To bridge this critical gap, we introduce VERIFY, a\nbenchmark explicitly designed to isolate and rigorously evaluate the visual\nreasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to\nreason primarily from visual information, providing minimal textual context to\nreduce reliance on domain-specific knowledge and linguistic biases. Each\nproblem is accompanied by a human-annotated reasoning path, making it the first\nto provide in-depth evaluation of model decision-making processes.\nAdditionally, we propose novel metrics that assess visual reasoning fidelity\nbeyond mere accuracy, highlighting critical imbalances in current model\nreasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers\nsignificant limitations, underscoring the need for a balanced and holistic\napproach to both perception and reasoning. For more teaser and testing, visit\nour project page (https://verify-eqh.pages.dev/).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/EqgBdRXowuWEve4uYBK4u.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/675cae2c127c72c5682a4df6/JQHAG-UHUMDX9hHHM9_Jk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11557.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675cae2c127c72c5682a4df6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/675cae2c127c72c5682a4df6/Fpkm1ilmoIpjPR1j8ipSD.png",
            "fullname": "jing bi",
            "name": "jing-bi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15354",
            "authors": [
                {
                    "_id": "67dc1db899d011efd7da645e",
                    "user": {
                        "_id": "642f742270daaa6e7209a2c8",
                        "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
                        "isPro": false,
                        "fullname": "Yining Lu",
                        "user": "ylu610",
                        "type": "user"
                    },
                    "name": "Yining Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T19:17:03.538Z",
                    "hidden": false
                },
                {
                    "_id": "67dc1db899d011efd7da645f",
                    "name": "Noah Ziems",
                    "hidden": false
                },
                {
                    "_id": "67dc1db899d011efd7da6460",
                    "name": "Hy Dang",
                    "hidden": false
                },
                {
                    "_id": "67dc1db899d011efd7da6461",
                    "name": "Meng Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T15:56:21.000Z",
            "submittedOnDailyAt": "2025-03-20T12:27:01.982Z",
            "title": "Optimizing Decomposition for Optimal Claim Verification",
            "submittedOnDailyBy": {
                "_id": "642f742270daaa6e7209a2c8",
                "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
                "isPro": false,
                "fullname": "Yining Lu",
                "user": "ylu610",
                "type": "user"
            },
            "summary": "Current research on the Decompose-Then-Verify paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
            "upvotes": 17,
            "discussionId": "67dc1db999d011efd7da64a7",
            "githubRepo": "https://github.com/yining610/dynamic-decomposition",
            "ai_keywords": [
                "bilevel optimization problem",
                "dynamic decomposition",
                "reinforcement learning framework"
            ]
        },
        "publishedAt": "2025-03-19T11:56:21.000Z",
        "title": "Optimizing Decomposition for Optimal Claim Verification",
        "summary": "Current research on the Decompose-Then-Verify paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15354.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642f742270daaa6e7209a2c8",
            "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
            "fullname": "Yining Lu",
            "name": "ylu610",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14891",
            "authors": [
                {
                    "_id": "67dc1aeab91614cb077cbdf4",
                    "user": {
                        "_id": "640d99628512ec51d7ef71c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
                        "isPro": false,
                        "fullname": "Honglin Lin",
                        "user": "LHL3341",
                        "type": "user"
                    },
                    "name": "Honglin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:31:18.591Z",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdf5",
                    "user": {
                        "_id": "6565e24fe5aac326bfd15a9d",
                        "avatarUrl": "/avatars/28ad90df0e0dbc10ef25ee6499a50dec.svg",
                        "isPro": false,
                        "fullname": "Zhuoshi Pan",
                        "user": "panzs",
                        "type": "user"
                    },
                    "name": "Zhuoshi Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:31:32.707Z",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdf6",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdf7",
                    "user": {
                        "_id": "6397f6081323f19c578f142e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                        "isPro": false,
                        "fullname": "QizhiPei",
                        "user": "QizhiPei",
                        "type": "user"
                    },
                    "name": "Qizhi Pei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:31:39.436Z",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdf8",
                    "name": "Xin Gao",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdf9",
                    "user": {
                        "_id": "677bc1cb02b4dd5d7b32747a",
                        "avatarUrl": "/avatars/199be58c7a91a8d52f88e51e49b63381.svg",
                        "isPro": false,
                        "fullname": "caimengzhang",
                        "user": "caimz",
                        "type": "user"
                    },
                    "name": "Mengzhang Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:31:50.091Z",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdfa",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:31:58.902Z",
                    "hidden": false
                },
                {
                    "_id": "67dc1aeab91614cb077cbdfb",
                    "user": {
                        "_id": "643e60d96db6ba8c5ee177ad",
                        "avatarUrl": "/avatars/73ac7740e462ba0b53a2f2480d9f1e3e.svg",
                        "isPro": false,
                        "fullname": "Lijun Wu",
                        "user": "apeters",
                        "type": "user"
                    },
                    "name": "Lijun Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:32:13.107Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T04:36:35.000Z",
            "submittedOnDailyAt": "2025-03-20T12:14:29.841Z",
            "title": "MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer",
            "submittedOnDailyBy": {
                "_id": "6397f6081323f19c578f142e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                "isPro": false,
                "fullname": "QizhiPei",
                "user": "QizhiPei",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\nMetaLadder, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (10.3\\% accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.",
            "upvotes": 16,
            "discussionId": "67dc1aeab91614cb077cbe31",
            "githubRepo": "https://github.com/LHL3341/MetaLadder",
            "ai_keywords": [
                "MetaLadder",
                "Chain-of-Thought (CoT)",
                "meta-problems",
                "problem-restating mechanism",
                "reasoning transfer",
                "learning from examples",
                "generalization abilities"
            ]
        },
        "publishedAt": "2025-03-19T00:36:35.000Z",
        "title": "MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer",
        "summary": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\nMetaLadder, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (10.3\\% accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "fullname": "QizhiPei",
            "name": "QizhiPei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12532",
            "authors": [
                {
                    "_id": "67da1df040371958e1732c83",
                    "user": {
                        "_id": "645b6094bc7518912e1fbc34",
                        "avatarUrl": "/avatars/5e1a875ba3ce350e71fe7049ca6a44c1.svg",
                        "isPro": false,
                        "fullname": "Fanbin Lu",
                        "user": "Fanbin",
                        "type": "user"
                    },
                    "name": "Fanbin Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:25:02.039Z",
                    "hidden": false
                },
                {
                    "_id": "67da1df040371958e1732c84",
                    "user": {
                        "_id": "66371198c8950692a42f19d9",
                        "avatarUrl": "/avatars/64802642be361df8219f4f645f1335c4.svg",
                        "isPro": false,
                        "fullname": "Zhisheng Zhong",
                        "user": "ZhishengZhong",
                        "type": "user"
                    },
                    "name": "Zhisheng Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:25:09.861Z",
                    "hidden": false
                },
                {
                    "_id": "67da1df040371958e1732c85",
                    "name": "Ziqin Wei",
                    "hidden": false
                },
                {
                    "_id": "67da1df040371958e1732c86",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "67da1df040371958e1732c87",
                    "name": "Chi-Wing Fu",
                    "hidden": false
                },
                {
                    "_id": "67da1df040371958e1732c88",
                    "name": "Jiaya Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T14:53:43.000Z",
            "submittedOnDailyAt": "2025-03-20T01:38:29.350Z",
            "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
            "submittedOnDailyBy": {
                "_id": "6418554a0956be7233a1023e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
                "isPro": false,
                "fullname": "zhang yuechen",
                "user": "julianjuaner",
                "type": "user"
            },
            "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
            "upvotes": 13,
            "discussionId": "67da1df240371958e1732d2f",
            "githubRepo": "https://github.com/FanbinLu/STEVE",
            "ai_keywords": [
                "behavior cloning",
                "trajectory data",
                "suboptimal agents",
                "GPT-4o",
                "step verification pipeline",
                "correctness verification",
                "Kahneman and Tversky Optimization",
                "positive actions",
                "negative actions",
                "vision-language model",
                "computer-use agent",
                "live desktop environment",
                "WinAgentArena"
            ]
        },
        "publishedAt": "2025-03-16T10:53:43.000Z",
        "title": "STEVE: AStep Verification Pipeline for Computer-use Agent Training",
        "summary": "Developing AI agents to autonomously manipulate graphical user interfaces is\na long challenging task. Recent advances in data scaling law inspire us to\ntrain computer-use agents with a scaled instruction set, yet using behavior\ncloning to train agents still requires immense high-quality trajectories. To\nmeet the scalability need, we designed STEVE, a step verification pipeline for\ncomputer-use agent training. First, we establish a large instruction set for\ncomputer-use agents and collect trajectory data with some suboptimal agents.\nGPT-4o is used to verify the correctness of each step in the trajectories based\non the screens before and after the action execution, assigning each step with\na binary label. Last, we adopt the Kahneman and Tversky Optimization to\noptimize the agent from the binary stepwise labels. Extensive experiments\nmanifest that our agent outperforms supervised finetuning by leveraging both\npositive and negative actions within a trajectory. Also, STEVE enables us to\ntrain a 7B vision-language model as a computer-use agent, achieving leading\nperformance in the challenging live desktop environment WinAgentArena with\ngreat efficiency at a reduced cost. Code and data:\nhttps://github.com/FanbinLu/STEVE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12532.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6418554a0956be7233a1023e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
            "fullname": "zhang yuechen",
            "name": "julianjuaner",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.15264",
            "authors": [
                {
                    "_id": "67dbbe8fafd5251fc6b55730",
                    "user": {
                        "_id": "66c598b536a75deef8bb21c0",
                        "avatarUrl": "/avatars/b3721202d01d3c947c36f0beae43ea1c.svg",
                        "isPro": false,
                        "fullname": "Hengrui Kang",
                        "user": "khr0516",
                        "type": "user"
                    },
                    "name": "Hengrui Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:05.833Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55731",
                    "name": "Siwei Wen",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55732",
                    "user": {
                        "_id": "653b8c3e97a4d71d950e2f20",
                        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                        "isPro": false,
                        "fullname": "Zichen Wen",
                        "user": "zichenwen",
                        "type": "user"
                    },
                    "name": "Zichen Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:45:04.004Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55733",
                    "user": {
                        "_id": "66978ee0b8656f6506b4acb2",
                        "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
                        "isPro": false,
                        "fullname": "Junyan Ye",
                        "user": "Yejy53",
                        "type": "user"
                    },
                    "name": "Junyan Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:27:18.626Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55734",
                    "user": {
                        "_id": "66d5b56c77a026c3d2086a79",
                        "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
                        "isPro": false,
                        "fullname": "Weijia Li",
                        "user": "liweijia",
                        "type": "user"
                    },
                    "name": "Weijia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:27:12.349Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55735",
                    "user": {
                        "_id": "65f95363ca387c9d45a2d2ad",
                        "avatarUrl": "/avatars/8e2e8ff5f3b8fbd6d24f0f8947ca5ef4.svg",
                        "isPro": false,
                        "fullname": "Peilin Feng",
                        "user": "Sssunset",
                        "type": "user"
                    },
                    "name": "Peilin Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:27:04.917Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55736",
                    "user": {
                        "_id": "646325085897b675c65aea0f",
                        "avatarUrl": "/avatars/28ce7388f9318b49bdd0a5594c0f6732.svg",
                        "isPro": false,
                        "fullname": "Baichuan Zhou",
                        "user": "bczhou",
                        "type": "user"
                    },
                    "name": "Baichuan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:26:58.914Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55737",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55738",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:26:52.306Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b55739",
                    "user": {
                        "_id": "642ec9831d1737803dc1c30a",
                        "avatarUrl": "/avatars/c9ded838bad09004c15a27200e66a108.svg",
                        "isPro": false,
                        "fullname": "linfeng zhang",
                        "user": "linfengZ",
                        "type": "user"
                    },
                    "name": "Linfeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:26:43.997Z",
                    "hidden": false
                },
                {
                    "_id": "67dbbe8fafd5251fc6b5573a",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:26:34.087Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T14:37:21.000Z",
            "submittedOnDailyAt": "2025-03-20T05:54:43.430Z",
            "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
            "submittedOnDailyBy": {
                "_id": "653b8c3e97a4d71d950e2f20",
                "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                "isPro": false,
                "fullname": "Zichen Wen",
                "user": "zichenwen",
                "type": "user"
            },
            "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
            "upvotes": 8,
            "discussionId": "67dbbe92afd5251fc6b55825",
            "projectPage": "https://opendatalab.github.io/LEGION",
            "githubRepo": "https://github.com/opendatalab/LEGION",
            "ai_keywords": [
                "SynthScars",
                "LEGION",
                "multimodal large language model",
                "image forgery analysis framework",
                "artifact detection",
                "segmentation",
                "explanation",
                "mIoU",
                "F1 score"
            ]
        },
        "publishedAt": "2025-03-19T10:37:21.000Z",
        "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection",
        "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15264.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "fullname": "Zichen Wen",
            "name": "zichenwen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14505",
            "authors": [
                {
                    "_id": "67db13f71956dcedf0b4d357",
                    "user": {
                        "_id": "635a6dd21668c4ead3ed19fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
                        "isPro": false,
                        "fullname": "Susung Hong",
                        "user": "susunghong",
                        "type": "user"
                    },
                    "name": "Susung Hong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:25:56.403Z",
                    "hidden": false
                },
                {
                    "_id": "67db13f71956dcedf0b4d358",
                    "user": {
                        "_id": "66d20e411ba71ac4c0488132",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kkqDAFT15lx4wrJVXiFGJ.png",
                        "isPro": false,
                        "fullname": "Ira Kemelmacher-Shlizerman",
                        "user": "kemelmi",
                        "type": "user"
                    },
                    "name": "Ira Kemelmacher-Shlizerman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:26:06.351Z",
                    "hidden": false
                },
                {
                    "_id": "67db13f71956dcedf0b4d359",
                    "name": "Brian Curless",
                    "hidden": false
                },
                {
                    "_id": "67db13f71956dcedf0b4d35a",
                    "name": "Steven M. Seitz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:59:58.000Z",
            "submittedOnDailyAt": "2025-03-20T02:39:46.392Z",
            "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
            "submittedOnDailyBy": {
                "_id": "635a6dd21668c4ead3ed19fa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
                "isPro": false,
                "fullname": "Susung Hong",
                "user": "susunghong",
                "type": "user"
            },
            "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
            "upvotes": 8,
            "discussionId": "67db13fc1956dcedf0b4d470",
            "ai_keywords": [
                "video diffusion models",
                "multimodal audio-video model",
                "music-video cross-attention",
                "low-rank adapter",
                "dance videos",
                "motion capture data",
                "music-driven video generation",
                "Video-LLMs"
            ]
        },
        "publishedAt": "2025-03-18T13:59:58.000Z",
        "title": "MusicInfuser: Making Video Diffusion Listen and Dance",
        "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14505.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635a6dd21668c4ead3ed19fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679159537671-635a6dd21668c4ead3ed19fa.jpeg",
            "fullname": "Susung Hong",
            "name": "susunghong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11227",
            "authors": [
                {
                    "_id": "67da533bb443470b7908a048",
                    "user": {
                        "_id": "658be7fe135580745c510323",
                        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
                        "isPro": false,
                        "fullname": "Jian Zhang",
                        "user": "VentureZJ",
                        "type": "user"
                    },
                    "name": "Jian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:44:13.546Z",
                    "hidden": false
                },
                {
                    "_id": "67da533bb443470b7908a049",
                    "name": "Bifan Wei",
                    "hidden": false
                },
                {
                    "_id": "67da533bb443470b7908a04a",
                    "name": "Shihao Qi",
                    "hidden": false
                },
                {
                    "_id": "67da533bb443470b7908a04b",
                    "name": "haiping Zhu",
                    "hidden": false
                },
                {
                    "_id": "67da533bb443470b7908a04c",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "67da533bb443470b7908a04d",
                    "user": {
                        "_id": "66ac77011cfb12c087605acb",
                        "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
                        "isPro": false,
                        "fullname": "Lin",
                        "user": "Qika",
                        "type": "user"
                    },
                    "name": "Qika Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:28:09.851Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T09:23:22.000Z",
            "submittedOnDailyAt": "2025-03-20T06:15:24.085Z",
            "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
            "submittedOnDailyBy": {
                "_id": "658be7fe135580745c510323",
                "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
                "isPro": false,
                "fullname": "Jian Zhang",
                "user": "VentureZJ",
                "type": "user"
            },
            "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
            "upvotes": 8,
            "discussionId": "67da533db443470b7908a0e6",
            "ai_keywords": [
                "knowledge graph",
                "event knowledge graph",
                "commonsense knowledge graph",
                "natural language processing",
                "unified framework",
                "in-sample data",
                "counter-task data",
                "out-of-distribution data",
                "three-stage curriculum learning fine-tuning framework",
                "Large Language Models"
            ]
        },
        "publishedAt": "2025-03-14T05:23:22.000Z",
        "title": "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
        "summary": "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11227.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "fullname": "Jian Zhang",
            "name": "VentureZJ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12769",
            "authors": [
                {
                    "_id": "67d8ded81a1b6ae91f79eb18",
                    "user": {
                        "_id": "67067633351e0c16a5c27497",
                        "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
                        "isPro": false,
                        "fullname": "Shenghao Fu",
                        "user": "fushh7",
                        "type": "user"
                    },
                    "name": "Shenghao Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:46:26.291Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb19",
                    "user": {
                        "_id": "66a097801a26a2350395edc7",
                        "avatarUrl": "/avatars/1e7e127cb7222df7d56e5bfda6bab519.svg",
                        "isPro": false,
                        "fullname": "Qize Yang",
                        "user": "PhilipC",
                        "type": "user"
                    },
                    "name": "Qize Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:46:23.971Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb1a",
                    "user": {
                        "_id": "644fe6a9e1d7a97f3b66e906",
                        "avatarUrl": "/avatars/ad1a45f0b1c8a4d03ba87f2a3ce5a8f8.svg",
                        "isPro": false,
                        "fullname": "Yuanming-Li",
                        "user": "Lymann",
                        "type": "user"
                    },
                    "name": "Yuan-Ming Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:46:20.167Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb1b",
                    "user": {
                        "_id": "66b02f7405e2b2771bb431db",
                        "avatarUrl": "/avatars/9d1ecb38e6cb2f0be33a5c1938bb1253.svg",
                        "isPro": false,
                        "fullname": "Yi-Xing Peng",
                        "user": "maybetomorrow",
                        "type": "user"
                    },
                    "name": "Yi-Xing Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:46:18.304Z",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb1c",
                    "name": "Kun-Yu Lin",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb1d",
                    "name": "Xihan Wei",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb1e",
                    "name": "Jian-Fang Hu",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb1f",
                    "name": "Xiaohua Xie",
                    "hidden": false
                },
                {
                    "_id": "67d8ded81a1b6ae91f79eb20",
                    "name": "Wei-Shi Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T03:05:31.000Z",
            "submittedOnDailyAt": "2025-03-20T00:48:47.112Z",
            "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
            "submittedOnDailyBy": {
                "_id": "6686044047f2a33570e59e31",
                "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
                "isPro": false,
                "fullname": "Jiaxing Zhao",
                "user": "StarJiaxing",
                "type": "user"
            },
            "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
            "upvotes": 7,
            "discussionId": "67d8ded91a1b6ae91f79eb5c",
            "ai_keywords": [
                "Large Multi-modal Models (LMMs)",
                "streaming video understanding",
                "Visual Instruction Feedback",
                "visual contents",
                "instructions",
                "gesture recognition",
                "user-agent interactions",
                "subtasks",
                "ViSpeak-Instruct dataset",
                "ViSpeak-Bench",
                "ViSpeak model",
                "GPT-4o-level performance",
                "streaming video understanding benchmarks",
                "finetuning"
            ]
        },
        "publishedAt": "2025-03-16T23:05:31.000Z",
        "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
        "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12769.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6686044047f2a33570e59e31",
            "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
            "fullname": "Jiaxing Zhao",
            "name": "StarJiaxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.13360",
            "authors": [
                {
                    "_id": "67d8e21dea26d6d743f2adde",
                    "user": {
                        "_id": "6623975c728f756224d4b768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
                        "isPro": false,
                        "fullname": "Allen Sun",
                        "user": "Allen8",
                        "type": "user"
                    },
                    "name": "Hai-Long Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:46:16.064Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e21dea26d6d743f2addf",
                    "user": {
                        "_id": "64b75c9c479b934973de0e98",
                        "avatarUrl": "/avatars/99a02865ddc6af27a77fcf7701f9f666.svg",
                        "isPro": false,
                        "fullname": "Zhun Sun",
                        "user": "zhunsun",
                        "type": "user"
                    },
                    "name": "Zhun Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:28:46.228Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e21dea26d6d743f2ade0",
                    "user": {
                        "_id": "631952b6f18d0b5d999ca397",
                        "avatarUrl": "/avatars/eb87dfc142a7602f8fb888f7b7b60d38.svg",
                        "isPro": false,
                        "fullname": "Peng",
                        "user": "Houwen",
                        "type": "user"
                    },
                    "name": "Houwen Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:28:40.023Z",
                    "hidden": false
                },
                {
                    "_id": "67d8e21dea26d6d743f2ade1",
                    "user": {
                        "_id": "67ab41b0ef4586b3ea65e7ed",
                        "avatarUrl": "/avatars/4fa69250c6657624276a9af2aea6cd89.svg",
                        "isPro": false,
                        "fullname": "Han-Jia Ye",
                        "user": "han-jia",
                        "type": "user"
                    },
                    "name": "Han-Jia Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:28:54.161Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T16:45:12.000Z",
            "submittedOnDailyAt": "2025-03-20T04:52:23.426Z",
            "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
            "submittedOnDailyBy": {
                "_id": "6623975c728f756224d4b768",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
                "isPro": false,
                "fullname": "Allen Sun",
                "user": "Allen8",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
            "upvotes": 5,
            "discussionId": "67d8e21eea26d6d743f2ae50",
            "ai_keywords": [
                "Chain-of-Thought (CoT)",
                "OpenAI o1",
                "Multimodal LLMs (MLLMs)",
                "attention to visual information",
                "text-over-relied outputs",
                "ablate image inputs",
                "long-chain reasoning",
                "MathVista's test-hard subset",
                "Take-along Visual Conditioning (TVC)",
                "critical reasoning stages",
                "dynamic pruning",
                "multimodal reasoning systems"
            ]
        },
        "publishedAt": "2025-03-17T12:45:12.000Z",
        "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
        "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13360.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6623975c728f756224d4b768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6623975c728f756224d4b768/US9n0k45Y1TF6WqOhgBhZ.jpeg",
            "fullname": "Allen Sun",
            "name": "Allen8",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12963",
            "authors": [
                {
                    "_id": "67da8735cae81b233ffd2927",
                    "user": {
                        "_id": "67da84f499b60bd76c91fd83",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5iUk_mhBg0lLbUbODEybH.png",
                        "isPro": false,
                        "fullname": "Chaolong Yang",
                        "user": "ChaolongYang",
                        "type": "user"
                    },
                    "name": "Chaolong Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T11:00:42.125Z",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd2928",
                    "user": {
                        "_id": "65d6adbe654f85ff0bb0c246",
                        "avatarUrl": "/avatars/a11b086ae1422b8d64d56d2c5825d3c0.svg",
                        "isPro": false,
                        "fullname": "kai yao",
                        "user": "KaiserYaoJM",
                        "type": "user"
                    },
                    "name": "Kai Yao",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-19T08:58:30.094Z",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd2929",
                    "user": {
                        "_id": "673aef01836947961d636a15",
                        "avatarUrl": "/avatars/3d45b5048d7a55a8a31a928081d95d8a.svg",
                        "isPro": false,
                        "fullname": "Yuyao Yan",
                        "user": "Ritayy",
                        "type": "user"
                    },
                    "name": "Yuyao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:29:13.789Z",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd292a",
                    "user": {
                        "_id": "6773b0515c88cb6ab34cc8fd",
                        "avatarUrl": "/avatars/5d4c4156463a7d69d88e579bf14215d0.svg",
                        "isPro": false,
                        "fullname": "chenru jiang",
                        "user": "chenrujiang",
                        "type": "user"
                    },
                    "name": "Chenru Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:29:20.450Z",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd292b",
                    "user": {
                        "_id": "672094a12174ca42d583e7bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OYcCwVQwR0oDQcVsMr22R.png",
                        "isPro": false,
                        "fullname": "Weiguang Zhao",
                        "user": "weiguangzhao",
                        "type": "user"
                    },
                    "name": "Weiguang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T11:05:14.366Z",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd292c",
                    "name": "Jie Sun",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd292d",
                    "name": "Guangliang Cheng",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd292e",
                    "name": "Yifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd292f",
                    "name": "Bin Dong",
                    "hidden": false
                },
                {
                    "_id": "67da8735cae81b233ffd2930",
                    "name": "Kaizhu Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T09:18:31.000Z",
            "submittedOnDailyAt": "2025-03-20T09:31:48.626Z",
            "title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait",
            "submittedOnDailyBy": {
                "_id": "67da84f499b60bd76c91fd83",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5iUk_mhBg0lLbUbODEybH.png",
                "isPro": false,
                "fullname": "Chaolong Yang",
                "user": "ChaolongYang",
                "type": "user"
            },
            "summary": "Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.",
            "upvotes": 5,
            "discussionId": "67da8736cae81b233ffd29b9",
            "ai_keywords": [
                "KDTalker",
                "unsupervised implicit 3D keypoint",
                "spatiotemporal diffusion model",
                "spatiotemporal attention mechanism",
                "lip synchronization",
                "temporally consistent",
                "computational efficiency"
            ]
        },
        "publishedAt": "2025-03-17T05:18:31.000Z",
        "title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait",
        "summary": "Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12963.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67da84f499b60bd76c91fd83",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5iUk_mhBg0lLbUbODEybH.png",
            "fullname": "Chaolong Yang",
            "name": "ChaolongYang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15450",
            "authors": [
                {
                    "_id": "67dc44a3a19fb08b924a9ea6",
                    "name": "Tongyao Zhu",
                    "hidden": false
                },
                {
                    "_id": "67dc44a3a19fb08b924a9ea7",
                    "user": {
                        "_id": "612ee6a7b960e78c6d2319d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T19:16:58.740Z",
                    "hidden": false
                },
                {
                    "_id": "67dc44a3a19fb08b924a9ea8",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "67dc44a3a19fb08b924a9ea9",
                    "name": "Shiqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67dc44a3a19fb08b924a9eaa",
                    "name": "Xiangming Gu",
                    "hidden": false
                },
                {
                    "_id": "67dc44a3a19fb08b924a9eab",
                    "user": {
                        "_id": "661a4a556fb488fa078c60aa",
                        "avatarUrl": "/avatars/c77401fa9c6d2db896b4a337bb3f8add.svg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "TIanyupang",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:30:51.650Z",
                    "hidden": false
                },
                {
                    "_id": "67dc44a3a19fb08b924a9eac",
                    "user": {
                        "_id": "628f137053028c04a3b750e4",
                        "avatarUrl": "/avatars/85d174cc041ab91e724bb9d9d4e46c88.svg",
                        "isPro": false,
                        "fullname": "Min-Yen Kan",
                        "user": "knmnyn",
                        "type": "user"
                    },
                    "name": "Min-Yen Kan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:30:38.894Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/TjBTIKLO0-u7vwmYlAn0Y.png"
            ],
            "publishedAt": "2025-03-19T17:31:15.000Z",
            "submittedOnDailyAt": "2025-03-20T15:12:18.224Z",
            "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
            "submittedOnDailyBy": {
                "_id": "612ee6a7b960e78c6d2319d4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                "isPro": false,
                "fullname": "Qian Liu",
                "user": "SivilTaram",
                "type": "user"
            },
            "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.",
            "upvotes": 4,
            "discussionId": "67dc44a5a19fb08b924a9f23",
            "githubRepo": "https://github.com/sail-sg/SkyLadder.",
            "ai_keywords": [
                "SkyLadder",
                "context window",
                "context window scheduling",
                "pretraining efficiency",
                "standard benchmark",
                "long-context capability",
                "1B-parameter models",
                "3B-parameter models"
            ]
        },
        "publishedAt": "2025-03-19T13:31:15.000Z",
        "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
        "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/TjBTIKLO0-u7vwmYlAn0Y.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15450.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "fullname": "Qian Liu",
            "name": "SivilTaram",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 77
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14830",
            "authors": [
                {
                    "_id": "67dc367858db060fe5a82916",
                    "name": "Junfeng Ni",
                    "hidden": false
                },
                {
                    "_id": "67dc367858db060fe5a82917",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "67dc367858db060fe5a82918",
                    "name": "Ruijie Lu",
                    "hidden": false
                },
                {
                    "_id": "67dc367858db060fe5a82919",
                    "name": "Zirui Zhou",
                    "hidden": false
                },
                {
                    "_id": "67dc367858db060fe5a8291a",
                    "name": "Song-Chun Zhu",
                    "hidden": false
                },
                {
                    "_id": "67dc367858db060fe5a8291b",
                    "name": "Yixin Chen",
                    "hidden": false
                },
                {
                    "_id": "67dc367858db060fe5a8291c",
                    "name": "Siyuan Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T02:11:31.000Z",
            "submittedOnDailyAt": "2025-03-20T14:09:28.615Z",
            "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion\n  Prior",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Decompositional reconstruction of 3D scenes, with complete shapes and\ndetailed texture of all objects within, is intriguing for downstream\napplications but remains challenging, particularly with sparse views as input.\nRecent approaches incorporate semantic or geometric regularization to address\nthis issue, but they suffer significant degradation in underconstrained areas\nand fail to recover occluded regions. We argue that the key to solving this\nproblem lies in supplementing missing information for these areas. To this end,\nwe propose DP-Recon, which employs diffusion priors in the form of Score\nDistillation Sampling (SDS) to optimize the neural representation of each\nindividual object under novel views. This provides additional information for\nthe underconstrained areas, but directly incorporating diffusion prior raises\npotential conflicts between the reconstruction and generative guidance.\nTherefore, we further introduce a visibility-guided approach to dynamically\nadjust the per-pixel SDS loss weights. Together these components enhance both\ngeometry and appearance recovery while remaining faithful to input images.\nExtensive experiments across Replica and ScanNet++ demonstrate that our method\nsignificantly outperforms SOTA methods. Notably, it achieves better object\nreconstruction under 10 views than the baselines under 100 views. Our method\nenables seamless text-based editing for geometry and appearance through SDS\noptimization and produces decomposed object meshes with detailed UV maps that\nsupport photorealistic Visual effects (VFX) editing. The project page is\navailable at https://dp-recon.github.io/.",
            "upvotes": 4,
            "discussionId": "67dc367958db060fe5a82980",
            "ai_keywords": [
                "diffusion priors",
                "Score Distillation Sampling (SDS)",
                "neural representation",
                "visibility-guided approach",
                "SDS loss weights",
                "geometry and appearance recovery",
                "input images",
                "Replica",
                "ScanNet++",
                "object reconstruction",
                "text-based editing",
                "decomposed object meshes",
                "detailed UV maps",
                "photorealistic Visual effects (VFX)"
            ]
        },
        "publishedAt": "2025-03-18T22:11:31.000Z",
        "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion\n  Prior",
        "summary": "Decompositional reconstruction of 3D scenes, with complete shapes and\ndetailed texture of all objects within, is intriguing for downstream\napplications but remains challenging, particularly with sparse views as input.\nRecent approaches incorporate semantic or geometric regularization to address\nthis issue, but they suffer significant degradation in underconstrained areas\nand fail to recover occluded regions. We argue that the key to solving this\nproblem lies in supplementing missing information for these areas. To this end,\nwe propose DP-Recon, which employs diffusion priors in the form of Score\nDistillation Sampling (SDS) to optimize the neural representation of each\nindividual object under novel views. This provides additional information for\nthe underconstrained areas, but directly incorporating diffusion prior raises\npotential conflicts between the reconstruction and generative guidance.\nTherefore, we further introduce a visibility-guided approach to dynamically\nadjust the per-pixel SDS loss weights. Together these components enhance both\ngeometry and appearance recovery while remaining faithful to input images.\nExtensive experiments across Replica and ScanNet++ demonstrate that our method\nsignificantly outperforms SOTA methods. Notably, it achieves better object\nreconstruction under 10 views than the baselines under 100 views. Our method\nenables seamless text-based editing for geometry and appearance through SDS\noptimization and produces decomposed object meshes with detailed UV maps that\nsupport photorealistic Visual effects (VFX) editing. The project page is\navailable at https://dp-recon.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14830.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6412
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14434",
            "authors": [
                {
                    "_id": "67dc4650129a710a8042aa99",
                    "user": {
                        "_id": "6490a82b231a197da764ace8",
                        "avatarUrl": "/avatars/5d8b7cafa2658a01e706037a3994ebb9.svg",
                        "isPro": false,
                        "fullname": "Nikhil Abhyankar",
                        "user": "nikhilsa",
                        "type": "user"
                    },
                    "name": "Nikhil Abhyankar",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-20T16:46:09.424Z",
                    "hidden": false
                },
                {
                    "_id": "67dc4650129a710a8042aa9a",
                    "user": {
                        "_id": "6520621836008ecc88699622",
                        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
                        "isPro": false,
                        "fullname": "Parshin Shojaee",
                        "user": "parshinsh",
                        "type": "user"
                    },
                    "name": "Parshin Shojaee",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-20T17:22:12.321Z",
                    "hidden": false
                },
                {
                    "_id": "67dc4650129a710a8042aa9b",
                    "name": "Chandan K. Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:11:24.000Z",
            "submittedOnDailyAt": "2025-03-20T15:19:09.831Z",
            "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers",
            "submittedOnDailyBy": {
                "_id": "6520621836008ecc88699622",
                "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
                "isPro": false,
                "fullname": "Parshin Shojaee",
                "user": "parshinsh",
                "type": "user"
            },
            "summary": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
            "upvotes": 3,
            "discussionId": "67dc4651129a710a8042ab0f",
            "githubRepo": "https://github.com/nikhilsab/LLMFE",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "automated feature engineering",
                "pre-defined transformations",
                "search spaces",
                "evolutionary search",
                "domain knowledge",
                "reasoning capabilities",
                "feature generation",
                "data-driven feedback",
                "program search problem",
                "feature transformation programs",
                "tabular learning tasks",
                "tabular prediction models",
                "classification",
                "regression",
                "benchmarks"
            ]
        },
        "publishedAt": "2025-03-18T13:11:24.000Z",
        "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers",
        "summary": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14434.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "fullname": "Parshin Shojaee",
            "name": "parshinsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15478",
            "authors": [
                {
                    "_id": "67db96db0d13633dcbe53fd6",
                    "user": {
                        "_id": "64fd0229e0dc35986bd3c0e5",
                        "avatarUrl": "/avatars/94f5698f9104dad7288edb4460026fd8.svg",
                        "isPro": false,
                        "fullname": "Yifei Zhou",
                        "user": "yifeizhou",
                        "type": "user"
                    },
                    "name": "Yifei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:29:41.066Z",
                    "hidden": false
                },
                {
                    "_id": "67db96db0d13633dcbe53fd7",
                    "user": {
                        "_id": "629ff655198a9a5a7562d437",
                        "avatarUrl": "/avatars/42fd229a1521dc218c1fb385dd867b3b.svg",
                        "isPro": false,
                        "fullname": "Song Jiang",
                        "user": "songjiang",
                        "type": "user"
                    },
                    "name": "Song Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:30:15.739Z",
                    "hidden": false
                },
                {
                    "_id": "67db96db0d13633dcbe53fd8",
                    "user": {
                        "_id": "6344cf73ee1504dbcd5bdfe7",
                        "avatarUrl": "/avatars/6dd2bf1f9c5679e5c8c85d62c9836aac.svg",
                        "isPro": false,
                        "fullname": "Yuandong Tian",
                        "user": "tydsh",
                        "type": "user"
                    },
                    "name": "Yuandong Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:30:08.515Z",
                    "hidden": false
                },
                {
                    "_id": "67db96db0d13633dcbe53fd9",
                    "user": {
                        "_id": "62f023a36a027498eaa2f9cc",
                        "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                        "isPro": false,
                        "fullname": "Jason Weston",
                        "user": "spermwhale",
                        "type": "user"
                    },
                    "name": "Jason Weston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:30:02.617Z",
                    "hidden": false
                },
                {
                    "_id": "67db96db0d13633dcbe53fda",
                    "user": {
                        "_id": "665ce54120a307a3754849dd",
                        "avatarUrl": "/avatars/e698726e9be61dd50ce2efe372ed5dac.svg",
                        "isPro": false,
                        "fullname": "Sergey Levine",
                        "user": "svlevine",
                        "type": "user"
                    },
                    "name": "Sergey Levine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:29:54.687Z",
                    "hidden": false
                },
                {
                    "_id": "67db96db0d13633dcbe53fdb",
                    "user": {
                        "_id": "66a8611eb51510d82ed54231",
                        "avatarUrl": "/avatars/ad559e774fee4914091b82c9831ae2a2.svg",
                        "isPro": false,
                        "fullname": "Sainbayar Sukhbaatar",
                        "user": "sainbar",
                        "type": "user"
                    },
                    "name": "Sainbayar Sukhbaatar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T19:29:48.384Z",
                    "hidden": false
                },
                {
                    "_id": "67db96db0d13633dcbe53fdc",
                    "name": "Xian Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T17:55:08.000Z",
            "submittedOnDailyAt": "2025-03-20T15:38:59.996Z",
            "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks",
            "submittedOnDailyBy": {
                "_id": "64fd0229e0dc35986bd3c0e5",
                "avatarUrl": "/avatars/94f5698f9104dad7288edb4460026fd8.svg",
                "isPro": false,
                "fullname": "Yifei Zhou",
                "user": "yifeizhou",
                "type": "user"
            },
            "summary": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.",
            "upvotes": 2,
            "discussionId": "67db96de0d13633dcbe53fdd",
            "ai_keywords": [
                "multi-turn interactions",
                "multi-turn RL algorithms",
                "credit assignment",
                "ColBench",
                "backend programming",
                "frontend design",
                "SWEET-RL (RL with Step-WisE Evaluation from Training-time information)",
                "critic model",
                "step-level rewards",
                "policy model",
                "Llama-3.1-8B",
                "GPT4-o",
                "collaborative content creation"
            ]
        },
        "publishedAt": "2025-03-19T13:55:08.000Z",
        "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks",
        "summary": "Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15478.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fd0229e0dc35986bd3c0e5",
            "avatarUrl": "/avatars/94f5698f9104dad7288edb4460026fd8.svg",
            "fullname": "Yifei Zhou",
            "name": "yifeizhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15055",
            "authors": [
                {
                    "_id": "67db9586a2f164ac51f84c72",
                    "user": {
                        "_id": "641ee9fe632a1ec42caf1fa6",
                        "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
                        "isPro": false,
                        "fullname": "Arina Razmyslovich",
                        "user": "lavriz",
                        "type": "user"
                    },
                    "name": "Arina Razmyslovich",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-20T04:13:32.101Z",
                    "hidden": false
                },
                {
                    "_id": "67db9586a2f164ac51f84c73",
                    "user": {
                        "_id": "6786af3f8c6e6a8a0fb39b45",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OBfFNaBuLyuEEuuszJfHj.png",
                        "isPro": false,
                        "fullname": "Kseniia Murasheva",
                        "user": "Kseniamorph",
                        "type": "user"
                    },
                    "name": "Kseniia Murasheva",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:29:50.922Z",
                    "hidden": false
                },
                {
                    "_id": "67db9586a2f164ac51f84c74",
                    "name": "Sofia Sedlova",
                    "hidden": false
                },
                {
                    "_id": "67db9586a2f164ac51f84c75",
                    "user": {
                        "_id": "64aab6e5d4d43713ceb83fb1",
                        "avatarUrl": "/avatars/3c636ed07e42c9883b0f7ad70b74f156.svg",
                        "isPro": false,
                        "fullname": "Julien Capitaine",
                        "user": "DrKapichu",
                        "type": "user"
                    },
                    "name": "Julien Capitaine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:30:01.959Z",
                    "hidden": false
                },
                {
                    "_id": "67db9586a2f164ac51f84c76",
                    "name": "Eugene Dmitriev",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T09:46:54.000Z",
            "submittedOnDailyAt": "2025-03-20T02:46:26.189Z",
            "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
            "submittedOnDailyBy": {
                "_id": "641ee9fe632a1ec42caf1fa6",
                "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
                "isPro": false,
                "fullname": "Arina Razmyslovich",
                "user": "lavriz",
                "type": "user"
            },
            "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
            "upvotes": 2,
            "discussionId": "67db958fa2f164ac51f84f51",
            "githubRepo": "https://github.com/1712n/eltex",
            "ai_keywords": [
                "ELTEX",
                "domain-driven framework",
                "high-quality synthetic training data",
                "Large Language Models (LLMs)",
                "cohort indicator extraction",
                "dynamic prompting",
                "critical domain knowledge",
                "blockchain-related cyberattack detection",
                "Gemma-2B",
                "performance competitive",
                "GPT-4",
                "standard classification metrics",
                "uncertainty calibration",
                "computational resources",
                "synthetic dataset",
                "social media texts",
                "domain-driven synthetic data generation",
                "performance gap",
                "resource-efficient models",
                "larger architectures"
            ]
        },
        "publishedAt": "2025-03-19T05:46:54.000Z",
        "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation",
        "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15055.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641ee9fe632a1ec42caf1fa6",
            "avatarUrl": "/avatars/4efb005ce798ce82d4053e00e0ba9f23.svg",
            "fullname": "Arina Razmyslovich",
            "name": "lavriz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13517",
            "authors": [
                {
                    "_id": "67dc13a0b0451eaa2c5e298a",
                    "name": "Hao Cui",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e298b",
                    "name": "Zahra Shamsi",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e298c",
                    "name": "Gowoon Cheon",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e298d",
                    "name": "Xuejian Ma",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e298e",
                    "user": {
                        "_id": "668ec66fd76c5aee9ba82a57",
                        "avatarUrl": "/avatars/4ba953faaf05c483615d2ea1ae0b5572.svg",
                        "isPro": false,
                        "fullname": "Shutong Li",
                        "user": "LigeiaE",
                        "type": "user"
                    },
                    "name": "Shutong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:31:25.035Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e298f",
                    "name": "Maria Tikhanovskaya",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2990",
                    "user": {
                        "_id": "6751fd0c907b6ca2f6c71179",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/75-MmaXqwW_HFMDoHntkU.jpeg",
                        "isPro": false,
                        "fullname": "Peter Norgaard",
                        "user": "PNorgMagician",
                        "type": "user"
                    },
                    "name": "Peter Norgaard",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:31:16.168Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2991",
                    "name": "Nayantara Mudur",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2992",
                    "user": {
                        "_id": "6251ade7183aa426692442a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6251ade7183aa426692442a0/1nP_mQqgcrSPgV1Cf2nMP.jpeg",
                        "isPro": false,
                        "fullname": "Martyna Plomecka",
                        "user": "Martynaplomecka",
                        "type": "user"
                    },
                    "name": "Martyna Plomecka",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:31:47.096Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2993",
                    "user": {
                        "_id": "667c456320d68cc5814ebc04",
                        "avatarUrl": "/avatars/4cb3e893f36aac3442e46afae9d725c7.svg",
                        "isPro": false,
                        "fullname": "Paul Raccuglia",
                        "user": "praccu-google",
                        "type": "user"
                    },
                    "name": "Paul Raccuglia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:31:53.506Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2994",
                    "name": "Yasaman Bahri",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2995",
                    "user": {
                        "_id": "66a7bb1d5f395fc0c6913f77",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sDFjAHabkD_SEYACVD5YU.jpeg",
                        "isPro": false,
                        "fullname": "Victor V. Albert",
                        "user": "valbert4",
                        "type": "user"
                    },
                    "name": "Victor V. Albert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:32:04.907Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2996",
                    "name": "Pranesh Srinivasan",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2997",
                    "user": {
                        "_id": "64664b4ecf550af36eb40363",
                        "avatarUrl": "/avatars/bf2b1ebbeac37f943578d5c93c834746.svg",
                        "isPro": false,
                        "fullname": "Haining Pan",
                        "user": "hainingpan",
                        "type": "user"
                    },
                    "name": "Haining Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:31:02.332Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2998",
                    "name": "Philippe Faist",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e2999",
                    "name": "Brian Rohr",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e299a",
                    "name": "Michael J. Statt",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e299b",
                    "user": {
                        "_id": "6386a8ed06858a85f595b533",
                        "avatarUrl": "/avatars/bdcd45db1dbc86d2f817bc682578cde3.svg",
                        "isPro": false,
                        "fullname": "Dan Morris",
                        "user": "danmorris427",
                        "type": "user"
                    },
                    "name": "Dan Morris",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-20T13:30:48.407Z",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e299c",
                    "name": "Drew Purves",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e299d",
                    "name": "Elise Kleeman",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e299e",
                    "name": "Ruth Alcantara",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e299f",
                    "name": "Matthew Abraham",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a0",
                    "name": "Muqthar Mohammad",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a1",
                    "name": "Ean Phing VanLee",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a2",
                    "name": "Chenfei Jiang",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a3",
                    "name": "Elizabeth Dorfman",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a4",
                    "name": "Eun-Ah Kim",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a5",
                    "name": "Michael P Brenner",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a6",
                    "name": "Viren Jain",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a7",
                    "name": "Sameera Ponda",
                    "hidden": false
                },
                {
                    "_id": "67dc13a0b0451eaa2c5e29a8",
                    "user": {
                        "_id": "646125a5933afb0106a9dabb",
                        "avatarUrl": "/avatars/ccef359e4442f560a87fd66228e54c8b.svg",
                        "isPro": false,
                        "fullname": "Subhashini",
                        "user": "vsubhashini",
                        "type": "user"
                    },
                    "name": "Subhashini Venugopalan",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-20T13:15:39.949Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646125a5933afb0106a9dabb/3PHAJKzNd2gQpNwQQYtdf.png"
            ],
            "publishedAt": "2025-03-14T17:53:03.000Z",
            "submittedOnDailyAt": "2025-03-20T11:42:32.021Z",
            "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning",
            "submittedOnDailyBy": {
                "_id": "646125a5933afb0106a9dabb",
                "avatarUrl": "/avatars/ccef359e4442f560a87fd66228e54c8b.svg",
                "isPro": false,
                "fullname": "Subhashini",
                "user": "vsubhashini",
                "type": "user"
            },
            "summary": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie",
            "upvotes": 2,
            "discussionId": "67dc13a4b0451eaa2c5e2ada",
            "projectPage": "https://github.com/google/curie",
            "githubRepo": "https://github.com/google/curie",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "scientific long-Context Understanding, Reasoning and Information Extraction benchmark (CURIE)",
                "domain expertise",
                "multi-step reasoning",
                "aa (amino acid) sequence tasks",
                "protein sequencing tasks"
            ]
        },
        "publishedAt": "2025-03-14T13:53:03.000Z",
        "title": "CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning",
        "summary": "Scientific problem-solving involves synthesizing information while applying\nexpert knowledge. We introduce CURIE, a scientific long-Context\nUnderstanding,Reasoning and Information Extraction benchmark to measure the\npotential of Large Language Models (LLMs) in scientific problem-solving and\nassisting scientists in realistic workflows. This benchmark introduces ten\nchallenging tasks with a total of 580 problems and solution pairs curated by\nexperts in six disciplines - materials science, condensed matter physics,\nquantum computing, geospatial analysis, biodiversity, and proteins - covering\nboth experimental and theoretical work-flows in science. We evaluate a range of\nclosed and open LLMs on tasks in CURIE which requires domain expertise,\ncomprehension of long in-context information,and multi-step reasoning. While\nGemini Flash 2.0 and Claude-3 show consistent high comprehension across\ndomains, the popular GPT-4o and command-R+ fail dramatically on protein\nsequencing tasks. With the best performance at 32% there is much room for\nimprovement for all models. We hope that insights gained from CURIE can guide\nthe future development of LLMs in sciences. Evaluation code and data are in\nhttps://github.com/google/curie",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646125a5933afb0106a9dabb/3PHAJKzNd2gQpNwQQYtdf.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13517.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646125a5933afb0106a9dabb",
            "avatarUrl": "/avatars/ccef359e4442f560a87fd66228e54c8b.svg",
            "fullname": "Subhashini",
            "name": "vsubhashini",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13553",
            "authors": [
                {
                    "_id": "67dc6adbdb4bc39b0ba2a4e6",
                    "user": {
                        "_id": "62a1e17591f85abff79c2cdf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Philipp Siedler",
                        "user": "philippds",
                        "type": "user"
                    },
                    "name": "Philipp D. Siedler",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-20T19:22:48.113Z",
                    "hidden": false
                },
                {
                    "_id": "67dc6adbdb4bc39b0ba2a4e7",
                    "name": "Ian Gemp",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/kkCV62BpJhpLeoIvRQ5Df.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/MoA0Fl8zl211BYwictw-b.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/2vqqCJ-V-ftWb0D1KZ2Ew.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/HDAUQeogAUA23Kp1bFT-8.png"
            ],
            "publishedAt": "2025-03-16T20:16:13.000Z",
            "submittedOnDailyAt": "2025-03-20T22:20:34.189Z",
            "title": "LLM-Mediated Guidance of MARL Systems",
            "submittedOnDailyBy": {
                "_id": "62a1e17591f85abff79c2cdf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
                "isPro": false,
                "fullname": "Philipp Siedler",
                "user": "philippds",
                "type": "user"
            },
            "summary": "In complex multi-agent environments, achieving efficient learning and\ndesirable behaviours is a significant challenge for Multi-Agent Reinforcement\nLearning (MARL) systems. This work explores the potential of combining MARL\nwith Large Language Model (LLM)-mediated interventions to guide agents toward\nmore desirable behaviours. Specifically, we investigate how LLMs can be used to\ninterpret and facilitate interventions that shape the learning trajectories of\nmultiple agents. We experimented with two types of interventions, referred to\nas controllers: a Natural Language (NL) Controller and a Rule-Based (RB)\nController. The NL Controller, which uses an LLM to simulate human-like\ninterventions, showed a stronger impact than the RB Controller. Our findings\nindicate that agents particularly benefit from early interventions, leading to\nmore efficient training and higher performance. Both intervention types\noutperform the baseline without interventions, highlighting the potential of\nLLM-mediated guidance to accelerate training and enhance MARL performance in\nchallenging environments.",
            "upvotes": 0,
            "discussionId": "67dc6adddb4bc39b0ba2a588",
            "githubRepo": "https://github.com/hivex-research/llm_mediated_guidance",
            "ai_keywords": [
                "Multi-Agent Reinforcement Learning (MARL)",
                "Large Language Model (LLM)",
                "interventions",
                "learning trajectories",
                "Natural Language (NL) Controller",
                "Rule-Based (RB) Controller"
            ]
        },
        "publishedAt": "2025-03-16T16:16:13.000Z",
        "title": "LLM-Mediated Guidance of MARL Systems",
        "summary": "In complex multi-agent environments, achieving efficient learning and\ndesirable behaviours is a significant challenge for Multi-Agent Reinforcement\nLearning (MARL) systems. This work explores the potential of combining MARL\nwith Large Language Model (LLM)-mediated interventions to guide agents toward\nmore desirable behaviours. Specifically, we investigate how LLMs can be used to\ninterpret and facilitate interventions that shape the learning trajectories of\nmultiple agents. We experimented with two types of interventions, referred to\nas controllers: a Natural Language (NL) Controller and a Rule-Based (RB)\nController. The NL Controller, which uses an LLM to simulate human-like\ninterventions, showed a stronger impact than the RB Controller. Our findings\nindicate that agents particularly benefit from early interventions, leading to\nmore efficient training and higher performance. Both intervention types\noutperform the baseline without interventions, highlighting the potential of\nLLM-mediated guidance to accelerate training and enhance MARL performance in\nchallenging environments.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/kkCV62BpJhpLeoIvRQ5Df.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/MoA0Fl8zl211BYwictw-b.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/2vqqCJ-V-ftWb0D1KZ2Ew.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62a1e17591f85abff79c2cdf/HDAUQeogAUA23Kp1bFT-8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13553.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62a1e17591f85abff79c2cdf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
            "fullname": "Philipp Siedler",
            "name": "philippds",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    }
]