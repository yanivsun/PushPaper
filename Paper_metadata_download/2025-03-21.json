[
    {
        "paper": {
            "id": "2503.13358",
            "authors": [
                {
                    "_id": "67dd2ed0d2550735426e7b6f",
                    "user": {
                        "_id": "64a42977250bfdecd9570a9e",
                        "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
                        "isPro": false,
                        "fullname": "Daniil Selikhanovych",
                        "user": "apryc1",
                        "type": "user"
                    },
                    "name": "Daniil Selikhanovych",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-21T09:18:55.946Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b70",
                    "user": {
                        "_id": "656a2e59b4020389028dc85f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656a2e59b4020389028dc85f/_OneIJ3ByHSgIZjjuU7HB.jpeg",
                        "isPro": false,
                        "fullname": "David Li",
                        "user": "kekchpek",
                        "type": "user"
                    },
                    "name": "David Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:32.766Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b71",
                    "name": "Aleksei Leonov",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b72",
                    "user": {
                        "_id": "672503c59f68afdd63cc81a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672503c59f68afdd63cc81a2/lw4ApCTwAKgt_uUyfSVRH.jpeg",
                        "isPro": false,
                        "fullname": "Nikita Gushchin",
                        "user": "ngushchin",
                        "type": "user"
                    },
                    "name": "Nikita Gushchin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:30.126Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b73",
                    "user": {
                        "_id": "644aa54d1acffad9353d5655",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644aa54d1acffad9353d5655/KE2nbf6UsmdNMFRtthwlH.jpeg",
                        "isPro": false,
                        "fullname": "Sergey Kushneryuk",
                        "user": "skushneryuk",
                        "type": "user"
                    },
                    "name": "Sergei Kushneriuk",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:27.722Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b74",
                    "user": {
                        "_id": "63190c17ca8b18deedc77734",
                        "avatarUrl": "/avatars/5b0e1850df82435b6875f85e03c3702f.svg",
                        "isPro": false,
                        "fullname": "Alexander Filippov",
                        "user": "agoxandr",
                        "type": "user"
                    },
                    "name": "Alexander Filippov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T11:53:41.165Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b75",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b76",
                    "user": {
                        "_id": "670558e7d2fc9d76b06c207e",
                        "avatarUrl": "/avatars/88a66ffb334a4fc9d4c150b6aae0f537.svg",
                        "isPro": false,
                        "fullname": "Iaroslav Koshelev",
                        "user": "ys-koshelev",
                        "type": "user"
                    },
                    "name": "Iaroslav Koshelev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T11:58:41.132Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2ed0d2550735426e7b77",
                    "user": {
                        "_id": "67a31c9ae5b870d5157657db",
                        "avatarUrl": "/avatars/ca5fd356e3656e1beacb5a28ecaad5be.svg",
                        "isPro": false,
                        "fullname": "Alexander Korotin",
                        "user": "akorotin",
                        "type": "user"
                    },
                    "name": "Alexander Korotin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T11:58:35.510Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T16:44:08.000Z",
            "submittedOnDailyAt": "2025-03-21T07:50:23.779Z",
            "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "64a42977250bfdecd9570a9e",
                "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
                "isPro": false,
                "fullname": "Daniil Selikhanovych",
                "user": "apryc1",
                "type": "user"
            },
            "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.",
            "upvotes": 71,
            "discussionId": "67dd2ed7d2550735426e7d7f",
            "ai_keywords": [
                "diffusion models",
                "super-resolution (SR)",
                "ResShift",
                "distillation method",
                "fake ResShift model",
                "single-step restoration",
                "SinSR",
                "perceptual quality",
                "degraded input images",
                "parameters",
                "GPU memory"
            ]
        },
        "publishedAt": "2025-03-17T12:44:08.000Z",
        "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
        "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13358.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a42977250bfdecd9570a9e",
            "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
            "fullname": "Daniil Selikhanovych",
            "name": "apryc1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16419",
            "authors": [
                {
                    "_id": "67dcdbfc71027d42fa46e3f2",
                    "user": {
                        "_id": "63787b13500186f250ba377c",
                        "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
                        "isPro": false,
                        "fullname": "yangsui",
                        "user": "yangsui",
                        "type": "user"
                    },
                    "name": "Yang Sui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:51.244Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f3",
                    "name": "Yu-Neng Chuang",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f4",
                    "name": "Guanchu Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f5",
                    "user": {
                        "_id": "63dcc9f3043d6c11093d3bd4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63dcc9f3043d6c11093d3bd4/t3Ai60QML7ZEwifVf3IF4.png",
                        "isPro": false,
                        "fullname": "Jiamu Zhang",
                        "user": "JiamuZhang",
                        "type": "user"
                    },
                    "name": "Jiamu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T11:59:21.471Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f6",
                    "name": "Tianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f7",
                    "user": {
                        "_id": "64b7879a6ab5d14ca7f9bdf4",
                        "avatarUrl": "/avatars/48990fe5b18ab17a3dedacdcc6ee3f3a.svg",
                        "isPro": false,
                        "fullname": "Jiayi Yuan",
                        "user": "jy-yuan",
                        "type": "user"
                    },
                    "name": "Jiayi Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T11:59:47.887Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f8",
                    "user": {
                        "_id": "65c540e2735dd9c81625fadd",
                        "avatarUrl": "/avatars/36713db387abb001cc1e20644ff1f1d4.svg",
                        "isPro": false,
                        "fullname": "Hongyi Liu",
                        "user": "HongyiLiuAI",
                        "type": "user"
                    },
                    "name": "Hongyi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T11:59:56.207Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3f9",
                    "user": {
                        "_id": "658fb57da02954c9820eeb29",
                        "avatarUrl": "/avatars/57a3a9c5175d0b23cc7d6cab0fec08b7.svg",
                        "isPro": false,
                        "fullname": "andrew wen",
                        "user": "andrewwen",
                        "type": "user"
                    },
                    "name": "Andrew Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:00:02.782Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3fa",
                    "name": "Shaochen",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3fb",
                    "name": "Zhong",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3fc",
                    "name": "Hanjie Chen",
                    "hidden": false
                },
                {
                    "_id": "67dcdbfc71027d42fa46e3fd",
                    "name": "Xia Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:38.000Z",
            "submittedOnDailyAt": "2025-03-21T01:56:58.604Z",
            "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "63787b13500186f250ba377c",
                "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
                "isPro": false,
                "fullname": "yangsui",
                "user": "yangsui",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
            "upvotes": 39,
            "discussionId": "67dcdbfd71027d42fa46e439",
            "githubRepo": "https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Large Reasoning Models (LRMs)",
                "OpenAI o1",
                "DeepSeek-R1",
                "supervised fine-tuning (SFT)",
                "reinforcement learning (RL)",
                "Chain-of-Thought (CoT) reasoning",
                "overthinking phenomenon",
                "model-based efficient reasoning",
                "reasoning output-based efficient reasoning",
                "input prompts-based efficient reasoning",
                "efficient data",
                "small language models",
                "evaluation methods",
                "benchmarking"
            ]
        },
        "publishedAt": "2025-03-20T13:59:38.000Z",
        "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16419.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63787b13500186f250ba377c",
            "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
            "fullname": "yangsui",
            "name": "yangsui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16416",
            "authors": [
                {
                    "_id": "67dd1d595fd14aedd30bb94a",
                    "user": {
                        "_id": "638324f862badff43269e588",
                        "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
                        "isPro": false,
                        "fullname": "Asaf Yehudai",
                        "user": "Asaf-Yehudai",
                        "type": "user"
                    },
                    "name": "Asaf Yehudai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:52.838Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb94b",
                    "user": {
                        "_id": "6694bdae1a0138cdbfc2799b",
                        "avatarUrl": "/avatars/8c42646280c327253493edc149d16a56.svg",
                        "isPro": false,
                        "fullname": "Lilach Eden",
                        "user": "LilachE",
                        "type": "user"
                    },
                    "name": "Lilach Eden",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:53:28.129Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb94c",
                    "user": {
                        "_id": "6209ab721c19916805e5b2d1",
                        "avatarUrl": "/avatars/67bbb958f19cedb2ea09461127f8ac92.svg",
                        "isPro": false,
                        "fullname": "Alan Li",
                        "user": "lihaoxin2020",
                        "type": "user"
                    },
                    "name": "Alan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:48.565Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb94d",
                    "name": "Guy Uziel",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb94e",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:53:35.221Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb94f",
                    "user": {
                        "_id": "666d8753e70e5838d9716446",
                        "avatarUrl": "/avatars/6390f9069372a72e79cd331bf043908b.svg",
                        "isPro": false,
                        "fullname": "Roy Bar Haim",
                        "user": "roybarhaim",
                        "type": "user"
                    },
                    "name": "Roy Bar-Haim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:53:41.335Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb950",
                    "user": {
                        "_id": "5f5ba21188f57f65f951f255",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
                        "isPro": false,
                        "fullname": "Arman Cohan",
                        "user": "armanc",
                        "type": "user"
                    },
                    "name": "Arman Cohan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:53:51.866Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1d595fd14aedd30bb951",
                    "name": "Michal Shmueli-Scheuer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:23.000Z",
            "submittedOnDailyAt": "2025-03-21T06:34:12.447Z",
            "title": "Survey on Evaluation of LLM-based Agents",
            "submittedOnDailyBy": {
                "_id": "638324f862badff43269e588",
                "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
                "isPro": false,
                "fullname": "Asaf Yehudai",
                "user": "Asaf-Yehudai",
                "type": "user"
            },
            "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
            "upvotes": 35,
            "discussionId": "67dd1d5a5fd14aedd30bb999",
            "ai_keywords": [
                "LLM-based agents",
                "planning",
                "tool use",
                "self-reflection",
                "memory",
                "evaluation benchmarks",
                "evaluation frameworks",
                "fundamental agent capabilities",
                "application-specific benchmarks",
                "web agents",
                "software engineering agents",
                "scientific agents",
                "conversational agents",
                "generalist agents",
                "cost-efficiency",
                "safety",
                "robustness",
                "fine-grained evaluation methods",
                "scalable evaluation methods"
            ]
        },
        "publishedAt": "2025-03-20T13:59:23.000Z",
        "title": "Survey on Evaluation of LLM-based Agents",
        "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16416.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638324f862badff43269e588",
            "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
            "fullname": "Asaf Yehudai",
            "name": "Asaf-Yehudai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16302",
            "authors": [
                {
                    "_id": "67dce2d2068292e7ef79b3dd",
                    "user": {
                        "_id": "63044b89eedc089484c995ad",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
                        "isPro": false,
                        "fullname": "Zeqiang Lai",
                        "user": "ZeqiangLai",
                        "type": "user"
                    },
                    "name": "Zeqiang Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:33.586Z",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3de",
                    "user": {
                        "_id": "619b8af6cbedb87e1a92e692",
                        "avatarUrl": "/avatars/1258fb2f2637adac8550100f3645651e.svg",
                        "isPro": false,
                        "fullname": "YunFei Zhao",
                        "user": "qikahh",
                        "type": "user"
                    },
                    "name": "Yunfei Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:00:37.346Z",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3df",
                    "user": {
                        "_id": "62d8ce11c60d1450a1ed8795",
                        "avatarUrl": "/avatars/26f1ca693ad7106be0f2f469070d8500.svg",
                        "isPro": false,
                        "fullname": "zibo.zhao",
                        "user": "cocacola",
                        "type": "user"
                    },
                    "name": "Zibo Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:00:43.511Z",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e0",
                    "name": "Haolin Liu",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e1",
                    "user": {
                        "_id": "63e9e92f20c109718713f5eb",
                        "avatarUrl": "/avatars/9ff312e854d803e1a2e9e685a21d12f8.svg",
                        "isPro": false,
                        "fullname": "Fu-Yun Wang",
                        "user": "wangfuyun",
                        "type": "user"
                    },
                    "name": "Fuyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:01:06.818Z",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e2",
                    "user": {
                        "_id": "67287a522ae45f363dd0ad43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67287a522ae45f363dd0ad43/H6eyuxxSk6a84PzRoYcIU.png",
                        "isPro": false,
                        "fullname": "huiwenshi",
                        "user": "Huiwenshi",
                        "type": "user"
                    },
                    "name": "Huiwen Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:01:13.316Z",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e3",
                    "name": "Xianghui Yang",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e4",
                    "name": "Qinxiang Lin",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e5",
                    "name": "Jinwei Huang",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e6",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e7",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e8",
                    "name": "Chunchao Guo",
                    "hidden": false
                },
                {
                    "_id": "67dce2d2068292e7ef79b3e9",
                    "user": {
                        "_id": "666a8f24e2990b0cb16b7bf9",
                        "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Yue",
                        "user": "xyyue",
                        "type": "user"
                    },
                    "name": "Xiangyu Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:52:21.020Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
            ],
            "publishedAt": "2025-03-20T16:23:44.000Z",
            "submittedOnDailyAt": "2025-03-21T02:25:30.177Z",
            "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
            "submittedOnDailyBy": {
                "_id": "63044b89eedc089484c995ad",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
                "isPro": false,
                "fullname": "Zeqiang Lai",
                "user": "ZeqiangLai",
                "type": "user"
            },
            "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
            "upvotes": 29,
            "discussionId": "67dce2d6068292e7ef79b556",
            "githubRepo": "https://github.com/Tencent/FlashVDM",
            "ai_keywords": [
                "3D diffusion",
                "Vecset Diffusion Model (VDM)",
                "diffusion sampling",
                "VAE",
                "DiT",
                "Progressive Flow Distillation",
                "lightning vecset decoder",
                "Adaptive KV Selection",
                "Hierarchical Volume Decoding",
                "Efficient Network Design",
                "FLOPs",
                "decoding overhead",
                "Hunyuan3D-2",
                "Hunyuan3D-2 Turbo"
            ]
        },
        "publishedAt": "2025-03-20T12:23:44.000Z",
        "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
        "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16302.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63044b89eedc089484c995ad",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
            "fullname": "Zeqiang Lai",
            "name": "ZeqiangLai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16397",
            "authors": [
                {
                    "_id": "67dd1227046f2c38458e9588",
                    "name": "Nikita Starodubcev",
                    "hidden": false
                },
                {
                    "_id": "67dd1227046f2c38458e9589",
                    "user": {
                        "_id": "629cf0475a13ba8233dd18c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654452258405-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Denis Kuznedelev",
                        "user": "SpiridonSunRotator",
                        "type": "user"
                    },
                    "name": "Denis Kuznedelev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:59:13.018Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1227046f2c38458e958a",
                    "name": "Artem Babenko",
                    "hidden": false
                },
                {
                    "_id": "67dd1227046f2c38458e958b",
                    "user": {
                        "_id": "62b6cc49752323892323bc04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
                        "isPro": true,
                        "fullname": "Dmitry Baranchuk",
                        "user": "dbaranchuk",
                        "type": "user"
                    },
                    "name": "Dmitry Baranchuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:59:23.033Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6410d3a4cfbe9c4400233d1e/lkzM32YzNPrhP9ESkaUpF.png"
            ],
            "publishedAt": "2025-03-20T17:54:02.000Z",
            "submittedOnDailyAt": "2025-03-21T08:05:39.256Z",
            "title": "Scale-wise Distillation of Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "6410d3a4cfbe9c4400233d1e",
                "avatarUrl": "/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg",
                "isPro": false,
                "fullname": "nikita",
                "user": "quickjkee",
                "type": "user"
            },
            "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
            "upvotes": 25,
            "discussionId": "67dd1229046f2c38458e9617",
            "projectPage": "https://yandex-research.github.io/swd/",
            "githubRepo": "https://github.com/yandex-research/swd",
            "ai_keywords": [
                "scale-wise distillation",
                "diffusion models",
                "next-scale prediction",
                "implicit spectral autoregression",
                "denoising",
                "computational costs",
                "distribution matching",
                "patch loss",
                "text-to-image diffusion models",
                "inference times",
                "computation budget",
                "automated metrics",
                "human preference studies"
            ]
        },
        "publishedAt": "2025-03-20T13:54:02.000Z",
        "title": "Scale-wise Distillation of Diffusion Models",
        "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6410d3a4cfbe9c4400233d1e/lkzM32YzNPrhP9ESkaUpF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16397.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6410d3a4cfbe9c4400233d1e",
            "avatarUrl": "/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg",
            "fullname": "nikita",
            "name": "quickjkee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.14487",
            "authors": [
                {
                    "_id": "67da83d1b05eff6d87a41f81",
                    "user": {
                        "_id": "662887715d246621f33d2ce6",
                        "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
                        "isPro": false,
                        "fullname": "Shi Minglei",
                        "user": "MingleiShi",
                        "type": "user"
                    },
                    "name": "Minglei Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-19T09:43:55.360Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f82",
                    "user": {
                        "_id": "66c15837b186e4f6a0dac80c",
                        "avatarUrl": "/avatars/a3b75d6945f1608e64a2fcff887a5024.svg",
                        "isPro": false,
                        "fullname": "Ziyang Yuan",
                        "user": "ziyangy",
                        "type": "user"
                    },
                    "name": "Ziyang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:54:09.791Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f83",
                    "user": {
                        "_id": "64ba53dcbc787364968a7649",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KgbDlawW8n2s_V3kzhJLW.png",
                        "isPro": false,
                        "fullname": "Haotian Yang",
                        "user": "haotianxoxo",
                        "type": "user"
                    },
                    "name": "Haotian Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:54:19.207Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f84",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T12:46:06.302Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f85",
                    "user": {
                        "_id": "647dcab4becb41a272921250",
                        "avatarUrl": "/avatars/1e4fa540109c4963f54ccad9bd9e9b42.svg",
                        "isPro": false,
                        "fullname": "Mingwu Zheng",
                        "user": "MuonZ",
                        "type": "user"
                    },
                    "name": "Mingwu Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:54:26.135Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f86",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f87",
                    "user": {
                        "_id": "65be89df5e342a230dd04375",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65be89df5e342a230dd04375/aoexyZRbHt2lwHE3iBrBv.jpeg",
                        "isPro": false,
                        "fullname": "Wenliang Zhao",
                        "user": "kennyzhao",
                        "type": "user"
                    },
                    "name": "Wenliang Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:54:33.665Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f88",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f89",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f8a",
                    "user": {
                        "_id": "66c44203ea476bea05e9fcd7",
                        "avatarUrl": "/avatars/b061eebec609446e669f5ad6365959f9.svg",
                        "isPro": false,
                        "fullname": "lu",
                        "user": "jiwenlu",
                        "type": "user"
                    },
                    "name": "Jiwen Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:55:09.982Z",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f8b",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f8c",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "67da83d1b05eff6d87a41f8d",
                    "name": "Kun Gai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:57:07.000Z",
            "submittedOnDailyAt": "2025-03-21T06:35:23.843Z",
            "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "662887715d246621f33d2ce6",
                "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
                "isPro": false,
                "fullname": "Shi Minglei",
                "user": "MingleiShi",
                "type": "user"
            },
            "summary": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
            "upvotes": 23,
            "discussionId": "67da83d3b05eff6d87a42049",
            "projectPage": "https://shiml20.github.io/DiffMoE/",
            "githubRepo": "https://github.com/KwaiVGI/DiffMoE",
            "ai_keywords": [
                "diffusion models",
                "ImageNet",
                "batch-level global token pool",
                "experts",
                "global token distributions",
                "capacity predictor",
                "computational resources",
                "noise levels",
                "sample complexity",
                "class-conditional generation",
                "text-to-image generation"
            ]
        },
        "publishedAt": "2025-03-18T13:57:07.000Z",
        "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
        "summary": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14487.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "662887715d246621f33d2ce6",
            "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
            "fullname": "Shi Minglei",
            "name": "MingleiShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16365",
            "authors": [
                {
                    "_id": "67dcdc98e406e84ea880ccab",
                    "name": "Muyao Li",
                    "hidden": false
                },
                {
                    "_id": "67dcdc98e406e84ea880ccac",
                    "user": {
                        "_id": "642e8c99c1b0f8e4e76bcaab",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
                        "isPro": false,
                        "fullname": "Zihao Wang",
                        "user": "zhwang4ai",
                        "type": "user"
                    },
                    "name": "Zihao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:06:04.185Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdc98e406e84ea880ccad",
                    "user": {
                        "_id": "6780ece9ff3623d9800b2183",
                        "avatarUrl": "/avatars/493e765bba03c115131ac16827c65d3e.svg",
                        "isPro": false,
                        "fullname": "kaichen he",
                        "user": "hkc20",
                        "type": "user"
                    },
                    "name": "Kaichen He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:05:06.674Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdc98e406e84ea880ccae",
                    "user": {
                        "_id": "60dd0e36a15ddd7d2006d2e9",
                        "avatarUrl": "/avatars/8bd98177a79efbf295be8f6457683297.svg",
                        "isPro": false,
                        "fullname": "Xiaojian Ma",
                        "user": "jeasinema",
                        "type": "user"
                    },
                    "name": "Xiaojian Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:04:56.172Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdc98e406e84ea880ccaf",
                    "user": {
                        "_id": "64683a5776bb704aa14588b7",
                        "avatarUrl": "/avatars/e532756f52c5b95981470ace41a10556.svg",
                        "isPro": false,
                        "fullname": "Yitao Liang",
                        "user": "YitaoLiang",
                        "type": "user"
                    },
                    "name": "Yitao Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:04:49.708Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
            ],
            "publishedAt": "2025-03-20T17:21:58.000Z",
            "submittedOnDailyAt": "2025-03-21T01:59:54.135Z",
            "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
            "submittedOnDailyBy": {
                "_id": "642e8c99c1b0f8e4e76bcaab",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
                "isPro": false,
                "fullname": "Zihao Wang",
                "user": "zhwang4ai",
                "type": "user"
            },
            "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
            "upvotes": 21,
            "discussionId": "67dcdc9ce406e84ea880ce67",
            "projectPage": "https://craftjarvis.github.io/JarvisVLA/",
            "githubRepo": "https://github.com/CraftJarvis/JarvisVLA",
            "ai_keywords": [
                "Visual Language Action (VLA) models",
                "Visual Language Models (VLMs)",
                "self-supervised manner",
                "world knowledge",
                "visual recognition",
                "spatial grounding",
                "atomic tasks",
                "crafting",
                "smelting",
                "cooking",
                "mining",
                "killing",
                "non-trajectory tasks",
                "imitation learning-based policies"
            ]
        },
        "publishedAt": "2025-03-20T13:21:58.000Z",
        "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
        "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16365.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642e8c99c1b0f8e4e76bcaab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
            "fullname": "Zihao Wang",
            "name": "zhwang4ai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15558",
            "authors": [
                {
                    "_id": "67dcadafb2cd7d4f3a266037",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266039",
                    "name": "Alisson Azzolini",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26603a",
                    "name": "Hannah Brandon",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26603b",
                    "user": {
                        "_id": "628d451386d23ad1560882c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628d451386d23ad1560882c4/UMxez0DEvX5qdP5ddqi-8.png",
                        "isPro": false,
                        "fullname": "Prithvijit Chattopadhyay",
                        "user": "prithv1",
                        "type": "user"
                    },
                    "name": "Prithvijit Chattopadhyay",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:55:45.738Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26603c",
                    "user": {
                        "_id": "6630f87ee53fcb71c3887df0",
                        "avatarUrl": "/avatars/50191a3d45bebf90cf08df09477e95db.svg",
                        "isPro": false,
                        "fullname": "HuayuChen",
                        "user": "HuayuChen",
                        "type": "user"
                    },
                    "name": "Huayu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:56:00.301Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26603d",
                    "name": "Jinju Chu",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26603e",
                    "name": "Yin Cui",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26603f",
                    "name": "Jenna Diamond",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266040",
                    "user": {
                        "_id": "5e08d1e0fcf41d740b69964f",
                        "avatarUrl": "/avatars/926fef14351d247a0287f6f9dc9e3cb6.svg",
                        "isPro": false,
                        "fullname": "Yifan Ding",
                        "user": "YifanDing",
                        "type": "user"
                    },
                    "name": "Yifan Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:56:21.932Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266041",
                    "user": {
                        "_id": "6353d3e95eac2d2efa7501f9",
                        "avatarUrl": "/avatars/4b063f54000bed4bfb1bfcc3cde1a09e.svg",
                        "isPro": false,
                        "fullname": "Francesco Ferroni",
                        "user": "fferroni",
                        "type": "user"
                    },
                    "name": "Francesco Ferroni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:41:11.413Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266042",
                    "name": "Rama Govindaraju",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266043",
                    "user": {
                        "_id": "647e8118770c299e56fc2bc8",
                        "avatarUrl": "/avatars/adf80f3473dda42450148789ae5c208f.svg",
                        "isPro": false,
                        "fullname": "Jinwei Gu",
                        "user": "jwgu",
                        "type": "user"
                    },
                    "name": "Jinwei Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:56:32.134Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266044",
                    "user": {
                        "_id": "679c387ac4fd876ea127b49f",
                        "avatarUrl": "/avatars/605f04dce6c2bf0f428642650d6642f5.svg",
                        "isPro": false,
                        "fullname": "Siddharth Gururani",
                        "user": "siddfisher",
                        "type": "user"
                    },
                    "name": "Siddharth Gururani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:56:41.456Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266045",
                    "name": "Imad El Hanafi",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266046",
                    "user": {
                        "_id": "6721a3afef642d85f1f6eed5",
                        "avatarUrl": "/avatars/1689e9ef452a7b71189b5d81fadeff01.svg",
                        "isPro": false,
                        "fullname": "Zekun Hao",
                        "user": "zekunhao",
                        "type": "user"
                    },
                    "name": "Zekun Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:56:51.384Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266047",
                    "name": "Jacob Huffman",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266048",
                    "user": {
                        "_id": "678ec9e693649b2688c1d2ad",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tLzqFzkmPu8o-A8PtViGO.png",
                        "isPro": false,
                        "fullname": "Jingyi Jin",
                        "user": "jingyijin03",
                        "type": "user"
                    },
                    "name": "Jingyi Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:57:01.419Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266049",
                    "user": {
                        "_id": "646f8b3d850a938d6c537775",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f8b3d850a938d6c537775/-bO4-nu6KocS9Wa1gcX4H.png",
                        "isPro": false,
                        "fullname": "Brendan Johnson",
                        "user": "BrendanBJohnson",
                        "type": "user"
                    },
                    "name": "Brendan Johnson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:57:09.652Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26604a",
                    "name": "Rizwan Khan",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26604b",
                    "user": {
                        "_id": "66d05ee16447c6c45f54d65d",
                        "avatarUrl": "/avatars/21df19be6a2e3426c858ad373e44f39b.svg",
                        "isPro": false,
                        "fullname": "George Kurian",
                        "user": "Duke09",
                        "type": "user"
                    },
                    "name": "George Kurian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:57:31.615Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26604c",
                    "name": "Elena Lantz",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26604d",
                    "name": "Nayeon Lee",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26604e",
                    "user": {
                        "_id": "647f13d85e1bc4753743f79e",
                        "avatarUrl": "/avatars/37ececdbede0be5770719884be4095d8.svg",
                        "isPro": false,
                        "fullname": "Zhaoshuo Li",
                        "user": "mli0603",
                        "type": "user"
                    },
                    "name": "Zhaoshuo Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:57:42.436Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26604f",
                    "name": "Xuan Li",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266050",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266051",
                    "user": {
                        "_id": "654c025dfda290a640090714",
                        "avatarUrl": "/avatars/83b730d2ed91f824e3f35e1999f5377f.svg",
                        "isPro": false,
                        "fullname": "Yen-Chen Lin",
                        "user": "yenchenl-nvidia",
                        "type": "user"
                    },
                    "name": "Yen-Chen Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:58:29.625Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266052",
                    "user": {
                        "_id": "62f049afdf4b93aad5c7f2d6",
                        "avatarUrl": "/avatars/e272e58ad996733d7098e50248e5b57e.svg",
                        "isPro": false,
                        "fullname": "Ming-Yu Liu",
                        "user": "mingyuliutw",
                        "type": "user"
                    },
                    "name": "Ming-Yu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:58:21.711Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266053",
                    "name": "Andrew Mathau",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266054",
                    "name": "Yun Ni",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266055",
                    "name": "Lindsey Pavao",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266056",
                    "name": "Wei Ping",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266057",
                    "name": "David W. Romero",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266058",
                    "user": {
                        "_id": "6606f3def4ab651901fe8602",
                        "avatarUrl": "/avatars/bacd64657a0154d2a319c9b411ef9fe4.svg",
                        "isPro": false,
                        "fullname": "Misha Smelyanskiy",
                        "user": "Msmelyan",
                        "type": "user"
                    },
                    "name": "Misha Smelyanskiy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:58:13.481Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266059",
                    "name": "Shuran Song",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26605a",
                    "name": "Lyne Tchapmi",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26605b",
                    "name": "Andrew Z. Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26605c",
                    "name": "Boxin Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26605d",
                    "name": "Haoxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26605e",
                    "name": "Fangyin Wei",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a26605f",
                    "name": "Jiashu Xu",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266060",
                    "name": "Yao Xu",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266061",
                    "user": {
                        "_id": "64ab727051756fb15c275c5d",
                        "avatarUrl": "/avatars/26f77a1f81f0f4703eb87e542cd81fa9.svg",
                        "isPro": false,
                        "fullname": "Yangxiaodong",
                        "user": "Xiaodongyang",
                        "type": "user"
                    },
                    "name": "Xiaodong Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:58:02.852Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266062",
                    "user": {
                        "_id": "67d75b0117c2acac528f47b6",
                        "avatarUrl": "/avatars/619aacd1a619aab64de3499ac3ee2229.svg",
                        "isPro": false,
                        "fullname": "Zhuolin Yang",
                        "user": "zhuoliny",
                        "type": "user"
                    },
                    "name": "Zhuolin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:41:13.584Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266063",
                    "user": {
                        "_id": "64d9cf178767727dff1da0c7",
                        "avatarUrl": "/avatars/c2ec61b511754a8d312e2db3229215ec.svg",
                        "isPro": false,
                        "fullname": "Xiaohui  ZENG",
                        "user": "eccony",
                        "type": "user"
                    },
                    "name": "Xiaohui Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:57:54.702Z",
                    "hidden": false
                },
                {
                    "_id": "67dcadafb2cd7d4f3a266064",
                    "name": "Zhe Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T22:06:58.000Z",
            "submittedOnDailyAt": "2025-03-21T01:09:51.519Z",
            "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
            "submittedOnDailyBy": {
                "_id": "649f05367b57fab3a5b27c8b",
                "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
                "isPro": false,
                "fullname": "Yin Cui",
                "user": "richardaecn",
                "type": "user"
            },
            "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
            "upvotes": 21,
            "discussionId": "67dcadb1b2cd7d4f3a2660f4",
            "projectPage": "https://research.nvidia.com/labs/dir/cosmos-reason1/",
            "githubRepo": "https://github.com/nvidia-cosmos/cosmos-reason1",
            "ai_keywords": [
                "hierarchical ontology",
                "two-dimensional ontology",
                "multimodal large language models",
                "vision pre-training",
                "long chain-of-thought reasoning",
                "Physical AI SFT",
                "Physical AI reinforcement learning",
                "embodied reasoning",
                "physical common sense"
            ]
        },
        "publishedAt": "2025-03-18T18:06:58.000Z",
        "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
        "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15558.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649f05367b57fab3a5b27c8b",
            "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
            "fullname": "Yin Cui",
            "name": "richardaecn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16257",
            "authors": [
                {
                    "_id": "67dd2cbabf4c007db3bc0b76",
                    "user": {
                        "_id": "65ce28c6340c3e914285aa58",
                        "avatarUrl": "/avatars/ffaa6d6ce92274bff960f8ea229a37f8.svg",
                        "isPro": false,
                        "fullname": "Keda TAO",
                        "user": "KD-TAO",
                        "type": "user"
                    },
                    "name": "Keda Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:38.025Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2cbabf4c007db3bc0b77",
                    "user": {
                        "_id": "65284ba3e3b3844f5e487a0b",
                        "avatarUrl": "/avatars/fce12a233bddb611d441f25a2ab17725.svg",
                        "isPro": false,
                        "fullname": "Haoxuan You",
                        "user": "HaoxuanYou",
                        "type": "user"
                    },
                    "name": "Haoxuan You",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:59:36.878Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2cbabf4c007db3bc0b78",
                    "name": "Yang Sui",
                    "hidden": false
                },
                {
                    "_id": "67dd2cbabf4c007db3bc0b79",
                    "user": {
                        "_id": "66a9bdfc35c89692442ba4b7",
                        "avatarUrl": "/avatars/ad87d1d2d81775eeb0920bf3ebe08cc2.svg",
                        "isPro": false,
                        "fullname": "Can Qin",
                        "user": "canqin001",
                        "type": "user"
                    },
                    "name": "Can Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T12:59:48.645Z",
                    "hidden": false
                },
                {
                    "_id": "67dd2cbabf4c007db3bc0b7a",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T15:52:43.000Z",
            "submittedOnDailyAt": "2025-03-21T07:41:12.399Z",
            "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "62b624f3b52bef716e248fd7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
                "isPro": false,
                "fullname": "Huan Wang",
                "user": "Huan-WhoRegisteredMyName",
                "type": "user"
            },
            "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
            "upvotes": 17,
            "discussionId": "67dd2cbebf4c007db3bc0cc4",
            "githubRepo": "https://github.com/KD-TAO/VidKV",
            "ai_keywords": [
                "large language models (LLMs)",
                "Video large language models (VideoLLMs)",
                "video frames",
                "key-value (KV) cache",
                "memory requirements",
                "inference speed",
                "KV cache quantization",
                "2-bit KV quantization",
                "VidKV",
                "mixed-precision quantization",
                "channel dimension",
                "anomalous channels",
                "1-bit quantization",
                "FFT",
                "1.58-bit quantization",
                "semantically salient visual tokens",
                "per-channel fashion",
                "per-token fashion",
                "LLaVA-OV-7B",
                "Qwen2.5-VL-7B",
                "benchmarks",
                "FP16 counterparts"
            ]
        },
        "publishedAt": "2025-03-20T11:52:43.000Z",
        "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
        "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16257.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b624f3b52bef716e248fd7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
            "fullname": "Huan Wang",
            "name": "Huan-WhoRegisteredMyName",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16212",
            "authors": [
                {
                    "_id": "67dcd33626989570158ce8cf",
                    "user": {
                        "_id": "6397f6081323f19c578f142e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                        "isPro": false,
                        "fullname": "QizhiPei",
                        "user": "QizhiPei",
                        "type": "user"
                    },
                    "name": "Qizhi Pei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:00:02.682Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d0",
                    "user": {
                        "_id": "643e60d96db6ba8c5ee177ad",
                        "avatarUrl": "/avatars/73ac7740e462ba0b53a2f2480d9f1e3e.svg",
                        "isPro": false,
                        "fullname": "Lijun Wu",
                        "user": "apeters",
                        "type": "user"
                    },
                    "name": "Lijun Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:00:09.197Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d1",
                    "name": "Zhuoshi Pan",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d2",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d3",
                    "user": {
                        "_id": "640d99628512ec51d7ef71c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
                        "isPro": false,
                        "fullname": "Honglin Lin",
                        "user": "LHL3341",
                        "type": "user"
                    },
                    "name": "Honglin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:00:16.585Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d4",
                    "user": {
                        "_id": "677e133ee86d0754dc7ce296",
                        "avatarUrl": "/avatars/c16511c1876b50c2d049925c5f320d15.svg",
                        "isPro": false,
                        "fullname": "mingchenlin",
                        "user": "mingchenlin2025",
                        "type": "user"
                    },
                    "name": "Chenlin Ming",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:00:29.586Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d5",
                    "name": "Xin Gao",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d6",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:00:36.973Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd33626989570158ce8d7",
                    "name": "Rui Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T15:00:41.000Z",
            "submittedOnDailyAt": "2025-03-21T01:18:55.765Z",
            "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
            "submittedOnDailyBy": {
                "_id": "6397f6081323f19c578f142e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                "isPro": false,
                "fullname": "QizhiPei",
                "user": "QizhiPei",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, MathFusionQA, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
            "upvotes": 17,
            "discussionId": "67dcd33726989570158ce90a",
            "githubRepo": "https://github.com/QizhiPei/MathFusion",
            "ai_keywords": [
                "MathFusion",
                "cross-problem instruction synthesis",
                "sequential fusion",
                "parallel fusion",
                "conditional fusion",
                "MathFusionQA",
                "DeepSeekMath-7B",
                "Mistral-7B",
                "Llama3-8B",
                "mathematical reasoning",
                "data efficiency"
            ]
        },
        "publishedAt": "2025-03-20T11:00:41.000Z",
        "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
        "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, MathFusionQA, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16212.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "fullname": "QizhiPei",
            "name": "QizhiPei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13657",
            "authors": [
                {
                    "_id": "67dc4391f618f3c7ba6a3b6d",
                    "name": "Mert Cemri",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b6e",
                    "name": "Melissa Z. Pan",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b6f",
                    "name": "Shuyi Yang",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b70",
                    "name": "Lakshya A. Agrawal",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b71",
                    "name": "Bhavya Chopra",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b72",
                    "name": "Rishabh Tiwari",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b73",
                    "name": "Kurt Keutzer",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b74",
                    "name": "Aditya Parameswaran",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b75",
                    "name": "Dan Klein",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b76",
                    "name": "Kannan Ramchandran",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b77",
                    "name": "Matei Zaharia",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b78",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "67dc4391f618f3c7ba6a3b79",
                    "name": "Ion Stoica",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T19:04:38.000Z",
            "submittedOnDailyAt": "2025-03-21T07:07:51.154Z",
            "title": "Why Do Multi-Agent LLM Systems Fail?",
            "submittedOnDailyBy": {
                "_id": "5ff5d596f244529b3ec0fb89",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png",
                "isPro": false,
                "fullname": "Philipp Schmid",
                "user": "philschmid",
                "type": "user"
            },
            "summary": "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
            "upvotes": 17,
            "discussionId": "67dc4392f618f3c7ba6a3be9",
            "githubRepo": "https://github.com/multi-agent-systems-failure-taxonomy/MASFT",
            "ai_keywords": [
                "Multi-Agent Systems (MAS)",
                "LLM agents",
                "performance gains",
                "single-agent frameworks",
                "failure modes",
                "Cohen's Kappa score",
                "specification and system design failures",
                "inter-agent misalignment",
                "task verification and termination",
                "LLM-as-a-Judge",
                "orchestration strategies"
            ]
        },
        "publishedAt": "2025-03-17T15:04:38.000Z",
        "title": "Why Do Multi-Agent LLM Systems Fail?",
        "summary": "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13657.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5ff5d596f244529b3ec0fb89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png",
            "fullname": "Philipp Schmid",
            "name": "philschmid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 812
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16418",
            "authors": [
                {
                    "_id": "67dcd0fe1f94b594ef4f3e8e",
                    "user": {
                        "_id": "632ea8a92a6ef6fb4acd6403",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664002049861-noauth.png",
                        "isPro": true,
                        "fullname": "Liming Jiang",
                        "user": "EndlessSora",
                        "type": "user"
                    },
                    "name": "Liming Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:03:33.323Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd0fe1f94b594ef4f3e8f",
                    "name": "Qing Yan",
                    "hidden": false
                },
                {
                    "_id": "67dcd0fe1f94b594ef4f3e90",
                    "name": "Yumin Jia",
                    "hidden": false
                },
                {
                    "_id": "67dcd0fe1f94b594ef4f3e91",
                    "user": {
                        "_id": "642f487cee432fcf2e9170c2",
                        "avatarUrl": "/avatars/aa03bf5f546dd732084f7ff7738a94ee.svg",
                        "isPro": false,
                        "fullname": "Zichuan Liu",
                        "user": "sun8878232",
                        "type": "user"
                    },
                    "name": "Zichuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:04:10.048Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd0fe1f94b594ef4f3e92",
                    "name": "Hao Kang",
                    "hidden": false
                },
                {
                    "_id": "67dcd0fe1f94b594ef4f3e93",
                    "name": "Xin Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:34.000Z",
            "submittedOnDailyAt": "2025-03-21T02:41:07.989Z",
            "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
            "upvotes": 16,
            "discussionId": "67dcd1001f94b594ef4f3f44",
            "ai_keywords": [
                "Diffusion Transformers (DiTs)",
                "FLUX",
                "InfiniteYou (InfU)",
                "InfuseNet",
                "residual connections",
                "synthetic single-person-multiple-sample (SPMS) data",
                "pretraining",
                "supervised fine-tuning (SFT)"
            ]
        },
        "publishedAt": "2025-03-20T13:59:34.000Z",
        "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
        "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16418.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6423
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.15299",
            "authors": [
                {
                    "_id": "67dd5e658dc9e947cee90f4a",
                    "user": {
                        "_id": "61499a8f8517f77f09a3a990",
                        "avatarUrl": "/avatars/fa15d117e8ce8c50669455d3e483408b.svg",
                        "isPro": false,
                        "fullname": "Zorik",
                        "user": "zorik",
                        "type": "user"
                    },
                    "name": "Zorik Gekhman",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-21T13:57:32.354Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f4b",
                    "name": "Eyal Ben David",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f4c",
                    "name": "Hadas Orgad",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f4d",
                    "name": "Eran Ofek",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f4e",
                    "name": "Yonatan Belinkov",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f4f",
                    "name": "Idan Szpector",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f50",
                    "name": "Jonathan Herzig",
                    "hidden": false
                },
                {
                    "_id": "67dd5e658dc9e947cee90f51",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T15:21:48.000Z",
            "submittedOnDailyAt": "2025-03-21T11:14:32.756Z",
            "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
            "submittedOnDailyBy": {
                "_id": "61499a8f8517f77f09a3a990",
                "avatarUrl": "/avatars/fa15d117e8ce8c50669455d3e483408b.svg",
                "isPro": false,
                "fullname": "Zorik",
                "user": "zorik",
                "type": "user"
            },
            "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
            "upvotes": 13,
            "discussionId": "67dd5e658dc9e947cee90f81",
            "ai_keywords": [
                "large language models (LLMs)",
                "knowledge",
                "correct-incorrect answer pairs",
                "external knowledge",
                "internal knowledge",
                "token-level probabilities",
                "intermediate computations",
                "hidden knowledge",
                "closed-book QA",
                "repeated sampling",
                "generation capabilities"
            ]
        },
        "publishedAt": "2025-03-19T11:21:48.000Z",
        "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
        "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15299.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61499a8f8517f77f09a3a990",
            "avatarUrl": "/avatars/fa15d117e8ce8c50669455d3e483408b.svg",
            "fullname": "Zorik",
            "name": "zorik",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16413",
            "authors": [
                {
                    "_id": "67dccb8ca33f11a56567bd61",
                    "name": "Xueyan Zou",
                    "hidden": false
                },
                {
                    "_id": "67dccb8ca33f11a56567bd62",
                    "name": "Yuchen Song",
                    "hidden": false
                },
                {
                    "_id": "67dccb8ca33f11a56567bd63",
                    "name": "Ri-Zhao Qiu",
                    "hidden": false
                },
                {
                    "_id": "67dccb8ca33f11a56567bd64",
                    "name": "Xuanbin Peng",
                    "hidden": false
                },
                {
                    "_id": "67dccb8ca33f11a56567bd65",
                    "name": "Jianglong Ye",
                    "hidden": false
                },
                {
                    "_id": "67dccb8ca33f11a56567bd66",
                    "name": "Sifei Liu",
                    "hidden": false
                },
                {
                    "_id": "67dccb8ca33f11a56567bd67",
                    "name": "Xiaolong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:12.000Z",
            "submittedOnDailyAt": "2025-03-21T00:52:49.431Z",
            "title": "M3: 3D-Spatial MultiModal Memory",
            "submittedOnDailyBy": {
                "_id": "62520988818a5dc29ab91d6f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
                "isPro": false,
                "fullname": "Xueyan Zou",
                "user": "xueyanz",
                "type": "user"
            },
            "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
            "upvotes": 12,
            "discussionId": "67dccb92a33f11a56567bf43",
            "projectPage": "https://m3-spatial-memory.github.io/",
            "githubRepo": "https://github.com/MaureenZOU/m3-spatial",
            "ai_keywords": [
                "3D Spatial MultiModal Memory (M3)",
                "3D Gaussian Splatting",
                "feature representations",
                "granularities",
                "principal scene components",
                "Gaussian memory attention",
                "feature splatting",
                "computational constraints",
                "high-dimensional features",
                "Gaussian primitive",
                "misalignment",
                "information loss",
                "distilled features",
                "foundation models",
                "vision-language models (VLMs)",
                "perception models",
                "large multimodal and language models (LMMs/LLMs)",
                "feature field",
                "quadruped robot",
                "core compression challenges",
                "3D feature distillation"
            ]
        },
        "publishedAt": "2025-03-20T13:59:12.000Z",
        "title": "M3: 3D-Spatial MultiModal Memory",
        "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16413.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62520988818a5dc29ab91d6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
            "fullname": "Xueyan Zou",
            "name": "xueyanz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16356",
            "authors": [
                {
                    "_id": "67dcd18ad2550735425351bf",
                    "user": {
                        "_id": "6122fbe636a4c36a99dbea7b",
                        "avatarUrl": "/avatars/c0cd2c1ef58e315d9adda9d26000f625.svg",
                        "isPro": false,
                        "fullname": "Yunzhi Yao",
                        "user": "cowTodd",
                        "type": "user"
                    },
                    "name": "Yunzhi Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:06:47.575Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd18ad2550735425351c0",
                    "user": {
                        "_id": "669663472d25bd04e9af1d66",
                        "avatarUrl": "/avatars/8b11d5d79d1b8b205baa498a942f573c.svg",
                        "isPro": false,
                        "fullname": "Jizhan Fang",
                        "user": "JizhanFang",
                        "type": "user"
                    },
                    "name": "Jizhan Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:06:55.484Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd18ad2550735425351c1",
                    "user": {
                        "_id": "677711d332399119a31bed14",
                        "avatarUrl": "/avatars/5648e8520c9ede4db42148f7da84464b.svg",
                        "isPro": false,
                        "fullname": "Jia-Chen Gu",
                        "user": "gujc",
                        "type": "user"
                    },
                    "name": "Jia-Chen Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:07:01.906Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd18ad2550735425351c2",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:59.405Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd18ad2550735425351c3",
                    "user": {
                        "_id": "6441f1d2603214724ec0c1c2",
                        "avatarUrl": "/avatars/d3c4b759e6a5635e37ff715fae52e5ba.svg",
                        "isPro": false,
                        "fullname": "Shumin Deng",
                        "user": "231sm",
                        "type": "user"
                    },
                    "name": "Shumin Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:07:08.198Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd18ad2550735425351c4",
                    "user": {
                        "_id": "67c06592ebc2e682b2628bde",
                        "avatarUrl": "/avatars/1032c0319eded956d2dd461b0e1b4ddb.svg",
                        "isPro": false,
                        "fullname": "Huajun Chen",
                        "user": "HuajunChen",
                        "type": "user"
                    },
                    "name": "Huajun Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:07:15.045Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd18ad2550735425351c5",
                    "name": "Nanyun Peng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
            ],
            "publishedAt": "2025-03-20T17:14:34.000Z",
            "submittedOnDailyAt": "2025-03-21T01:12:07.189Z",
            "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
            "upvotes": 11,
            "discussionId": "67dcd18bd255073542535223",
            "githubRepo": "https://github.com/zjunlp/CaKE",
            "ai_keywords": [
                "Knowledge Editing (KE)",
                "large language models (LLMs)",
                "multi-hop reasoning tasks",
                "reasoning circuits",
                "neural pathways",
                "knowledge-based inference",
                "MEMIT",
                "WISE",
                "layer-localized KE approaches",
                "CaKE (Circuit-aware Knowledge Editing)",
                "strategically curated data",
                "circuits-based analysis",
                "MQuAKE dataset"
            ]
        },
        "publishedAt": "2025-03-20T13:14:34.000Z",
        "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
        "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16356.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16422",
            "authors": [
                {
                    "_id": "67dcd5da7f5c5665205b11c0",
                    "user": {
                        "_id": "67ac56c90f861b63617d153d",
                        "avatarUrl": "/avatars/96f5e60031fe4fe677159dccfecaa65c.svg",
                        "isPro": false,
                        "fullname": "Yuan Yuheng",
                        "user": "nopyyh",
                        "type": "user"
                    },
                    "name": "Yuheng Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:54.364Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd5da7f5c5665205b11c1",
                    "name": "Qiuhong Shen",
                    "hidden": false
                },
                {
                    "_id": "67dcd5da7f5c5665205b11c2",
                    "name": "Xingyi Yang",
                    "hidden": false
                },
                {
                    "_id": "67dcd5da7f5c5665205b11c3",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
            ],
            "publishedAt": "2025-03-20T17:59:44.000Z",
            "submittedOnDailyAt": "2025-03-21T01:29:12.177Z",
            "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
            "submittedOnDailyBy": {
                "_id": "634cfebc350bcee9bed20a4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                "isPro": false,
                "fullname": "Xingyi Yang",
                "user": "adamdad",
                "type": "user"
            },
            "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) Inactive Gaussians:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a 41times reduction in storage and 9times faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
            "upvotes": 10,
            "discussionId": "67dcd5e17f5c5665205b1422",
            "ai_keywords": [
                "4D Gaussian Splatting (4DGS)",
                "temporal redundancy",
                "Short-Lifespan Gaussians",
                "inactive Gaussians",
                "rasterization",
                "Spatial-Temporal Variation Score",
                "4DGS-1K",
                "FPS",
                "modern GPUs",
                "storage",
                "rasterization speed",
                "visual quality"
            ]
        },
        "publishedAt": "2025-03-20T13:59:44.000Z",
        "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
        "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) Inactive Gaussians:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a 41times reduction in storage and 9times faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16422.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "fullname": "Xingyi Yang",
            "name": "adamdad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16420",
            "authors": [
                {
                    "_id": "67dd8e2b0824309332ce3b97",
                    "name": "Paul Engstler",
                    "hidden": false
                },
                {
                    "_id": "67dd8e2b0824309332ce3b98",
                    "name": "Aleksandar Shtedritski",
                    "hidden": false
                },
                {
                    "_id": "67dd8e2b0824309332ce3b99",
                    "name": "Iro Laina",
                    "hidden": false
                },
                {
                    "_id": "67dd8e2b0824309332ce3b9a",
                    "name": "Christian Rupprecht",
                    "hidden": false
                },
                {
                    "_id": "67dd8e2b0824309332ce3b9b",
                    "name": "Andrea Vedaldi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:40.000Z",
            "submittedOnDailyAt": "2025-03-21T14:36:02.165Z",
            "title": "SynCity: Training-Free Generation of 3D Worlds",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We address the challenge of generating 3D worlds from textual descriptions.\nWe propose SynCity, a training- and optimization-free approach, which leverages\nthe geometric precision of pre-trained 3D generative models and the artistic\nversatility of 2D image generators to create large, high-quality 3D spaces.\nWhile most 3D generative models are object-centric and cannot generate\nlarge-scale worlds, we show how 3D and 2D generators can be combined to\ngenerate ever-expanding scenes. Through a tile-based approach, we allow\nfine-grained control over the layout and the appearance of scenes. The world is\ngenerated tile-by-tile, and each new tile is generated within its world-context\nand then fused with the scene. SynCity generates compelling and immersive\nscenes that are rich in detail and diversity.",
            "upvotes": 10,
            "discussionId": "67dd8e2d0824309332ce3c2a",
            "projectPage": "https://research.paulengstler.com/syncity/",
            "githubRepo": "https://github.com/paulengstler/syncity",
            "ai_keywords": [
                "3D generative models",
                "2D image generators",
                "object-centric",
                "large-scale worlds",
                "tile-based approach",
                "world-context",
                "compelling and immersive scenes"
            ]
        },
        "publishedAt": "2025-03-20T13:59:40.000Z",
        "title": "SynCity: Training-Free Generation of 3D Worlds",
        "summary": "We address the challenge of generating 3D worlds from textual descriptions.\nWe propose SynCity, a training- and optimization-free approach, which leverages\nthe geometric precision of pre-trained 3D generative models and the artistic\nversatility of 2D image generators to create large, high-quality 3D spaces.\nWhile most 3D generative models are object-centric and cannot generate\nlarge-scale worlds, we show how 3D and 2D generators can be combined to\ngenerate ever-expanding scenes. Through a tile-based approach, we allow\nfine-grained control over the layout and the appearance of scenes. The world is\ngenerated tile-by-tile, and each new tile is generated within its world-context\nand then fused with the scene. SynCity generates compelling and immersive\nscenes that are rich in detail and diversity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16420.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6423
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16322",
            "authors": [
                {
                    "_id": "67dce4c10784200359ab2494",
                    "name": "Ruonan Yu",
                    "hidden": false
                },
                {
                    "_id": "67dce4c10784200359ab2495",
                    "name": "Songhua Liu",
                    "hidden": false
                },
                {
                    "_id": "67dce4c10784200359ab2496",
                    "name": "Zhenxiong Tan",
                    "hidden": false
                },
                {
                    "_id": "67dce4c10784200359ab2497",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T16:44:43.000Z",
            "submittedOnDailyAt": "2025-03-21T02:36:06.925Z",
            "title": "Ultra-Resolution Adaptation with Ease",
            "submittedOnDailyBy": {
                "_id": "6486fb33570a419f41a882e4",
                "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
                "isPro": false,
                "fullname": "Ruonan Yu",
                "user": "roseannelexie",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed URAE. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, i.e., setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\nhttps://github.com/Huage001/URAE{here}.",
            "upvotes": 10,
            "discussionId": "67dce4c50784200359ab25dc",
            "ai_keywords": [
                "text-to-image diffusion models",
                "high-resolution image generation",
                "training data",
                "computational resources",
                "data efficiency",
                "synthetic data",
                "teacher models",
                "training convergence",
                "parameter efficiency",
                "weight matrices",
                "low-rank adapters",
                "guidance distillation",
                "FLUX",
                "classifier-free guidance",
                "guidance scale",
                "2K-generation performance",
                "FLUX1.1",
                "4K-resolution generation"
            ]
        },
        "publishedAt": "2025-03-20T12:44:43.000Z",
        "title": "Ultra-Resolution Adaptation with Ease",
        "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed URAE. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, i.e., setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\nhttps://github.com/Huage001/URAE{here}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16322.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6486fb33570a419f41a882e4",
            "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
            "fullname": "Ruonan Yu",
            "name": "roseannelexie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16057",
            "authors": [
                {
                    "_id": "67dd04563b4c256a9809cc96",
                    "user": {
                        "_id": "6656c30934f77bc39689a478",
                        "avatarUrl": "/avatars/226a797834d7353420ffe60cc93e8ee2.svg",
                        "isPro": false,
                        "fullname": "Yike Yuan",
                        "user": "yyk-wew",
                        "type": "user"
                    },
                    "name": "Yike Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:58.139Z",
                    "hidden": false
                },
                {
                    "_id": "67dd04563b4c256a9809cc97",
                    "name": "Ziyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67dd04563b4c256a9809cc98",
                    "user": {
                        "_id": "65a62085576772f531e13856",
                        "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg",
                        "isPro": false,
                        "fullname": "Huang Zihao",
                        "user": "FetchFortune",
                        "type": "user"
                    },
                    "name": "Zihao Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:38:05.513Z",
                    "hidden": false
                },
                {
                    "_id": "67dd04563b4c256a9809cc99",
                    "name": "Defa Zhu",
                    "hidden": false
                },
                {
                    "_id": "67dd04563b4c256a9809cc9a",
                    "name": "Xun Zhou",
                    "hidden": false
                },
                {
                    "_id": "67dd04563b4c256a9809cc9b",
                    "name": "Jingyi Yu",
                    "hidden": false
                },
                {
                    "_id": "67dd04563b4c256a9809cc9c",
                    "name": "Qiyang Min",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T11:45:08.000Z",
            "submittedOnDailyAt": "2025-03-21T04:50:45.905Z",
            "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
            "submittedOnDailyBy": {
                "_id": "667505f4361b960c79e35486",
                "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
                "isPro": false,
                "fullname": "Defa Zhu",
                "user": "mathfinder",
                "type": "user"
            },
            "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
            "upvotes": 10,
            "discussionId": "67dd045a3b4c256a9809cdb1",
            "ai_keywords": [
                "diffusion models",
                "Mixture of Experts (MoE)",
                "Race-DiT",
                "diffusion transformers",
                "Expert Race",
                "tokens",
                "experts",
                "per-layer regularization",
                "router similarity loss",
                "mode collapse",
                "ImageNet"
            ]
        },
        "publishedAt": "2025-03-20T07:45:08.000Z",
        "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
        "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16057.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667505f4361b960c79e35486",
            "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
            "fullname": "Defa Zhu",
            "name": "mathfinder",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16428",
            "authors": [
                {
                    "_id": "67dcd7a53c21e084fe58c3a8",
                    "name": "Ruyi Xu",
                    "hidden": false
                },
                {
                    "_id": "67dcd7a53c21e084fe58c3a9",
                    "name": "Guangxuan Xiao",
                    "hidden": false
                },
                {
                    "_id": "67dcd7a53c21e084fe58c3aa",
                    "user": {
                        "_id": "63797f727df2fefdcaf3ff7e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668906853549-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "songhan",
                        "type": "user"
                    },
                    "name": "Haofeng Huang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-21T03:06:14.875Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd7a53c21e084fe58c3ab",
                    "name": "Junxian Guo",
                    "hidden": false
                },
                {
                    "_id": "67dcd7a53c21e084fe58c3ac",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:58.000Z",
            "submittedOnDailyAt": "2025-03-21T01:36:33.593Z",
            "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
            "upvotes": 8,
            "discussionId": "67dcd7a63c21e084fe58c422",
            "ai_keywords": [
                "Long-Context Transformer Models (LCTMs)",
                "attention's quadratic complexity",
                "block-sparse attention",
                "block importance",
                "XAttention",
                "sparse attention",
                "attention matrix",
                "antidiagonal values",
                "block importance proxy",
                "precision identification",
                "block pruning",
                "high sparsity",
                "inference acceleration",
                "RULER benchmark",
                "LongBench benchmark",
                "VideoMME benchmark",
                "VBench benchmark",
                "video understanding",
                "video generation"
            ]
        },
        "publishedAt": "2025-03-20T13:59:58.000Z",
        "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
        "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6423
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16425",
            "authors": [
                {
                    "_id": "67dd38adbf4c007db3bf4b98",
                    "name": "Zigang Geng",
                    "hidden": false
                },
                {
                    "_id": "67dd38adbf4c007db3bf4b99",
                    "name": "Mengde Xu",
                    "hidden": false
                },
                {
                    "_id": "67dd38adbf4c007db3bf4b9a",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "67dd38adbf4c007db3bf4b9b",
                    "name": "Shuyang Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:51.000Z",
            "submittedOnDailyAt": "2025-03-21T09:16:02.739Z",
            "title": "Tokenize Image as a Set",
            "submittedOnDailyBy": {
                "_id": "64c38fcf573c5a427e12cd37",
                "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
                "isPro": false,
                "fullname": "cientgu",
                "user": "cientgu",
                "type": "user"
            },
            "summary": "This paper proposes a fundamentally new paradigm for image generation through\nset-based tokenization and distribution modeling. Unlike conventional methods\nthat serialize images into fixed-position latent codes with a uniform\ncompression ratio, we introduce an unordered token set representation to\ndynamically allocate coding capacity based on regional semantic complexity.\nThis TokenSet enhances global context aggregation and improves robustness\nagainst local perturbations. To address the critical challenge of modeling\ndiscrete sets, we devise a dual transformation mechanism that bijectively\nconverts sets into fixed-length integer sequences with summation constraints.\nFurther, we propose Fixed-Sum Discrete Diffusion--the first framework to\nsimultaneously handle discrete values, fixed sequence length, and summation\ninvariance--enabling effective set distribution modeling. Experiments\ndemonstrate our method's superiority in semantic-aware representation and\ngeneration quality. Our innovations, spanning novel representation and modeling\nstrategies, advance visual generation beyond traditional sequential token\nparadigms. Our code and models are publicly available at\nhttps://github.com/Gengzigang/TokenSet.",
            "upvotes": 8,
            "discussionId": "67dd38b0bf4c007db3bf4c8f",
            "ai_keywords": [
                "set-based tokenization",
                "TokenSet",
                "discrete sets",
                "dual transformation mechanism",
                "bijective conversion",
                "fixed-length integer sequences",
                "summation constraints",
                "Fixed-Sum Discrete Diffusion",
                "discrete values",
                "fixed sequence length",
                "summation invariance",
                "set distribution modeling",
                "semantic-aware representation",
                "generation quality"
            ]
        },
        "publishedAt": "2025-03-20T13:59:51.000Z",
        "title": "Tokenize Image as a Set",
        "summary": "This paper proposes a fundamentally new paradigm for image generation through\nset-based tokenization and distribution modeling. Unlike conventional methods\nthat serialize images into fixed-position latent codes with a uniform\ncompression ratio, we introduce an unordered token set representation to\ndynamically allocate coding capacity based on regional semantic complexity.\nThis TokenSet enhances global context aggregation and improves robustness\nagainst local perturbations. To address the critical challenge of modeling\ndiscrete sets, we devise a dual transformation mechanism that bijectively\nconverts sets into fixed-length integer sequences with summation constraints.\nFurther, we propose Fixed-Sum Discrete Diffusion--the first framework to\nsimultaneously handle discrete values, fixed sequence length, and summation\ninvariance--enabling effective set distribution modeling. Experiments\ndemonstrate our method's superiority in semantic-aware representation and\ngeneration quality. Our innovations, spanning novel representation and modeling\nstrategies, advance visual generation beyond traditional sequential token\nparadigms. Our code and models are publicly available at\nhttps://github.com/Gengzigang/TokenSet.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16425.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64c38fcf573c5a427e12cd37",
            "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
            "fullname": "cientgu",
            "name": "cientgu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16421",
            "authors": [
                {
                    "_id": "67dcd5913713a0e1da19bbe5",
                    "user": {
                        "_id": "65818917554131ec95df3829",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65818917554131ec95df3829/YIbgLCllm-0u88pRLhpVZ.jpeg",
                        "isPro": false,
                        "fullname": "Quanhao Li",
                        "user": "quanhaol",
                        "type": "user"
                    },
                    "name": "Quanhao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:57.203Z",
                    "hidden": false
                },
                {
                    "_id": "67dcd5913713a0e1da19bbe6",
                    "name": "Zhen Xing",
                    "hidden": false
                },
                {
                    "_id": "67dcd5913713a0e1da19bbe7",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcd5913713a0e1da19bbe8",
                    "name": "Hui Zhang",
                    "hidden": false
                },
                {
                    "_id": "67dcd5913713a0e1da19bbe9",
                    "name": "Qi Dai",
                    "hidden": false
                },
                {
                    "_id": "67dcd5913713a0e1da19bbea",
                    "name": "Zuxuan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:42.000Z",
            "submittedOnDailyAt": "2025-03-21T01:28:06.140Z",
            "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
            "upvotes": 8,
            "discussionId": "67dcd5953713a0e1da19bd51",
            "projectPage": "https://quanhaol.github.io/magicmotion-site",
            "githubRepo": "https://github.com/quanhaol/MagicMotion",
            "ai_keywords": [
                "trajectory-controllable video generation",
                "dense conditions",
                "sparse conditions",
                "masks",
                "bounding boxes",
                "sparse boxes",
                "object consistency",
                "MagicMotion",
                "MagicData",
                "MagicBench"
            ]
        },
        "publishedAt": "2025-03-20T13:59:42.000Z",
        "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
        "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16421.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6423
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16252",
            "authors": [
                {
                    "_id": "67dcc7b29c17514cb9815abd",
                    "name": "Zhaowei Liu",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815abe",
                    "name": "Xin Guo",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815abf",
                    "name": "Fangqi Lou",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac0",
                    "name": "Lingfeng Zeng",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac1",
                    "name": "Jinyi Niu",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac2",
                    "name": "Zixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac3",
                    "name": "Jiajie Xu",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac4",
                    "name": "Weige Cai",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac5",
                    "name": "Ziwei Yang",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac6",
                    "name": "Xueqian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac7",
                    "name": "Chao Li",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac8",
                    "name": "Sheng Xu",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815ac9",
                    "name": "Dezhi Chen",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815aca",
                    "name": "Yun Chen",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815acb",
                    "name": "Zuo Bai",
                    "hidden": false
                },
                {
                    "_id": "67dcc7b29c17514cb9815acc",
                    "name": "Liwen Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T15:46:18.000Z",
            "submittedOnDailyAt": "2025-03-21T00:54:01.337Z",
            "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
            "upvotes": 7,
            "discussionId": "67dcc7b69c17514cb9815c1d",
            "githubRepo": "https://github.com/SUFE-AIFLM-Lab/Fin-R1",
            "ai_keywords": [
                "reasoning large language models",
                "two-stage architecture",
                "financial reasoning dataset",
                "DeepSeek-R1",
                "supervised fine-tuning (SFT)",
                "reinforcement learning (RL)",
                "state-of-the-art (SOTA)",
                "FinQA",
                "ConvFinQA"
            ]
        },
        "publishedAt": "2025-03-20T11:46:18.000Z",
        "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
        "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16252.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6423
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16219",
            "authors": [
                {
                    "_id": "67dd1a9cfa598c90d14e9b47",
                    "user": {
                        "_id": "645b663eca5d8a297712f2e1",
                        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
                        "isPro": false,
                        "fullname": "Quy-Anh Dang",
                        "user": "quyanh",
                        "type": "user"
                    },
                    "name": "Quy-Anh Dang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-21T07:52:47.629Z",
                    "hidden": false
                },
                {
                    "_id": "67dd1a9cfa598c90d14e9b48",
                    "name": "Chris Ngo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/EIt1JuoKkqWkGY1Pied64.png"
            ],
            "publishedAt": "2025-03-20T15:13:23.000Z",
            "submittedOnDailyAt": "2025-03-21T07:03:47.115Z",
            "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
            "submittedOnDailyBy": {
                "_id": "645b663eca5d8a297712f2e1",
                "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
                "isPro": false,
                "fullname": "Quy-Anh Dang",
                "user": "quyanh",
                "type": "user"
            },
            "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
            "upvotes": 7,
            "discussionId": "67dd1a9dfa598c90d14e9ba4",
            "githubRepo": "https://github.com/knoveleng/open-rs",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "Group Relative Policy Optimization (GRPO)",
                "mathematical reasoning dataset",
                "AMC23",
                "AIME24",
                "optimization instability",
                "RL-based fine-tuning",
                "scalable",
                "reasoning-capable LLMs"
            ]
        },
        "publishedAt": "2025-03-20T11:13:23.000Z",
        "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
        "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/EIt1JuoKkqWkGY1Pied64.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16219.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "fullname": "Quy-Anh Dang",
            "name": "quyanh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16055",
            "authors": [
                {
                    "_id": "67dd0d7c68dc6463747e6cac",
                    "name": "Abdelrahman Elsayed",
                    "hidden": false
                },
                {
                    "_id": "67dd0d7c68dc6463747e6cad",
                    "user": {
                        "_id": "62676a94dacab364889bb36c",
                        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
                        "isPro": false,
                        "fullname": "SARIM HASHMI",
                        "user": "Sarim-Hash",
                        "type": "user"
                    },
                    "name": "Sarim Hashmi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:54.986Z",
                    "hidden": false
                },
                {
                    "_id": "67dd0d7c68dc6463747e6cae",
                    "name": "Mohammed Elseiagy",
                    "hidden": false
                },
                {
                    "_id": "67dd0d7c68dc6463747e6caf",
                    "name": "Hu Wang",
                    "hidden": false
                },
                {
                    "_id": "67dd0d7c68dc6463747e6cb0",
                    "name": "Mohammad Yaqub",
                    "hidden": false
                },
                {
                    "_id": "67dd0d7c68dc6463747e6cb1",
                    "name": "Ibrahim Almakky",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T11:42:41.000Z",
            "submittedOnDailyAt": "2025-03-21T05:26:30.501Z",
            "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
            "submittedOnDailyBy": {
                "_id": "62676a94dacab364889bb36c",
                "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
                "isPro": false,
                "fullname": "SARIM HASHMI",
                "user": "Sarim-Hash",
                "type": "user"
            },
            "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
            "upvotes": 7,
            "discussionId": "67dd0d7e68dc6463747e6d03",
            "githubRepo": "https://github.com/BioMedIA-MBZUAI/SALT",
            "ai_keywords": [
                "SALT",
                "Singular Value Adaptation with Low-Rank Transformation",
                "Low-Rank Adaptation",
                "LoRA",
                "Singular Value Decomposition",
                "SVD",
                "Dice",
                "Parameter-Efficient Fine-Tuning",
                "trainable parameters"
            ]
        },
        "publishedAt": "2025-03-20T07:42:41.000Z",
        "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
        "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16055.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62676a94dacab364889bb36c",
            "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
            "fullname": "SARIM HASHMI",
            "name": "Sarim-Hash",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15242",
            "authors": [
                {
                    "_id": "67dbd897a3b9c06eb0b6988e",
                    "user": {
                        "_id": "65e8c0e40a099ee0c1302e57",
                        "avatarUrl": "/avatars/3905f563be224c0c296dc5c942329d8d.svg",
                        "isPro": false,
                        "fullname": "Pierre Chambon",
                        "user": "pierrechambon",
                        "type": "user"
                    },
                    "name": "Pierre Chambon",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-20T08:58:00.216Z",
                    "hidden": false
                },
                {
                    "_id": "67dbd897a3b9c06eb0b6988f",
                    "user": {
                        "_id": "62d063dac375d0c84255b9a1",
                        "avatarUrl": "/avatars/de0fc34bad8c761210c0895ebfa4feba.svg",
                        "isPro": false,
                        "fullname": "Baptiste Roziere",
                        "user": "broz",
                        "type": "user"
                    },
                    "name": "Baptiste Roziere",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:09:01.044Z",
                    "hidden": false
                },
                {
                    "_id": "67dbd897a3b9c06eb0b69890",
                    "user": {
                        "_id": "602ba2a739515f8d31237967",
                        "avatarUrl": "/avatars/ef534712ca682bf74ec7eef17d3d1b5f.svg",
                        "isPro": false,
                        "fullname": "Benot Sagot",
                        "user": "sagot",
                        "type": "user"
                    },
                    "name": "Benoit Sagot",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:09:08.259Z",
                    "hidden": false
                },
                {
                    "_id": "67dbd897a3b9c06eb0b69891",
                    "user": {
                        "_id": "630eac7931970d1cd4fbacf2",
                        "avatarUrl": "/avatars/b7ccbddfa745db854dc342be1327cd53.svg",
                        "isPro": false,
                        "fullname": "Gabriel Synnaeve",
                        "user": "gsynnaeve",
                        "type": "user"
                    },
                    "name": "Gabriel Synnaeve",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:09:15.204Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T14:19:57.000Z",
            "submittedOnDailyAt": "2025-03-21T09:23:13.946Z",
            "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
            "submittedOnDailyBy": {
                "_id": "65e8c0e40a099ee0c1302e57",
                "avatarUrl": "/avatars/3905f563be224c0c296dc5c942329d8d.svg",
                "isPro": false,
                "fullname": "Pierre Chambon",
                "user": "pierrechambon",
                "type": "user"
            },
            "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
            "upvotes": 7,
            "discussionId": "67dbd898a3b9c06eb0b69923",
            "projectPage": "https://facebookresearch.github.io/BigOBench/",
            "githubRepo": "https://github.com/facebookresearch/bigobench",
            "ai_keywords": [
                "generative language models",
                "time and space complexities",
                "algorithmic complexity",
                "complexity framework",
                "runtime",
                "memory footprint",
                "token-space reasoning"
            ]
        },
        "publishedAt": "2025-03-19T10:19:57.000Z",
        "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
        "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15242.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e8c0e40a099ee0c1302e57",
            "avatarUrl": "/avatars/3905f563be224c0c296dc5c942329d8d.svg",
            "fullname": "Pierre Chambon",
            "name": "pierrechambon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.10625",
            "authors": [
                {
                    "_id": "67dacb439c49701f604e4257",
                    "name": "Lingteng Qiu",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e4258",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e4259",
                    "name": "Peihao Li",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e425a",
                    "user": {
                        "_id": "64d0d72e15b26cc7f704a60f",
                        "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
                        "isPro": true,
                        "fullname": "Qi Zuo",
                        "user": "DyrusQZ",
                        "type": "user"
                    },
                    "name": "Qi Zuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-20T10:46:05.121Z",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e425b",
                    "name": "Weichao Shen",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e425c",
                    "name": "Junfei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e425d",
                    "name": "Kejie Qiu",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e425e",
                    "name": "Weihao Yuan",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e425f",
                    "name": "Guanying Chen",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e4260",
                    "name": "Zilong Dong",
                    "hidden": false
                },
                {
                    "_id": "67dacb439c49701f604e4261",
                    "name": "Liefeng Bo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
            ],
            "publishedAt": "2025-03-13T17:59:21.000Z",
            "submittedOnDailyAt": "2025-03-21T05:52:51.662Z",
            "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
            "submittedOnDailyBy": {
                "_id": "64d0d72e15b26cc7f704a60f",
                "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
                "isPro": true,
                "fullname": "Qi Zuo",
                "user": "DyrusQZ",
                "type": "user"
            },
            "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
            "upvotes": 7,
            "discussionId": "67dacb499c49701f604e4454",
            "ai_keywords": [
                "3D Gaussian splatting",
                "multimodal transformer architecture",
                "attention mechanism",
                "head feature pyramid encoding scheme"
            ]
        },
        "publishedAt": "2025-03-13T13:59:21.000Z",
        "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
        "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10625.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d0d72e15b26cc7f704a60f",
            "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
            "fullname": "Qi Zuo",
            "name": "DyrusQZ",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16375",
            "authors": [
                {
                    "_id": "67dd269625f9991caf94c667",
                    "user": {
                        "_id": "6313c7a754e6e5d9f0fa4d81",
                        "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
                        "isPro": true,
                        "fullname": "HAN-HUNG LEE",
                        "user": "rexleeppp",
                        "type": "user"
                    },
                    "name": "Han-Hung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:37:43.579Z",
                    "hidden": false
                },
                {
                    "_id": "67dd269625f9991caf94c668",
                    "name": "Qinghong Han",
                    "hidden": false
                },
                {
                    "_id": "67dd269625f9991caf94c669",
                    "name": "Angel X. Chang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6313c7a754e6e5d9f0fa4d81/U4GisQW1kt0WXR-0HiBLv.mp4"
            ],
            "publishedAt": "2025-03-20T17:37:43.000Z",
            "submittedOnDailyAt": "2025-03-21T07:15:44.580Z",
            "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
            "submittedOnDailyBy": {
                "_id": "6313c7a754e6e5d9f0fa4d81",
                "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
                "isPro": true,
                "fullname": "HAN-HUNG LEE",
                "user": "rexleeppp",
                "type": "user"
            },
            "summary": "In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training.",
            "upvotes": 6,
            "discussionId": "67dd269c25f9991caf94c87b",
            "projectPage": "https://3dlg-hcvc.github.io/NuiScene/",
            "githubRepo": "https://github.com/3dlg-hcvc/NuiScene",
            "ai_keywords": [
                "outpainting model",
                "explicit outpainting",
                "diffusion steps"
            ]
        },
        "publishedAt": "2025-03-20T13:37:43.000Z",
        "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
        "summary": "In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6313c7a754e6e5d9f0fa4d81/U4GisQW1kt0WXR-0HiBLv.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16375.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6313c7a754e6e5d9f0fa4d81",
            "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
            "fullname": "HAN-HUNG LEE",
            "name": "rexleeppp",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15567",
            "authors": [
                {
                    "_id": "67dcc734067589b43b19af7a",
                    "name": "Yanchen Luo",
                    "hidden": false
                },
                {
                    "_id": "67dcc734067589b43b19af7b",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67dcc734067589b43b19af7c",
                    "name": "Yi Zhao",
                    "hidden": false
                },
                {
                    "_id": "67dcc734067589b43b19af7d",
                    "name": "Sihang Li",
                    "hidden": false
                },
                {
                    "_id": "67dcc734067589b43b19af7e",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                },
                {
                    "_id": "67dcc734067589b43b19af7f",
                    "name": "Tat-Seng Chua",
                    "hidden": false
                },
                {
                    "_id": "67dcc734067589b43b19af80",
                    "name": "Xiang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T08:56:13.000Z",
            "submittedOnDailyAt": "2025-03-21T00:33:21.187Z",
            "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
            "submittedOnDailyBy": {
                "_id": "64f04a28f3cd962c21726459",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
                "isPro": false,
                "fullname": "LuoYanchen",
                "user": "lyc0930",
                "type": "user"
            },
            "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose Unified\nVariational Auto-Encoder for 3D Molecular Latent\nDiffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both de novo and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
            "upvotes": 6,
            "discussionId": "67dcc735067589b43b19afd9",
            "ai_keywords": [
                "SE(3) equivariance",
                "latent spaces",
                "multi-modal VAE",
                "latent sequences",
                "unified latent space",
                "latent diffusion modeling",
                "Diffusion Transformer",
                "GEOM-Drugs dataset",
                "QM9 dataset",
                "de novo molecule generation",
                "conditional molecule generation"
            ]
        },
        "publishedAt": "2025-03-19T04:56:13.000Z",
        "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
        "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose Unified\nVariational Auto-Encoder for 3D Molecular Latent\nDiffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both de novo and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15567.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f04a28f3cd962c21726459",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
            "fullname": "LuoYanchen",
            "name": "lyc0930",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.13356",
            "authors": [
                {
                    "_id": "67dcfafa73e582f950d360f3",
                    "user": {
                        "_id": "672db76fa34d64e774fc42c9",
                        "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
                        "isPro": false,
                        "fullname": "Zhongwen Xu",
                        "user": "zhongwenxu",
                        "type": "user"
                    },
                    "name": "Zhongwen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:17.259Z",
                    "hidden": false
                },
                {
                    "_id": "67dcfafa73e582f950d360f4",
                    "name": "Xianliang Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcfafa73e582f950d360f5",
                    "name": "Siyi Li",
                    "hidden": false
                },
                {
                    "_id": "67dcfafa73e582f950d360f6",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "67dcfafa73e582f950d360f7",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcfafa73e582f950d360f8",
                    "name": "Qiang Fu",
                    "hidden": false
                },
                {
                    "_id": "67dcfafa73e582f950d360f9",
                    "name": "Wei Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T16:42:34.000Z",
            "submittedOnDailyAt": "2025-03-21T13:26:11.525Z",
            "title": "Agents Play Thousands of 3D Video Games",
            "submittedOnDailyBy": {
                "_id": "672db76fa34d64e774fc42c9",
                "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
                "isPro": false,
                "fullname": "Zhongwen Xu",
                "user": "zhongwenxu",
                "type": "user"
            },
            "summary": "We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal .",
            "upvotes": 6,
            "discussionId": "67dcfb0273e582f950d36384",
            "ai_keywords": [
                "PORTAL",
                "large language models (LLMs)",
                "behavior trees",
                "domain-specific language (DSL)",
                "rule-based nodes",
                "neural network components",
                "hybrid policy structure",
                "dual-feedback mechanism",
                "quantitative game metrics",
                "vision-language model analysis",
                "first-person shooter (FPS) games"
            ]
        },
        "publishedAt": "2025-03-17T12:42:34.000Z",
        "title": "Agents Play Thousands of 3D Video Games",
        "summary": "We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13356.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672db76fa34d64e774fc42c9",
            "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
            "fullname": "Zhongwen Xu",
            "name": "zhongwenxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16278",
            "authors": [
                {
                    "_id": "67dcc98b54dcfdc1fd17d9b6",
                    "name": "Shuqi Lu",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9b7",
                    "name": "Haowei Lin",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9b8",
                    "name": "Lin Yao",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9b9",
                    "name": "Zhifeng Gao",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9ba",
                    "name": "Xiaohong Ji",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9bb",
                    "name": "Weinan E",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9bc",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67dcc98b54dcfdc1fd17d9bd",
                    "user": {
                        "_id": "6348de0c62c668c7b48d83c9",
                        "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
                        "isPro": false,
                        "fullname": "Guolin Ke",
                        "user": "guolinke",
                        "type": "user"
                    },
                    "name": "Guolin Ke",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:41:08.637Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T16:07:04.000Z",
            "submittedOnDailyAt": "2025-03-21T00:36:39.769Z",
            "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
            "submittedOnDailyBy": {
                "_id": "6348de0c62c668c7b48d83c9",
                "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
                "isPro": false,
                "fullname": "Guolin Ke",
                "user": "guolinke",
                "type": "user"
            },
            "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
            "upvotes": 5,
            "discussionId": "67dcc98c54dcfdc1fd17da0f",
            "projectPage": "https://uni-3dar.github.io",
            "githubRepo": "https://github.com/dptech-corp/Uni-3DAR",
            "ai_keywords": [
                "hierarchical tokenization",
                "octree",
                "two-level subtree compression strategy",
                "masked next-token prediction mechanism",
                "Uni-3DAR",
                "3D GU (3D generation and understanding)",
                "autoregressive prediction",
                "state-of-the-art diffusion models"
            ]
        },
        "publishedAt": "2025-03-20T12:07:04.000Z",
        "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
        "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16278.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6348de0c62c668c7b48d83c9",
            "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
            "fullname": "Guolin Ke",
            "name": "guolinke",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16188",
            "authors": [
                {
                    "_id": "67dce5afbabeda89ca6071c3",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67dce5afbabeda89ca6071c4",
                    "user": {
                        "_id": "62c66504031996c36c86976a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
                        "isPro": false,
                        "fullname": "steve z",
                        "user": "stzhao",
                        "type": "user"
                    },
                    "name": "Shitian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:22.815Z",
                    "hidden": false
                },
                {
                    "_id": "67dce5afbabeda89ca6071c5",
                    "name": "Jike Zhong",
                    "hidden": false
                },
                {
                    "_id": "67dce5afbabeda89ca6071c6",
                    "user": {
                        "_id": "671a569a97cb6656e4859ea8",
                        "avatarUrl": "/avatars/9eb7966b3ff39ee342e9e681256c86a7.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Lai",
                        "user": "yuxianglai117",
                        "type": "user"
                    },
                    "name": "Yuxiang Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:25.107Z",
                    "hidden": false
                },
                {
                    "_id": "67dce5afbabeda89ca6071c7",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T14:37:45.000Z",
            "submittedOnDailyAt": "2025-03-21T03:47:03.083Z",
            "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
            "upvotes": 5,
            "discussionId": "67dce5b1babeda89ca607241",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "few-shot MLLM classification fine-tuning",
                "SFT",
                "overfitting issues",
                "zero-shot approach",
                "CLS-RL",
                "verifiable signals",
                "reward",
                "free-lunch phenomenon",
                "base-to-new",
                "few-shot learning",
                "RL-based methods",
                "inference time thinking",
                "thinking process",
                "No-Thinking-CLS-RL",
                "equality accuracy reward"
            ]
        },
        "publishedAt": "2025-03-20T10:37:45.000Z",
        "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
        "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16188.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.15851",
            "authors": [
                {
                    "_id": "67dcff844aa37abf77ae7338",
                    "user": {
                        "_id": "6425318d175bd2952281065e",
                        "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
                        "isPro": false,
                        "fullname": "ZhenglinZhou",
                        "user": "zhenglin",
                        "type": "user"
                    },
                    "name": "Zhou Zhenglin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:15.305Z",
                    "hidden": false
                },
                {
                    "_id": "67dcff844aa37abf77ae7339",
                    "name": "Ma Fan",
                    "hidden": false
                },
                {
                    "_id": "67dcff844aa37abf77ae733a",
                    "name": "Fan Hehe",
                    "hidden": false
                },
                {
                    "_id": "67dcff844aa37abf77ae733b",
                    "name": "Chua Tat-Seng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T05:07:46.000Z",
            "submittedOnDailyAt": "2025-03-21T04:27:49.029Z",
            "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
            "submittedOnDailyBy": {
                "_id": "6425318d175bd2952281065e",
                "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
                "isPro": false,
                "fullname": "ZhenglinZhou",
                "user": "zhenglin",
                "type": "user"
            },
            "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
            "upvotes": 5,
            "discussionId": "67dcff884aa37abf77ae7415",
            "ai_keywords": [
                "diffusion models",
                "score distillation sampling (SDS)",
                "pseudo ground-truth outputs",
                "video diffusion",
                "spatial consistency",
                "temporal consistency",
                "Zero-1-to-A",
                "front-to-side views",
                "progressive learning",
                "spatial consistency learning",
                "temporal consistency learning",
                "4D avatars",
                "avatar quality",
                "fidelity",
                "animation quality",
                "rendering speed"
            ]
        },
        "publishedAt": "2025-03-20T01:07:46.000Z",
        "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
        "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15851.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6425318d175bd2952281065e",
            "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
            "fullname": "ZhenglinZhou",
            "name": "zhenglin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14237",
            "authors": [
                {
                    "_id": "67db7a33b1d42828a18fd0c8",
                    "name": "Chenting Wang",
                    "hidden": false
                },
                {
                    "_id": "67db7a33b1d42828a18fd0c9",
                    "name": "Kunchang Li",
                    "hidden": false
                },
                {
                    "_id": "67db7a33b1d42828a18fd0ca",
                    "name": "Tianxiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "67db7a33b1d42828a18fd0cb",
                    "name": "Xiangyu Zeng",
                    "hidden": false
                },
                {
                    "_id": "67db7a33b1d42828a18fd0cc",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "67db7a33b1d42828a18fd0cd",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T13:15:58.000Z",
            "submittedOnDailyAt": "2025-03-21T07:23:35.664Z",
            "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
            "submittedOnDailyBy": {
                "_id": "62aafa49f29ff279b51f0182",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
                "isPro": false,
                "fullname": "yinanhe",
                "user": "ynhe",
                "type": "user"
            },
            "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.",
            "upvotes": 5,
            "discussionId": "67db7a35b1d42828a18fd11f",
            "ai_keywords": [
                "token optimization",
                "spatiotemporal grid",
                "token selection",
                "Flux augmentation tool",
                "FluxViT",
                "video pre-training"
            ]
        },
        "publishedAt": "2025-03-18T09:15:58.000Z",
        "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
        "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14237.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62aafa49f29ff279b51f0182",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
            "fullname": "yinanhe",
            "name": "ynhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16429",
            "authors": [
                {
                    "_id": "67dd5935b4cce80ee64cd98f",
                    "user": {
                        "_id": "643e5d6a1d0e956d94bb3608",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoyang Wu",
                        "user": "Gofinge",
                        "type": "user"
                    },
                    "name": "Xiaoyang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T12:46:08.995Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd990",
                    "user": {
                        "_id": "66f1e84ea4d3e85a77bdb545",
                        "avatarUrl": "/avatars/4e24b0bdd7a8c86ba5f43f4b042817ea.svg",
                        "isPro": false,
                        "fullname": "Daniel DeTone",
                        "user": "ddetone",
                        "type": "user"
                    },
                    "name": "Daniel DeTone",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:09:34.106Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd991",
                    "user": {
                        "_id": "65fc12c6fc9132a2dfe9e8b3",
                        "avatarUrl": "/avatars/3d04d79721f4ec6b0b090e0bbbf85f70.svg",
                        "isPro": false,
                        "fullname": "Duncan Frost",
                        "user": "frosd01",
                        "type": "user"
                    },
                    "name": "Duncan Frost",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:09:54.292Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd992",
                    "user": {
                        "_id": "66e0950be1bfac4b463fb941",
                        "avatarUrl": "/avatars/6506d169af3ead9b7d7d30a1013857b1.svg",
                        "isPro": false,
                        "fullname": "Tianwei Shen",
                        "user": "tshenaa",
                        "type": "user"
                    },
                    "name": "Tianwei Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:10:00.797Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd993",
                    "name": "Chris Xie",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd994",
                    "name": "Nan Yang",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd995",
                    "user": {
                        "_id": "62861d244713da0266f29f99",
                        "avatarUrl": "/avatars/819abb2f7d02d65cbaa4a571b8b13169.svg",
                        "isPro": false,
                        "fullname": "Jakob Engel",
                        "user": "JakobEngel",
                        "type": "user"
                    },
                    "name": "Jakob Engel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:10:15.646Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd996",
                    "user": {
                        "_id": "633ea8814b469e3151bb38f2",
                        "avatarUrl": "/avatars/245f849cde665f1d7dfec15dead6b880.svg",
                        "isPro": false,
                        "fullname": "Richard Newcombe",
                        "user": "MONGIE30",
                        "type": "user"
                    },
                    "name": "Richard Newcombe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:10:22.340Z",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd997",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67dd5935b4cce80ee64cd998",
                    "user": {
                        "_id": "673927a0ab4212713230d456",
                        "avatarUrl": "/avatars/b90972cefa847649a6acc47e1f100b3b.svg",
                        "isPro": false,
                        "fullname": "Julian Straub",
                        "user": "jstraub",
                        "type": "user"
                    },
                    "name": "Julian Straub",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:10:37.008Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:59.000Z",
            "submittedOnDailyAt": "2025-03-21T10:49:40.228Z",
            "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
            "submittedOnDailyBy": {
                "_id": "643e5d6a1d0e956d94bb3608",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
                "isPro": false,
                "fullname": "Xiaoyang Wu",
                "user": "Gofinge",
                "type": "user"
            },
            "summary": "In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.",
            "upvotes": 4,
            "discussionId": "67dd5937b4cce80ee64cda31",
            "ai_keywords": [
                "self-supervised point cloud model",
                "linear probing",
                "representation quality",
                "geometric shortcut",
                "low-level spatial features",
                "point cloud data",
                "obscuring spatial information",
                "enhancing the reliance on input features",
                "self-distillation",
                "semantic grouping",
                "nearest-neighbor relationships",
                "parameter efficiency",
                "data efficiency",
                "linear probing accuracy",
                "ScanNet",
                "full fine-tuning",
                "SOTA (state-of-the-art)",
                "3D indoor perception tasks",
                "3D outdoor perception tasks"
            ]
        },
        "publishedAt": "2025-03-20T13:59:59.000Z",
        "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
        "summary": "In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16429.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643e5d6a1d0e956d94bb3608",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e5d6a1d0e956d94bb3608/mTnu7Dts2RmDklHlp9Gqu.jpeg",
            "fullname": "Xiaoyang Wu",
            "name": "Gofinge",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16031",
            "authors": [
                {
                    "_id": "67dcc5e41f94b594ef4c0312",
                    "user": {
                        "_id": "651692d718f3a57f869a5a0a",
                        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                        "isPro": false,
                        "fullname": "Sai Kartheek Reddy",
                        "user": "UVSKKR",
                        "type": "user"
                    },
                    "name": "Sai Kartheek Reddy Kasu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-21T01:51:18.039Z",
                    "hidden": false
                },
                {
                    "_id": "67dcc5e41f94b594ef4c0313",
                    "user": {
                        "_id": "6443c91b79e7797ce719dbaf",
                        "avatarUrl": "/avatars/5c981c39075b35cc0ea69bf0253829a0.svg",
                        "isPro": false,
                        "fullname": "shankar Biradar",
                        "user": "shankarB",
                        "type": "user"
                    },
                    "name": "Shankar Biradar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:11:38.661Z",
                    "hidden": false
                },
                {
                    "_id": "67dcc5e41f94b594ef4c0314",
                    "name": "Sunil Saumya",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T10:58:02.000Z",
            "submittedOnDailyAt": "2025-03-21T00:28:13.365Z",
            "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
            "submittedOnDailyBy": {
                "_id": "651692d718f3a57f869a5a0a",
                "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                "isPro": false,
                "fullname": "Sai Kartheek Reddy",
                "user": "UVSKKR",
                "type": "user"
            },
            "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
            "upvotes": 3,
            "discussionId": "67dcc5e41f94b594ef4c0353"
        },
        "publishedAt": "2025-03-20T06:58:02.000Z",
        "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
        "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16031.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "fullname": "Sai Kartheek Reddy",
            "name": "UVSKKR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15451",
            "authors": [
                {
                    "_id": "67dd03fb2672aa3643252e8c",
                    "user": {
                        "_id": "65220fedc709aaca9aa63061",
                        "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
                        "isPro": false,
                        "fullname": "Lixing Xiao",
                        "user": "lxxiao",
                        "type": "user"
                    },
                    "name": "Lixing Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:07.470Z",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e8d",
                    "name": "Shunlin Lu",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e8e",
                    "name": "Huaijin Pi",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e8f",
                    "name": "Ke Fan",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e90",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e91",
                    "name": "Yueer Zhou",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e92",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e93",
                    "name": "Xiaowei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e94",
                    "name": "Sida Peng",
                    "hidden": false
                },
                {
                    "_id": "67dd03fb2672aa3643252e95",
                    "name": "Jingbo Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T17:32:24.000Z",
            "submittedOnDailyAt": "2025-03-21T05:04:41.437Z",
            "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
            "submittedOnDailyBy": {
                "_id": "65220fedc709aaca9aa63061",
                "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
                "isPro": false,
                "fullname": "Lixing Xiao",
                "user": "lxxiao",
                "type": "user"
            },
            "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
            "upvotes": 3,
            "discussionId": "67dd03fd2672aa3643252f2a",
            "projectPage": "https://zju3dv.github.io/MotionStreamer/",
            "ai_keywords": [
                "diffusion models",
                "streaming motion generation",
                "human pose prediction",
                "historical motions",
                "incoming texts",
                "GPT-based methods",
                "continuous causal latent space",
                "probabilistic autoregressive model",
                "information loss",
                "error accumulation",
                "temporal causal dependencies",
                "online motion decoding",
                "multi-round generation",
                "long-term generation",
                "dynamic motion composition"
            ]
        },
        "publishedAt": "2025-03-19T13:32:24.000Z",
        "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
        "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15451.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65220fedc709aaca9aa63061",
            "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
            "fullname": "Lixing Xiao",
            "name": "lxxiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12689",
            "authors": [
                {
                    "_id": "67dcc2e10df3501c657ef478",
                    "name": "Hengjia Li",
                    "hidden": false
                },
                {
                    "_id": "67dcc2e10df3501c657ef479",
                    "name": "Lifan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67dcc2e10df3501c657ef47a",
                    "name": "Xi Xiao",
                    "hidden": false
                },
                {
                    "_id": "67dcc2e10df3501c657ef47b",
                    "name": "Tianyang Wang",
                    "hidden": false
                },
                {
                    "_id": "67dcc2e10df3501c657ef47c",
                    "name": "Hongwei Yi",
                    "hidden": false
                },
                {
                    "_id": "67dcc2e10df3501c657ef47d",
                    "name": "Boxi Wu",
                    "hidden": false
                },
                {
                    "_id": "67dcc2e10df3501c657ef47e",
                    "user": {
                        "_id": "646735a1946476c5d214e5b2",
                        "avatarUrl": "/avatars/23392fda23fa6288fcb309f7eaa73472.svg",
                        "isPro": false,
                        "fullname": "Deng Cai",
                        "user": "jcyk",
                        "type": "user"
                    },
                    "name": "Deng Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T15:01:22.490Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-16T23:15:09.000Z",
            "submittedOnDailyAt": "2025-03-21T02:25:48.893Z",
            "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\nMagicID, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
            "upvotes": 3,
            "discussionId": "67dcc2e50df3501c657ef56b",
            "projectPage": "https://echopluto.github.io/MagicID-project/",
            "githubRepo": "https://github.com/EchoPluto/MagicID",
            "ai_keywords": [
                "pairwise preference video data",
                "identity and dynamic rewards",
                "preference learning",
                "hybrid sampling strategy",
                "static videos",
                "Frontier-based sampling method",
                "reward differences",
                "customized preferences",
                "identity-preserving",
                "dynamic motion quality",
                "high-fidelity videos",
                "consistent identity",
                "natural dynamics"
            ]
        },
        "publishedAt": "2025-03-16T19:15:09.000Z",
        "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
        "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\nMagicID, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 35
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.09949",
            "authors": [
                {
                    "_id": "67dd70153851bd50bfa58b42",
                    "name": "Yuanxin Liu",
                    "hidden": false
                },
                {
                    "_id": "67dd70153851bd50bfa58b43",
                    "name": "Rui Zhu",
                    "hidden": false
                },
                {
                    "_id": "67dd70153851bd50bfa58b44",
                    "name": "Shuhuai Ren",
                    "hidden": false
                },
                {
                    "_id": "67dd70153851bd50bfa58b45",
                    "name": "Jiacong Wang",
                    "hidden": false
                },
                {
                    "_id": "67dd70153851bd50bfa58b46",
                    "name": "Haoyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "67dd70153851bd50bfa58b47",
                    "name": "Xu Sun",
                    "hidden": false
                },
                {
                    "_id": "67dd70153851bd50bfa58b48",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-13T01:52:27.000Z",
            "submittedOnDailyAt": "2025-03-21T12:45:29.574Z",
            "title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?",
            "submittedOnDailyBy": {
                "_id": "6489761dcaea79f577897f98",
                "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
                "isPro": false,
                "fullname": "Yuanxin Liu",
                "user": "lyx97",
                "type": "user"
            },
            "summary": "With the rapid growth of video generative models (VGMs), it is essential to\ndevelop reliable and comprehensive automatic metrics for AI-generated videos\n(AIGVs). Existing methods either use off-the-shelf models optimized for other\ntasks or rely on human assessment data to train specialized evaluators. These\napproaches are constrained to specific evaluation aspects and are difficult to\nscale with the increasing demands for finer-grained and more comprehensive\nevaluations. To address this issue, this work investigates the feasibility of\nusing multimodal large language models (MLLMs) as a unified evaluator for\nAIGVs, leveraging their strong visual perception and language understanding\ncapabilities. To evaluate the performance of automatic metrics in unified AIGV\nevaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects\nvideos generated by state-of-the-art VGMs and provides pairwise human\npreference annotations across 15 evaluation aspects. Using UVE-Bench, we\nextensively evaluate 16 MLLMs. Our empirical results suggest that while\nadvanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human\nevaluators, they demonstrate promising ability in unified AIGV evaluation,\nsignificantly surpassing existing specialized evaluation methods. Additionally,\nwe conduct an in-depth analysis of key design choices that impact the\nperformance of MLLM-driven evaluators, offering valuable insights for future\nresearch on AIGV evaluation. The code is available at\nhttps://github.com/bytedance/UVE.",
            "upvotes": 3,
            "discussionId": "67dd70193851bd50bfa58c70",
            "ai_keywords": [
                "video generative models (VGMs)",
                "AI-generated videos (AIGVs)",
                "off-the-shelf models",
                "specialized evaluators",
                "multimodal large language models (MLLMs)",
                "visual perception",
                "language understanding",
                "UVE-Bench",
                "pairwise human preference annotations",
                "advanced MLLMs"
            ]
        },
        "publishedAt": "2025-03-12T21:52:27.000Z",
        "title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?",
        "summary": "With the rapid growth of video generative models (VGMs), it is essential to\ndevelop reliable and comprehensive automatic metrics for AI-generated videos\n(AIGVs). Existing methods either use off-the-shelf models optimized for other\ntasks or rely on human assessment data to train specialized evaluators. These\napproaches are constrained to specific evaluation aspects and are difficult to\nscale with the increasing demands for finer-grained and more comprehensive\nevaluations. To address this issue, this work investigates the feasibility of\nusing multimodal large language models (MLLMs) as a unified evaluator for\nAIGVs, leveraging their strong visual perception and language understanding\ncapabilities. To evaluate the performance of automatic metrics in unified AIGV\nevaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects\nvideos generated by state-of-the-art VGMs and provides pairwise human\npreference annotations across 15 evaluation aspects. Using UVE-Bench, we\nextensively evaluate 16 MLLMs. Our empirical results suggest that while\nadvanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human\nevaluators, they demonstrate promising ability in unified AIGV evaluation,\nsignificantly surpassing existing specialized evaluation methods. Additionally,\nwe conduct an in-depth analysis of key design choices that impact the\nperformance of MLLM-driven evaluators, offering valuable insights for future\nresearch on AIGV evaluation. The code is available at\nhttps://github.com/bytedance/UVE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09949.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6489761dcaea79f577897f98",
            "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
            "fullname": "Yuanxin Liu",
            "name": "lyx97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16194",
            "authors": [
                {
                    "_id": "67dcf6375fd14aedd3005237",
                    "user": {
                        "_id": "64f1ba1cc640c6d957204786",
                        "avatarUrl": "/avatars/28ede115da5334d2f22a4b5698df1c30.svg",
                        "isPro": false,
                        "fullname": "Ziyao Guo",
                        "user": "ZiyaoGuo",
                        "type": "user"
                    },
                    "name": "Ziyao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:11:50.727Z",
                    "hidden": false
                },
                {
                    "_id": "67dcf6375fd14aedd3005238",
                    "user": {
                        "_id": "65f1713552c38a91e0a445e8",
                        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                        "isPro": false,
                        "fullname": "kaipeng",
                        "user": "kpzhang996",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:12:11.686Z",
                    "hidden": false
                },
                {
                    "_id": "67dcf6375fd14aedd3005239",
                    "user": {
                        "_id": "64d48e9ef2a3941669810f23",
                        "avatarUrl": "/avatars/495e6785abc44d1facfcf385edd6a222.svg",
                        "isPro": false,
                        "fullname": "Michael Shieh",
                        "user": "Michael2008S",
                        "type": "user"
                    },
                    "name": "Michael Qizhe Shieh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:12:23.239Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T14:41:29.000Z",
            "submittedOnDailyAt": "2025-03-21T03:46:49.486Z",
            "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
            "upvotes": 2,
            "discussionId": "67dcf6375fd14aedd300527b",
            "ai_keywords": [
                "autoregressive models",
                "image generation",
                "sequential prediction",
                "language modeling",
                "VQ-VAE",
                "vector quantization",
                "codebooks",
                "token",
                "codeword representations",
                "coarse to fine (CTF)",
                "inference step",
                "Inception Score",
                "sampling speeds"
            ]
        },
        "publishedAt": "2025-03-20T10:41:29.000Z",
        "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
        "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16194.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15855",
            "authors": [
                {
                    "_id": "67dd072f1c182b6168eaa004",
                    "user": {
                        "_id": "649f65a4ca03a1a35e3dac14",
                        "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
                        "isPro": false,
                        "fullname": "Hyojun GO",
                        "user": "HJGO",
                        "type": "user"
                    },
                    "name": "Hyojun Go",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:10:51.488Z",
                    "hidden": false
                },
                {
                    "_id": "67dd072f1c182b6168eaa005",
                    "user": {
                        "_id": "653929a66da48e0d21e65e17",
                        "avatarUrl": "/avatars/e34ae3411d689b4280ff34c1b680f283.svg",
                        "isPro": false,
                        "fullname": "Byeongjun Park",
                        "user": "byeongjun-park",
                        "type": "user"
                    },
                    "name": "Byeongjun Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:10:58.350Z",
                    "hidden": false
                },
                {
                    "_id": "67dd072f1c182b6168eaa006",
                    "user": {
                        "_id": "668e2d6bf7c08c3f9e837b1e",
                        "avatarUrl": "/avatars/ad769e4b5af78e6da8a1090b379932c9.svg",
                        "isPro": false,
                        "fullname": "hyelin nam",
                        "user": "hyelinnam",
                        "type": "user"
                    },
                    "name": "Hyelin Nam",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:11:04.355Z",
                    "hidden": false
                },
                {
                    "_id": "67dd072f1c182b6168eaa007",
                    "name": "Byung-Hoon Kim",
                    "hidden": false
                },
                {
                    "_id": "67dd072f1c182b6168eaa008",
                    "user": {
                        "_id": "62b5e7543797f21de0bd00db",
                        "avatarUrl": "/avatars/b8131c8fd30c35b1776a94ea6aa51995.svg",
                        "isPro": false,
                        "fullname": "Hyungjin Chung",
                        "user": "95harry",
                        "type": "user"
                    },
                    "name": "Hyungjin Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:11:13.503Z",
                    "hidden": false
                },
                {
                    "_id": "67dd072f1c182b6168eaa009",
                    "user": {
                        "_id": "676e5f07aa6895d518748e52",
                        "avatarUrl": "/avatars/11e30698017dd98eedff3274c9cc5f19.svg",
                        "isPro": false,
                        "fullname": "Changick Kim",
                        "user": "CK8308",
                        "type": "user"
                    },
                    "name": "Changick Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:11:19.337Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T05:26:09.000Z",
            "submittedOnDailyAt": "2025-03-21T07:39:17.497Z",
            "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling",
            "submittedOnDailyBy": {
                "_id": "649f65a4ca03a1a35e3dac14",
                "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
                "isPro": false,
                "fullname": "Hyojun GO",
                "user": "HJGO",
                "type": "user"
            },
            "summary": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.",
            "upvotes": 2,
            "discussionId": "67dd07351c182b6168eaa1fc",
            "ai_keywords": [
                "VideoRFSplat",
                "text-to-3D model",
                "video generation model",
                "realistic 3D Gaussian Splatting (3DGS)",
                "camera poses",
                "unbounded real-world scenes",
                "multi-view images",
                "dual-stream architecture",
                "pose generation model",
                "communication blocks",
                "asynchronous sampling strategy",
                "denoising",
                "mutual ambiguity",
                "cross-modal consistency",
                "RealEstate10K",
                "MVImgNet",
                "DL3DV-10K",
                "ACID",
                "score distillation sampling"
            ]
        },
        "publishedAt": "2025-03-20T01:26:09.000Z",
        "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling",
        "summary": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15855.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649f65a4ca03a1a35e3dac14",
            "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
            "fullname": "Hyojun GO",
            "name": "HJGO",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13834",
            "authors": [
                {
                    "_id": "67dcf6456b575dc3179e05a2",
                    "user": {
                        "_id": "6756f15abba07064c254d8d3",
                        "avatarUrl": "/avatars/0b7067c2475d5cb1950579cbb98b54f3.svg",
                        "isPro": false,
                        "fullname": "June Hyoung Kwon",
                        "user": "herbwood27",
                        "type": "user"
                    },
                    "name": "JuneHyoung Kwon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:08:35.264Z",
                    "hidden": false
                },
                {
                    "_id": "67dcf6456b575dc3179e05a3",
                    "name": "MiHyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "67dcf6456b575dc3179e05a4",
                    "name": "Eunju Lee",
                    "hidden": false
                },
                {
                    "_id": "67dcf6456b575dc3179e05a5",
                    "user": {
                        "_id": "65646b22ac9d3c2bd7b14788",
                        "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
                        "isPro": false,
                        "fullname": "Juhwan Choi",
                        "user": "c-juhwan",
                        "type": "user"
                    },
                    "name": "Juhwan Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:08:21.660Z",
                    "hidden": false
                },
                {
                    "_id": "67dcf6456b575dc3179e05a6",
                    "name": "YoungBin Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T02:17:41.000Z",
            "submittedOnDailyAt": "2025-03-21T03:48:06.030Z",
            "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
            "submittedOnDailyBy": {
                "_id": "65646b22ac9d3c2bd7b14788",
                "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
                "isPro": false,
                "fullname": "Juhwan Choi",
                "user": "c-juhwan",
                "type": "user"
            },
            "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
            "upvotes": 2,
            "discussionId": "67dcf64a6b575dc3179e0754",
            "ai_keywords": [
                "dominant modality bias",
                "unaligned gradients",
                "gradient magnitudes",
                "inter-modality gradient reweighting",
                "KL divergence",
                "inter-task gradient projection"
            ]
        },
        "publishedAt": "2025-03-17T22:17:41.000Z",
        "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
        "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13834.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65646b22ac9d3c2bd7b14788",
            "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
            "fullname": "Juhwan Choi",
            "name": "c-juhwan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.07906",
            "authors": [
                {
                    "_id": "67dde94f169ce083a5098930",
                    "name": "Qinghao Ye",
                    "hidden": false
                },
                {
                    "_id": "67dde94f169ce083a5098931",
                    "name": "Xianhan Zeng",
                    "hidden": false
                },
                {
                    "_id": "67dde94f169ce083a5098932",
                    "name": "Fu Li",
                    "hidden": false
                },
                {
                    "_id": "67dde94f169ce083a5098933",
                    "name": "Chunyuan Li",
                    "hidden": false
                },
                {
                    "_id": "67dde94f169ce083a5098934",
                    "name": "Haoqi Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-10T22:53:56.000Z",
            "submittedOnDailyAt": "2025-03-21T21:04:12.065Z",
            "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark\n  and Alignment Learning",
            "submittedOnDailyBy": {
                "_id": "64530fc01a57e1179c1fe4c0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/0lncTpIHXn6suB0p-oSma.jpeg",
                "isPro": false,
                "fullname": "QinghaoYe",
                "user": "MAGAer13",
                "type": "user"
            },
            "summary": "Image captioning has long been a pivotal task in visual understanding, with\nrecent advancements in vision-language models (VLMs) significantly enhancing\nthe ability to generate detailed image captions. However, the evaluation of\ndetailed image captioning remains underexplored due to outdated evaluation\nmetrics and coarse annotations. In this paper, we introduce DeCapBench along\nwith a novel metric, DCScore, specifically designed for detailed captioning\ntasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by\ndeconstructing responses into the smallest self-sufficient units, termed\nprimitive information units, and assessing them individually. Our evaluation\nshows that DCScore aligns more closely with human judgment than other\nrule-based or model-based metrics. Concurrently, DeCapBench exhibits a high\ncorrelation with VLM arena results on descriptive tasks, surpassing existing\nbenchmarks for vision-language models. Additionally, we present an automatic\nfine-grained feedback collection method, FeedQuill, for preference optimization\nbased on our advanced metric, showing robust generalization capabilities across\nauto-generated preference data. Extensive experiments on multiple VLMs\ndemonstrate that our method not only significantly reduces hallucinations but\nalso enhances performance across various benchmarks, achieving superior detail\ncaptioning performance while surpassing GPT-4o.",
            "upvotes": 2,
            "discussionId": "67dde951169ce083a509898a",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "detailed image captioning",
                "evaluation metrics",
                "DeCapBench",
                "DCScore",
                "hallucinations",
                "fine-grained comprehensiveness",
                "primitive information units",
                "human judgment",
                "FeedQuill",
                "preference optimization",
                "auto-generated preference data",
                "detail captioning performance",
                "GPT-4"
            ]
        },
        "publishedAt": "2025-03-10T18:53:56.000Z",
        "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark\n  and Alignment Learning",
        "summary": "Image captioning has long been a pivotal task in visual understanding, with\nrecent advancements in vision-language models (VLMs) significantly enhancing\nthe ability to generate detailed image captions. However, the evaluation of\ndetailed image captioning remains underexplored due to outdated evaluation\nmetrics and coarse annotations. In this paper, we introduce DeCapBench along\nwith a novel metric, DCScore, specifically designed for detailed captioning\ntasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by\ndeconstructing responses into the smallest self-sufficient units, termed\nprimitive information units, and assessing them individually. Our evaluation\nshows that DCScore aligns more closely with human judgment than other\nrule-based or model-based metrics. Concurrently, DeCapBench exhibits a high\ncorrelation with VLM arena results on descriptive tasks, surpassing existing\nbenchmarks for vision-language models. Additionally, we present an automatic\nfine-grained feedback collection method, FeedQuill, for preference optimization\nbased on our advanced metric, showing robust generalization capabilities across\nauto-generated preference data. Extensive experiments on multiple VLMs\ndemonstrate that our method not only significantly reduces hallucinations but\nalso enhances performance across various benchmarks, achieving superior detail\ncaptioning performance while surpassing GPT-4o.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07906.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64530fc01a57e1179c1fe4c0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/0lncTpIHXn6suB0p-oSma.jpeg",
            "fullname": "QinghaoYe",
            "name": "MAGAer13",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16091",
            "authors": [
                {
                    "_id": "67dd4b64a412018fab289466",
                    "user": {
                        "_id": "657e623983543a061b585a9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657e623983543a061b585a9c/Zg9WWHQYPOtuYt9kDsq46.jpeg",
                        "isPro": false,
                        "fullname": "Abdullah Mamun",
                        "user": "ab9mamun",
                        "type": "user"
                    },
                    "name": "Abdullah Mamun",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-21T11:20:04.980Z",
                    "hidden": false
                },
                {
                    "_id": "67dd4b64a412018fab289467",
                    "name": "Diane J. Cook",
                    "hidden": false
                },
                {
                    "_id": "67dd4b64a412018fab289468",
                    "user": {
                        "_id": "64dcf2572057a9cd9ac1bc2e",
                        "avatarUrl": "/avatars/ecbe33158bf7d854ac64666c5dee64a6.svg",
                        "isPro": false,
                        "fullname": "Hassan Ghasemzadeh",
                        "user": "HassanGM",
                        "type": "user"
                    },
                    "name": "Hassan Ghasemzadeh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-21T13:07:49.859Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657e623983543a061b585a9c/8Kgmx3SBM2qa3jJllXnQc.png",
                "https://cdn-uploads.huggingface.co/production/uploads/657e623983543a061b585a9c/n5id2jrngGSq-lncjy6zp.png",
                "https://cdn-uploads.huggingface.co/production/uploads/657e623983543a061b585a9c/4t-Zx9kXpWr1v8BYHChrZ.png"
            ],
            "publishedAt": "2025-03-20T12:32:35.000Z",
            "submittedOnDailyAt": "2025-03-21T09:55:50.000Z",
            "title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event\n  Forecasting for Treatment Adherence",
            "submittedOnDailyBy": {
                "_id": "657e623983543a061b585a9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657e623983543a061b585a9c/Zg9WWHQYPOtuYt9kDsq46.jpeg",
                "isPro": false,
                "fullname": "Abdullah Mamun",
                "user": "ab9mamun",
                "type": "user"
            },
            "summary": "Adherence to prescribed treatments is crucial for individuals with chronic\nconditions to avoid costly or adverse health outcomes. For certain patient\ngroups, intensive lifestyle interventions are vital for enhancing medication\nadherence. Accurate forecasting of treatment adherence can open pathways to\ndeveloping an on-demand intervention tool, enabling timely and personalized\nsupport. With the increasing popularity of smartphones and wearables, it is now\neasier than ever to develop and deploy smart activity monitoring systems.\nHowever, effective forecasting systems for treatment adherence based on\nwearable sensors are still not widely available. We close this gap by proposing\nAdherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI\nis a knowledge-guided adherence forecasting system that leverages smartphone\nsensors and previous medication history to estimate the likelihood of\nforgetting to take a prescribed medication. A user study was conducted with 27\nparticipants who took daily medications to manage their cardiovascular\ndiseases. We designed and developed CNN and LSTM-based forecasting models with\nvarious combinations of input features and found that LSTM models can forecast\nmedication adherence with an accuracy of 0.932 and an F-1 score of 0.936.\nMoreover, through a series of ablation studies involving convolutional and\nrecurrent neural network architectures, we demonstrate that leveraging known\nknowledge about future and personalized training enhances the accuracy of\nmedication adherence forecasting. Code available:\nhttps://github.com/ab9mamun/AIMI.",
            "upvotes": 1,
            "discussionId": "67dd4b64a412018fab2894c0",
            "githubRepo": "https://github.com/ab9mamun/AIMI",
            "ai_keywords": [
                "CNN",
                "LSTM",
                "adherence forecasting",
                "knowledge-guided",
                "convolutional neural network",
                "recurrent neural network",
                "medication adherence forecasting"
            ]
        },
        "publishedAt": "2025-03-20T08:32:35.000Z",
        "title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event\n  Forecasting for Treatment Adherence",
        "summary": "Adherence to prescribed treatments is crucial for individuals with chronic\nconditions to avoid costly or adverse health outcomes. For certain patient\ngroups, intensive lifestyle interventions are vital for enhancing medication\nadherence. Accurate forecasting of treatment adherence can open pathways to\ndeveloping an on-demand intervention tool, enabling timely and personalized\nsupport. With the increasing popularity of smartphones and wearables, it is now\neasier than ever to develop and deploy smart activity monitoring systems.\nHowever, effective forecasting systems for treatment adherence based on\nwearable sensors are still not widely available. We close this gap by proposing\nAdherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI\nis a knowledge-guided adherence forecasting system that leverages smartphone\nsensors and previous medication history to estimate the likelihood of\nforgetting to take a prescribed medication. A user study was conducted with 27\nparticipants who took daily medications to manage their cardiovascular\ndiseases. We designed and developed CNN and LSTM-based forecasting models with\nvarious combinations of input features and found that LSTM models can forecast\nmedication adherence with an accuracy of 0.932 and an F-1 score of 0.936.\nMoreover, through a series of ablation studies involving convolutional and\nrecurrent neural network architectures, we demonstrate that leveraging known\nknowledge about future and personalized training enhances the accuracy of\nmedication adherence forecasting. Code available:\nhttps://github.com/ab9mamun/AIMI.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/657e623983543a061b585a9c/8Kgmx3SBM2qa3jJllXnQc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/657e623983543a061b585a9c/n5id2jrngGSq-lncjy6zp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/657e623983543a061b585a9c/4t-Zx9kXpWr1v8BYHChrZ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16091.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657e623983543a061b585a9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657e623983543a061b585a9c/Zg9WWHQYPOtuYt9kDsq46.jpeg",
            "fullname": "Abdullah Mamun",
            "name": "ab9mamun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14201",
            "authors": [
                {
                    "_id": "67ddc4f45c0bd58dff86341d",
                    "user": {
                        "_id": "663486a1f64712540644cb68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
                        "isPro": true,
                        "fullname": "Alessandro",
                        "user": "Devy1",
                        "type": "user"
                    },
                    "name": "Alessandro Giagnorio",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-21T19:59:32.283Z",
                    "hidden": false
                },
                {
                    "_id": "67ddc4f45c0bd58dff86341e",
                    "name": "Alberto Martin-Lopez",
                    "hidden": false
                },
                {
                    "_id": "67ddc4f45c0bd58dff86341f",
                    "user": {
                        "_id": "6638bea59e57161faac814e7",
                        "avatarUrl": "/avatars/91375b88945af50e51b7229a789a31b8.svg",
                        "isPro": false,
                        "fullname": "Gabriele Bavota",
                        "user": "gbavota",
                        "type": "user"
                    },
                    "name": "Gabriele Bavota",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-21T19:58:45.144Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T12:26:06.000Z",
            "submittedOnDailyAt": "2025-03-21T18:35:03.541Z",
            "title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
            "submittedOnDailyBy": {
                "_id": "663486a1f64712540644cb68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
                "isPro": true,
                "fullname": "Alessandro",
                "user": "Devy1",
                "type": "user"
            },
            "summary": "Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being sim10times larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed).",
            "upvotes": 1,
            "discussionId": "67ddc4f55c0bd58dff86349b",
            "githubRepo": "https://github.com/Devy99/comp-personalization",
            "ai_keywords": [
                "fine-tuning",
                "T5",
                "Code Llama",
                "parameter-efficient fine-tuning",
                "pre-trained",
                "organization-specific",
                "developer-specific"
            ]
        },
        "publishedAt": "2025-03-18T08:26:06.000Z",
        "title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
        "summary": "Deep learning (DL)-based code completion tools have transformed software\ndevelopment by enabling advanced code generation. These tools leverage models\ntrained on vast amounts of code from numerous repositories, capturing general\ncoding patterns. However, the impact of fine-tuning these models for specific\norganizations or developers to boost their performance on such subjects remains\nunexplored. In this work, we fill this gap by presenting solid empirical\nevidence answering this question. More specifically, we consider 136 developers\nfrom two organizations (Apache and Spring), two model architectures (T5 and\nCode Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5\nmodels (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source\nprojects, excluding the subject organizations' data, and compared against\nversions fine-tuned on organization- and developer-specific datasets. For the\nCode Llama model (7B), we compared the performance of the already pre-trained\nmodel publicly available online with the same model fine-tuned via\nparameter-efficient fine-tuning on organization- and developer-specific\ndatasets. Our results show that there is a boost in prediction capabilities\nprovided by both an organization-specific and a developer-specific additional\nfine-tuning, with the former being particularly performant. Such a finding\ngeneralizes across (i) the two subject organizations (i.e., Apache and Spring)\nand (ii) models of completely different magnitude (from 60M to 7B trainable\nparameters). Finally, we show that DL models fine-tuned on an\norganization-specific dataset achieve the same completion performance of\npre-trained code models used out of the box and being sim10times larger,\nwith consequent savings in terms of deployment and inference cost (e.g.,\nsmaller GPUs needed).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14201.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "663486a1f64712540644cb68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
            "fullname": "Alessandro",
            "name": "Devy1",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11509",
            "authors": [
                {
                    "_id": "67d7e187ee9f6c22e2b1fa0a",
                    "name": "Jonas Belouadi",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa0b",
                    "name": "Eddy Ilg",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa0c",
                    "name": "Margret Keuper",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa0d",
                    "name": "Hideki Tanaka",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa0e",
                    "name": "Masao Utiyama",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa0f",
                    "name": "Raj Dabre",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa10",
                    "name": "Steffen Eger",
                    "hidden": false
                },
                {
                    "_id": "67d7e187ee9f6c22e2b1fa11",
                    "name": "Simone Paolo Ponzetto",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62e402ec6a82e063860729f4/8b7v3fbda7daquqknHOC0.png"
            ],
            "publishedAt": "2025-03-14T15:29:58.000Z",
            "submittedOnDailyAt": "2025-03-21T15:27:29.362Z",
            "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
            "submittedOnDailyBy": {
                "_id": "62e402ec6a82e063860729f4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659110120002-noauth.png",
                "isPro": false,
                "fullname": "Jonas Belouadi",
                "user": "potamides",
                "type": "user"
            },
            "summary": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.",
            "upvotes": 1,
            "discussionId": "67d7e188ee9f6c22e2b1fa72",
            "githubRepo": "https://github.com/potamides/DeTikZify",
            "ai_keywords": [
                "decouples graphics program generation",
                "image representations",
                "text-guided graphics program synthesis",
                "zero-shot"
            ]
        },
        "publishedAt": "2025-03-14T11:29:58.000Z",
        "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
        "summary": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62e402ec6a82e063860729f4/8b7v3fbda7daquqknHOC0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11509.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e402ec6a82e063860729f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659110120002-noauth.png",
            "fullname": "Jonas Belouadi",
            "name": "potamides",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.15672",
            "authors": [
                {
                    "_id": "67ddab768f06eb5bf437487a",
                    "name": "William Ljungbergh",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf437487b",
                    "name": "Adam Lilja",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf437487c",
                    "name": "Adam Tonderski. Arvid Laveno Ling",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf437487d",
                    "name": "Carl Lindstrm",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf437487e",
                    "name": "Willem Verbeke",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf437487f",
                    "name": "Junsheng Fu",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf4374880",
                    "name": "Christoffer Petersson",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf4374881",
                    "name": "Lars Hammarstrand",
                    "hidden": false
                },
                {
                    "_id": "67ddab768f06eb5bf4374882",
                    "name": "Michael Felsberg",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T20:00:27.000Z",
            "submittedOnDailyAt": "2025-03-21T16:40:30.561Z",
            "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "651eab8480f9f9c5acf5213d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651eab8480f9f9c5acf5213d/1rtxhnXRgEOecdCSMKics.png",
                "isPro": false,
                "fullname": "William Ljungbergh",
                "user": "wljungbergh",
                "type": "user"
            },
            "summary": "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https://research.zenseact.com/publications/gasp/.",
            "upvotes": 0,
            "discussionId": "67ddab788f06eb5bf43748ee",
            "ai_keywords": [
                "self-supervised pre-training",
                "next-token prediction",
                "large language models",
                "spatiotemporal data",
                "geometric structure",
                "semantic structure",
                "autonomous driving",
                "3D scene",
                "ego vehicle",
                "vision foundation model",
                "4D occupancy fields",
                "structured representation",
                "generalizable representation",
                "semantic occupancy forecasting",
                "online mapping",
                "ego trajectory prediction",
                "continuous 4D geometric and semantic occupancy prediction"
            ]
        },
        "publishedAt": "2025-03-19T16:00:27.000Z",
        "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
        "summary": "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https://research.zenseact.com/publications/gasp/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15672.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651eab8480f9f9c5acf5213d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651eab8480f9f9c5acf5213d/1rtxhnXRgEOecdCSMKics.png",
            "fullname": "William Ljungbergh",
            "name": "wljungbergh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.13891",
            "authors": [
                {
                    "_id": "67ddcdd46dc3daedb5e5fe4a",
                    "name": "Xiaoying Xing",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe4b",
                    "name": "Chia-Wen Kuo",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe4c",
                    "name": "Li Fuxin",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe4d",
                    "name": "Yulei Niu",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe4e",
                    "name": "Fan Chen",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe4f",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe50",
                    "name": "Ying Wu",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe51",
                    "name": "Longyin Wen",
                    "hidden": false
                },
                {
                    "_id": "67ddcdd46dc3daedb5e5fe52",
                    "user": {
                        "_id": "65cbdea6d6c974694f09249a",
                        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
                        "isPro": false,
                        "fullname": "Jay",
                        "user": "Zilence006",
                        "type": "user"
                    },
                    "name": "Sijie Zhu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-21T20:41:23.891Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T04:34:43.000Z",
            "submittedOnDailyAt": "2025-03-21T19:09:28.389Z",
            "title": "Where do Large Vision-Language Models Look at when Answering Questions?",
            "submittedOnDailyBy": {
                "_id": "65cbdea6d6c974694f09249a",
                "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
                "isPro": false,
                "fullname": "Jay",
                "user": "Zilence006",
                "type": "user"
            },
            "summary": "Large Vision-Language Models (LVLMs) have shown promising performance in\nvision-language understanding and reasoning tasks. However, their visual\nunderstanding behaviors remain underexplored. A fundamental question arises: to\nwhat extent do LVLMs rely on visual input, and which image regions contribute\nto their responses? It is non-trivial to interpret the free-form generation of\nLVLMs due to their complicated visual architecture (e.g., multiple encoders and\nmulti-resolution) and variable-length outputs. In this paper, we extend\nexisting heatmap visualization methods (e.g., iGOS++) to support LVLMs for\nopen-ended visual question answering. We propose a method to select visually\nrelevant tokens that reflect the relevance between generated answers and input\nimage. Furthermore, we conduct a comprehensive analysis of state-of-the-art\nLVLMs on benchmarks designed to require visual information to answer. Our\nfindings offer several insights into LVLM behavior, including the relationship\nbetween focus region and answer correctness, differences in visual attention\nacross architectures, and the impact of LLM scale on visual understanding. The\ncode and data are available at\nhttps://github.com/bytedance/LVLM_Interpretation.",
            "upvotes": 0,
            "discussionId": "67ddcdd56dc3daedb5e5feb6",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "vision-language understanding",
                "reasonning tasks",
                "visual input",
                "image regions",
                "free-form generation",
                "heatmap visualization methods",
                "iGOS++",
                "visually relevant tokens",
                "open-ended visual question answering",
                "focus region",
                "answer correctness",
                "visual attention",
                "LLM scale",
                "visual understanding"
            ]
        },
        "publishedAt": "2025-03-18T00:34:43.000Z",
        "title": "Where do Large Vision-Language Models Look at when Answering Questions?",
        "summary": "Large Vision-Language Models (LVLMs) have shown promising performance in\nvision-language understanding and reasoning tasks. However, their visual\nunderstanding behaviors remain underexplored. A fundamental question arises: to\nwhat extent do LVLMs rely on visual input, and which image regions contribute\nto their responses? It is non-trivial to interpret the free-form generation of\nLVLMs due to their complicated visual architecture (e.g., multiple encoders and\nmulti-resolution) and variable-length outputs. In this paper, we extend\nexisting heatmap visualization methods (e.g., iGOS++) to support LVLMs for\nopen-ended visual question answering. We propose a method to select visually\nrelevant tokens that reflect the relevance between generated answers and input\nimage. Furthermore, we conduct a comprehensive analysis of state-of-the-art\nLVLMs on benchmarks designed to require visual information to answer. Our\nfindings offer several insights into LVLM behavior, including the relationship\nbetween focus region and answer correctness, differences in visual attention\nacross architectures, and the impact of LLM scale on visual understanding. The\ncode and data are available at\nhttps://github.com/bytedance/LVLM_Interpretation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65cbdea6d6c974694f09249a",
            "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
            "fullname": "Jay",
            "name": "Zilence006",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]