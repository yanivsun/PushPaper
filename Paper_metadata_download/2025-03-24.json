[
    {
        "paper": {
            "id": "2503.16660",
            "authors": [
                {
                    "_id": "67e0ffb029682c8065e1c223",
                    "name": "Eduard Allakhverdov",
                    "hidden": false
                },
                {
                    "_id": "67e0ffb029682c8065e1c224",
                    "user": {
                        "_id": "6310ff34bc152fa3e810c186",
                        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
                        "isPro": false,
                        "fullname": "Elizaveta Goncharova",
                        "user": "Elizaveta",
                        "type": "user"
                    },
                    "name": "Elizaveta Goncharova",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:38:44.495Z",
                    "hidden": false
                },
                {
                    "_id": "67e0ffb029682c8065e1c225",
                    "user": {
                        "_id": "643984dceb7c5616ef3f5d54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                        "isPro": false,
                        "fullname": "Andrey Kuznetsov",
                        "user": "kuznetsoffandrey",
                        "type": "user"
                    },
                    "name": "Andrey Kuznetsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T14:49:31.067Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T19:17:08.000Z",
            "submittedOnDailyAt": "2025-03-24T05:20:19.676Z",
            "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
            "submittedOnDailyBy": {
                "_id": "6310ff34bc152fa3e810c186",
                "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
                "isPro": false,
                "fullname": "Elizaveta Goncharova",
                "user": "Elizaveta",
                "type": "user"
            },
            "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
            "upvotes": 55,
            "discussionId": "67e0ffb229682c8065e1c2c6",
            "ai_keywords": [
                "autoencoder",
                "Gumbel-Softmax selection mechanism",
                "feature utility",
                "LLaVA-NeXT model",
                "OCR-based tasks",
                "visual context",
                "performance loss",
                "general-domain tasks",
                "multimodal pruning"
            ]
        },
        "publishedAt": "2025-03-20T15:17:08.000Z",
        "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
        "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16660.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6310ff34bc152fa3e810c186",
            "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
            "fullname": "Elizaveta Goncharova",
            "name": "Elizaveta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16905",
            "authors": [
                {
                    "_id": "67e0c13fe5fa0da84e134581",
                    "user": {
                        "_id": "658be7fe135580745c510323",
                        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
                        "isPro": false,
                        "fullname": "Jian Zhang",
                        "user": "VentureZJ",
                        "type": "user"
                    },
                    "name": "Jian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:07:08.476Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134582",
                    "name": "Zhiyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134583",
                    "name": "Zhangqi Wang",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134584",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134585",
                    "user": {
                        "_id": "64e6cf78ecce34cb442dc889",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                        "isPro": false,
                        "fullname": "Fangzhi Xu",
                        "user": "xufangzhi",
                        "type": "user"
                    },
                    "name": "Fangzhi Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:33:42.732Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134586",
                    "user": {
                        "_id": "66ac77011cfb12c087605acb",
                        "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
                        "isPro": false,
                        "fullname": "Lin",
                        "user": "Qika",
                        "type": "user"
                    },
                    "name": "Qika Lin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-24T02:19:51.913Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134587",
                    "name": "Rui Mao",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134588",
                    "name": "Erik Cambria",
                    "hidden": false
                },
                {
                    "_id": "67e0c13fe5fa0da84e134589",
                    "name": "Jun Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T07:13:45.000Z",
            "submittedOnDailyAt": "2025-03-24T00:51:41.644Z",
            "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
            "submittedOnDailyBy": {
                "_id": "658be7fe135580745c510323",
                "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
                "isPro": false,
                "fullname": "Jian Zhang",
                "user": "VentureZJ",
                "type": "user"
            },
            "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
            "upvotes": 45,
            "discussionId": "67e0c147e5fa0da84e1347f5",
            "githubRepo": "https://github.com/exoskeletonzj/MAPS"
        },
        "publishedAt": "2025-03-21T03:13:45.000Z",
        "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
        "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "fullname": "Jian Zhang",
            "name": "VentureZJ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16874",
            "authors": [
                {
                    "_id": "67e0c1ce151ca9ed9284dc52",
                    "user": {
                        "_id": "658be7fe135580745c510323",
                        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
                        "isPro": false,
                        "fullname": "Jian Zhang",
                        "user": "VentureZJ",
                        "type": "user"
                    },
                    "name": "Jian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:07:05.271Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c1ce151ca9ed9284dc53",
                    "name": "Zhangqi Wang",
                    "hidden": false
                },
                {
                    "_id": "67e0c1ce151ca9ed9284dc54",
                    "name": "Haiping Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e0c1ce151ca9ed9284dc55",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "67e0c1ce151ca9ed9284dc56",
                    "user": {
                        "_id": "66ac77011cfb12c087605acb",
                        "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
                        "isPro": false,
                        "fullname": "Lin",
                        "user": "Qika",
                        "type": "user"
                    },
                    "name": "Qika Lin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-24T02:22:15.812Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c1ce151ca9ed9284dc57",
                    "name": "Erik Cambria",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T06:19:55.000Z",
            "submittedOnDailyAt": "2025-03-24T00:56:26.218Z",
            "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization",
            "submittedOnDailyBy": {
                "_id": "658be7fe135580745c510323",
                "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
                "isPro": false,
                "fullname": "Jian Zhang",
                "user": "VentureZJ",
                "type": "user"
            },
            "summary": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
            "upvotes": 38,
            "discussionId": "67e0c1d7151ca9ed9284ded7",
            "githubRepo": "https://github.com/exoskeletonzj/MARS",
            "ai_keywords": [
                "Multi-Agent framework",
                "Socratic guidance",
                "multi-agent fusion technology",
                "Planner",
                "Teacher-Critic-Student Socratic dialogue pattern"
            ]
        },
        "publishedAt": "2025-03-21T02:19:55.000Z",
        "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization",
        "summary": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16874.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "fullname": "Jian Zhang",
            "name": "VentureZJ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16408",
            "authors": [
                {
                    "_id": "67dcdedbeff29d0d52c739e4",
                    "user": {
                        "_id": "658a6c1399ed106ac8c822b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
                        "isPro": false,
                        "fullname": "yiranqin",
                        "user": "IranQin",
                        "type": "user"
                    },
                    "name": "Yiran Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:36.686Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739e5",
                    "user": {
                        "_id": "64eadcb03d76028d805a7818",
                        "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
                        "isPro": false,
                        "fullname": "Li Kang",
                        "user": "FACEONG",
                        "type": "user"
                    },
                    "name": "Li Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:38.834Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739e6",
                    "user": {
                        "_id": "64cdf8230fbfb00b91225087",
                        "avatarUrl": "/avatars/5379727f55782e146302ed20c7661932.svg",
                        "isPro": false,
                        "fullname": "Xiufeng Song",
                        "user": "sparklexfantasy",
                        "type": "user"
                    },
                    "name": "Xiufeng Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:35:50.462Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739e7",
                    "user": {
                        "_id": "64e314ad24809d7fa0f20fbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg",
                        "isPro": false,
                        "fullname": "Zhenfei Yin",
                        "user": "JeremyYin",
                        "type": "user"
                    },
                    "name": "Zhenfei Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:35:56.882Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739e8",
                    "name": "Xiaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739e9",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:36:19.727Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739ea",
                    "user": {
                        "_id": "65324848220f490dbaa3b8c9",
                        "avatarUrl": "/avatars/cec7dcadf4657bb4f61e2c9a8075521b.svg",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "Ruimao",
                        "type": "user"
                    },
                    "name": "Ruimao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:37:18.101Z",
                    "hidden": false
                },
                {
                    "_id": "67dcdedbeff29d0d52c739eb",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:58:38.000Z",
            "submittedOnDailyAt": "2025-03-24T01:35:09.139Z",
            "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints",
            "submittedOnDailyBy": {
                "_id": "658a6c1399ed106ac8c822b1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
                "isPro": false,
                "fullname": "yiranqin",
                "user": "IranQin",
                "type": "user"
            },
            "summary": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
            "upvotes": 32,
            "discussionId": "67dcdedeeff29d0d52c73abc",
            "projectPage": "https://iranqin.github.io/robofactory/",
            "ai_keywords": [
                "compositional constraints",
                "embodied multi-agent systems",
                "data collection framework",
                "benchmark",
                "RoboFactory",
                "imitation learning",
                "multi-agent imitation learning",
                "training strategies"
            ]
        },
        "publishedAt": "2025-03-20T13:58:38.000Z",
        "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints",
        "summary": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16408.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658a6c1399ed106ac8c822b1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
            "fullname": "yiranqin",
            "name": "IranQin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16430",
            "authors": [
                {
                    "_id": "67e0bd81b04d9e836829c468",
                    "user": {
                        "_id": "63ea23b9dedfeebe54d02bdf",
                        "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
                        "isPro": false,
                        "fullname": "Yuqing Wang",
                        "user": "Epiphqny",
                        "type": "user"
                    },
                    "name": "Yuqing Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:10:42.267Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bd81b04d9e836829c469",
                    "user": {
                        "_id": "64415957bd0c9726529802f6",
                        "avatarUrl": "/avatars/1132d1ee68fb58ec635d57c8175caacd.svg",
                        "isPro": false,
                        "fullname": "Zhijie Lin",
                        "user": "Ikuinen",
                        "type": "user"
                    },
                    "name": "Zhijie Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:40:54.208Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bd81b04d9e836829c46a",
                    "user": {
                        "_id": "6427e08288215cee63b1c44d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
                        "isPro": false,
                        "fullname": "yao teng",
                        "user": "tytyt",
                        "type": "user"
                    },
                    "name": "Yao Teng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:41:01.459Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bd81b04d9e836829c46b",
                    "user": {
                        "_id": "627d2723401f42c57b6b7c0c",
                        "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg",
                        "isPro": false,
                        "fullname": "Yuanzhi Zhu",
                        "user": "Yuanzhi",
                        "type": "user"
                    },
                    "name": "Yuanzhi Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:41:07.856Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bd81b04d9e836829c46c",
                    "user": {
                        "_id": "60d2e681b8448e1785bbda06",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Shuhuai Ren",
                        "user": "ShuhuaiRen",
                        "type": "user"
                    },
                    "name": "Shuhuai Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:10:44.045Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bd81b04d9e836829c46d",
                    "user": {
                        "_id": "67298e44017b96a1d0101dc4",
                        "avatarUrl": "/avatars/1f8ed1a3e911e6a3021087b9371d284c.svg",
                        "isPro": false,
                        "fullname": "Jiashi Feng",
                        "user": "jshfeng",
                        "type": "user"
                    },
                    "name": "Jiashi Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:41:16.160Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bd81b04d9e836829c46e",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:41:22.229Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:59.000Z",
            "submittedOnDailyAt": "2025-03-24T00:50:05.627Z",
            "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63ea23b9dedfeebe54d02bdf",
                "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
                "isPro": false,
                "fullname": "Yuqing Wang",
                "user": "Epiphqny",
                "type": "user"
            },
            "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
            "upvotes": 28,
            "discussionId": "67e0bd85b04d9e836829c55f",
            "projectPage": "https://yuqingwang1029.github.io/TokenBridge/",
            "githubRepo": "https://github.com/YuqingWang1029/TokenBridge",
            "ai_keywords": [
                "autoregressive visual generation models",
                "tokenizers",
                "tokens",
                "discrete tokens",
                "continuous tokens",
                "cross-entropy loss",
                "tokenizer training",
                "TokenBridge",
                "post-training quantization",
                "dimension-wise quantization",
                "lightweight autoregressive prediction mechanism",
                "reconstruction quality",
                "generation quality"
            ]
        },
        "publishedAt": "2025-03-20T13:59:59.000Z",
        "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
        "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16430.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "63ea23b9dedfeebe54d02bdf",
            "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
            "fullname": "Yuqing Wang",
            "name": "Epiphqny",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17352",
            "authors": [
                {
                    "_id": "67e0bcc9e5fa0da84e121032",
                    "user": {
                        "_id": "642f4c789b2484d7d8551a93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
                        "isPro": true,
                        "fullname": "Yihe Deng",
                        "user": "ydeng9",
                        "type": "user"
                    },
                    "name": "Yihe Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:41:41.202Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bcc9e5fa0da84e121033",
                    "user": {
                        "_id": "61c5c25705aa54027c52f7b3",
                        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
                        "isPro": false,
                        "fullname": "Hritik Bansal",
                        "user": "hbXNov",
                        "type": "user"
                    },
                    "name": "Hritik Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:41:47.419Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bcc9e5fa0da84e121034",
                    "user": {
                        "_id": "639bd19e445b133a4e94c3ee",
                        "avatarUrl": "/avatars/63ab9919b5e67261cce9007192a70deb.svg",
                        "isPro": false,
                        "fullname": "Fan Yin",
                        "user": "fanyin3639",
                        "type": "user"
                    },
                    "name": "Fan Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:42:02.800Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bcc9e5fa0da84e121035",
                    "name": "Nanyun Peng",
                    "hidden": false
                },
                {
                    "_id": "67e0bcc9e5fa0da84e121036",
                    "user": {
                        "_id": "62fa0ffe0697d224219a0cb7",
                        "avatarUrl": "/avatars/f0ef59e1c0cf4ab4fe5cee08d488bd03.svg",
                        "isPro": false,
                        "fullname": "Wei Wang",
                        "user": "WeiWang",
                        "type": "user"
                    },
                    "name": "Wei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:42:09.903Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bcc9e5fa0da84e121037",
                    "user": {
                        "_id": "60b7b9d71b90c5d07c23fbd0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1622653364258-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai-Wei Chang",
                        "user": "kaiweichang",
                        "type": "user"
                    },
                    "name": "Kai-Wei Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:42:16.596Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T17:52:43.000Z",
            "submittedOnDailyAt": "2025-03-24T00:31:06.884Z",
            "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
            "submittedOnDailyBy": {
                "_id": "642f4c789b2484d7d8551a93",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
                "isPro": true,
                "fullname": "Yihe Deng",
                "user": "ydeng9",
                "type": "user"
            },
            "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
            "upvotes": 20,
            "discussionId": "67e0bccae5fa0da84e121079",
            "projectPage": "https://yihe-deng.notion.site/openvlthinker",
            "githubRepo": "https://github.com/yihedeng9/OpenVLThinker",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "verifiable rewards",
                "large language models (LLMs)",
                "self-verification",
                "self-correction",
                "large vision-language models (LVLMs)",
                "multimodal reasoning tasks",
                "supervised fine-tuning (SFT)",
                "lightweight training data",
                "reasoning steps",
                "high-quality captions",
                "diversity",
                "visual datasets",
                "iterative process",
                "OpenVLThinker",
                "reasoning performance",
                "challenging benchmarks",
                "MathVista",
                "MathVerse",
                "MathVision",
                "robust vision-language reasoning"
            ]
        },
        "publishedAt": "2025-03-21T13:52:43.000Z",
        "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
        "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17352.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642f4c789b2484d7d8551a93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
            "fullname": "Yihe Deng",
            "name": "ydeng9",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17126",
            "authors": [
                {
                    "_id": "67e0beb474fc794321fb4ad7",
                    "user": {
                        "_id": "630c92f6c169245d78fcfa19",
                        "avatarUrl": "/avatars/a4dd46b1b61f43530a1c934de29420af.svg",
                        "isPro": false,
                        "fullname": "John Joon Young Chung",
                        "user": "johnr0",
                        "type": "user"
                    },
                    "name": "John Joon Young Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:42:42.640Z",
                    "hidden": false
                },
                {
                    "_id": "67e0beb474fc794321fb4ad8",
                    "user": {
                        "_id": "62a96c134a8579cc0be1eb16",
                        "avatarUrl": "/avatars/d1862985db7769f4d5e4ab82ecdc6796.svg",
                        "isPro": false,
                        "fullname": "Vishakh Padmakumar",
                        "user": "vishakhpk",
                        "type": "user"
                    },
                    "name": "Vishakh Padmakumar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:42:49.425Z",
                    "hidden": false
                },
                {
                    "_id": "67e0beb474fc794321fb4ad9",
                    "user": {
                        "_id": "63a0ef9945edac9f75042337",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671491435912-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Melissa Roemmele",
                        "user": "roemmele",
                        "type": "user"
                    },
                    "name": "Melissa Roemmele",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:42:55.929Z",
                    "hidden": false
                },
                {
                    "_id": "67e0beb474fc794321fb4ada",
                    "name": "Yuqian Sun",
                    "hidden": false
                },
                {
                    "_id": "67e0beb474fc794321fb4adb",
                    "name": "Max Kreminski",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T13:21:45.000Z",
            "submittedOnDailyAt": "2025-03-24T00:39:24.717Z",
            "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
            "upvotes": 18,
            "discussionId": "67e0beb574fc794321fb4b04",
            "ai_keywords": [
                "direct preference optimization (DPO)",
                "odds ratio preference optimization (ORPO)",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-03-21T09:21:45.000Z",
        "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
        "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17126.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6450
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17032",
            "authors": [
                {
                    "_id": "67e132d77d88e5241c019390",
                    "user": {
                        "_id": "6794e5faaae193052d6fc1b8",
                        "avatarUrl": "/avatars/357560910abdbd52392a3a2fe0202217.svg",
                        "isPro": false,
                        "fullname": "Jianchuan Chen",
                        "user": "Janaldo",
                        "type": "user"
                    },
                    "name": "Jianchuan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:53:32.888Z",
                    "hidden": false
                },
                {
                    "_id": "67e132d77d88e5241c019391",
                    "name": "Jingchuan Hu",
                    "hidden": false
                },
                {
                    "_id": "67e132d77d88e5241c019392",
                    "user": {
                        "_id": "64647d4f07e49885b4c7892d",
                        "avatarUrl": "/avatars/806b7a842d260990731f954af73896a3.svg",
                        "isPro": false,
                        "fullname": "Wang Gaige",
                        "user": "WangGaige",
                        "type": "user"
                    },
                    "name": "Gaige Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:53:54.415Z",
                    "hidden": false
                },
                {
                    "_id": "67e132d77d88e5241c019393",
                    "name": "Zhonghua Jiang",
                    "hidden": false
                },
                {
                    "_id": "67e132d77d88e5241c019394",
                    "user": {
                        "_id": "649d55b91e9de7b743897ba6",
                        "avatarUrl": "/avatars/c8d3be7df5e2f7a6ffda1adcae1e7aac.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Tiansong",
                        "type": "user"
                    },
                    "name": "Tiansong Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:54:12.524Z",
                    "hidden": false
                },
                {
                    "_id": "67e132d77d88e5241c019395",
                    "user": {
                        "_id": "6729d2b802305fd2366b30a0",
                        "avatarUrl": "/avatars/e5be25eecf5694e3b63a20c89a20653b.svg",
                        "isPro": false,
                        "fullname": "Zhiwen Chen",
                        "user": "chenzhiwen",
                        "type": "user"
                    },
                    "name": "Zhiwen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:54:25.569Z",
                    "hidden": false
                },
                {
                    "_id": "67e132d77d88e5241c019396",
                    "name": "Chengfei Lv",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T10:40:37.000Z",
            "submittedOnDailyAt": "2025-03-24T08:56:11.529Z",
            "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.",
            "upvotes": 13,
            "discussionId": "67e132da7d88e5241c019421",
            "projectPage": "https://pixelai-team.github.io/TaoAvatar/",
            "ai_keywords": [
                "3D Gaussian Splatting (3DGS)",
                "fine-grained control",
                "facial expressions",
                "body movements",
                "high-fidelity",
                "lightweight",
                "personalized clothed human parametric template",
                "Gaussians",
                "StyleUnet",
                "non-rigid deformation",
                "high-frequency appearance details",
                "resource-intensive",
                "MLP-based network",
                "distillation technique",
                "blend shapes",
                "rendering quality",
                "real-time",
                "high-definition stereo devices",
                "Apple Vision Pro"
            ]
        },
        "publishedAt": "2025-03-21T06:40:37.000Z",
        "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting",
        "summary": "Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17032.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6450
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16549",
            "authors": [
                {
                    "_id": "67e0d11eb04d9e83682f222a",
                    "user": {
                        "_id": "642c1581fa6e17ba1e28f075",
                        "avatarUrl": "/avatars/e57ffb69a4882ff95717b484bcfb71ff.svg",
                        "isPro": false,
                        "fullname": "Felix Chen",
                        "user": "felixchen",
                        "type": "user"
                    },
                    "name": "Felix Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:43:40.264Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f222b",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:06:59.943Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f222c",
                    "user": {
                        "_id": "646c77911ee398a4e9404b8b",
                        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
                        "isPro": false,
                        "fullname": "Yunqiu Xu",
                        "user": "Yunqiu",
                        "type": "user"
                    },
                    "name": "Yunqiu Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:45:05.982Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f222d",
                    "user": {
                        "_id": "66269e8b3385ba4e6ed75834",
                        "avatarUrl": "/avatars/a3188fbb4526f62db92dc65ce42598bc.svg",
                        "isPro": false,
                        "fullname": "Tao Feng",
                        "user": "taofeng",
                        "type": "user"
                    },
                    "name": "Tao Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:44:22.418Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f222e",
                    "name": "Jun Cen",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f222f",
                    "user": {
                        "_id": "66f0e9bb16362f8e97b601e7",
                        "avatarUrl": "/avatars/2d76a832dd1b8708ac2f3c168cb2ceaf.svg",
                        "isPro": false,
                        "fullname": "pengwei liu",
                        "user": "wayyyy",
                        "type": "user"
                    },
                    "name": "Pengwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:44:07.900Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f2230",
                    "name": "Zeying Huang",
                    "hidden": false
                },
                {
                    "_id": "67e0d11eb04d9e83682f2231",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T11:46:19.000Z",
            "submittedOnDailyAt": "2025-03-24T01:59:23.638Z",
            "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.",
            "upvotes": 11,
            "discussionId": "67e0d11fb04d9e83682f2267",
            "githubRepo": "https://github.com/MathFlow-zju/MathFlow",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "visual mathematical problem-solving",
                "diagrams",
                "perception capabilities",
                "inference processes",
                "FlowVerse",
                "problem-solving",
                "essential information",
                "reasoned property",
                "MathFlow",
                "problem-solving pipeline",
                "perception model",
                "MathFlow-P-7B",
                "closed-source inference models",
                "open-source inference models"
            ]
        },
        "publishedAt": "2025-03-19T07:46:19.000Z",
        "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems",
        "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16549.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16983",
            "authors": [
                {
                    "_id": "67e0c303ff27a08e3896134a",
                    "name": "Xu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e3896134b",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e3896134c",
                    "name": "Haoming Qin",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e3896134d",
                    "user": {
                        "_id": "64a42dc94c10e3675498ad92",
                        "avatarUrl": "/avatars/1f3e027d27af8444a437399d9ee6dd8f.svg",
                        "isPro": false,
                        "fullname": "xiaobin Lu",
                        "user": "xilu5047",
                        "type": "user"
                    },
                    "name": "Xiaobin Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:47:23.819Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e3896134e",
                    "user": {
                        "_id": "67c4610fa8ec9d71bf3b2a48",
                        "avatarUrl": "/avatars/88eb9641e1dc7a237498c5257a0232aa.svg",
                        "isPro": false,
                        "fullname": "yanjiaxing",
                        "user": "YanJiaXing",
                        "type": "user"
                    },
                    "name": "Jiaxing Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:46:19.494Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e3896134f",
                    "user": {
                        "_id": "677f5eca19fb7f8c452e0876",
                        "avatarUrl": "/avatars/f053709ece377164975e266e52ab23af.svg",
                        "isPro": false,
                        "fullname": "wang guanzhong",
                        "user": "jerrywgz",
                        "type": "user"
                    },
                    "name": "Guanzhong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:47:37.219Z",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e38961350",
                    "name": "Zeyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67e0c303ff27a08e38961351",
                    "name": "Yi Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T09:48:00.000Z",
            "submittedOnDailyAt": "2025-03-24T00:58:01.102Z",
            "title": "Enabling Versatile Controls for Video Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
            "upvotes": 9,
            "discussionId": "67e0c306ff27a08e38961428",
            "ai_keywords": [
                "VCtrl",
                "PP-VCtrl",
                "fine-grained control",
                "pre-trained video diffusion models",
                "conditional module",
                "Canny edges",
                "segmentation masks",
                "human keypoints",
                "unified control signal encoding pipeline",
                "sparse residual connection mechanism",
                "controllability",
                "PaddlePaddle",
                "PaddleMIX",
                "ppdiffusers"
            ]
        },
        "publishedAt": "2025-03-21T05:48:00.000Z",
        "title": "Enabling Versatile Controls for Video Diffusion Models",
        "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16983.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6450
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16025",
            "authors": [
                {
                    "_id": "67dd02594aa37abf77af416b",
                    "user": {
                        "_id": "63eb8b1113a3eb9b0dc89d8c",
                        "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
                        "isPro": false,
                        "fullname": "Yair Shpitzer",
                        "user": "yairshp",
                        "type": "user"
                    },
                    "name": "Yair Shpitzer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-21T11:40:09.783Z",
                    "hidden": false
                },
                {
                    "_id": "67dd02594aa37abf77af416c",
                    "name": "Gal Chechik",
                    "hidden": false
                },
                {
                    "_id": "67dd02594aa37abf77af416d",
                    "name": "Idan Schwartz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T10:45:04.000Z",
            "submittedOnDailyAt": "2025-03-24T08:16:33.033Z",
            "title": "Single Image Iterative Subject-driven Generation and Editing",
            "submittedOnDailyBy": {
                "_id": "63eb8b1113a3eb9b0dc89d8c",
                "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
                "isPro": false,
                "fullname": "Yair Shpitzer",
                "user": "yairshp",
                "type": "user"
            },
            "summary": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
            "upvotes": 9,
            "discussionId": "67dd025f4aa37abf77af42db",
            "projectPage": "https://siso-paper.github.io/",
            "githubRepo": "https://github.com/yairshp/SISO",
            "ai_keywords": [
                "concept learning",
                "encoder",
                "pre-training",
                "similarity score",
                "iterative generation",
                "model optimization",
                "plug-and-play optimization"
            ]
        },
        "publishedAt": "2025-03-20T06:45:04.000Z",
        "title": "Single Image Iterative Subject-driven Generation and Editing",
        "summary": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16025.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63eb8b1113a3eb9b0dc89d8c",
            "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
            "fullname": "Yair Shpitzer",
            "name": "yairshp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17287",
            "authors": [
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc0",
                    "user": {
                        "_id": "66aca01e33f6b27979856f6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
                        "isPro": false,
                        "fullname": "Mingyang Song",
                        "user": "hitsmy",
                        "type": "user"
                    },
                    "name": "Mingyang Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:52:03.201Z",
                    "hidden": true
                },
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc1",
                    "name": "Mao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc2",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc3",
                    "name": "Wenjie Yang",
                    "hidden": false
                },
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc4",
                    "name": "Xuan Luo",
                    "hidden": false
                },
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc5",
                    "name": "Yue Pan",
                    "hidden": false
                },
                {
                    "_id": "67e0bfcc8fb92b0edaa78dc6",
                    "name": "Feng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T16:35:31.000Z",
            "submittedOnDailyAt": "2025-03-24T00:43:41.116Z",
            "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
            "upvotes": 8,
            "discussionId": "67e0bfcd8fb92b0edaa78e17",
            "githubRepo": "https://github.com/nick7nlp/FastCuRL",
            "ai_keywords": [
                "Curriculum Reinforcement Learning",
                "context window",
                "reinforcement learning",
                "training efficiency",
                "R1-like reasoning models",
                "long chain-of-thought",
                "rationales",
                "length-aware training",
                "training data segmentation",
                "context window extension",
                "progressively increasing context window length",
                "DeepScaleR-1.5B-Preview",
                "MATH 500",
                "AIME 2024",
                "AMC 2023",
                "Minerva Math",
                "OlympiadBench",
                "training steps"
            ]
        },
        "publishedAt": "2025-03-21T12:35:31.000Z",
        "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
        "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17287.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6450
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16867",
            "authors": [
                {
                    "_id": "67e0d31f151ca9ed92898fff",
                    "user": {
                        "_id": "63bbf071d8d676a2299c7d0b",
                        "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
                        "isPro": false,
                        "fullname": "Guan",
                        "user": "Guan123",
                        "type": "user"
                    },
                    "name": "Kaisi Guan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:06:57.786Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899000",
                    "user": {
                        "_id": "66b5295f83425904fa7a1a6a",
                        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
                        "isPro": false,
                        "fullname": "Zhengfeng Lai",
                        "user": "jefflai",
                        "type": "user"
                    },
                    "name": "Zhengfeng Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:49:46.588Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899001",
                    "user": {
                        "_id": "645e0f1aa45b4182d7f69176",
                        "avatarUrl": "/avatars/6b87f203e781b6ec734d2b58084ab013.svg",
                        "isPro": false,
                        "fullname": "yuchong sun",
                        "user": "ycsun",
                        "type": "user"
                    },
                    "name": "Yuchong Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:49:53.004Z",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899002",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899003",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899004",
                    "name": "Kieran Liu",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899005",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "67e0d31f151ca9ed92899006",
                    "name": "Ruihua Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T05:52:50.000Z",
            "submittedOnDailyAt": "2025-03-24T07:22:50.935Z",
            "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
            "submittedOnDailyBy": {
                "_id": "63bbf071d8d676a2299c7d0b",
                "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
                "isPro": false,
                "fullname": "Guan",
                "user": "Guan123",
                "type": "user"
            },
            "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
            "upvotes": 8,
            "discussionId": "67e0d322151ca9ed928990c0",
            "projectPage": "https://eftv-eval.github.io/etva-eval/",
            "ai_keywords": [
                "semantic alignment",
                "Text-to-Video (T2V) Generation",
                "CLIPScore",
                "fine-grained alignment details",
                "multi-agent system",
                "semantic scene graphs",
                "atomic questions",
                "knowledge-augmented",
                "multi-stage reasoning framework",
                "auxiliary LLM",
                "common-sense knowledge",
                "video LLM",
                "multi-stage reasoning mechanism",
                "Spearman's correlation coefficient",
                "benchmark",
                "text-to-video models"
            ]
        },
        "publishedAt": "2025-03-21T01:52:50.000Z",
        "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
        "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16867.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bbf071d8d676a2299c7d0b",
            "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
            "fullname": "Guan",
            "name": "Guan123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.12821",
            "authors": [
                {
                    "_id": "67e0cc432bbf376bdb18623b",
                    "user": {
                        "_id": "66aca01e33f6b27979856f6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
                        "isPro": false,
                        "fullname": "Mingyang Song",
                        "user": "hitsmy",
                        "type": "user"
                    },
                    "name": "Mingyang Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:07:01.991Z",
                    "hidden": false
                },
                {
                    "_id": "67e0cc432bbf376bdb18623c",
                    "user": {
                        "_id": "64cb54da1af278541d663708",
                        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                        "isPro": false,
                        "fullname": "Xiaoye Qu",
                        "user": "Xiaoye08",
                        "type": "user"
                    },
                    "name": "Xiaoye Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:51:10.424Z",
                    "hidden": false
                },
                {
                    "_id": "67e0cc432bbf376bdb18623d",
                    "name": "Jiawei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e0cc432bbf376bdb18623e",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-17T05:01:09.000Z",
            "submittedOnDailyAt": "2025-03-24T02:58:13.280Z",
            "title": "From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration",
            "submittedOnDailyBy": {
                "_id": "66aca01e33f6b27979856f6f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
                "isPro": false,
                "fullname": "Mingyang Song",
                "user": "hitsmy",
                "type": "user"
            },
            "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an Adaptive\nData Refinement Framework (ADR), which\nconsists of two stages: Data Rebalancing (DR)\nand Data Synthesis (DS). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.",
            "upvotes": 7,
            "discussionId": "67e0cc442bbf376bdb186293",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "Long-Tail (LT) problems",
                "CLIP",
                "ViT",
                "LLaVA",
                "Visual Question Answering",
                "Visual Reasoning",
                "Adaptive Data Refinement Framework (ADR)",
                "Data Rebalancing (DR)",
                "Data Synthesis (DS)",
                "Denoising Diffusion Probabilistic Models (DDPMs)"
            ]
        },
        "publishedAt": "2025-03-17T01:01:09.000Z",
        "title": "From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration",
        "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an Adaptive\nData Refinement Framework (ADR), which\nconsists of two stages: Data Rebalancing (DR)\nand Data Synthesis (DS). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66aca01e33f6b27979856f6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
            "fullname": "Mingyang Song",
            "name": "hitsmy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17069",
            "authors": [
                {
                    "_id": "67e0ba47753cfd5e438d3814",
                    "user": {
                        "_id": "63be636387619d1458c2e8e0",
                        "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
                        "isPro": false,
                        "fullname": "SHI YUFEI",
                        "user": "Master-Shi",
                        "type": "user"
                    },
                    "name": "Yufei Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:10:50.772Z",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d3815",
                    "user": {
                        "_id": "668192a3f35c3ff47a8438ee",
                        "avatarUrl": "/avatars/6b293341f5dc51f574252c6f57cfd293.svg",
                        "isPro": false,
                        "fullname": "Weilong Yan",
                        "user": "DavidYan2001",
                        "type": "user"
                    },
                    "name": "Weilong Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:54:43.276Z",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d3816",
                    "user": {
                        "_id": "676cbdc1dd95830fd9137137",
                        "avatarUrl": "/avatars/71f36c630b475c08198d75ec377b6f3f.svg",
                        "isPro": false,
                        "fullname": "xu",
                        "user": "gangXu",
                        "type": "user"
                    },
                    "name": "Gang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:54:52.020Z",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d3817",
                    "name": "Yumeng Li",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d3818",
                    "user": {
                        "_id": "646cdb564e59b82995df856b",
                        "avatarUrl": "/avatars/4accb3f3d0289f8a5848e973ddcfcb92.svg",
                        "isPro": false,
                        "fullname": "Jonathan Li",
                        "user": "yumengli",
                        "type": "user"
                    },
                    "name": "Yuchen Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:54:59.453Z",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d3819",
                    "user": {
                        "_id": "67198fff75da7b2981620711",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9pBj2JrSeYQHZg43jV6Fa.png",
                        "isPro": false,
                        "fullname": "Zhenxi Li",
                        "user": "wangxiangyu1",
                        "type": "user"
                    },
                    "name": "Zhenxi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:55:27.186Z",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d381a",
                    "name": "Fei Richard Yu",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d381b",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67e0ba47753cfd5e438d381c",
                    "name": "Si Yong Yeo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T11:50:06.000Z",
            "submittedOnDailyAt": "2025-03-24T07:28:42.136Z",
            "title": "PVChat: Personalized Video Chat with One-Shot Learning",
            "submittedOnDailyBy": {
                "_id": "63be636387619d1458c2e8e0",
                "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
                "isPro": false,
                "fullname": "SHI YUFEI",
                "user": "Master-Shi",
                "type": "user"
            },
            "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
            "upvotes": 6,
            "discussionId": "67e0ba4b753cfd5e438d391e",
            "ai_keywords": [
                "ViLLMs",
                "one-shot learning",
                "PVChat",
                "Mixture-of-Heads (MoH)",
                "image-to-video learning",
                "automated augmentation pipeline",
                "identity-preserving positive samples",
                "hard negatives",
                "QA types",
                "ReLU Routing MoH attention mechanism",
                "Smooth Proximity Regularization",
                "Head Activation Enhancement",
                "two-stage training strategy",
                "image pre-training",
                "video fine-tuning",
                "personalized feature understanding",
                "state-of-the-art ViLLMs"
            ]
        },
        "publishedAt": "2025-03-21T07:50:06.000Z",
        "title": "PVChat: Personalized Video Chat with One-Shot Learning",
        "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17069.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63be636387619d1458c2e8e0",
            "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
            "fullname": "SHI YUFEI",
            "name": "Master-Shi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16921",
            "authors": [
                {
                    "_id": "67e0bb1665e294ad989334ea",
                    "name": "Lingfan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334eb",
                    "name": "Chen Liu",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334ec",
                    "user": {
                        "_id": "652fab9d04a34a9282bf29d6",
                        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                        "isPro": false,
                        "fullname": "Chengming Xu",
                        "user": "ChengmingX",
                        "type": "user"
                    },
                    "name": "Chengming Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:49:08.734Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334ed",
                    "name": "Kai Hu",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334ee",
                    "name": "Donghao Luo",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334ef",
                    "user": {
                        "_id": "65729087379284b904a6d81d",
                        "avatarUrl": "/avatars/0e7968b1d9221d523a54e3ac787e449d.svg",
                        "isPro": false,
                        "fullname": "Chengjie Wang",
                        "user": "chengjie-wang",
                        "type": "user"
                    },
                    "name": "Chengjie Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:48:42.939Z",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334f0",
                    "name": "Yanwei Fu",
                    "hidden": false
                },
                {
                    "_id": "67e0bb1665e294ad989334f1",
                    "user": {
                        "_id": "664618d83a7681520570b58a",
                        "avatarUrl": "/avatars/c03e25ee9bbf3504cdf2959acf81e0fe.svg",
                        "isPro": false,
                        "fullname": "Yuan Yao",
                        "user": "yuanyao",
                        "type": "user"
                    },
                    "name": "Yuan Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:48:12.962Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T07:33:44.000Z",
            "submittedOnDailyAt": "2025-03-24T00:24:41.729Z",
            "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
            "submittedOnDailyBy": {
                "_id": "652fab9d04a34a9282bf29d6",
                "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                "isPro": false,
                "fullname": "Chengming Xu",
                "user": "ChengmingX",
                "type": "user"
            },
            "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
            "upvotes": 6,
            "discussionId": "67e0bb1a65e294ad9893361c",
            "ai_keywords": [
                "diffusion models",
                "Diffusion-DPO",
                "adaptive-DPO",
                "intra-annotator confidence",
                "inter-annotator stability",
                "DPO objective",
                "Adaptive-DPO loss function"
            ]
        },
        "publishedAt": "2025-03-21T03:33:44.000Z",
        "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
        "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652fab9d04a34a9282bf29d6",
            "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
            "fullname": "Chengming Xu",
            "name": "ChengmingX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.11572",
            "authors": [
                {
                    "_id": "67e0530f151ca9ed9265e949",
                    "user": {
                        "_id": "64c5d832d68946edad7d5536",
                        "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
                        "isPro": false,
                        "fullname": "Messi Lee",
                        "user": "l048596",
                        "type": "user"
                    },
                    "name": "Messi H. J. Lee",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-23T18:30:22.473Z",
                    "hidden": false
                },
                {
                    "_id": "67e0530f151ca9ed9265e94a",
                    "name": "Calvin K. Lai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T16:40:02.000Z",
            "submittedOnDailyAt": "2025-03-24T02:15:38.327Z",
            "title": "Implicit Bias-Like Patterns in Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "64c5d832d68946edad7d5536",
                "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
                "isPro": false,
                "fullname": "Messi Lee",
                "user": "l048596",
                "type": "user"
            },
            "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
            "upvotes": 6,
            "discussionId": "67e05311151ca9ed9265e9c1",
            "githubRepo": "https://github.com/lee-messi/RM-IAT",
            "ai_keywords": [
                "Reasoning Model Implicit Association Test (RM-IAT)",
                "reasoning models",
                "LLMs (Large Language Models)",
                "tokens",
                "association-incompatible information",
                "association-compatible information",
                "implicit bias-like patterns"
            ]
        },
        "publishedAt": "2025-03-14T12:40:02.000Z",
        "title": "Implicit Bias-Like Patterns in Reasoning Models",
        "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11572.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c5d832d68946edad7d5536",
            "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
            "fullname": "Messi Lee",
            "name": "l048596",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16423",
            "authors": [
                {
                    "_id": "67def53e2c86b9aaee43f306",
                    "user": {
                        "_id": "63dd76b63880ace5ca41c133",
                        "avatarUrl": "/avatars/d8593fc89c00eeadc7941be040174d35.svg",
                        "isPro": false,
                        "fullname": "Ron Campos",
                        "user": "rjccv",
                        "type": "user"
                    },
                    "name": "Ron Campos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T14:49:32.851Z",
                    "hidden": false
                },
                {
                    "_id": "67def53e2c86b9aaee43f307",
                    "name": "Ashmal Vayani",
                    "hidden": false
                },
                {
                    "_id": "67def53e2c86b9aaee43f308",
                    "name": "Parth Parag Kulkarni",
                    "hidden": false
                },
                {
                    "_id": "67def53e2c86b9aaee43f309",
                    "name": "Rohit Gupta",
                    "hidden": false
                },
                {
                    "_id": "67def53e2c86b9aaee43f30a",
                    "user": {
                        "_id": "67d58d156db0e6f0c33c0f60",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d58d156db0e6f0c33c0f60/9KiFw0iEMZQHcHnm1k0ED.jpeg",
                        "isPro": false,
                        "fullname": "Aritra Dutta",
                        "user": "aritradutta",
                        "type": "user"
                    },
                    "name": "Aritra Dutta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:52:11.893Z",
                    "hidden": false
                },
                {
                    "_id": "67def53e2c86b9aaee43f30b",
                    "name": "Mubarak Shah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:59:47.000Z",
            "submittedOnDailyAt": "2025-03-24T12:08:38.647Z",
            "title": "GAEA: A Geolocation Aware Conversational Model",
            "submittedOnDailyBy": {
                "_id": "67d58d156db0e6f0c33c0f60",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d58d156db0e6f0c33c0f60/9KiFw0iEMZQHcHnm1k0ED.jpeg",
                "isPro": false,
                "fullname": "Aritra Dutta",
                "user": "aritradutta",
                "type": "user"
            },
            "summary": "Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs)\nproprietary and open-source researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose a comprehensive dataset GAEA\nwith 800K images and around 1.6M question answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark comprising 4K\nimage-text pairs to evaluate conversational capabilities equipped with diverse\nquestion types. We consider 11 state-of-the-art open-source and proprietary\nLMMs and demonstrate that GAEA significantly outperforms the best open-source\nmodel, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by\n8.28%. Our dataset, model and codes are available",
            "upvotes": 5,
            "discussionId": "67def5402c86b9aaee43f397",
            "projectPage": "https://ucf-crcv.github.io/GAEA/",
            "githubRepo": "https://github.com/UCF-CRCV/GAEA",
            "ai_keywords": [
                "conversational model",
                "multimodal models",
                "GAEA",
                "large-scale dataset",
                "OpenStreetMap (OSM)",
                "geographical context clues",
                "image-text pairs",
                "conversational capabilities",
                "state-of-the-art",
                "LLaVA-OneVision",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-03-20T13:59:47.000Z",
        "title": "GAEA: A Geolocation Aware Conversational Model",
        "summary": "Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs)\nproprietary and open-source researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose a comprehensive dataset GAEA\nwith 800K images and around 1.6M question answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark comprising 4K\nimage-text pairs to evaluate conversational capabilities equipped with diverse\nquestion types. We consider 11 state-of-the-art open-source and proprietary\nLMMs and demonstrate that GAEA significantly outperforms the best open-source\nmodel, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by\n8.28%. Our dataset, model and codes are available",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16423.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d58d156db0e6f0c33c0f60",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d58d156db0e6f0c33c0f60/9KiFw0iEMZQHcHnm1k0ED.jpeg",
            "fullname": "Aritra Dutta",
            "name": "aritradutta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17407",
            "authors": [
                {
                    "_id": "67e204089e7142f579abb84a",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb84b",
                    "name": "Dawei Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb84c",
                    "name": "Zhiqi Bai",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb84d",
                    "name": "Yancheng He",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb84e",
                    "name": "Huanxuan Liao",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb84f",
                    "name": "Haoran Que",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb850",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb851",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb852",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb853",
                    "name": "Jiebin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb854",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb855",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb856",
                    "name": "Hangyu Guo",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb857",
                    "name": "Shilong Li",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb858",
                    "name": "Ziqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb859",
                    "name": "Yong Shan",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb85a",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb85b",
                    "name": "Jiayi Tian",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb85c",
                    "name": "Wenhao Wu",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb85d",
                    "name": "Zhejian Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb85e",
                    "name": "Ruijie Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb85f",
                    "name": "Junlan Feng",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb860",
                    "name": "Yang Gao",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb861",
                    "name": "Shizhu He",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb862",
                    "name": "Zhoujun Li",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb863",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb864",
                    "name": "Fanyu Meng",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb865",
                    "name": "Wenbo Su",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb866",
                    "name": "Yingshui Tan",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb867",
                    "name": "Zili Wang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb868",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb869",
                    "name": "Wei Ye",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb86a",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb86b",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb86c",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb86d",
                    "name": "Sujian Li",
                    "hidden": false
                },
                {
                    "_id": "67e204089e7142f579abb86e",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T17:06:28.000Z",
            "submittedOnDailyAt": "2025-03-24T23:50:44.202Z",
            "title": "A Comprehensive Survey on Long Context Language Modeling",
            "submittedOnDailyBy": {
                "_id": "64d2fce8129a210e569e0c76",
                "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
                "isPro": false,
                "fullname": "Dawei Zhu",
                "user": "dwzhu",
                "type": "user"
            },
            "summary": "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\nhttps://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling{\\color[RGB]{175,36,67}{LCLM-Horizon}}.",
            "upvotes": 5,
            "discussionId": "67e2040b9e7142f579abb93a",
            "projectPage": "https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling",
            "githubRepo": "https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling",
            "ai_keywords": [
                "Long Context Language Models (LCLMs)",
                "long-context modeling",
                "large language models (LLMs)",
                "long context processing",
                "data strategies",
                "architectural designs",
                "workflow approaches",
                "LCLM training",
                "LCLM inference",
                "long-context comprehension",
                "long-form generation",
                "behavioral analysis",
                "mechanism interpretability",
                "application scenarios"
            ]
        },
        "publishedAt": "2025-03-20T13:06:28.000Z",
        "title": "A Comprehensive Survey on Long Context Language Modeling",
        "summary": "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\nhttps://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling{\\color[RGB]{175,36,67}{LCLM-Horizon}}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17407.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d2fce8129a210e569e0c76",
            "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
            "fullname": "Dawei Zhu",
            "name": "dwzhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16282",
            "authors": [
                {
                    "_id": "67e13fcab21a55baf5fc6768",
                    "user": {
                        "_id": "65e5eae6958b39864e8b683e",
                        "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
                        "isPro": false,
                        "fullname": "Zhaochong An",
                        "user": "ZhaochongAn",
                        "type": "user"
                    },
                    "name": "Zhaochong An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T12:32:41.985Z",
                    "hidden": false
                },
                {
                    "_id": "67e13fcab21a55baf5fc6769",
                    "user": {
                        "_id": "653c2b3777c2f094527b7a9c",
                        "avatarUrl": "/avatars/b27ac82a79a2d62cbecda303239efc2c.svg",
                        "isPro": false,
                        "fullname": "Guolei Sun",
                        "user": "GuoleiSun",
                        "type": "user"
                    },
                    "name": "Guolei Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:55:55.488Z",
                    "hidden": false
                },
                {
                    "_id": "67e13fcab21a55baf5fc676a",
                    "name": "Yun Liu",
                    "hidden": false
                },
                {
                    "_id": "67e13fcab21a55baf5fc676b",
                    "user": {
                        "_id": "638e29cf319f9c746b87ad4b",
                        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
                        "isPro": false,
                        "fullname": "Runjia Li",
                        "user": "liguang0115",
                        "type": "user"
                    },
                    "name": "Runjia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:56:19.776Z",
                    "hidden": false
                },
                {
                    "_id": "67e13fcab21a55baf5fc676c",
                    "user": {
                        "_id": "636e6ee287545ca5a136b4c3",
                        "avatarUrl": "/avatars/208d32b1202e2da210146027212dbdd3.svg",
                        "isPro": false,
                        "fullname": "Junlin Han",
                        "user": "Junlinh",
                        "type": "user"
                    },
                    "name": "Junlin Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:56:41.798Z",
                    "hidden": false
                },
                {
                    "_id": "67e13fcab21a55baf5fc676d",
                    "name": "Ender Konukoglu",
                    "hidden": false
                },
                {
                    "_id": "67e13fcab21a55baf5fc676e",
                    "name": "Serge Belongie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T16:10:33.000Z",
            "submittedOnDailyAt": "2025-03-24T09:53:31.665Z",
            "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model",
            "submittedOnDailyBy": {
                "_id": "65e5eae6958b39864e8b683e",
                "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
                "isPro": false,
                "fullname": "Zhaochong An",
                "user": "ZhaochongAn",
                "type": "user"
            },
            "summary": "Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL",
            "upvotes": 5,
            "discussionId": "67e13fcbb21a55baf5fc67c2",
            "ai_keywords": [
                "few-shot 3D point cloud segmentation",
                "GFS-PCS",
                "prototypes",
                "support features",
                "query features",
                "3D vision-language models",
                "3D VLMs",
                "open-world novel classes",
                "pseudo-labels",
                "prototype-guided pseudo-label selection",
                "adaptive infilling strategy",
                "novel-base mix strategy",
                "GFS-VL",
                "benchmark"
            ]
        },
        "publishedAt": "2025-03-20T12:10:33.000Z",
        "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model",
        "summary": "Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16282.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65e5eae6958b39864e8b683e",
            "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg",
            "fullname": "Zhaochong An",
            "name": "ZhaochongAn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14607",
            "authors": [
                {
                    "_id": "67e192ae359a7a8b5fb32877",
                    "name": "Shuo Xing",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb32878",
                    "name": "Zezhou Sun",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb32879",
                    "name": "Shuangyu Xie",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb3287a",
                    "name": "Kaiyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb3287b",
                    "name": "Yanjia Huang",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb3287c",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb3287d",
                    "name": "Jiachen Li",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb3287e",
                    "name": "Dezhen Song",
                    "hidden": false
                },
                {
                    "_id": "67e192ae359a7a8b5fb3287f",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T18:05:38.000Z",
            "submittedOnDailyAt": "2025-03-24T15:44:02.936Z",
            "title": "Can Large Vision Language Models Read Maps Like a Human?",
            "submittedOnDailyBy": {
                "_id": "62df9ee3c7e6f74fc92a1351",
                "avatarUrl": "/avatars/2a4f60ecc2fc4be2a8ac21c3f2ceb3cc.svg",
                "isPro": false,
                "fullname": "Shuo Xing",
                "user": "shuoxing",
                "type": "user"
            },
            "summary": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench.",
            "upvotes": 4,
            "discussionId": "67e192b2359a7a8b5fb329d7",
            "ai_keywords": [
                "MapBench",
                "LVLMs",
                "pixel-based map-based outdoor navigation",
                "pixel space map path finding problems",
                "Map Space Scene Graph (MSSG)",
                "language-based navigation instructions",
                "zero-shot prompting",
                "Chain-of-Thought (CoT)",
                "sequential cognitive processes",
                "spatial reasoning",
                "structured decision-making capabilities"
            ]
        },
        "publishedAt": "2025-03-18T14:05:38.000Z",
        "title": "Can Large Vision Language Models Read Maps Like a Human?",
        "summary": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14607.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62df9ee3c7e6f74fc92a1351",
            "avatarUrl": "/avatars/2a4f60ecc2fc4be2a8ac21c3f2ceb3cc.svg",
            "fullname": "Shuo Xing",
            "name": "shuoxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17095",
            "authors": [
                {
                    "_id": "67e147d28e8ab85fb6d72244",
                    "user": {
                        "_id": "639d445524af4747d8d2af52",
                        "avatarUrl": "/avatars/6ca870edc993fd3db6b0db4e4848ef7a.svg",
                        "isPro": false,
                        "fullname": "kwan yun",
                        "user": "kwanY",
                        "type": "user"
                    },
                    "name": "Kwan Yun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-24T12:57:18.816Z",
                    "hidden": false
                },
                {
                    "_id": "67e147d28e8ab85fb6d72245",
                    "name": "Chaelin Kim",
                    "hidden": false
                },
                {
                    "_id": "67e147d28e8ab85fb6d72246",
                    "name": "Hangyeul Shin",
                    "hidden": false
                },
                {
                    "_id": "67e147d28e8ab85fb6d72247",
                    "name": "Junyong Noh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/639d445524af4747d8d2af52/QNTV4dy014LzqHWvGDWnK.png"
            ],
            "publishedAt": "2025-03-21T12:24:58.000Z",
            "submittedOnDailyAt": "2025-03-24T10:26:35.086Z",
            "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
            "submittedOnDailyBy": {
                "_id": "639d445524af4747d8d2af52",
                "avatarUrl": "/avatars/6ca870edc993fd3db6b0db4e4848ef7a.svg",
                "isPro": false,
                "fullname": "kwan yun",
                "user": "kwanY",
                "type": "user"
            },
            "summary": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{https://kwanyun.github.io/FFaceNeRF_page/{project-page}}.",
            "upvotes": 2,
            "discussionId": "67e147d88e8ab85fb6d72429",
            "projectPage": "https://kwanyun.github.io/FFaceNeRF_page/",
            "githubRepo": "https://github.com/kwanyun/FFaceNeRF",
            "ai_keywords": [
                "Neural Radiance Fields (NeRF)",
                "geometry adapter",
                "feature injection",
                "latent mixing",
                "tri-plane augmentation",
                "3D face editing",
                "mask-based face editing"
            ]
        },
        "publishedAt": "2025-03-21T08:24:58.000Z",
        "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
        "summary": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{https://kwanyun.github.io/FFaceNeRF_page/{project-page}}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/639d445524af4747d8d2af52/QNTV4dy014LzqHWvGDWnK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17095.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639d445524af4747d8d2af52",
            "avatarUrl": "/avatars/6ca870edc993fd3db6b0db4e4848ef7a.svg",
            "fullname": "kwan yun",
            "name": "kwanY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]