[
    {
        "paper": {
            "id": "2503.18878",
            "authors": [
                {
                    "_id": "67e25fe88e6c927eb7794abd",
                    "name": "Andrey Galichin",
                    "hidden": false
                },
                {
                    "_id": "67e25fe88e6c927eb7794abe",
                    "user": {
                        "_id": "60cd95ee15ecba5f2200304a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
                        "isPro": false,
                        "fullname": "Alexey Dontsov",
                        "user": "therem",
                        "type": "user"
                    },
                    "name": "Alexey Dontsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:18:43.467Z",
                    "hidden": false
                },
                {
                    "_id": "67e25fe88e6c927eb7794abf",
                    "name": "Polina Druzhinina",
                    "hidden": false
                },
                {
                    "_id": "67e25fe88e6c927eb7794ac0",
                    "user": {
                        "_id": "6172aaeec8e66e2aa84c06b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
                        "isPro": false,
                        "fullname": "Anton Razzhigaev",
                        "user": "razzant",
                        "type": "user"
                    },
                    "name": "Anton Razzhigaev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T09:05:58.409Z",
                    "hidden": false
                },
                {
                    "_id": "67e25fe88e6c927eb7794ac1",
                    "name": "Oleg Y. Rogov",
                    "hidden": false
                },
                {
                    "_id": "67e25fe88e6c927eb7794ac2",
                    "user": {
                        "_id": "662f8d645c4db70c77a203b0",
                        "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
                        "isPro": false,
                        "fullname": "Elena Tutubalina",
                        "user": "tlenusik",
                        "type": "user"
                    },
                    "name": "Elena Tutubalina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T09:05:56.401Z",
                    "hidden": false
                },
                {
                    "_id": "67e25fe88e6c927eb7794ac3",
                    "user": {
                        "_id": "6169a581d05945bfd8718dfa",
                        "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
                        "isPro": false,
                        "fullname": "Ivan Oseledets",
                        "user": "oseledets",
                        "type": "user"
                    },
                    "name": "Ivan Oseledets",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T19:02:24.076Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T16:54:26.000Z",
            "submittedOnDailyAt": "2025-03-25T06:45:16.781Z",
            "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
            "submittedOnDailyBy": {
                "_id": "60cd95ee15ecba5f2200304a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
                "isPro": false,
                "fullname": "Alexey Dontsov",
                "user": "therem",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
            "upvotes": 84,
            "discussionId": "67e25fea8e6c927eb7794b25",
            "ai_keywords": [
                "Sparse Autoencoders (SAEs)",
                "latent representations",
                "interpretable features",
                "reasoning features",
                "empirical analysis",
                "interpretability methods",
                "systematic enhancement"
            ]
        },
        "publishedAt": "2025-03-24T12:54:26.000Z",
        "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18878.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60cd95ee15ecba5f2200304a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg",
            "fullname": "Alexey Dontsov",
            "name": "therem",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17359",
            "authors": [
                {
                    "_id": "67e16a266280a70b45b8a16c",
                    "user": {
                        "_id": "64105a6d14215c0775dfdd14",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
                        "isPro": false,
                        "fullname": "Jiwen Yu",
                        "user": "VictorYuki",
                        "type": "user"
                    },
                    "name": "Jiwen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:05:33.251Z",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a16d",
                    "name": "Yiran Qin",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a16e",
                    "user": {
                        "_id": "652404d0050781c16f1c51b0",
                        "avatarUrl": "/avatars/4ad62f2c65406dd0af36c6d0697ae599.svg",
                        "isPro": false,
                        "fullname": "Haoxuan Che",
                        "user": "chehx",
                        "type": "user"
                    },
                    "name": "Haoxuan Che",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:06:13.592Z",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a16f",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a170",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:06:29.181Z",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a171",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a172",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:06:50.971Z",
                    "hidden": false
                },
                {
                    "_id": "67e16a266280a70b45b8a173",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:06:56.864Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T17:59:22.000Z",
            "submittedOnDailyAt": "2025-03-25T01:42:15.880Z",
            "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
            "submittedOnDailyBy": {
                "_id": "64105a6d14215c0775dfdd14",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
                "isPro": false,
                "fullname": "Jiwen Yu",
                "user": "VictorYuki",
                "type": "user"
            },
            "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
            "upvotes": 53,
            "discussionId": "67e16a276280a70b45b8a214",
            "ai_keywords": [
                "Interactive Generative Video (IGV)",
                "Generative Game Engines (GGE)",
                "video generation models",
                "high-quality content synthesis",
                "physics-aware world modeling",
                "user-controlled interactivity",
                "long-term memory capabilities",
                "causal reasoning",
                "hierarchical maturity roadmap (L0-L4)"
            ]
        },
        "publishedAt": "2025-03-21T13:59:22.000Z",
        "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
        "summary": "Modern game development faces significant challenges in creativity and cost\ndue to predetermined content in traditional game engines. Recent breakthroughs\nin video generation models, capable of synthesizing realistic and interactive\nvirtual environments, present an opportunity to revolutionize game creation. In\nthis position paper, we propose Interactive Generative Video (IGV) as the\nfoundation for Generative Game Engines (GGE), enabling unlimited novel content\ngeneration in next-generation gaming. GGE leverages IGV's unique strengths in\nunlimited high-quality content synthesis, physics-aware world modeling,\nuser-controlled interactivity, long-term memory capabilities, and causal\nreasoning. We present a comprehensive framework detailing GGE's core modules\nand a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work\ncharts a new course for game development in the AI era, envisioning a future\nwhere AI-powered generative systems fundamentally reshape how games are created\nand experienced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17359.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64105a6d14215c0775dfdd14",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
            "fullname": "Jiwen Yu",
            "name": "VictorYuki",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18942",
            "authors": [
                {
                    "_id": "67e226039cd910bee045e38f",
                    "user": {
                        "_id": "6505a02f9310ce8c400edc63",
                        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                        "isPro": false,
                        "fullname": "Fangfu Liu",
                        "user": "Liuff23",
                        "type": "user"
                    },
                    "name": "Fangfu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:07:42.279Z",
                    "hidden": false
                },
                {
                    "_id": "67e226039cd910bee045e390",
                    "name": "Hanyang Wang",
                    "hidden": false
                },
                {
                    "_id": "67e226039cd910bee045e391",
                    "name": "Yimo Cai",
                    "hidden": false
                },
                {
                    "_id": "67e226039cd910bee045e392",
                    "user": {
                        "_id": "60bc94cd85a3ab33829b6211",
                        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "iseesaw",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:20.614Z",
                    "hidden": false
                },
                {
                    "_id": "67e226039cd910bee045e393",
                    "user": {
                        "_id": "6528fc319474946b8541b36f",
                        "avatarUrl": "/avatars/08ea388cbcd7c0f1361980127a8d33c3.svg",
                        "isPro": false,
                        "fullname": "Xiaohang Zhan",
                        "user": "xhangzhan",
                        "type": "user"
                    },
                    "name": "Xiaohang Zhan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:08:05.983Z",
                    "hidden": false
                },
                {
                    "_id": "67e226039cd910bee045e394",
                    "user": {
                        "_id": "66c8131afafc0fc87ca99650",
                        "avatarUrl": "/avatars/a6eeba2ccf011d5c9964fd38f85bd671.svg",
                        "isPro": false,
                        "fullname": "Yueqi Duan",
                        "user": "duanyueqi",
                        "type": "user"
                    },
                    "name": "Yueqi Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:08:11.759Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:59:04.000Z",
            "submittedOnDailyAt": "2025-03-25T02:12:44.893Z",
            "title": "Video-T1: Test-Time Scaling for Video Generation",
            "submittedOnDailyBy": {
                "_id": "6505a02f9310ce8c400edc63",
                "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                "isPro": false,
                "fullname": "Fangfu Liu",
                "user": "Liuff23",
                "type": "user"
            },
            "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
            "upvotes": 51,
            "discussionId": "67e226059cd910bee045e42b",
            "projectPage": "https://liuff19.github.io/Video-T1/",
            "githubRepo": "https://github.com/liuff19/Video-T1",
            "ai_keywords": [
                "Test-Time Scaling (TTS)",
                "video foundation models",
                "inference-time computation",
                "Gaussian noise space",
                "target video distribution",
                "test-time verifiers",
                "heuristic algorithms",
                "linear search strategy",
                "noise candidates",
                "full-step denoising",
                "inference time",
                "Tree-of-Frames (ToF)",
                "autoregressive manner",
                "text-conditioned video generation benchmarks"
            ]
        },
        "publishedAt": "2025-03-24T13:59:04.000Z",
        "title": "Video-T1: Test-Time Scaling for Video Generation",
        "summary": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18942.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6505a02f9310ce8c400edc63",
            "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
            "fullname": "Fangfu Liu",
            "name": "Liuff23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18892",
            "authors": [
                {
                    "_id": "67e22ce1155ea10f2fdbe5d2",
                    "user": {
                        "_id": "62751082b43ccfeef483424f",
                        "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
                        "isPro": false,
                        "fullname": "WeihaoZeng",
                        "user": "AndrewZeng",
                        "type": "user"
                    },
                    "name": "Weihao Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:18.367Z",
                    "hidden": false
                },
                {
                    "_id": "67e22ce1155ea10f2fdbe5d3",
                    "user": {
                        "_id": "6462def82a83863b97c0611e",
                        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
                        "isPro": false,
                        "fullname": "Yuzhen Huang",
                        "user": "yuzhen17",
                        "type": "user"
                    },
                    "name": "Yuzhen Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:13.781Z",
                    "hidden": false
                },
                {
                    "_id": "67e22ce1155ea10f2fdbe5d4",
                    "user": {
                        "_id": "612ee6a7b960e78c6d2319d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:15.927Z",
                    "hidden": false
                },
                {
                    "_id": "67e22ce1155ea10f2fdbe5d5",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "67e22ce1155ea10f2fdbe5d6",
                    "user": {
                        "_id": "64bf71792915a87970c07446",
                        "avatarUrl": "/avatars/b24403f9fa699e0143e441b56528e6af.svg",
                        "isPro": false,
                        "fullname": "Keqing He",
                        "user": "HelicHe",
                        "type": "user"
                    },
                    "name": "Keqing He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:17:09.770Z",
                    "hidden": false
                },
                {
                    "_id": "67e22ce1155ea10f2fdbe5d7",
                    "name": "Zejun Ma",
                    "hidden": false
                },
                {
                    "_id": "67e22ce1155ea10f2fdbe5d8",
                    "user": {
                        "_id": "615f34ec3f6d24d67c1b5c78",
                        "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
                        "isPro": false,
                        "fullname": "Junxian He",
                        "user": "jxhe",
                        "type": "user"
                    },
                    "name": "Junxian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:16:49.208Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:06:10.000Z",
            "submittedOnDailyAt": "2025-03-25T02:41:40.812Z",
            "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
            "submittedOnDailyBy": {
                "_id": "62751082b43ccfeef483424f",
                "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
                "isPro": false,
                "fullname": "WeihaoZeng",
                "user": "AndrewZeng",
                "type": "user"
            },
            "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
            "upvotes": 23,
            "discussionId": "67e22ce3155ea10f2fdbe6c0",
            "githubRepo": "https://github.com/hkust-nlp/simpleRL-reason",
            "ai_keywords": [
                "reinforcement learning",
                "rule-based rewards",
                "zero RL training",
                "long chain-of-thought (CoT) reasoning",
                "instruction-following",
                "self-reflection",
                "base models",
                "Qwen2.5 model series",
                "LLama3-8B",
                "Mistral-7B/24B",
                "DeepSeek-Math-7B",
                "Qwen2.5-math-7B",
                "response length",
                "reasoning accuracy",
                "cognitive behaviors",
                "verification",
                "training dynamics"
            ]
        },
        "publishedAt": "2025-03-24T13:06:10.000Z",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild",
        "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18892.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62751082b43ccfeef483424f",
            "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
            "fullname": "WeihaoZeng",
            "name": "AndrewZeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18945",
            "authors": [
                {
                    "_id": "67e22eca9455abdd1d257263",
                    "name": "Aether Team",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d257264",
                    "user": {
                        "_id": "6283546209aa80237c6c482c",
                        "avatarUrl": "/avatars/0d6fc5846c0456d5282d82d5bf4d7056.svg",
                        "isPro": false,
                        "fullname": "Haoyi Zhu",
                        "user": "HaoyiZhu",
                        "type": "user"
                    },
                    "name": "Haoyi Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:15:13.586Z",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d257265",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d257266",
                    "user": {
                        "_id": "667e81565934c9fae29207ef",
                        "avatarUrl": "/avatars/431e777c71fccf7cf48ce013e5f6f1cb.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "ZhouTimeMachine",
                        "type": "user"
                    },
                    "name": "Jianjun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T09:05:54.719Z",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d257267",
                    "user": {
                        "_id": "67a5b0fe5a8652514e67c38c",
                        "avatarUrl": "/avatars/28da8e93ee00fd77c7e62d16f9b94045.svg",
                        "isPro": false,
                        "fullname": "Wenzheng Chang",
                        "user": "AmberHeart",
                        "type": "user"
                    },
                    "name": "Wenzheng Chang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:10.947Z",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d257268",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d257269",
                    "user": {
                        "_id": "65e7eb86c7a0617cc71d3df4",
                        "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
                        "isPro": false,
                        "fullname": "lizizun",
                        "user": "lizizun",
                        "type": "user"
                    },
                    "name": "Zizun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:16:06.912Z",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d25726a",
                    "user": {
                        "_id": "6679bb85972a0f224cde335c",
                        "avatarUrl": "/avatars/bf0649645458e206ba5224b001723641.svg",
                        "isPro": false,
                        "fullname": "Junyi Chen",
                        "user": "Junyichen",
                        "type": "user"
                    },
                    "name": "Junyi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:16:13.751Z",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d25726b",
                    "name": "Chunhua Shen",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d25726c",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:16:26.855Z",
                    "hidden": false
                },
                {
                    "_id": "67e22eca9455abdd1d25726d",
                    "user": {
                        "_id": "64478c64e2148488340229db",
                        "avatarUrl": "/avatars/f5c23489a068e896381cdc25836ce3dd.svg",
                        "isPro": false,
                        "fullname": "he",
                        "user": "tonghe",
                        "type": "user"
                    },
                    "name": "Tong He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:16:35.824Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:59:51.000Z",
            "submittedOnDailyAt": "2025-03-25T02:50:34.610Z",
            "title": "Aether: Geometric-Aware Unified World Modeling",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
            "upvotes": 22,
            "discussionId": "67e22ecb9455abdd1d2572af",
            "projectPage": "https://aether-world.github.io/",
            "githubRepo": "https://github.com/OpenRobotLab/Aether",
            "ai_keywords": [
                "Aether",
                "4D dynamic reconstruction",
                "action-conditioned video prediction",
                "goal-conditioned visual planning",
                "task-interleaved feature learning",
                "video generation models",
                "synthetic-to-real generalization",
                "zero-shot generalization",
                "geometric modeling",
                "geometry-informed action space",
                "autonomous trajectory planning",
                "physically-reasonable world modeling"
            ]
        },
        "publishedAt": "2025-03-24T13:59:51.000Z",
        "title": "Aether: Geometric-Aware Unified World Modeling",
        "summary": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance far\nexceeds that of domain-specific models. Additionally, Aether leverages a\ngeometry-informed action space to seamlessly translate predictions into\nactions, enabling effective autonomous trajectory planning. We hope our work\ninspires the community to explore new frontiers in physically-reasonable world\nmodeling and its applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18945.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6463
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18033",
            "authors": [
                {
                    "_id": "67e2706af6cf2764a534d4a5",
                    "user": {
                        "_id": "630f0d48982455e61cc4cc08",
                        "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
                        "isPro": false,
                        "fullname": "Samuel",
                        "user": "Dvir",
                        "type": "user"
                    },
                    "name": "Dvir Samuel",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-25T09:00:01.542Z",
                    "hidden": false
                },
                {
                    "_id": "67e2706af6cf2764a534d4a6",
                    "user": {
                        "_id": "66633be10875aaaa9153c963",
                        "avatarUrl": "/avatars/f47aaaf7b029ad3e99f49676a8f9a479.svg",
                        "isPro": false,
                        "fullname": "Matan Levy",
                        "user": "m98levy",
                        "type": "user"
                    },
                    "name": "Matan Levy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:25:56.024Z",
                    "hidden": false
                },
                {
                    "_id": "67e2706af6cf2764a534d4a7",
                    "name": "Nir Darshan",
                    "hidden": false
                },
                {
                    "_id": "67e2706af6cf2764a534d4a8",
                    "user": {
                        "_id": "6493393f357b252af72196c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
                        "isPro": false,
                        "fullname": "Gal Chechik",
                        "user": "galchechik",
                        "type": "user"
                    },
                    "name": "Gal Chechik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:26:05.156Z",
                    "hidden": false
                },
                {
                    "_id": "67e2706af6cf2764a534d4a9",
                    "user": {
                        "_id": "64c5f22c2581696666ebed88",
                        "avatarUrl": "/avatars/e85cd2d82f16ec10cad2b63929b2f05a.svg",
                        "isPro": false,
                        "fullname": "Rami Ben-Ari",
                        "user": "ramiben",
                        "type": "user"
                    },
                    "name": "Rami Ben-Ari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:26:11.853Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-23T11:26:48.000Z",
            "submittedOnDailyAt": "2025-03-25T07:31:06.785Z",
            "title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "630f0d48982455e61cc4cc08",
                "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
                "isPro": false,
                "fullname": "Samuel",
                "user": "Dvir",
                "type": "user"
            },
            "summary": "Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime.",
            "upvotes": 20,
            "discussionId": "67e2706df6cf2764a534d570",
            "projectPage": "https://dvirsamuel.github.io/omnimattezero.github.io/",
            "ai_keywords": [
                "diffusion models",
                "zero-shot image inpainting",
                "self-attention maps",
                "latent arithmetic",
                "real-time performance",
                "frame runtime"
            ]
        },
        "publishedAt": "2025-03-23T07:26:48.000Z",
        "title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models",
        "summary": "Omnimatte aims to decompose a given video into semantically meaningful\nlayers, including the background and individual objects along with their\nassociated effects, such as shadows and reflections. Existing methods often\nrequire extensive training or costly self-supervised optimization. In this\npaper, we present OmnimatteZero, a training-free approach that leverages\noff-the-shelf pre-trained video diffusion models for omnimatte. It can remove\nobjects from videos, extract individual object layers along with their effects,\nand composite those objects onto new videos. We accomplish this by adapting\nzero-shot image inpainting techniques for video object removal, a task they\nfail to handle effectively out-of-the-box. We then show that self-attention\nmaps capture information about the object and its footprints and use them to\ninpaint the object's effects, leaving a clean background. Additionally, through\nsimple latent arithmetic, object layers can be isolated and recombined\nseamlessly with new video layers to produce new videos. Evaluations show that\nOmnimatteZero not only achieves superior performance in terms of background\nreconstruction but also sets a new record for the fastest Omnimatte approach,\nachieving real-time performance with minimal frame runtime.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18033.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630f0d48982455e61cc4cc08",
            "avatarUrl": "/avatars/eea6ed2e112e830effa98a4661c5474f.svg",
            "fullname": "Samuel",
            "name": "Dvir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17489",
            "authors": [
                {
                    "_id": "67e21f300e6b6fcc3eb38ae1",
                    "name": "Shu Pu",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae2",
                    "name": "Yaochen Wang",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae3",
                    "user": {
                        "_id": "65e2be1e630e2db23829ee8d",
                        "avatarUrl": "/avatars/294f9ba909037f03669dc0bb80cabfe3.svg",
                        "isPro": false,
                        "fullname": "Dongping Chen",
                        "user": "fjchendp",
                        "type": "user"
                    },
                    "name": "Dongping Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:20:55.986Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae4",
                    "user": {
                        "_id": "64964aae457f60023c6a6f9d",
                        "avatarUrl": "/avatars/342603e0028204f33fe7f5e3f3da1aa3.svg",
                        "isPro": false,
                        "fullname": "Yuhang Chen",
                        "user": "yuhangchen",
                        "type": "user"
                    },
                    "name": "Yuhang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:21:02.911Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae5",
                    "user": {
                        "_id": "67c94fd48670a35a7c05f36c",
                        "avatarUrl": "/avatars/a59a7872bcc58fec7747225f2d3da3f9.svg",
                        "isPro": false,
                        "fullname": "Guohao Wang",
                        "user": "NiuniuWang",
                        "type": "user"
                    },
                    "name": "Guohao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:21:09.345Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae6",
                    "name": "Qi Qin",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae7",
                    "name": "Zhongyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae8",
                    "name": "Zhiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38ae9",
                    "user": {
                        "_id": "6697e7e55ef2828a1ff371c3",
                        "avatarUrl": "/avatars/b361ea817760f7cb5c5d39028ee6b507.svg",
                        "isPro": false,
                        "fullname": "Zetong Zhou",
                        "user": "Frywind",
                        "type": "user"
                    },
                    "name": "Zetong Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:21:29.118Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38aea",
                    "user": {
                        "_id": "67575cac2f7acf9a8b4626fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1OkoZh8A4jPKHpTg5iSXP.png",
                        "isPro": false,
                        "fullname": "Shuang Gong",
                        "user": "shuang72",
                        "type": "user"
                    },
                    "name": "Shuang Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:21:35.465Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38aeb",
                    "name": "Yi Gui",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38aec",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "67e21f300e6b6fcc3eb38aed",
                    "name": "Philip S. Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T18:59:20.000Z",
            "submittedOnDailyAt": "2025-03-25T01:46:07.985Z",
            "title": "Judge Anything: MLLM as a Judge Across Any Modality",
            "submittedOnDailyBy": {
                "_id": "643be8879f5d314db2d9ed23",
                "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
                "isPro": false,
                "fullname": "Chen Dongping",
                "user": "shuaishuaicdp",
                "type": "user"
            },
            "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
            "upvotes": 16,
            "discussionId": "67e21f350e6b6fcc3eb38c35",
            "ai_keywords": [
                "Multimodal LLMs (MLLMs)",
                "TaskAnything",
                "JudgeAnything",
                "open-ended multimodal understanding (MMU)",
                "open-ended multimodal generation (MMG)",
                "cross-modal interactions",
                "vision-language understanding tasks",
                "any-to-any modality tasks",
                "Pair Comparison",
                "Score Evaluation",
                "omni-models",
                "multimodal reward models",
                "cross-modality biases",
                "hallucination issues"
            ]
        },
        "publishedAt": "2025-03-21T14:59:20.000Z",
        "title": "Judge Anything: MLLM as a Judge Across Any Modality",
        "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17489.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18908",
            "authors": [
                {
                    "_id": "67e230fd4b9f234b60d06389",
                    "user": {
                        "_id": "66857bd849a4ed9de4c31936",
                        "avatarUrl": "/avatars/f6f016bf36fad5b29f30fbec6cde3e4d.svg",
                        "isPro": false,
                        "fullname": "Akhiad Bercovich",
                        "user": "abercovich",
                        "type": "user"
                    },
                    "name": "Akhiad Bercovich",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:30:49.482Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0638a",
                    "user": {
                        "_id": "6756aa3741b39ab0d327de52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VjjYWljIgPn9HEMDyKtft.png",
                        "isPro": false,
                        "fullname": "Mohammad Dabbah",
                        "user": "mdabbah-nvidia",
                        "type": "user"
                    },
                    "name": "Mohammad Dabbah",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:31:05.696Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0638b",
                    "user": {
                        "_id": "6509a96c61c4bb4636fd0fd2",
                        "avatarUrl": "/avatars/8ffa9b4dd698469f7d70d4d9144aac82.svg",
                        "isPro": false,
                        "fullname": "Omri Puny",
                        "user": "omripuny",
                        "type": "user"
                    },
                    "name": "Omri Puny",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:31:13.161Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0638c",
                    "name": "Ido Galil",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0638d",
                    "user": {
                        "_id": "65006cac12c1442d993d6d51",
                        "avatarUrl": "/avatars/6700109303b902d453f3d8e2b45a103f.svg",
                        "isPro": false,
                        "fullname": "Geifman",
                        "user": "AmnonGeifman",
                        "type": "user"
                    },
                    "name": "Amnon Geifman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:31:22.938Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0638e",
                    "user": {
                        "_id": "604bc69e0fe8ff3ec13d71cd",
                        "avatarUrl": "/avatars/fe4b14b24befdbed02eecb43a25c67f4.svg",
                        "isPro": false,
                        "fullname": "Yonatan Geifman",
                        "user": "geifmany",
                        "type": "user"
                    },
                    "name": "Yonatan Geifman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:31:30.392Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0638f",
                    "name": "Izhak Golan",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06390",
                    "name": "Ehud Karpas",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06391",
                    "user": {
                        "_id": "668578fdd24e614fec97eac8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668578fdd24e614fec97eac8/n5xYnqo5nQbVX2tgaRfEi.jpeg",
                        "isPro": false,
                        "fullname": "Itay Levy",
                        "user": "itlevy",
                        "type": "user"
                    },
                    "name": "Itay Levy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:31:50.928Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06392",
                    "user": {
                        "_id": "61ee58f1af500c0acfc4d8eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643010228464-noauth.png",
                        "isPro": false,
                        "fullname": "Zach Moshe",
                        "user": "zachmoshe",
                        "type": "user"
                    },
                    "name": "Zach Moshe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:31:56.910Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06393",
                    "user": {
                        "_id": "63a16d5d5d09b819fee9a350",
                        "avatarUrl": "/avatars/d1a3fef0131688e92e272cbd80856fc3.svg",
                        "isPro": false,
                        "fullname": "Najeeb Nabwani",
                        "user": "NajeebDeci",
                        "type": "user"
                    },
                    "name": "Najeeb Nabwani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:02.613Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06394",
                    "user": {
                        "_id": "6671634f1820f293a9995b12",
                        "avatarUrl": "/avatars/50c8f7b4bfb00f2169b808f3c72c7686.svg",
                        "isPro": false,
                        "fullname": "Tomer Ronen",
                        "user": "tomer-nv",
                        "type": "user"
                    },
                    "name": "Tomer Ronen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:10.072Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06395",
                    "user": {
                        "_id": "665f0a46f065b1d42806000d",
                        "avatarUrl": "/avatars/927f042a3c95c5846621e2a381c66bbf.svg",
                        "isPro": false,
                        "fullname": "Itamar Schen",
                        "user": "ischen-nvidia",
                        "type": "user"
                    },
                    "name": "Itamar Schen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:17.605Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06396",
                    "user": {
                        "_id": "5f5b0efe10b2753d9000c888",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628140531144-5f5b0efe10b2753d9000c888.jpeg",
                        "isPro": false,
                        "fullname": "Elad Segal",
                        "user": "eladsegal",
                        "type": "user"
                    },
                    "name": "Elad Segal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:24.511Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06397",
                    "user": {
                        "_id": "666ef13c14f1c262feeb706c",
                        "avatarUrl": "/avatars/7dca59acf5e069d96bdbb98dace9199b.svg",
                        "isPro": false,
                        "fullname": "Ido Shahaf",
                        "user": "ishahaf",
                        "type": "user"
                    },
                    "name": "Ido Shahaf",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:30.927Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06398",
                    "user": {
                        "_id": "66b089f14ae4ae811218cdb6",
                        "avatarUrl": "/avatars/a50fe725922dfdbe0e731fade381b22e.svg",
                        "isPro": false,
                        "fullname": "Oren Tropp",
                        "user": "otropp",
                        "type": "user"
                    },
                    "name": "Oren Tropp",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:38.161Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d06399",
                    "user": {
                        "_id": "666027917c3f9c72113cc75c",
                        "avatarUrl": "/avatars/a276ebe8e2731b6a05e3c61c2ae0ddae.svg",
                        "isPro": false,
                        "fullname": "Ran Zilberstein",
                        "user": "RanZilberstein-Nvidia",
                        "type": "user"
                    },
                    "name": "Ran Zilberstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:44.768Z",
                    "hidden": false
                },
                {
                    "_id": "67e230fd4b9f234b60d0639a",
                    "user": {
                        "_id": "65758349983403462a54ac06",
                        "avatarUrl": "/avatars/4f337c732f31bd748738c2717b50a99c.svg",
                        "isPro": false,
                        "fullname": "Ran El-Yaniv",
                        "user": "ranielyaniv",
                        "type": "user"
                    },
                    "name": "Ran El-Yaniv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:32:56.726Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:20:35.000Z",
            "submittedOnDailyAt": "2025-03-25T02:59:12.174Z",
            "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
            "upvotes": 14,
            "discussionId": "67e230fe4b9f234b60d063ec",
            "ai_keywords": [
                "FFN Fusion",
                "Feed-Forward Network (FFN)",
                "parallelization",
                "inference latency",
                "Llama-3.1-405B-Instruct",
                "Ultra-253B-Base",
                "model behavior",
                "per-token cost",
                "benchmarks",
                "transformer blocks",
                "quantization",
                "pruning"
            ]
        },
        "publishedAt": "2025-03-24T13:20:35.000Z",
        "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
        "summary": "We introduce FFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities for parallelization. Our key insight is that sequences of\nFeed-Forward Network (FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduce inference latency while preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup in inference latency and 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques like quantization and pruning. Most\nintriguingly, we find that even full transformer blocks containing both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18908.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6463
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18813",
            "authors": [
                {
                    "_id": "67e24a997210beea5ecae330",
                    "user": {
                        "_id": "631dd96f6d6a5870f3d42528",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631dd96f6d6a5870f3d42528/YMsxboRfIBBGE-z-_DeNx.jpeg",
                        "isPro": false,
                        "fullname": "Edoardo Debenedetti",
                        "user": "dedeswim",
                        "type": "user"
                    },
                    "name": "Edoardo Debenedetti",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:26:38.111Z",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae331",
                    "user": {
                        "_id": "6475c2794766357252e69e9f",
                        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                        "isPro": false,
                        "fullname": "i",
                        "user": "iliashum",
                        "type": "user"
                    },
                    "name": "Ilia Shumailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:27:10.362Z",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae332",
                    "name": "Tianqi Fan",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae333",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae334",
                    "user": {
                        "_id": "6303fa9ba362e7e8b51d8f2a",
                        "avatarUrl": "/avatars/53e53c84f987989deb351dd2ae6ee558.svg",
                        "isPro": false,
                        "fullname": "Nicholas Carlini",
                        "user": "carlini",
                        "type": "user"
                    },
                    "name": "Nicholas Carlini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:27:26.752Z",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae335",
                    "name": "Daniel Fabian",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae336",
                    "name": "Christoph Kern",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae337",
                    "name": "Chongyang Shi",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae338",
                    "name": "Andreas Terzis",
                    "hidden": false
                },
                {
                    "_id": "67e24a997210beea5ecae339",
                    "user": {
                        "_id": "63568f18ba90b4ea9fe91cb5",
                        "avatarUrl": "/avatars/3e8b3c573e20cf80d329a312bfc34728.svg",
                        "isPro": false,
                        "fullname": "Florian Tramer",
                        "user": "ftramer",
                        "type": "user"
                    },
                    "name": "Florian Tramèr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:27:51.364Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T15:54:10.000Z",
            "submittedOnDailyAt": "2025-03-25T04:48:52.478Z",
            "title": "Defeating Prompt Injections by Design",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
            "upvotes": 14,
            "discussionId": "67e24a9b7210beea5ecae3a0",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "prompt injection attacks",
                "protective system layer",
                "trusted query",
                "untrusted data",
                "program flow",
                "capability",
                "private data exfiltration",
                "unauthorized data flows",
                "AgentDojo"
            ]
        },
        "publishedAt": "2025-03-24T11:54:10.000Z",
        "title": "Defeating Prompt Injections by Design",
        "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18813.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18102",
            "authors": [
                {
                    "_id": "67e22206ddc9b120cbde6fbe",
                    "name": "Samuel Schmidgall",
                    "hidden": false
                },
                {
                    "_id": "67e22206ddc9b120cbde6fbf",
                    "user": {
                        "_id": "6438d1d843d932c462404500",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
                        "isPro": false,
                        "fullname": "Michael Moor",
                        "user": "mdmoor",
                        "type": "user"
                    },
                    "name": "Michael Moor",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:34:00.877Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-23T15:16:42.000Z",
            "submittedOnDailyAt": "2025-03-25T05:09:36.523Z",
            "title": "AgentRxiv: Towards Collaborative Autonomous Research",
            "submittedOnDailyBy": {
                "_id": "6438d1d843d932c462404500",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
                "isPro": false,
                "fullname": "Michael Moor",
                "user": "mdmoor",
                "type": "user"
            },
            "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
            "upvotes": 14,
            "discussionId": "67e22207ddc9b120cbde702c",
            "projectPage": "https://agentrxiv.github.io/",
            "ai_keywords": [
                "LLM (Large Language Model)",
                "agent laboratories",
                "preprint server",
                "reasoning techniques",
                "prompting techniques",
                "performance improvements",
                "benchmarks",
                "accuracy"
            ]
        },
        "publishedAt": "2025-03-23T11:16:42.000Z",
        "title": "AgentRxiv: Towards Collaborative Autonomous Research",
        "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18102.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "fullname": "Michael Moor",
            "name": "mdmoor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18013",
            "authors": [
                {
                    "_id": "67e22902af6628c90b525a2b",
                    "name": "Yufei Zhan",
                    "hidden": false
                },
                {
                    "_id": "67e22902af6628c90b525a2c",
                    "name": "Yousong Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e22902af6628c90b525a2d",
                    "name": "Shurong Zheng",
                    "hidden": false
                },
                {
                    "_id": "67e22902af6628c90b525a2e",
                    "name": "Hongyin Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e22902af6628c90b525a2f",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "67e22902af6628c90b525a30",
                    "name": "Ming Tang",
                    "hidden": false
                },
                {
                    "_id": "67e22902af6628c90b525a31",
                    "name": "Jinqiao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-23T10:21:14.000Z",
            "submittedOnDailyAt": "2025-03-25T02:25:32.811Z",
            "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
            "upvotes": 13,
            "discussionId": "67e22903af6628c90b525a71",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "pretraining",
                "supervised fine-tuning",
                "preference optimization",
                "reinforcement learning",
                "Vision-R1",
                "criterion-driven reward function",
                "progressive rule refinement strategy",
                "reward criteria",
                "model completions",
                "vision task logic",
                "reward hacking",
                "state-of-the-art"
            ]
        },
        "publishedAt": "2025-03-23T06:21:14.000Z",
        "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning",
        "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18013.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6463
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17439",
            "authors": [
                {
                    "_id": "67e21f63fb4213c53714be08",
                    "user": {
                        "_id": "6565e24fe5aac326bfd15a9d",
                        "avatarUrl": "/avatars/28ad90df0e0dbc10ef25ee6499a50dec.svg",
                        "isPro": false,
                        "fullname": "Zhuoshi Pan",
                        "user": "panzs",
                        "type": "user"
                    },
                    "name": "Zhuoshi Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:17:23.682Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be09",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be0a",
                    "user": {
                        "_id": "640d99628512ec51d7ef71c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg",
                        "isPro": false,
                        "fullname": "Honglin Lin",
                        "user": "LHL3341",
                        "type": "user"
                    },
                    "name": "Honglin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:17:30.635Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be0b",
                    "user": {
                        "_id": "6397f6081323f19c578f142e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                        "isPro": false,
                        "fullname": "QizhiPei",
                        "user": "QizhiPei",
                        "type": "user"
                    },
                    "name": "Qizhi Pei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:17:42.836Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be0c",
                    "user": {
                        "_id": "66580d3d80ee5b1e11a94e57",
                        "avatarUrl": "/avatars/1a88e7337f9095c40c6d402fab797d83.svg",
                        "isPro": false,
                        "fullname": "Zinan Tang",
                        "user": "Word2Li",
                        "type": "user"
                    },
                    "name": "Zinan Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:17:50.791Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be0d",
                    "name": "Wei Wu",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be0e",
                    "user": {
                        "_id": "677e133ee86d0754dc7ce296",
                        "avatarUrl": "/avatars/c16511c1876b50c2d049925c5f320d15.svg",
                        "isPro": false,
                        "fullname": "mingchenlin",
                        "user": "mingchenlin2025",
                        "type": "user"
                    },
                    "name": "Chenlin Ming",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:18:12.485Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be0f",
                    "name": "H. Vicky Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be10",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:18:25.368Z",
                    "hidden": false
                },
                {
                    "_id": "67e21f63fb4213c53714be11",
                    "name": "Lijun Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T17:59:10.000Z",
            "submittedOnDailyAt": "2025-03-25T01:47:52.213Z",
            "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
            "submittedOnDailyBy": {
                "_id": "6397f6081323f19c578f142e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                "isPro": false,
                "fullname": "QizhiPei",
                "user": "QizhiPei",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
            "upvotes": 13,
            "discussionId": "67e21f64fb4213c53714be6b",
            "githubRepo": "https://github.com/pzs19/LEMMA",
            "ai_keywords": [
                "Learning from Errors for Mathematical Advancement (LEMMA)",
                "mistake augmentation",
                "model-aware smooth reflection connection",
                "autonomous error correction"
            ]
        },
        "publishedAt": "2025-03-21T13:59:10.000Z",
        "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs",
        "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17439.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "fullname": "QizhiPei",
            "name": "QizhiPei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18940",
            "authors": [
                {
                    "_id": "67e21b305d20ec3277dac34a",
                    "user": {
                        "_id": "64e357dd825f4133e7427bf8",
                        "avatarUrl": "/avatars/aeb6869d075f65a581797df2aabfb02f.svg",
                        "isPro": false,
                        "fullname": "tyfeld",
                        "user": "tyfeld",
                        "type": "user"
                    },
                    "name": "Ye Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:24.659Z",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac34b",
                    "name": "Xin Xia",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac34c",
                    "user": {
                        "_id": "6618d5e83b412cdc85334ca8",
                        "avatarUrl": "/avatars/5fe356d58c4c822a60370dbee8d78a69.svg",
                        "isPro": false,
                        "fullname": "renyuxi",
                        "user": "renyuxi",
                        "type": "user"
                    },
                    "name": "Yuxi Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:19:25.116Z",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac34d",
                    "name": "Shanchuan Lin",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac34e",
                    "name": "Xing Wang",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac34f",
                    "user": {
                        "_id": "646b7f71df2609a541c1ab9f",
                        "avatarUrl": "/avatars/48b82e5fd9b06f41ff825507c36816cd.svg",
                        "isPro": false,
                        "fullname": "Xuefeng Xiao",
                        "user": "xiaoxuefeng",
                        "type": "user"
                    },
                    "name": "Xuefeng Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:19:05.020Z",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac350",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac351",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:19:44.463Z",
                    "hidden": false
                },
                {
                    "_id": "67e21b305d20ec3277dac352",
                    "user": {
                        "_id": "67b2795f0bd4ddcd84426bb4",
                        "avatarUrl": "/avatars/d4346ac5a0ebbaeb828d832cc6ca9f0b.svg",
                        "isPro": false,
                        "fullname": "Bin Cui",
                        "user": "lazybone128",
                        "type": "user"
                    },
                    "name": "Bin Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:18:48.809Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:59:02.000Z",
            "submittedOnDailyAt": "2025-03-25T01:26:48.606Z",
            "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
            "upvotes": 12,
            "discussionId": "67e21b365d20ec3277dac500",
            "projectPage": "https://tyfeld.github.io/BottleneckSampling.github.io",
            "githubRepo": "https://github.com/tyfeld/Bottleneck-Sampling",
            "ai_keywords": [
                "diffusion models",
                "self-attention",
                "computational overhead",
                "low-resolution priors",
                "Bottleneck Sampling",
                "denoising workflow",
                "high-resolution denoising",
                "aliasing",
                "blurring artifacts",
                "resolution transition points",
                "adaptive timesteps"
            ]
        },
        "publishedAt": "2025-03-24T13:59:02.000Z",
        "title": "Training-free Diffusion Acceleration with Bottleneck Sampling",
        "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3times for image generation and 2.5times for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18940.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18886",
            "authors": [
                {
                    "_id": "67e21d3484513315a9169aae",
                    "user": {
                        "_id": "6481764e8af4675862efb22e",
                        "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
                        "isPro": true,
                        "fullname": "weichenfan",
                        "user": "weepiess2383",
                        "type": "user"
                    },
                    "name": "Weichen Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:22.659Z",
                    "hidden": false
                },
                {
                    "_id": "67e21d3484513315a9169aaf",
                    "user": {
                        "_id": "6429ac8e8136224fee087253",
                        "avatarUrl": "/avatars/f7e4f0885e8c4a90c75c4e36ae3fba6e.svg",
                        "isPro": false,
                        "fullname": "Amber Yijia Zheng",
                        "user": "amberyzheng",
                        "type": "user"
                    },
                    "name": "Amber Yijia Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T14:36:26.742Z",
                    "hidden": false
                },
                {
                    "_id": "67e21d3484513315a9169ab0",
                    "name": "Raymond A. Yeh",
                    "hidden": false
                },
                {
                    "_id": "67e21d3484513315a9169ab1",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T16:59:57.000Z",
            "submittedOnDailyAt": "2025-03-25T06:34:48.098Z",
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "submittedOnDailyBy": {
                "_id": "6481764e8af4675862efb22e",
                "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
                "isPro": true,
                "fullname": "weichenfan",
                "user": "weepiess2383",
                "type": "user"
            },
            "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)",
            "upvotes": 12,
            "discussionId": "67e21d3884513315a9169bba",
            "projectPage": "https://weichenfan.github.io/webpage-cfg-zero-star/",
            "githubRepo": "https://github.com/WeichenFan/CFG-Zero-star",
            "ai_keywords": [
                "Classifier-Free Guidance (CFG)",
                "diffusion/flow models",
                "image fidelity",
                "controllability",
                "flow matching models",
                "Gaussian mixtures",
                "ground-truth flow",
                "flow estimation",
                "estimated velocity",
                "scalar optimization",
                "ODE solver",
                "text-to-image",
                "Lumina-Next",
                "Stable Diffusion 3",
                "Flux",
                "text-to-video",
                "Wan-2.1",
                "CFG-Zero*"
            ]
        },
        "publishedAt": "2025-03-24T12:59:57.000Z",
        "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
        "summary": "Classifier-Free Guidance (CFG) is a widely adopted technique in\ndiffusion/flow models to improve image fidelity and controllability. In this\nwork, we first analytically study the effect of CFG on flow matching models\ntrained on Gaussian mixtures where the ground-truth flow can be derived. We\nobserve that in the early stages of training, when the flow estimation is\ninaccurate, CFG directs samples toward incorrect trajectories. Building on this\nobservation, we propose CFG-Zero*, an improved CFG with two contributions: (a)\noptimized scale, where a scalar is optimized to correct for the inaccuracies in\nthe estimated velocity, hence the * in the name; and (b) zero-init, which\ninvolves zeroing out the first few steps of the ODE solver. Experiments on both\ntext-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video\n(Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG,\nhighlighting its effectiveness in guiding Flow Matching models. (Code is\navailable at github.com/WeichenFan/CFG-Zero-star)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18886.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6481764e8af4675862efb22e",
            "avatarUrl": "/avatars/fc2e076bc861693f598a528a068a696e.svg",
            "fullname": "weichenfan",
            "name": "weepiess2383",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18948",
            "authors": [
                {
                    "_id": "67e24217db11e1d382285cd4",
                    "user": {
                        "_id": "6447a5806ffed6ece1fcf723",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NjOA7G_QCa3bCluA69hSs.jpeg",
                        "isPro": false,
                        "fullname": "Ruixiao Dong",
                        "user": "dongruixiao",
                        "type": "user"
                    },
                    "name": "Ruixiao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:20:32.045Z",
                    "hidden": false
                },
                {
                    "_id": "67e24217db11e1d382285cd5",
                    "user": {
                        "_id": "63f5993afcf95ecac2b419b5",
                        "avatarUrl": "/avatars/a8c020080a84d9a663789c4fb19270e9.svg",
                        "isPro": false,
                        "fullname": "Mengde Xu",
                        "user": "Mendel192",
                        "type": "user"
                    },
                    "name": "Mengde Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:20:25.156Z",
                    "hidden": false
                },
                {
                    "_id": "67e24217db11e1d382285cd6",
                    "name": "Zigang Geng",
                    "hidden": false
                },
                {
                    "_id": "67e24217db11e1d382285cd7",
                    "name": "Li Li",
                    "hidden": false
                },
                {
                    "_id": "67e24217db11e1d382285cd8",
                    "user": {
                        "_id": "665d88640e92f92b0e7eb17f",
                        "avatarUrl": "/avatars/ff3a410e1e7bfb00ff0ec8ce4d5b1463.svg",
                        "isPro": false,
                        "fullname": "han hu",
                        "user": "hanhu2",
                        "type": "user"
                    },
                    "name": "Han Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:20:10.271Z",
                    "hidden": false
                },
                {
                    "_id": "67e24217db11e1d382285cd9",
                    "name": "Shuyang Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:59:57.000Z",
            "submittedOnDailyAt": "2025-03-25T05:57:15.975Z",
            "title": "Equivariant Image Modeling",
            "submittedOnDailyBy": {
                "_id": "64c38fcf573c5a427e12cd37",
                "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
                "isPro": false,
                "fullname": "cientgu",
                "user": "cientgu",
                "type": "user"
            },
            "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
            "upvotes": 11,
            "discussionId": "67e2421edb11e1d382285f9b",
            "githubRepo": "https://github.com/drx-code/EquivariantModeling",
            "ai_keywords": [
                "autoregressive",
                "diffusion approaches",
                "high-dimensional data distribution learning",
                "subtasks",
                "joint optimization",
                "equivariant image modeling framework",
                "translation invariance",
                "column-wise tokenization",
                "translational symmetry",
                "windowed causal attention",
                "contextual relationships",
                "class-conditioned ImageNet generation",
                "state-of-the-art AR models",
                "computational resources",
                "enhanced equivariance",
                "zero-shot generalization",
                "ultra-long image synthesis",
                "task-aligned decomposition",
                "efficient parameter sharing",
                "conflict-free optimization"
            ]
        },
        "publishedAt": "2025-03-24T13:59:57.000Z",
        "title": "Equivariant Image Modeling",
        "summary": "Current generative models, such as autoregressive and diffusion approaches,\ndecompose high-dimensional data distribution learning into a series of simpler\nsubtasks. However, inherent conflicts arise during the joint optimization of\nthese subtasks, and existing solutions fail to resolve such conflicts without\nsacrificing efficiency or scalability. We propose a novel equivariant image\nmodeling framework that inherently aligns optimization targets across subtasks\nby leveraging the translation invariance of natural visual signals. Our method\nintroduces (1) column-wise tokenization which enhances translational symmetry\nalong the horizontal axis, and (2) windowed causal attention which enforces\nconsistent contextual relationships across positions. Evaluated on\nclass-conditioned ImageNet generation at 256x256 resolution, our approach\nachieves performance comparable to state-of-the-art AR models while using fewer\ncomputational resources. Systematic analysis demonstrates that enhanced\nequivariance reduces inter-task conflicts, significantly improving zero-shot\ngeneralization and enabling ultra-long image synthesis. This work establishes\nthe first framework for task-aligned decomposition in generative modeling,\noffering insights into efficient parameter sharing and conflict-free\noptimization. The code and models are publicly available at\nhttps://github.com/drx-code/EquivariantModeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18948.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c38fcf573c5a427e12cd37",
            "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
            "fullname": "cientgu",
            "name": "cientgu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18923",
            "authors": [
                {
                    "_id": "67e226f401cdb8cf3a1c7cd8",
                    "user": {
                        "_id": "640222f83e3d0f2745b097b2",
                        "avatarUrl": "/avatars/c5dbac84734855369a7f57b051f16caa.svg",
                        "isPro": false,
                        "fullname": "Meng Cao",
                        "user": "mengcao",
                        "type": "user"
                    },
                    "name": "Meng Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:22:38.738Z",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cd9",
                    "name": "Pengfei Hu",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cda",
                    "name": "Yingyao Wang",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cdb",
                    "user": {
                        "_id": "65733c1b244aefdfc45cc771",
                        "avatarUrl": "/avatars/7223cedbeed065c28a400e130cea30ae.svg",
                        "isPro": false,
                        "fullname": "Jihao Guo",
                        "user": "grejioh",
                        "type": "user"
                    },
                    "name": "Jihao Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:22:23.718Z",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cdc",
                    "name": "Haoran Tang",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cdd",
                    "name": "Haoze Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cde",
                    "name": "Jiahua Dong",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7cdf",
                    "user": {
                        "_id": "63f095be6309c84d5f48848a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
                        "isPro": false,
                        "fullname": "Wangbo Yu",
                        "user": "Drexubery",
                        "type": "user"
                    },
                    "name": "Wangbo Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:23:02.455Z",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7ce0",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:22:08.334Z",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7ce1",
                    "name": "Ian Reid",
                    "hidden": false
                },
                {
                    "_id": "67e226f401cdb8cf3a1c7ce2",
                    "name": "Xiaodan Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:46:09.000Z",
            "submittedOnDailyAt": "2025-03-25T02:17:41.453Z",
            "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
            "upvotes": 10,
            "discussionId": "67e226f601cdb8cf3a1c7d73",
            "projectPage": "https://videosimpleqa.github.io",
            "ai_keywords": [
                "Large Video Language Models (LVLMs)",
                "multi-modal understanding",
                "factuality evaluation",
                "Video SimpleQA",
                "external knowledge",
                "objective events",
                "relationships",
                "short-form answer",
                "LLM-as-a-judge",
                "automated evaluation",
                "scQUIre",
                "authoritative external references",
                "temporal reasoning",
                "long-context dependencies",
                "F-score",
                "test-time compute",
                "Retrieval-Augmented Generation",
                "inference time overhead",
                "efficiency-performance trade-off"
            ]
        },
        "publishedAt": "2025-03-24T13:46:09.000Z",
        "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models",
        "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18923.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17811",
            "authors": [
                {
                    "_id": "67e27583cf3845b431740913",
                    "user": {
                        "_id": "65e7d63b14856e8859f1924c",
                        "avatarUrl": "/avatars/bc58ab252c7b4d95ff99e4506fb8d3e9.svg",
                        "isPro": false,
                        "fullname": "Pei Wenqi",
                        "user": "CedPei",
                        "type": "user"
                    },
                    "name": "Wenqi Pei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T14:36:24.705Z",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b431740914",
                    "name": "Hailing Xu",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b431740915",
                    "name": "Hengyuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b431740916",
                    "name": "Shizheng Hou",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b431740917",
                    "user": {
                        "_id": "6399c67bf78f75ae73146760",
                        "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
                        "isPro": false,
                        "fullname": "CHEN Han",
                        "user": "Concyclics",
                        "type": "user"
                    },
                    "name": "Han Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T14:36:43.002Z",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b431740918",
                    "name": "Zining Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b431740919",
                    "name": "Pingyi Luo",
                    "hidden": false
                },
                {
                    "_id": "67e27583cf3845b43174091a",
                    "name": "Bingsheng He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-22T16:22:53.000Z",
            "submittedOnDailyAt": "2025-03-25T12:48:09.140Z",
            "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models",
            "submittedOnDailyBy": {
                "_id": "65e7d63b14856e8859f1924c",
                "avatarUrl": "/avatars/bc58ab252c7b4d95ff99e4506fb8d3e9.svg",
                "isPro": false,
                "fullname": "Pei Wenqi",
                "user": "CedPei",
                "type": "user"
            },
            "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness.",
            "upvotes": 10,
            "discussionId": "67e27584cf3845b43174097f",
            "ai_keywords": [
                "schema pruning",
                "multi-path generation",
                "multi-candidate generation",
                "1+1 Model Collaboration Paradigm",
                "general-purpose chat model",
                "SQL specialist",
                "analytical reasoning",
                "high-precision SQL generation"
            ]
        },
        "publishedAt": "2025-03-22T12:22:53.000Z",
        "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models",
        "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17811.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e7d63b14856e8859f1924c",
            "avatarUrl": "/avatars/bc58ab252c7b4d95ff99e4506fb8d3e9.svg",
            "fullname": "Pei Wenqi",
            "name": "CedPei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18866",
            "authors": [
                {
                    "_id": "67e2290da4525cbb1d718ae2",
                    "user": {
                        "_id": "65619949d2e4352d64365606",
                        "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
                        "isPro": true,
                        "fullname": "Yangjun Ruan",
                        "user": "ryoungj",
                        "type": "user"
                    },
                    "name": "Yangjun Ruan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:33:10.423Z",
                    "hidden": false
                },
                {
                    "_id": "67e2290da4525cbb1d718ae3",
                    "user": {
                        "_id": "630bc38809eceb8fafe5ed7f",
                        "avatarUrl": "/avatars/5f2a1268f8a7b51cca8446ef0be6445f.svg",
                        "isPro": true,
                        "fullname": "Neil Band",
                        "user": "nband",
                        "type": "user"
                    },
                    "name": "Neil Band",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:33:16.895Z",
                    "hidden": false
                },
                {
                    "_id": "67e2290da4525cbb1d718ae4",
                    "user": {
                        "_id": "66a7f54fbb22d7e78a2aeaf4",
                        "avatarUrl": "/avatars/3ab8899935f8f7b14e89c623cc6c0fd2.svg",
                        "isPro": false,
                        "fullname": "Chris J. Maddison",
                        "user": "cmaddis",
                        "type": "user"
                    },
                    "name": "Chris J. Maddison",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:33:22.633Z",
                    "hidden": false
                },
                {
                    "_id": "67e2290da4525cbb1d718ae5",
                    "name": "Tatsunori Hashimoto",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T16:41:23.000Z",
            "submittedOnDailyAt": "2025-03-25T02:25:31.127Z",
            "title": "Reasoning to Learn from Latent Thoughts",
            "submittedOnDailyBy": {
                "_id": "65619949d2e4352d64365606",
                "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
                "isPro": true,
                "fullname": "Yangjun Ruan",
                "user": "ryoungj",
                "type": "user"
            },
            "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
            "upvotes": 8,
            "discussionId": "67e2290ea4525cbb1d718b18",
            "ai_keywords": [
                "latent thoughts",
                "data-efficient learning",
                "web text",
                "verbose human thought process",
                "synthetic data",
                "data-constrained regime",
                "EM algorithm",
                "thought-augmented pretraining data",
                "inference compute",
                "data-constrained pretraining"
            ]
        },
        "publishedAt": "2025-03-24T12:41:23.000Z",
        "title": "Reasoning to Learn from Latent Thoughts",
        "summary": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18866.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65619949d2e4352d64365606",
            "avatarUrl": "/avatars/4241b6e901e146a9ff8b5bc7de3f960a.svg",
            "fullname": "Yangjun Ruan",
            "name": "ryoungj",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14428",
            "authors": [
                {
                    "_id": "67e217941cb9bded659267f0",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f1",
                    "user": {
                        "_id": "64210d1fd039a891a914986d",
                        "avatarUrl": "/avatars/b178a768657eb223bdbfbd9e0a2000ff.svg",
                        "isPro": false,
                        "fullname": "Yufan Deng",
                        "user": "dyf",
                        "type": "user"
                    },
                    "name": "Yufan Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:28:50.390Z",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f2",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:33.315Z",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f3",
                    "user": {
                        "_id": "63ad0b04e3b217fb36d36c13",
                        "avatarUrl": "/avatars/5a3715ba20859052ba04c048db9e03c2.svg",
                        "isPro": false,
                        "fullname": "Peng Jin",
                        "user": "Pengjin",
                        "type": "user"
                    },
                    "name": "Peng Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:29:15.403Z",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f4",
                    "user": {
                        "_id": "65b2529285b6c21448a10d65",
                        "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg",
                        "isPro": false,
                        "fullname": "Zesen Cheng",
                        "user": "ClownRat",
                        "type": "user"
                    },
                    "name": "Zesen Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:28:42.575Z",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f5",
                    "name": "Yian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f6",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "67e217941cb9bded659267f7",
                    "name": "Jie Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-18T17:02:14.000Z",
            "submittedOnDailyAt": "2025-03-25T01:10:50.245Z",
            "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
            "upvotes": 8,
            "discussionId": "67e217981cb9bded65926978",
            "projectPage": "https://hong-yu-zhang.github.io/MagicComp-Page/",
            "githubRepo": "https://github.com/Hong-yu-Zhang/MagicComp",
            "ai_keywords": [
                "Semantic Anchor Disambiguation",
                "Dynamic Layout Fusion Attention",
                "grounding priors",
                "model-adaptive spatial perception",
                "masked attention modulation"
            ]
        },
        "publishedAt": "2025-03-18T13:02:14.000Z",
        "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
        "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 35
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18769",
            "authors": [
                {
                    "_id": "67e2177c77d32fd1ed8a496b",
                    "user": {
                        "_id": "62d7b2339b629105a5d6888a",
                        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                        "isPro": false,
                        "fullname": "Alan Dao",
                        "user": "alandao",
                        "type": "user"
                    },
                    "name": "Alan Dao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:35.578Z",
                    "hidden": false
                },
                {
                    "_id": "67e2177c77d32fd1ed8a496c",
                    "name": "Dinh Bach Vu",
                    "hidden": false
                },
                {
                    "_id": "67e2177c77d32fd1ed8a496d",
                    "name": "Bui Quang Huy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
            ],
            "publishedAt": "2025-03-24T15:16:51.000Z",
            "submittedOnDailyAt": "2025-03-25T01:11:03.544Z",
            "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
            "submittedOnDailyBy": {
                "_id": "62d7b2339b629105a5d6888a",
                "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                "isPro": false,
                "fullname": "Alan Dao",
                "user": "alandao",
                "type": "user"
            },
            "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
            "upvotes": 6,
            "discussionId": "67e2177d77d32fd1ed8a49ac",
            "ai_keywords": [
                "semantics-based tokenization",
                "semantic tokens",
                "Cartesian space",
                "3D Cartesian space",
                "positioning",
                "manipulation subtasks"
            ]
        },
        "publishedAt": "2025-03-24T11:16:51.000Z",
        "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning",
        "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/505sM0N1UIUHUOunaHtAG.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18769.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "fullname": "Alan Dao",
            "name": "alandao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15879",
            "authors": [
                {
                    "_id": "67dea7cc5b44ace7a30e237e",
                    "user": {
                        "_id": "6540fbf9cb7fffd683942b43",
                        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
                        "isPro": false,
                        "fullname": "DongGeon Lee",
                        "user": "oneonlee",
                        "type": "user"
                    },
                    "name": "DongGeon Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-24T08:52:18.850Z",
                    "hidden": false
                },
                {
                    "_id": "67dea7cc5b44ace7a30e237f",
                    "name": "Ahjeong Park",
                    "hidden": false
                },
                {
                    "_id": "67dea7cc5b44ace7a30e2380",
                    "user": {
                        "_id": "666a8be869a08ea4aac5e73e",
                        "avatarUrl": "/avatars/be42632414bafb0af74b5f4d4f03d223.svg",
                        "isPro": false,
                        "fullname": "keira lee",
                        "user": "keirahrlee",
                        "type": "user"
                    },
                    "name": "Hyeri Lee",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-22T12:06:36.938Z",
                    "hidden": false
                },
                {
                    "_id": "67dea7cc5b44ace7a30e2381",
                    "name": "Hyeonseo Nam",
                    "hidden": false
                },
                {
                    "_id": "67dea7cc5b44ace7a30e2382",
                    "name": "Yunho Maeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T06:04:12.000Z",
            "submittedOnDailyAt": "2025-03-25T03:48:51.972Z",
            "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
            "submittedOnDailyBy": {
                "_id": "6540fbf9cb7fffd683942b43",
                "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
                "isPro": false,
                "fullname": "DongGeon Lee",
                "user": "oneonlee",
                "type": "user"
            },
            "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
            "upvotes": 6,
            "discussionId": "67dea7cc5b44ace7a30e23b8",
            "githubRepo": "https://github.com/TeamNLP/Typed-RAG",
            "ai_keywords": [
                "retrieval-augmented generation (RAG)",
                "non-factoid question-answering (NFQA)",
                "multi-aspect reasoning",
                "type-aware multi-aspect decomposition framework",
                "single-aspect sub-queries",
                "Wiki-NFQA",
                "type-aware decomposition"
            ]
        },
        "publishedAt": "2025-03-20T02:04:12.000Z",
        "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering",
        "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\nhttps://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15879.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "fullname": "DongGeon Lee",
            "name": "oneonlee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17422",
            "authors": [
                {
                    "_id": "67e226b3b1acaf8a7680e926",
                    "name": "Javier J. Poveda Rodrigo",
                    "hidden": false
                },
                {
                    "_id": "67e226b3b1acaf8a7680e927",
                    "name": "Mohamed Amine Ahmdi",
                    "hidden": false
                },
                {
                    "_id": "67e226b3b1acaf8a7680e928",
                    "name": "Alessio Burrello",
                    "hidden": false
                },
                {
                    "_id": "67e226b3b1acaf8a7680e929",
                    "name": "Daniele Jahier Pagliari",
                    "hidden": false
                },
                {
                    "_id": "67e226b3b1acaf8a7680e92a",
                    "name": "Luca Benini",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T09:00:19.000Z",
            "submittedOnDailyAt": "2025-03-25T02:15:09.508Z",
            "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
            "upvotes": 5,
            "discussionId": "67e226b3b1acaf8a7680e96b",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "GPU-based systems",
                "CPUs",
                "RISC-V",
                "ISA",
                "RISC-V hardware",
                "software ecosystem",
                "domain-specific tuning",
                "Sophon SG2042",
                "many-core RISC-V CPU",
                "vector processing capabilities",
                "DeepSeek R1 Distill Llama 8B",
                "DeepSeek R1 Distill QWEN 14B",
                "token generation",
                "prompt processing"
            ]
        },
        "publishedAt": "2025-03-21T05:00:19.000Z",
        "title": "V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms",
        "summary": "The recent exponential growth of Large Language Models (LLMs) has relied on\nGPU-based systems. However, CPUs are emerging as a flexible and lower-cost\nalternative, especially when targeting inference and reasoning workloads.\nRISC-V is rapidly gaining traction in this area, given its open and\nvendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the\ncorresponding software ecosystem are not fully mature and streamlined, given\nthe requirement of domain-specific tuning. This paper aims at filling this gap,\nfocusing on optimizing LLM inference on the Sophon SG2042, the first\ncommercially available many-core RISC-V CPU with vector processing\ncapabilities.\n  On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1\nDistill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s\nfor token generation and 6.54/3.68 token/s for prompt processing, with a speed\nup of up 2.9x/3.0x compared to our baseline.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17422.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6463
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18559",
            "authors": [
                {
                    "_id": "67e22d5236076dc847989434",
                    "name": "Takashi Isobe",
                    "hidden": false
                },
                {
                    "_id": "67e22d5236076dc847989435",
                    "name": "He Cui",
                    "hidden": false
                },
                {
                    "_id": "67e22d5236076dc847989436",
                    "name": "Dong Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e22d5236076dc847989437",
                    "user": {
                        "_id": "6463685fd2044cd1d7c74b81",
                        "avatarUrl": "/avatars/334637e2d63efb7cc2129fec6ea54725.svg",
                        "isPro": false,
                        "fullname": "gemengmeng",
                        "user": "gemengmeng",
                        "type": "user"
                    },
                    "name": "Mengmeng Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:30:24.973Z",
                    "hidden": false
                },
                {
                    "_id": "67e22d5236076dc847989438",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "67e22d5236076dc847989439",
                    "user": {
                        "_id": "65adc9d086f88a686be41215",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65adc9d086f88a686be41215/xizVHuZPkE0Gu8_ulx0Fm.jpeg",
                        "isPro": false,
                        "fullname": "Emad Barsoum",
                        "user": "ebarsoum",
                        "type": "user"
                    },
                    "name": "Emad Barsoum",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:30:09.267Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T11:13:33.000Z",
            "submittedOnDailyAt": "2025-03-25T02:43:21.795Z",
            "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
            "upvotes": 4,
            "discussionId": "67e22d5836076dc84798964e",
            "ai_keywords": [
                "U-Net",
                "Visual feedback learning",
                "Large Language Models (LLMs)",
                "Video Quality Assessment (VQA)",
                "VBench"
            ]
        },
        "publishedAt": "2025-03-24T07:13:33.000Z",
        "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
        "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18559.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6463
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18018",
            "authors": [
                {
                    "_id": "67e289d7e5bbf1a125feebf3",
                    "user": {
                        "_id": "67e28c6b0bab9de7e86d9a66",
                        "avatarUrl": "/avatars/d2d56dfe464c8aeb087eeb27572af8f3.svg",
                        "isPro": false,
                        "fullname": "Aabid Karim",
                        "user": "abedk",
                        "type": "user"
                    },
                    "name": "Aabid Karim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T12:19:05.289Z",
                    "hidden": false
                },
                {
                    "_id": "67e289d7e5bbf1a125feebf4",
                    "user": {
                        "_id": "63eb20fbd0b894bbc77de9df",
                        "avatarUrl": "/avatars/3489e391206d461e880c0dd8adbc6abe.svg",
                        "isPro": false,
                        "fullname": "Abdul Karim",
                        "user": "Abdul084",
                        "type": "user"
                    },
                    "name": "Abdul Karim",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-25T11:01:05.131Z",
                    "hidden": false
                },
                {
                    "_id": "67e289d7e5bbf1a125feebf5",
                    "name": "Bhoomika Lohana",
                    "hidden": false
                },
                {
                    "_id": "67e289d7e5bbf1a125feebf6",
                    "user": {
                        "_id": "66321599027c1f96a39508df",
                        "avatarUrl": "/avatars/71dd68d7f0bfc0fac4d047b82de5446a.svg",
                        "isPro": false,
                        "fullname": "Koo",
                        "user": "55mv",
                        "type": "user"
                    },
                    "name": "Matt Keon",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-25T10:47:52.707Z",
                    "hidden": false
                },
                {
                    "_id": "67e289d7e5bbf1a125feebf7",
                    "user": {
                        "_id": "63da27dee697e5898cb73d1a",
                        "avatarUrl": "/avatars/68b2dffba35e425085e33b4a7cb52a08.svg",
                        "isPro": false,
                        "fullname": "Jaswinder Singh",
                        "user": "jaswindersingh2",
                        "type": "user"
                    },
                    "name": "Jaswinder Singh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T12:19:02.963Z",
                    "hidden": false
                },
                {
                    "_id": "67e289d7e5bbf1a125feebf8",
                    "name": "Abdul Sattar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-23T10:35:39.000Z",
            "submittedOnDailyAt": "2025-03-25T09:34:34.203Z",
            "title": "Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural\n  Contexts?",
            "submittedOnDailyBy": {
                "_id": "63eb20fbd0b894bbc77de9df",
                "avatarUrl": "/avatars/3489e391206d461e880c0dd8adbc6abe.svg",
                "isPro": false,
                "fullname": "Abdul Karim",
                "user": "Abdul084",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have significantly advanced various fields,\nparticularly coding, mathematical reasoning, and logical problem solving.\nHowever, a critical question remains: Do these mathematical reasoning abilities\npersist when LLMs are presented with culturally adapted math problems?\nSpecifically, how do LLMs perform when faced with math problems embedded in\ncultural contexts that have no significant representation in main stream\nweb-scale AI training data? To explore this, we generated six synthetic\ncultural datasets from GSM8K, a widely used benchmark for assessing LLMs'\nmathematical reasoning skills. While preserving the mathematical logic and\nnumerical values of the original GSM8K test set, we modify cultural elements\nsuch as personal names, food items, place names, etc. These culturally adapted\ndatasets provide a more reliable framework for evaluating LLMs' mathematical\nreasoning under shifting cultural contexts. Our findings reveal that LLMs\nstruggle with math problems when cultural references change, even though the\nunderlying mathematical structure remains constant. Smaller models exhibit\ngreater performance drops compared to larger models. Interestingly, our results\nalso suggest that cultural familiarity can enhance mathematical reasoning. Even\nmodels with no explicit mathematical training but exposure to relevant cultural\ncontexts sometimes outperform larger, mathematically proficient models on\nculturally embedded math problems. This study highlights the impact of cultural\ncontext on the mathematical reasoning abilities of LLMs, underscoring the need\nfor more diverse and representative training data to improve robustness in\nreal-world applications. The benchmark data sets and script for reproducing the\nresults are available at\nhttps://github.com/akarim23131/Lost_in_Cultural_Translation",
            "upvotes": 4,
            "discussionId": "67e289d8e5bbf1a125feec5f",
            "githubRepo": "https://github.com/akarim23131/Lost_in_Cultural_Translation",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "GSM8K",
                "synthetic cultural datasets",
                "mathematical reasoning",
                "logical problem solving",
                "cultural adaptation",
                "cultural elements",
                "cultural references",
                "cultural familiarity"
            ]
        },
        "publishedAt": "2025-03-23T06:35:39.000Z",
        "title": "Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural\n  Contexts?",
        "summary": "Large Language Models (LLMs) have significantly advanced various fields,\nparticularly coding, mathematical reasoning, and logical problem solving.\nHowever, a critical question remains: Do these mathematical reasoning abilities\npersist when LLMs are presented with culturally adapted math problems?\nSpecifically, how do LLMs perform when faced with math problems embedded in\ncultural contexts that have no significant representation in main stream\nweb-scale AI training data? To explore this, we generated six synthetic\ncultural datasets from GSM8K, a widely used benchmark for assessing LLMs'\nmathematical reasoning skills. While preserving the mathematical logic and\nnumerical values of the original GSM8K test set, we modify cultural elements\nsuch as personal names, food items, place names, etc. These culturally adapted\ndatasets provide a more reliable framework for evaluating LLMs' mathematical\nreasoning under shifting cultural contexts. Our findings reveal that LLMs\nstruggle with math problems when cultural references change, even though the\nunderlying mathematical structure remains constant. Smaller models exhibit\ngreater performance drops compared to larger models. Interestingly, our results\nalso suggest that cultural familiarity can enhance mathematical reasoning. Even\nmodels with no explicit mathematical training but exposure to relevant cultural\ncontexts sometimes outperform larger, mathematically proficient models on\nculturally embedded math problems. This study highlights the impact of cultural\ncontext on the mathematical reasoning abilities of LLMs, underscoring the need\nfor more diverse and representative training data to improve robustness in\nreal-world applications. The benchmark data sets and script for reproducing the\nresults are available at\nhttps://github.com/akarim23131/Lost_in_Cultural_Translation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18018.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63eb20fbd0b894bbc77de9df",
            "avatarUrl": "/avatars/3489e391206d461e880c0dd8adbc6abe.svg",
            "fullname": "Abdul Karim",
            "name": "Abdul084",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17500",
            "authors": [
                {
                    "_id": "67e23d503ef5318b1550f1bc",
                    "user": {
                        "_id": "6071c4b270e11b30cfcfd7a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
                        "isPro": false,
                        "fullname": "Louis Owen",
                        "user": "louisowen6",
                        "type": "user"
                    },
                    "name": "Louis Owen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:19:59.211Z",
                    "hidden": false
                },
                {
                    "_id": "67e23d503ef5318b1550f1bd",
                    "user": {
                        "_id": "62cd4b03c5cc157be82f0b56",
                        "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
                        "isPro": false,
                        "fullname": "Abhay kumar",
                        "user": "akanyaani",
                        "type": "user"
                    },
                    "name": "Abhay Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T09:23:34.033Z",
                    "hidden": false
                },
                {
                    "_id": "67e23d503ef5318b1550f1be",
                    "user": {
                        "_id": "645a0d3dd6648853107c5fdc",
                        "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
                        "isPro": false,
                        "fullname": "Nilabhra Roy Chowdhury",
                        "user": "nilabhra",
                        "type": "user"
                    },
                    "name": "Nilabhra Roy Chowdhury",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T09:15:43.961Z",
                    "hidden": false
                },
                {
                    "_id": "67e23d503ef5318b1550f1bf",
                    "user": {
                        "_id": "65e4be59e8b017ee1310a1b6",
                        "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
                        "isPro": false,
                        "fullname": "Fabian",
                        "user": "gueraf",
                        "type": "user"
                    },
                    "name": "Fabian Güra",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:19:56.909Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
            ],
            "publishedAt": "2025-03-21T19:23:08.000Z",
            "submittedOnDailyAt": "2025-03-25T03:52:18.158Z",
            "title": "Variance Control via Weight Rescaling in LLM Pre-training",
            "submittedOnDailyBy": {
                "_id": "6071c4b270e11b30cfcfd7a3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
                "isPro": false,
                "fullname": "Louis Owen",
                "user": "louisowen6",
                "type": "user"
            },
            "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
            "upvotes": 4,
            "discussionId": "67e23d513ef5318b1550f22c",
            "githubRepo": "https://github.com/bluorion-com/weight_rescaling",
            "ai_keywords": [
                "Layer Index Rescaling (LIR)",
                "Target Variance Rescaling (TVR)",
                "weight initialization",
                "variance control"
            ]
        },
        "publishedAt": "2025-03-21T15:23:08.000Z",
        "title": "Variance Control via Weight Rescaling in LLM Pre-training",
        "summary": "The outcome of Large Language Model (LLM) pre-training strongly depends on\nweight initialization and variance control strategies. Although the importance\nof initial variance control has been well documented in neural networks in\ngeneral, the literature on initialization and management of its growth during\nLLM pre-training, specifically, is somewhat sparse. In this paper, we introduce\nthe Layer Index Rescaling (LIR) weight initialization scheme, and the Target\nVariance Rescaling (TVR) variance control strategy. Experiments on a 1B\nparameter LLaMA model demonstrate that better variance management using these\ntechniques yields substantial improvements in downstream task performance (up\nto 4.6% on common pre-training benchmarks) and reduces extreme activation\nvalues, thus mitigating challenges associated with quantization and\nlow-precision training. Our code is available at:\nhttps://github.com/bluorion-com/weight_rescaling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6071c4b270e11b30cfcfd7a3/AaY617TTwvPiB-ub2B9bA.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17500.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "fullname": "Louis Owen",
            "name": "louisowen6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18352",
            "authors": [
                {
                    "_id": "67e217a272e17348c5b3f0a2",
                    "user": {
                        "_id": "63a95f207930fa8c7dd6634e",
                        "avatarUrl": "/avatars/0dcf0e263300e3773e5da0b46ab4aa6f.svg",
                        "isPro": false,
                        "fullname": "Jinjin Zhang",
                        "user": "zhang0jhon",
                        "type": "user"
                    },
                    "name": "Jinjin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T19:02:26.353Z",
                    "hidden": false
                },
                {
                    "_id": "67e217a272e17348c5b3f0a3",
                    "user": {
                        "_id": "6708e399672d9dcd31575fbc",
                        "avatarUrl": "/avatars/0f947f17b5426186aadaa4224571f47b.svg",
                        "isPro": false,
                        "fullname": "qiuyuhuang",
                        "user": "qiuyuhuang",
                        "type": "user"
                    },
                    "name": "Qiuyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:24:20.054Z",
                    "hidden": false
                },
                {
                    "_id": "67e217a272e17348c5b3f0a4",
                    "name": "Junjie Liu",
                    "hidden": false
                },
                {
                    "_id": "67e217a272e17348c5b3f0a5",
                    "user": {
                        "_id": "64905cd589f22918ecaca080",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2S7I7uZL49CXbUN2T7p63.jpeg",
                        "isPro": false,
                        "fullname": "Xiefan Guo",
                        "user": "xiefan-guo",
                        "type": "user"
                    },
                    "name": "Xiefan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:24:02.736Z",
                    "hidden": false
                },
                {
                    "_id": "67e217a272e17348c5b3f0a6",
                    "name": "Di Huang",
                    "hidden": true
                }
            ],
            "publishedAt": "2025-03-24T05:25:07.000Z",
            "submittedOnDailyAt": "2025-03-25T07:48:32.671Z",
            "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.",
            "upvotes": 3,
            "discussionId": "67e217a772e17348c5b3f20a",
            "ai_keywords": [
                "diffusion models",
                "text-to-image diffusion models",
                "Aesthetic-4K Benchmark",
                "wavelet-based fine-tuning",
                "latent diffusion models",
                "SD3-2B",
                "Flux-12B",
                "GLCM Score",
                "Compression Ratio",
                "FID",
                "Aesthetics",
                "CLIPScore",
                "ultra-high-resolution image synthesis",
                "photorealistic 4K images",
                "high-quality image synthesis",
                "text prompt adherence"
            ]
        },
        "publishedAt": "2025-03-24T01:25:07.000Z",
        "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models",
        "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18352.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 801
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18470",
            "authors": [
                {
                    "_id": "67e23d7ddb11e1d38226cafd",
                    "user": {
                        "_id": "669794c5813d96b4eb0b3fd6",
                        "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
                        "isPro": true,
                        "fullname": "Zhenyu Pan",
                        "user": "zhenyupan",
                        "type": "user"
                    },
                    "name": "Zhenyu Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:19:54.569Z",
                    "hidden": false
                },
                {
                    "_id": "67e23d7ddb11e1d38226cafe",
                    "name": "Han Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T09:18:01.000Z",
            "submittedOnDailyAt": "2025-03-25T06:54:21.536Z",
            "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
            "submittedOnDailyBy": {
                "_id": "669794c5813d96b4eb0b3fd6",
                "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
                "isPro": true,
                "fullname": "Zhenyu Pan",
                "user": "zhenyupan",
                "type": "user"
            },
            "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.",
            "upvotes": 2,
            "discussionId": "67e23d7fdb11e1d38226cb7b",
            "projectPage": "https://github.com/PzySeere/MetaSpatial",
            "githubRepo": "https://github.com/PzySeere/MetaSpatial",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "vision-language models (VLMs)",
                "3D spatial reasoning",
                "real-time 3D scene generation",
                "internalized 3D spatial reasoning",
                "supervised fine-tuning (SFT)",
                "multi-turn RL-based optimization",
                "physics-aware constraints",
                "rendered image evaluations",
                "adaptive, iterative reasoning process",
                "scene coherence",
                "spatial consistency",
                "formatting stability",
                "object placements",
                "metaverse",
                "AR/VR",
                "digital twins",
                "game development",
                "empirical evaluations"
            ]
        },
        "publishedAt": "2025-03-24T05:18:01.000Z",
        "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
        "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18470.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669794c5813d96b4eb0b3fd6",
            "avatarUrl": "/avatars/b4e2bb7b07cc281932d783dcbb64f211.svg",
            "fullname": "Zhenyu Pan",
            "name": "zhenyupan",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17760",
            "authors": [
                {
                    "_id": "67e2b2c1a46a0939c33119b5",
                    "user": {
                        "_id": "64f2a14396a5ffb820ab39a8",
                        "avatarUrl": "/avatars/3d7c3be17c53eb52bd8a75d3c51e6891.svg",
                        "isPro": false,
                        "fullname": "Zeyu Liu",
                        "user": "shuidi0020",
                        "type": "user"
                    },
                    "name": "Zeyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T14:35:58.054Z",
                    "hidden": false
                },
                {
                    "_id": "67e2b2c1a46a0939c33119b6",
                    "name": "Zanlin Ni",
                    "hidden": false
                },
                {
                    "_id": "67e2b2c1a46a0939c33119b7",
                    "name": "Yeguo Hua",
                    "hidden": false
                },
                {
                    "_id": "67e2b2c1a46a0939c33119b8",
                    "name": "Xin Deng",
                    "hidden": false
                },
                {
                    "_id": "67e2b2c1a46a0939c33119b9",
                    "name": "Xiao Ma",
                    "hidden": false
                },
                {
                    "_id": "67e2b2c1a46a0939c33119ba",
                    "name": "Cheng Zhong",
                    "hidden": false
                },
                {
                    "_id": "67e2b2c1a46a0939c33119bb",
                    "name": "Gao Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-22T12:59:00.000Z",
            "submittedOnDailyAt": "2025-03-25T12:34:00.612Z",
            "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
            "submittedOnDailyBy": {
                "_id": "63987ffb2ceb55aabe0852f3",
                "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg",
                "isPro": false,
                "fullname": "Zanlin Ni",
                "user": "nzl-thu",
                "type": "user"
            },
            "summary": "Discrete visual tokenizers transform images into a sequence of tokens,\nenabling token-based visual generation akin to language models. However, this\nprocess is inherently challenging, as it requires both compressing visual\nsignals into a compact representation and discretizing them into a fixed set of\ncodes. Traditional discrete tokenizers typically learn the two tasks jointly,\noften leading to unstable training, low codebook utilization, and limited\nreconstruction quality. In this paper, we introduce\nCODA(COntinuous-to-Discrete Adaptation), a\nframework that decouples compression and discretization. Instead of training\ndiscrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs --\nalready optimized for perceptual compression -- into discrete tokenizers via a\ncarefully designed discretization process. By primarily focusing on\ndiscretization, CODA ensures stable and efficient training while retaining the\nstrong visual fidelity of continuous VAEs. Empirically, with 6\ntimes less training budget than standard VQGAN, our approach achieves a\nremarkable codebook utilization of 100% and notable reconstruction FID (rFID)\nof 0.43 and 1.34 for 8 times and 16 times\ncompression on ImageNet 256times 256 benchmark.",
            "upvotes": 2,
            "discussionId": "67e2b2c2a46a0939c3311a3c",
            "ai_keywords": [
                "discrete visual tokenizers",
                "token-based visual generation",
                "continuous VAEs",
                "perceptual compression",
                "CODA",
                "Continuous-to-Discrete Adaptation",
                "codebook utilization",
                "reconstruction FID (rFID)",
                "ImageNet 256$\\times$ 256 benchmark"
            ]
        },
        "publishedAt": "2025-03-22T08:59:00.000Z",
        "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
        "summary": "Discrete visual tokenizers transform images into a sequence of tokens,\nenabling token-based visual generation akin to language models. However, this\nprocess is inherently challenging, as it requires both compressing visual\nsignals into a compact representation and discretizing them into a fixed set of\ncodes. Traditional discrete tokenizers typically learn the two tasks jointly,\noften leading to unstable training, low codebook utilization, and limited\nreconstruction quality. In this paper, we introduce\nCODA(COntinuous-to-Discrete Adaptation), a\nframework that decouples compression and discretization. Instead of training\ndiscrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs --\nalready optimized for perceptual compression -- into discrete tokenizers via a\ncarefully designed discretization process. By primarily focusing on\ndiscretization, CODA ensures stable and efficient training while retaining the\nstrong visual fidelity of continuous VAEs. Empirically, with 6\ntimes less training budget than standard VQGAN, our approach achieves a\nremarkable codebook utilization of 100% and notable reconstruction FID (rFID)\nof 0.43 and 1.34 for 8 times and 16 times\ncompression on ImageNet 256times 256 benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17760.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63987ffb2ceb55aabe0852f3",
            "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg",
            "fullname": "Zanlin Ni",
            "name": "nzl-thu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17735",
            "authors": [
                {
                    "_id": "67e22bc349edf14060e5747a",
                    "name": "Zhiqiang Yuan",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e5747b",
                    "name": "Ting Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e5747c",
                    "name": "Ying Deng",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e5747d",
                    "name": "Jiapei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e5747e",
                    "name": "Yeshuang Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e5747f",
                    "name": "Zexi Jia",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e57480",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e22bc349edf14060e57481",
                    "name": "Jinchao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-22T11:28:25.000Z",
            "submittedOnDailyAt": "2025-03-25T02:36:41.359Z",
            "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
            "upvotes": 2,
            "discussionId": "67e22bc449edf14060e574e3",
            "ai_keywords": [
                "parameter-efficient tuning",
                "Adapter",
                "Lora",
                "discrete frame generation network",
                "dual-mask based data utilization strategy",
                "curriculum learning method",
                "difficulty-adaptive curriculum learning",
                "sample entropy",
                "I2V-Adapter",
                "SimDA"
            ]
        },
        "publishedAt": "2025-03-22T07:28:25.000Z",
        "title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation",
        "summary": "Recently, great progress has been made in video generation technology,\nattracting the widespread attention of scholars. To apply this technology to\ndownstream applications under resource-constrained conditions, researchers\nusually fine-tune the pre-trained models based on parameter-efficient tuning\nmethods such as Adapter or Lora. Although these methods can transfer the\nknowledge from the source domain to the target domain, fewer training\nparameters lead to poor fitting ability, and the knowledge from the source\ndomain may lead to the inference process deviating from the target domain. In\nthis paper, we argue that under constrained resources, training a smaller video\ngeneration model from scratch using only million-level samples can outperform\nparameter-efficient tuning on larger models in downstream applications: the\ncore lies in the effective utilization of data and curriculum strategy. Take\nanimated sticker generation (ASG) as a case study, we first construct a\ndiscrete frame generation network for stickers with low frame rates, ensuring\nthat its parameters meet the requirements of model training under constrained\nresources. In order to provide data support for models trained from scratch, we\ncome up with a dual-mask based data utilization strategy, which manages to\nimprove the availability and expand the diversity of limited data. To\nfacilitate convergence under dual-mask situation, we propose a\ndifficulty-adaptive curriculum learning method, which decomposes the sample\nentropy into static and adaptive components so as to obtain samples from easy\nto difficult. The experiment demonstrates that our resource-efficient dual-mask\ntraining framework is quantitatively and qualitatively superior to\nefficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the\nfeasibility of our method on downstream tasks under constrained resources. Code\nwill be available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17735.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6463
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16924",
            "authors": [
                {
                    "_id": "67e230f384513315a91c5602",
                    "user": {
                        "_id": "664207e5af62c6c26653b369",
                        "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
                        "isPro": false,
                        "fullname": "Joo Chan Lee",
                        "user": "maincold2",
                        "type": "user"
                    },
                    "name": "Joo Chan Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:07.628Z",
                    "hidden": false
                },
                {
                    "_id": "67e230f384513315a91c5603",
                    "name": "Jong Hwan Ko",
                    "hidden": false
                },
                {
                    "_id": "67e230f384513315a91c5604",
                    "user": {
                        "_id": "655e0141d36a195f663ee4b0",
                        "avatarUrl": "/avatars/97bb695ccefdcb2139b94bcae808cf99.svg",
                        "isPro": false,
                        "fullname": "Eunbyung Park",
                        "user": "epark",
                        "type": "user"
                    },
                    "name": "Eunbyung Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T09:25:09.995Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T07:41:45.000Z",
            "submittedOnDailyAt": "2025-03-25T02:59:24.076Z",
            "title": "Optimized Minimal 3D Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "664207e5af62c6c26653b369",
                "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
                "isPro": false,
                "fullname": "Joo Chan Lee",
                "user": "maincold2",
                "type": "user"
            },
            "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
            "upvotes": 2,
            "discussionId": "67e230f484513315a91c5678",
            "projectPage": "https://maincold2.github.io/omg/",
            "githubRepo": "https://github.com/maincold2/OMG",
            "ai_keywords": [
                "Gaussian Splatting (3DGS)",
                "real-time",
                "high-performance rendering",
                "3D scenes",
                "explicit Gaussian primitives",
                "storage",
                "memory overhead",
                "high-quality rendering",
                "attribute compression",
                "quality degradation",
                "computational costs",
                "Optimized Minimal Gaussians representation (OMG)",
                "distinct Gaussian",
                "redundancy",
                "attribute representation",
                "continuity",
                "irregularity",
                "sub-vector quantization",
                "codebook size",
                "FPS rendering",
                "rendering quality"
            ]
        },
        "publishedAt": "2025-03-21T03:41:45.000Z",
        "title": "Optimized Minimal 3D Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nreal-time, high-performance rendering, enabling a wide range of applications.\nHowever, representing 3D scenes with numerous explicit Gaussian primitives\nimposes significant storage and memory overhead. Recent studies have shown that\nhigh-quality rendering can be achieved with a substantially reduced number of\nGaussians when represented with high-precision attributes. Nevertheless,\nexisting 3DGS compression methods still rely on a relatively large number of\nGaussians, focusing primarily on attribute compression. This is because a\nsmaller set of Gaussians becomes increasingly sensitive to lossy attribute\ncompression, leading to severe quality degradation. Since the number of\nGaussians is directly tied to computational costs, it is essential to reduce\nthe number of Gaussians effectively rather than only optimizing storage. In\nthis paper, we propose Optimized Minimal Gaussians representation (OMG), which\nsignificantly reduces storage while using a minimal number of primitives.\nFirst, we determine the distinct Gaussian from the near ones, minimizing\nredundancy without sacrificing quality. Second, we propose a compact and\nprecise attribute representation that efficiently captures both continuity and\nirregularity among primitives. Additionally, we propose a sub-vector\nquantization technique for improved irregularity representation, maintaining\nfast training with a negligible codebook size. Extensive experiments\ndemonstrate that OMG reduces storage requirements by nearly 50% compared to the\nprevious state-of-the-art and enables 600+ FPS rendering while maintaining high\nrendering quality. Our source code is available at\nhttps://maincold2.github.io/omg/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664207e5af62c6c26653b369",
            "avatarUrl": "/avatars/031453340e114efb10b7d650e1d1c384.svg",
            "fullname": "Joo Chan Lee",
            "name": "maincold2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18494",
            "authors": [
                {
                    "_id": "67e21a81e2e69ea26eee4f67",
                    "user": {
                        "_id": "65bef46337491e7adc5ee7c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
                        "isPro": false,
                        "fullname": "Hao-Yuan Chen",
                        "user": "MarkChenX",
                        "type": "user"
                    },
                    "name": "Hao-Yuan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:20:30.277Z",
                    "hidden": false
                },
                {
                    "_id": "67e21a81e2e69ea26eee4f68",
                    "name": "Cheng-Pong Huang",
                    "hidden": false
                },
                {
                    "_id": "67e21a81e2e69ea26eee4f69",
                    "name": "Jui-Ming Yao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
            ],
            "publishedAt": "2025-03-24T09:48:59.000Z",
            "submittedOnDailyAt": "2025-03-25T06:52:38.289Z",
            "title": "Verbal Process Supervision Elicits Better Coding Agents",
            "submittedOnDailyBy": {
                "_id": "65bef46337491e7adc5ee7c9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
                "isPro": false,
                "fullname": "Hao-Yuan Chen",
                "user": "MarkChenX",
                "type": "user"
            },
            "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
            "upvotes": 1,
            "discussionId": "67e21a82e2e69ea26eee4fba",
            "ai_keywords": [
                "large language models (LLMs)",
                "code generation",
                "software engineering tasks",
                "CURA",
                "code understanding and reasoning agent system",
                "verbal process supervision (VPS)",
                "BigCodeBench",
                "o3-mini model",
                "reasoning-driven architectures",
                "agentic reasoning"
            ]
        },
        "publishedAt": "2025-03-24T05:48:59.000Z",
        "title": "Verbal Process Supervision Elicits Better Coding Agents",
        "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/kVdnqHbvP4VXfEWb2MAs_.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/amlMqiikDojWAwsnS3d5e.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65bef46337491e7adc5ee7c9/cndDykrtJViO6gRavkF9D.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18494.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65bef46337491e7adc5ee7c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bQ2WY0bU65IQ3IBn6SrnA.png",
            "fullname": "Hao-Yuan Chen",
            "name": "MarkChenX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18406",
            "authors": [
                {
                    "_id": "67e31b0c3ba899a93859c755",
                    "name": "Sherry X. Chen",
                    "hidden": false
                },
                {
                    "_id": "67e31b0c3ba899a93859c756",
                    "name": "Misha Sra",
                    "hidden": false
                },
                {
                    "_id": "67e31b0c3ba899a93859c757",
                    "name": "Pradeep Sen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T07:25:44.000Z",
            "submittedOnDailyAt": "2025-03-25T19:38:53.151Z",
            "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated\n  Data Refinement Using Contrastive Learning",
            "submittedOnDailyBy": {
                "_id": "6350af4d229624a4d4a6bd98",
                "avatarUrl": "/avatars/2cbc606f23893288ab94b8c8e9b680d3.svg",
                "isPro": false,
                "fullname": "Sherry Chen",
                "user": "SherryXTChen",
                "type": "user"
            },
            "summary": "Although natural language instructions offer an intuitive way to guide\nautomated image editing, deep-learning models often struggle to achieve\nhigh-quality results, largely due to challenges in creating large, high-quality\ntraining datasets. Previous work has typically relied on text-toimage (T2I)\ngenerative models to produce pairs of original and edited images that simulate\nthe input/output of an instruction-guided image-editing model. However, these\nimage pairs often fail to align with the specified edit instructions due to the\nlimitations of T2I models, which negatively impacts models trained on such\ndatasets. To address this, we present Instruct-CLIP, a self-supervised method\nthat learns the semantic changes between original and edited images to refine\nand better align the instructions in existing datasets. Furthermore, we adapt\nInstruct-CLIP to handle noisy latent images and diffusion timesteps so that it\ncan be used to train latent diffusion models (LDMs) [19] and efficiently\nenforce alignment between the edit instruction and the image changes in latent\nspace at any step of the diffusion pipeline. We use Instruct-CLIP to correct\nthe InstructPix2Pix dataset and get over 120K refined samples we then use to\nfine-tune their model, guided by our novel Instruct-CLIP-based loss function.\nThe resulting model can produce edits that are more aligned with the given\ninstructions. Our code and dataset are available at\nhttps://github.com/SherryXTChen/Instruct-CLIP.git.",
            "upvotes": 1,
            "discussionId": "67e31b113ba899a93859c909",
            "githubRepo": "https://github.com/SherryXTChen/Instruct-CLIP",
            "ai_keywords": [
                "self-supervised method",
                "semantic changes",
                "text-to-image (T2I)",
                "latent images",
                "diffusion timesteps",
                "latent diffusion models (LDMs)",
                "diffusion pipeline",
                "Instruct-CLIP-based loss function"
            ]
        },
        "publishedAt": "2025-03-24T03:25:44.000Z",
        "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated\n  Data Refinement Using Contrastive Learning",
        "summary": "Although natural language instructions offer an intuitive way to guide\nautomated image editing, deep-learning models often struggle to achieve\nhigh-quality results, largely due to challenges in creating large, high-quality\ntraining datasets. Previous work has typically relied on text-toimage (T2I)\ngenerative models to produce pairs of original and edited images that simulate\nthe input/output of an instruction-guided image-editing model. However, these\nimage pairs often fail to align with the specified edit instructions due to the\nlimitations of T2I models, which negatively impacts models trained on such\ndatasets. To address this, we present Instruct-CLIP, a self-supervised method\nthat learns the semantic changes between original and edited images to refine\nand better align the instructions in existing datasets. Furthermore, we adapt\nInstruct-CLIP to handle noisy latent images and diffusion timesteps so that it\ncan be used to train latent diffusion models (LDMs) [19] and efficiently\nenforce alignment between the edit instruction and the image changes in latent\nspace at any step of the diffusion pipeline. We use Instruct-CLIP to correct\nthe InstructPix2Pix dataset and get over 120K refined samples we then use to\nfine-tune their model, guided by our novel Instruct-CLIP-based loss function.\nThe resulting model can produce edits that are more aligned with the given\ninstructions. Our code and dataset are available at\nhttps://github.com/SherryXTChen/Instruct-CLIP.git.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18406.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6350af4d229624a4d4a6bd98",
            "avatarUrl": "/avatars/2cbc606f23893288ab94b8c8e9b680d3.svg",
            "fullname": "Sherry Chen",
            "name": "SherryXTChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18071",
            "authors": [
                {
                    "_id": "67e249babf8cc1d141a54299",
                    "user": {
                        "_id": "6489b2b9a9d27ef5f8c824c7",
                        "avatarUrl": "/avatars/14655bb803ba0db259b7a52ea96fcd09.svg",
                        "isPro": false,
                        "fullname": "zyllin",
                        "user": "zygg",
                        "type": "user"
                    },
                    "name": "Zhiyu Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:19:25.946Z",
                    "hidden": false
                },
                {
                    "_id": "67e249babf8cc1d141a5429a",
                    "name": "Yifei Gao",
                    "hidden": false
                },
                {
                    "_id": "67e249babf8cc1d141a5429b",
                    "name": "Xian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e249babf8cc1d141a5429c",
                    "name": "Yunfan Yang",
                    "hidden": false
                },
                {
                    "_id": "67e249babf8cc1d141a5429d",
                    "name": "Jitao Sang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6489b2b9a9d27ef5f8c824c7/mlkEenSXDE4zWAEbO2El3.png"
            ],
            "publishedAt": "2025-03-23T13:40:44.000Z",
            "submittedOnDailyAt": "2025-03-25T14:04:24.299Z",
            "title": "Mind with Eyes: from Language Reasoning to Multimodal Reasoning",
            "submittedOnDailyBy": {
                "_id": "6489b2b9a9d27ef5f8c824c7",
                "avatarUrl": "/avatars/14655bb803ba0db259b7a52ea96fcd09.svg",
                "isPro": false,
                "fullname": "zyllin",
                "user": "zygg",
                "type": "user"
            },
            "summary": "Language models have recently advanced into the realm of reasoning, yet it is\nthrough multimodal reasoning that we can fully unlock the potential to achieve\nmore comprehensive, human-like cognitive capabilities. This survey provides a\nsystematic overview of the recent multimodal reasoning approaches, categorizing\nthem into two levels: language-centric multimodal reasoning and collaborative\nmultimodal reasoning. The former encompasses one-pass visual perception and\nactive visual perception, where vision primarily serves a supporting role in\nlanguage reasoning. The latter involves action generation and state update\nwithin reasoning process, enabling a more dynamic interaction between\nmodalities. Furthermore, we analyze the technical evolution of these methods,\ndiscuss their inherent challenges, and introduce key benchmark tasks and\nevaluation metrics for assessing multimodal reasoning performance. Finally, we\nprovide insights into future research directions from the following two\nperspectives: (i) from visual-language reasoning to omnimodal reasoning and\n(ii) from multimodal reasoning to multimodal agents. This survey aims to\nprovide a structured overview that will inspire further advancements in\nmultimodal reasoning research.",
            "upvotes": 1,
            "discussionId": "67e249bbbf8cc1d141a542dd",
            "ai_keywords": [
                "multimodal reasoning",
                "language-centric multimodal reasoning",
                "collaborative multimodal reasoning",
                "one-pass visual perception",
                "active visual perception",
                "action generation",
                "state update",
                "benchmark tasks",
                "evaluation metrics",
                "omnimodal reasoning",
                "multimodal agents"
            ]
        },
        "publishedAt": "2025-03-23T09:40:44.000Z",
        "title": "Mind with Eyes: from Language Reasoning to Multimodal Reasoning",
        "summary": "Language models have recently advanced into the realm of reasoning, yet it is\nthrough multimodal reasoning that we can fully unlock the potential to achieve\nmore comprehensive, human-like cognitive capabilities. This survey provides a\nsystematic overview of the recent multimodal reasoning approaches, categorizing\nthem into two levels: language-centric multimodal reasoning and collaborative\nmultimodal reasoning. The former encompasses one-pass visual perception and\nactive visual perception, where vision primarily serves a supporting role in\nlanguage reasoning. The latter involves action generation and state update\nwithin reasoning process, enabling a more dynamic interaction between\nmodalities. Furthermore, we analyze the technical evolution of these methods,\ndiscuss their inherent challenges, and introduce key benchmark tasks and\nevaluation metrics for assessing multimodal reasoning performance. Finally, we\nprovide insights into future research directions from the following two\nperspectives: (i) from visual-language reasoning to omnimodal reasoning and\n(ii) from multimodal reasoning to multimodal agents. This survey aims to\nprovide a structured overview that will inspire further advancements in\nmultimodal reasoning research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6489b2b9a9d27ef5f8c824c7/mlkEenSXDE4zWAEbO2El3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18071.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6489b2b9a9d27ef5f8c824c7",
            "avatarUrl": "/avatars/14655bb803ba0db259b7a52ea96fcd09.svg",
            "fullname": "zyllin",
            "name": "zygg",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14774",
            "authors": [
                {
                    "_id": "67e2c6540a17d66e31b9fa9c",
                    "user": {
                        "_id": "6479b418bfaa9e96b850959b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6479b418bfaa9e96b850959b/qQSSgCK2M8ATutL6H0Yvv.jpeg",
                        "isPro": false,
                        "fullname": "David Serrano Lozano",
                        "user": "davidserra9",
                        "type": "user"
                    },
                    "name": "David Serrano-Lozano",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T19:02:12.938Z",
                    "hidden": false
                },
                {
                    "_id": "67e2c6540a17d66e31b9fa9d",
                    "name": "Aditya Arora",
                    "hidden": false
                },
                {
                    "_id": "67e2c6540a17d66e31b9fa9e",
                    "name": "Luis Herranz",
                    "hidden": false
                },
                {
                    "_id": "67e2c6540a17d66e31b9fa9f",
                    "name": "Konstantinos G. Derpanis",
                    "hidden": false
                },
                {
                    "_id": "67e2c6540a17d66e31b9faa0",
                    "name": "Michael S. Brown",
                    "hidden": false
                },
                {
                    "_id": "67e2c6540a17d66e31b9faa1",
                    "name": "Javier Vazquez-Corral",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6479b418bfaa9e96b850959b/eJTcKtl1z4IUMaaYWOE7-.png"
            ],
            "publishedAt": "2025-03-18T23:01:22.000Z",
            "submittedOnDailyAt": "2025-03-25T13:37:28.261Z",
            "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction",
            "submittedOnDailyBy": {
                "_id": "6479b418bfaa9e96b850959b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6479b418bfaa9e96b850959b/qQSSgCK2M8ATutL6H0Yvv.jpeg",
                "isPro": false,
                "fullname": "David Serrano Lozano",
                "user": "davidserra9",
                "type": "user"
            },
            "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.",
            "upvotes": 1,
            "discussionId": "67e2c6570a17d66e31b9fbb2",
            "ai_keywords": [
                "transformer-based model",
                "spatial dependencies",
                "multi-illuminant scenarios",
                "sRGB WB datasets",
                "sRGB WB presets",
                "multi-illuminant dataset",
                "WB-corrected images",
                "image fusion dataset"
            ]
        },
        "publishedAt": "2025-03-18T19:01:22.000Z",
        "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction",
        "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6479b418bfaa9e96b850959b/eJTcKtl1z4IUMaaYWOE7-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14774.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6479b418bfaa9e96b850959b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6479b418bfaa9e96b850959b/qQSSgCK2M8ATutL6H0Yvv.jpeg",
            "fullname": "David Serrano Lozano",
            "name": "davidserra9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.13074",
            "authors": [
                {
                    "_id": "67e2c563ecba0eb812dec03b",
                    "name": "Shaolin Su",
                    "hidden": false
                },
                {
                    "_id": "67e2c563ecba0eb812dec03c",
                    "name": "Josep M. Rocafort",
                    "hidden": false
                },
                {
                    "_id": "67e2c563ecba0eb812dec03d",
                    "name": "Danna Xue",
                    "hidden": false
                },
                {
                    "_id": "67e2c563ecba0eb812dec03e",
                    "user": {
                        "_id": "6479b418bfaa9e96b850959b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6479b418bfaa9e96b850959b/qQSSgCK2M8ATutL6H0Yvv.jpeg",
                        "isPro": false,
                        "fullname": "David Serrano Lozano",
                        "user": "davidserra9",
                        "type": "user"
                    },
                    "name": "David Serrano-Lozano",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T19:02:21.584Z",
                    "hidden": false
                },
                {
                    "_id": "67e2c563ecba0eb812dec03f",
                    "name": "Lei Sun",
                    "hidden": false
                },
                {
                    "_id": "67e2c563ecba0eb812dec040",
                    "name": "Javier Vazquez-Corral",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6479b418bfaa9e96b850959b/9x3X3CR7kTvocKSy90OKA.png"
            ],
            "publishedAt": "2025-03-17T11:25:48.000Z",
            "submittedOnDailyAt": "2025-03-25T13:34:34.161Z",
            "title": "Rethinking Image Evaluation in Super-Resolution",
            "submittedOnDailyBy": {
                "_id": "6479b418bfaa9e96b850959b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6479b418bfaa9e96b850959b/qQSSgCK2M8ATutL6H0Yvv.jpeg",
                "isPro": false,
                "fullname": "David Serrano Lozano",
                "user": "davidserra9",
                "type": "user"
            },
            "summary": "While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100% trustworthy\nfor model evaluations? How does GT quality affect this evaluation? And how to\nmake fair evaluations if there exist imperfect GTs? To answer these questions,\nthis paper presents two main contributions. First, by systematically analyzing\nseven state-of-the-art SR models across three real-world SR datasets, we show\nthat SR performances can be consistently affected across models by low-quality\nGTs, and models can perform quite differently when GT quality is controlled.\nSecond, we propose a novel perceptual quality metric, Relative Quality Index\n(RQI), that measures the relative quality discrepancy of image pairs, thus\nissuing the biased evaluations caused by unreliable GTs. Our proposed model\nachieves significantly better consistency with human opinions. We expect our\nwork to provide insights for the SR community on how future datasets, models,\nand metrics should be developed.",
            "upvotes": 1,
            "discussionId": "67e2c569ecba0eb812dec293",
            "projectPage": "https://rqi-sr.github.io/",
            "ai_keywords": [
                "image super-resolution (SR)",
                "perceptual quality",
                "quantitative evaluations",
                "image metrics",
                "reference ground truth (GT)",
                "state-of-the-art SR models",
                "real-world SR datasets",
                "low-quality GTs",
                "Relative Quality Index (RQI)",
                "human opinions"
            ]
        },
        "publishedAt": "2025-03-17T07:25:48.000Z",
        "title": "Rethinking Image Evaluation in Super-Resolution",
        "summary": "While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100% trustworthy\nfor model evaluations? How does GT quality affect this evaluation? And how to\nmake fair evaluations if there exist imperfect GTs? To answer these questions,\nthis paper presents two main contributions. First, by systematically analyzing\nseven state-of-the-art SR models across three real-world SR datasets, we show\nthat SR performances can be consistently affected across models by low-quality\nGTs, and models can perform quite differently when GT quality is controlled.\nSecond, we propose a novel perceptual quality metric, Relative Quality Index\n(RQI), that measures the relative quality discrepancy of image pairs, thus\nissuing the biased evaluations caused by unreliable GTs. Our proposed model\nachieves significantly better consistency with human opinions. We expect our\nwork to provide insights for the SR community on how future datasets, models,\nand metrics should be developed.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6479b418bfaa9e96b850959b/9x3X3CR7kTvocKSy90OKA.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13074.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6479b418bfaa9e96b850959b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6479b418bfaa9e96b850959b/qQSSgCK2M8ATutL6H0Yvv.jpeg",
            "fullname": "David Serrano Lozano",
            "name": "davidserra9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18674",
            "authors": [
                {
                    "_id": "67e2c6f567aea67078c6a5f0",
                    "user": {
                        "_id": "641af8477c21ab946bf18a91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641af8477c21ab946bf18a91/iuJpnsvfMEPP-YWcXyh1D.jpeg",
                        "isPro": false,
                        "fullname": "Edoardo De Matteis",
                        "user": "edodema",
                        "type": "user"
                    },
                    "name": "Edoardo De Matteis",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T19:01:57.585Z",
                    "hidden": false
                },
                {
                    "_id": "67e2c6f567aea67078c6a5f1",
                    "name": "Matteo Migliarini",
                    "hidden": false
                },
                {
                    "_id": "67e2c6f567aea67078c6a5f2",
                    "name": "Alessio Sampieri",
                    "hidden": false
                },
                {
                    "_id": "67e2c6f567aea67078c6a5f3",
                    "name": "Indro Spinelli",
                    "hidden": false
                },
                {
                    "_id": "67e2c6f567aea67078c6a5f4",
                    "name": "Fabio Galasso",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T13:46:27.000Z",
            "submittedOnDailyAt": "2025-03-25T13:44:43.353Z",
            "title": "Human Motion Unlearning",
            "submittedOnDailyBy": {
                "_id": "641af8477c21ab946bf18a91",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641af8477c21ab946bf18a91/iuJpnsvfMEPP-YWcXyh1D.jpeg",
                "isPro": false,
                "fullname": "Edoardo De Matteis",
                "user": "edodema",
                "type": "user"
            },
            "summary": "We introduce the task of human motion unlearning to prevent the synthesis of\ntoxic animations while preserving the general text-to-motion generative\nperformance. Unlearning toxic motions is challenging as those can be generated\nfrom explicit text prompts and from implicit toxic combinations of safe motions\n(e.g., ``kicking\" is ``loading and swinging a leg\"). We propose the first\nmotion unlearning benchmark by filtering toxic motions from the large and\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\nby adapting state-of-the-art image unlearning techniques to process\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\ndiffusion models. LCR is simple and consistently outperforms baselines\nqualitatively and quantitatively. Project page:\nhttps://www.pinlab.org/hmu{https://www.pinlab.org/hmu}.",
            "upvotes": 0,
            "discussionId": "67e2c6f667aea67078c6a65a",
            "projectPage": "https://www.pinlab.org/hmu",
            "githubRepo": "https://github.com/edodema/human-motion-unlearning/",
            "ai_keywords": [
                "text-to-motion",
                "human motion unlearning",
                "toxic animations",
                "HumanML3D",
                "Motion-X",
                "spatio-temporal signals",
                "Latent Code Replacement",
                "LCR",
                "text-to-motion diffusion models"
            ]
        },
        "publishedAt": "2025-03-24T09:46:27.000Z",
        "title": "Human Motion Unlearning",
        "summary": "We introduce the task of human motion unlearning to prevent the synthesis of\ntoxic animations while preserving the general text-to-motion generative\nperformance. Unlearning toxic motions is challenging as those can be generated\nfrom explicit text prompts and from implicit toxic combinations of safe motions\n(e.g., ``kicking\" is ``loading and swinging a leg\"). We propose the first\nmotion unlearning benchmark by filtering toxic motions from the large and\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\nby adapting state-of-the-art image unlearning techniques to process\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\ndiffusion models. LCR is simple and consistently outperforms baselines\nqualitatively and quantitatively. Project page:\nhttps://www.pinlab.org/hmu{https://www.pinlab.org/hmu}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18674.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641af8477c21ab946bf18a91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641af8477c21ab946bf18a91/iuJpnsvfMEPP-YWcXyh1D.jpeg",
            "fullname": "Edoardo De Matteis",
            "name": "edodema",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18476",
            "authors": [
                {
                    "_id": "67e29a0af38c0d7de45c28ee",
                    "user": {
                        "_id": "65816205e6b9dfd2cd2cff14",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yfzfGuD0PRYiumnI-Ei3-.jpeg",
                        "isPro": false,
                        "fullname": "Wei",
                        "user": "WeiDeng1999",
                        "type": "user"
                    },
                    "name": "Wei Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T12:18:58.901Z",
                    "hidden": false
                },
                {
                    "_id": "67e29a0af38c0d7de45c28ef",
                    "name": "Mengshi Qi",
                    "hidden": false
                },
                {
                    "_id": "67e29a0af38c0d7de45c28f0",
                    "name": "Huadong Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T09:21:13.000Z",
            "submittedOnDailyAt": "2025-03-25T23:07:08.087Z",
            "title": "Global-Local Tree Search for Language Guided 3D Scene Generation",
            "submittedOnDailyBy": {
                "_id": "65816205e6b9dfd2cd2cff14",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yfzfGuD0PRYiumnI-Ei3-.jpeg",
                "isPro": false,
                "fullname": "Wei",
                "user": "WeiDeng1999",
                "type": "user"
            },
            "summary": "Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable\nsuccess across various fields. However, there are few studies on 3D indoor\nscene generation with VLMs. This paper considers this task as a planning\nproblem subject to spatial and layout common sense constraints. To solve the\nproblem with a VLM, we propose a new global-local tree search algorithm.\nGlobally, the method places each object sequentially and explores multiple\nplacements during each placement process, where the problem space is\nrepresented as a tree. To reduce the depth of the tree, we decompose the scene\nstructure hierarchically, i.e. room level, region level, floor object level,\nand supported object level. The algorithm independently generates the floor\nobjects in different regions and supported objects placed on different floor\nobjects. Locally, we also decompose the sub-task, the placement of each object,\ninto multiple steps. The algorithm searches the tree of problem space. To\nleverage the VLM model to produce positions of objects, we discretize the\ntop-down view space as a dense grid and fill each cell with diverse emojis to\nmake to cells distinct. We prompt the VLM with the emoji grid and the VLM\nproduces a reasonable location for the object by describing the position with\nthe name of emojis. The quantitative and qualitative experimental results\nillustrate our approach generates more plausible 3D scenes than\nstate-of-the-art approaches. Our source code is available at\nhttps://github.com/dw-dengwei/TreeSearchGen .",
            "upvotes": 0,
            "discussionId": "67e29a0cf38c0d7de45c297d",
            "ai_keywords": [
                "Global-local tree search algorithm",
                "spatial and layout common sense constraints",
                "problem space",
                "tree representation",
                "scene structure hierarchy",
                "room level",
                "region level",
                "floor object level",
                "supported object level",
                "top-down view space",
                "dense grid",
                "emojis",
                "prompt",
                "VLM (Vision-Language Model)",
                "3D scene generation"
            ]
        },
        "publishedAt": "2025-03-24T05:21:13.000Z",
        "title": "Global-Local Tree Search for Language Guided 3D Scene Generation",
        "summary": "Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable\nsuccess across various fields. However, there are few studies on 3D indoor\nscene generation with VLMs. This paper considers this task as a planning\nproblem subject to spatial and layout common sense constraints. To solve the\nproblem with a VLM, we propose a new global-local tree search algorithm.\nGlobally, the method places each object sequentially and explores multiple\nplacements during each placement process, where the problem space is\nrepresented as a tree. To reduce the depth of the tree, we decompose the scene\nstructure hierarchically, i.e. room level, region level, floor object level,\nand supported object level. The algorithm independently generates the floor\nobjects in different regions and supported objects placed on different floor\nobjects. Locally, we also decompose the sub-task, the placement of each object,\ninto multiple steps. The algorithm searches the tree of problem space. To\nleverage the VLM model to produce positions of objects, we discretize the\ntop-down view space as a dense grid and fill each cell with diverse emojis to\nmake to cells distinct. We prompt the VLM with the emoji grid and the VLM\nproduces a reasonable location for the object by describing the position with\nthe name of emojis. The quantitative and qualitative experimental results\nillustrate our approach generates more plausible 3D scenes than\nstate-of-the-art approaches. Our source code is available at\nhttps://github.com/dw-dengwei/TreeSearchGen .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18476.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65816205e6b9dfd2cd2cff14",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yfzfGuD0PRYiumnI-Ei3-.jpeg",
            "fullname": "Wei",
            "name": "WeiDeng1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16709",
            "authors": [
                {
                    "_id": "67e314352fb3b4f2e999ec50",
                    "name": "Xuan Shen",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec51",
                    "name": "Weize Ma",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec52",
                    "name": "Jing Liu",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec53",
                    "name": "Changdi Yang",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec54",
                    "name": "Rui Ding",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec55",
                    "name": "Quanyi Wang",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec56",
                    "name": "Henghui Ding",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec57",
                    "name": "Wei Niu",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec58",
                    "name": "Yanzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec59",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec5a",
                    "name": "Jun Lin",
                    "hidden": false
                },
                {
                    "_id": "67e314352fb3b4f2e999ec5b",
                    "name": "Jiuxiang Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T21:03:10.000Z",
            "submittedOnDailyAt": "2025-03-25T19:08:32.789Z",
            "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on\n  the Edge",
            "submittedOnDailyBy": {
                "_id": "65f7197c66afd39dd7f58522",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zx-6RlvxTATmIO3dCIYez.png",
                "isPro": false,
                "fullname": "Shawn Shen",
                "user": "shawnricecake",
                "type": "user"
            },
            "summary": "Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer\nvision, supporting numerous real-world applications. However, deploying\naccurate depth estimation models on resource-limited edge devices, especially\nApplication-Specific Integrated Circuits (ASICs), is challenging due to the\nhigh computational and memory demands. Recent advancements in foundational\ndepth estimation deliver impressive results but further amplify the difficulty\nof deployment on ASICs. To address this, we propose QuartDepth which adopts\npost-training quantization to quantize MDE models with hardware accelerations\nfor ASICs. Our approach involves quantizing both weights and activations to\n4-bit precision, reducing the model size and computation cost. To mitigate the\nperformance degradation, we introduce activation polishing and compensation\nalgorithm applied before and after activation quantization, as well as a weight\nreconstruction method for minimizing errors in weight quantization.\nFurthermore, we design a flexible and programmable hardware accelerator by\nsupporting kernel fusion and customized instruction programmability, enhancing\nthroughput and efficiency. Experimental results demonstrate that our framework\nachieves competitive accuracy while enabling fast inference and higher energy\nefficiency on ASICs, bridging the gap between high-performance depth estimation\nand practical edge-device applicability. Code:\nhttps://github.com/shawnricecake/quart-depth",
            "upvotes": 0,
            "discussionId": "67e314362fb3b4f2e999eca6",
            "ai_keywords": [
                "post-training quantization",
                "weights quantization",
                "activations quantization",
                "activation polishing",
                "compensation algorithm",
                "weight reconstruction method",
                "kernel fusion",
                "customized instruction programmability",
                "hardware accelerator"
            ]
        },
        "publishedAt": "2025-03-20T17:03:10.000Z",
        "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on\n  the Edge",
        "summary": "Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer\nvision, supporting numerous real-world applications. However, deploying\naccurate depth estimation models on resource-limited edge devices, especially\nApplication-Specific Integrated Circuits (ASICs), is challenging due to the\nhigh computational and memory demands. Recent advancements in foundational\ndepth estimation deliver impressive results but further amplify the difficulty\nof deployment on ASICs. To address this, we propose QuartDepth which adopts\npost-training quantization to quantize MDE models with hardware accelerations\nfor ASICs. Our approach involves quantizing both weights and activations to\n4-bit precision, reducing the model size and computation cost. To mitigate the\nperformance degradation, we introduce activation polishing and compensation\nalgorithm applied before and after activation quantization, as well as a weight\nreconstruction method for minimizing errors in weight quantization.\nFurthermore, we design a flexible and programmable hardware accelerator by\nsupporting kernel fusion and customized instruction programmability, enhancing\nthroughput and efficiency. Experimental results demonstrate that our framework\nachieves competitive accuracy while enabling fast inference and higher energy\nefficiency on ASICs, bridging the gap between high-performance depth estimation\nand practical edge-device applicability. Code:\nhttps://github.com/shawnricecake/quart-depth",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16709.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f7197c66afd39dd7f58522",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zx-6RlvxTATmIO3dCIYez.png",
            "fullname": "Shawn Shen",
            "name": "shawnricecake",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.16426",
            "authors": [
                {
                    "_id": "67e2ac3b768537c7d7da2a88",
                    "user": {
                        "_id": "62adcbf533c317b9b5dcf794",
                        "avatarUrl": "/avatars/b4b8e2ecab25801b6b2ba4175073f605.svg",
                        "isPro": false,
                        "fullname": "KeyanChen",
                        "user": "KyanChen",
                        "type": "user"
                    },
                    "name": "Keyan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T14:39:01.351Z",
                    "hidden": false
                },
                {
                    "_id": "67e2ac3b768537c7d7da2a89",
                    "name": "Chenyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67e2ac3b768537c7d7da2a8a",
                    "name": "Bowen Chen",
                    "hidden": false
                },
                {
                    "_id": "67e2ac3b768537c7d7da2a8b",
                    "user": {
                        "_id": "65029e5889707f182388f8d4",
                        "avatarUrl": "/avatars/af50b72b78f35b2af7a0e12b59cbf1bd.svg",
                        "isPro": false,
                        "fullname": "wenyuan Li",
                        "user": "WenyuanLi",
                        "type": "user"
                    },
                    "name": "Wenyuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T14:40:00.722Z",
                    "hidden": false
                },
                {
                    "_id": "67e2ac3b768537c7d7da2a8c",
                    "user": {
                        "_id": "62cfc18780c9d4ceb2fedebf",
                        "avatarUrl": "/avatars/a1cd4571cbda924d50eadd37c56079d9.svg",
                        "isPro": false,
                        "fullname": "Zhengxia Zou",
                        "user": "Jiupinjia",
                        "type": "user"
                    },
                    "name": "Zhengxia Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-25T14:39:36.651Z",
                    "hidden": false
                },
                {
                    "_id": "67e2ac3b768537c7d7da2a8d",
                    "name": "Zhenwei Shi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62adcbf533c317b9b5dcf794/gPaOVlZy-_6MI6l7CMZpk.png"
            ],
            "publishedAt": "2025-03-20T17:59:54.000Z",
            "submittedOnDailyAt": "2025-03-25T11:45:54.183Z",
            "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding",
            "submittedOnDailyBy": {
                "_id": "62adcbf533c317b9b5dcf794",
                "avatarUrl": "/avatars/b4b8e2ecab25801b6b2ba4175073f605.svg",
                "isPro": false,
                "fullname": "KeyanChen",
                "user": "KyanChen",
                "type": "user"
            },
            "summary": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
            "upvotes": 0,
            "discussionId": "67e2ac40768537c7d7da2bfb",
            "projectPage": "https://github.com/KyanChen/DynamicVis",
            "githubRepo": "https://github.com/KyanChen/DynamicVis",
            "ai_keywords": [
                "dynamic visual perception foundation model",
                "dynamic region perception backbone",
                "selective state space model",
                "multi-instance learning paradigm",
                "meta-embedding representations",
                "multi-level feature modeling"
            ]
        },
        "publishedAt": "2025-03-20T13:59:54.000Z",
        "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding",
        "summary": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62adcbf533c317b9b5dcf794/gPaOVlZy-_6MI6l7CMZpk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16426.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62adcbf533c317b9b5dcf794",
            "avatarUrl": "/avatars/b4b8e2ecab25801b6b2ba4175073f605.svg",
            "fullname": "KeyanChen",
            "name": "KyanChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    }
]