[
    {
        "paper": {
            "id": "2503.19325",
            "authors": [
                {
                    "_id": "67e35f6fc9d8214b5e1c64c3",
                    "name": "Yuchao Gu",
                    "hidden": false
                },
                {
                    "_id": "67e35f6fc9d8214b5e1c64c4",
                    "name": "Weijia Mao",
                    "hidden": false
                },
                {
                    "_id": "67e35f6fc9d8214b5e1c64c5",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
            ],
            "publishedAt": "2025-03-25T03:38:06.000Z",
            "submittedOnDailyAt": "2025-03-26T00:37:14.940Z",
            "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
            "submittedOnDailyBy": {
                "_id": "63021630a35b21bd8a53305a",
                "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
                "isPro": true,
                "fullname": "Gu Yuchao",
                "user": "guyuchao",
                "type": "user"
            },
            "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
            "upvotes": 59,
            "discussionId": "67e35f72c9d8214b5e1c659b",
            "ai_keywords": [
                "Frame AutoRegressive (FAR)",
                "Token AR",
                "video autoregressive modeling",
                "visual redundancy",
                "RoPE (Rotary Position Embedding)",
                "temporal decay",
                "FlexRoPE",
                "long short-term context modeling",
                "high-resolution short-term context window",
                "long-term context window",
                "state-of-the-art performance",
                "video generation"
            ]
        },
        "publishedAt": "2025-03-24T23:38:06.000Z",
        "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
        "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63021630a35b21bd8a53305a",
            "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
            "fullname": "Gu Yuchao",
            "name": "guyuchao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19622",
            "authors": [
                {
                    "_id": "67e3706bc9d8214b5e219149",
                    "name": "Hongcheng Gao",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e21914a",
                    "user": {
                        "_id": "66b980d66d490d845f8a697e",
                        "avatarUrl": "/avatars/6ab2d08436851063f55baaadae8c4bc0.svg",
                        "isPro": false,
                        "fullname": "Joshua Qu",
                        "user": "Joshua999",
                        "type": "user"
                    },
                    "name": "Jiashu Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:30.396Z",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e21914b",
                    "name": "Jingyi Tang",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e21914c",
                    "name": "Baolong Bi",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e21914d",
                    "name": "Yue Liu",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e21914e",
                    "name": "Hongyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e21914f",
                    "name": "Li Liang",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e219150",
                    "name": "Li Su",
                    "hidden": false
                },
                {
                    "_id": "67e3706bc9d8214b5e219151",
                    "name": "Qingming Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T13:12:17.000Z",
            "submittedOnDailyAt": "2025-03-26T01:44:03.080Z",
            "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
            "submittedOnDailyBy": {
                "_id": "62728f4f6253fe2068da1021",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                "isPro": false,
                "fullname": "Hongcheng Gao",
                "user": "HongchengGao",
                "type": "user"
            },
            "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
            "upvotes": 26,
            "discussionId": "67e3706dc9d8214b5e2191e0",
            "githubRepo": "https://github.com/Hongcheng-Gao/HAVEN",
            "ai_keywords": [
                "multimodal models (LMMs)",
                "hallucination",
                "video modality",
                "video understanding",
                "HAVEN",
                "hallucination causes",
                "hallucination aspects",
                "question formats",
                "duration time",
                "model sizes",
                "model reasoning",
                "supervised reasoning fine-tuning (SRFT)",
                "direct preference optimization (TDPO)",
                "video-thinking model",
                "accuracy",
                "bias score"
            ]
        },
        "publishedAt": "2025-03-25T09:12:17.000Z",
        "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
        "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "fullname": "Hongcheng Gao",
            "name": "HongchengGao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18931",
            "authors": [
                {
                    "_id": "67e25c4d1908043170bd551d",
                    "user": {
                        "_id": "64651db3611ae99d14d392ea",
                        "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
                        "isPro": false,
                        "fullname": "cyt",
                        "user": "Row11n",
                        "type": "user"
                    },
                    "name": "Yitong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T08:18:45.692Z",
                    "hidden": false
                },
                {
                    "_id": "67e25c4d1908043170bd551e",
                    "name": "Lingchen Meng",
                    "hidden": false
                },
                {
                    "_id": "67e25c4d1908043170bd551f",
                    "user": {
                        "_id": "649bce4f200e2dff194d9883",
                        "avatarUrl": "/avatars/b55a8bdc6f7e2bf9de5f26dc1d87bee3.svg",
                        "isPro": false,
                        "fullname": "Wujian Peng",
                        "user": "wjpoom",
                        "type": "user"
                    },
                    "name": "Wujian Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:49:16.314Z",
                    "hidden": false
                },
                {
                    "_id": "67e25c4d1908043170bd5520",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "67e25c4d1908043170bd5521",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:52:47.000Z",
            "submittedOnDailyAt": "2025-03-26T01:10:42.553Z",
            "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
            "submittedOnDailyBy": {
                "_id": "64651db3611ae99d14d392ea",
                "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
                "isPro": false,
                "fullname": "cyt",
                "user": "Row11n",
                "type": "user"
            },
            "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
            "upvotes": 26,
            "discussionId": "67e25c4f1908043170bd55a8",
            "projectPage": "https://slimm-x.github.io/comp/",
            "githubRepo": "https://github.com/SliMM-X/CoMP-MM",
            "ai_keywords": [
                "Vision Foundation Models (VFMs)",
                "Continual Rotary Position Embedding",
                "Alignment Loss",
                "language prototypes",
                "multimodal pre-training pipeline",
                "three-stage training",
                "multimodal understanding",
                "classification",
                "segmentation",
                "ChartQA",
                "DocVQA",
                "LLM",
                "ImageNet-1K",
                "ADE20K",
                "frozen chunk evaluation"
            ]
        },
        "publishedAt": "2025-03-24T13:52:47.000Z",
        "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
        "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64651db3611ae99d14d392ea",
            "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
            "fullname": "cyt",
            "name": "Row11n",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19385",
            "authors": [
                {
                    "_id": "67e36241d8da46951f858026",
                    "name": "Jaihoon Kim",
                    "hidden": false
                },
                {
                    "_id": "67e36241d8da46951f858027",
                    "name": "Taehoon Yoon",
                    "hidden": false
                },
                {
                    "_id": "67e36241d8da46951f858028",
                    "name": "Jisung Hwang",
                    "hidden": false
                },
                {
                    "_id": "67e36241d8da46951f858029",
                    "name": "Minhyuk Sung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T06:30:45.000Z",
            "submittedOnDailyAt": "2025-03-26T00:49:38.583Z",
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
            "submittedOnDailyBy": {
                "_id": "6342796a0875f2c99cfd313b",
                "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
                "isPro": false,
                "fullname": "Yuseung \"Phillip\" Lee",
                "user": "phillipinseoul",
                "type": "user"
            },
            "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
            "upvotes": 25,
            "discussionId": "67e36245d8da46951f85802c",
            "projectPage": "https://flow-inference-time-scaling.github.io/",
            "ai_keywords": [
                "flow models",
                "inference-time scaling",
                "LLMs",
                "diffusion models",
                "sample quality",
                "user preferences",
                "particle sampling",
                "stochasticity",
                "denoising steps",
                "generative process",
                "SDE-based generation",
                "interpolant conversion",
                "sample diversity",
                "Rollover Budget Forcing (RBF)",
                "adaptive allocation",
                "computational resources",
                "timesteps",
                "budget utilization",
                "variance-preserving (VP)",
                "VP interpolant-based generation"
            ]
        },
        "publishedAt": "2025-03-25T02:30:45.000Z",
        "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
        "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6342796a0875f2c99cfd313b",
            "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
            "fullname": "Yuseung \"Phillip\" Lee",
            "name": "phillipinseoul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19903",
            "authors": [
                {
                    "_id": "67e375d3cc93cc8c42da7699",
                    "user": {
                        "_id": "649004218f7cbbc94c782db6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
                        "isPro": false,
                        "fullname": "Baifeng Shi",
                        "user": "bfshi",
                        "type": "user"
                    },
                    "name": "Baifeng Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:24.349Z",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da769a",
                    "name": "Boyi Li",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da769b",
                    "name": "Han Cai",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da769c",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da769d",
                    "name": "Sifei Liu",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da769e",
                    "name": "Marco Pavone",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da769f",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da76a0",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da76a1",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da76a2",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "67e375d3cc93cc8c42da76a3",
                    "name": "Hongxu Yin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
            ],
            "publishedAt": "2025-03-25T17:58:37.000Z",
            "submittedOnDailyAt": "2025-03-26T02:13:20.800Z",
            "title": "Scaling Vision Pre-Training to 4K Resolution",
            "submittedOnDailyBy": {
                "_id": "649004218f7cbbc94c782db6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
                "isPro": false,
                "fullname": "Baifeng Shi",
                "user": "bfshi",
                "type": "user"
            },
            "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
            "upvotes": 21,
            "discussionId": "67e375d9cc93cc8c42da785f",
            "projectPage": "https://nvlabs.github.io/PS3/",
            "githubRepo": "https://github.com/NVlabs/PS3",
            "ai_keywords": [
                "PS3",
                "CLIP-style vision pre-training",
                "contrastive learning",
                "local regions",
                "local detailed captions",
                "high-resolution representation learning",
                "computational overhead",
                "saliency",
                "text prompt",
                "VILA-HD",
                "multi-modal LLM",
                "high-resolution visual perception",
                "AnyRes",
                "S^2",
                "scaling properties",
                "test-time compute",
                "NVILA",
                "Qwen2-VL",
                "benchmarks",
                "token pruning approaches",
                "4KPer",
                "image QA",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-03-25T13:58:37.000Z",
        "title": "Scaling Vision Pre-Training to 4K Resolution",
        "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649004218f7cbbc94c782db6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
            "fullname": "Baifeng Shi",
            "name": "bfshi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14905",
            "authors": [
                {
                    "_id": "67e250450487eeecfd9a5880",
                    "name": "Siwei Wen",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5881",
                    "name": "Junyan Ye",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5882",
                    "name": "Peilin Feng",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5883",
                    "name": "Hengrui Kang",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5884",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5885",
                    "name": "Yize Chen",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5886",
                    "name": "Jiang Wu",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5887",
                    "name": "Wenjun Wu",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5888",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "67e250450487eeecfd9a5889",
                    "name": "Weijia Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T05:14:44.000Z",
            "submittedOnDailyAt": "2025-03-26T04:00:13.753Z",
            "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
            "submittedOnDailyBy": {
                "_id": "653b8c3e97a4d71d950e2f20",
                "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                "isPro": false,
                "fullname": "Zichen Wen",
                "user": "zichenwen",
                "type": "user"
            },
            "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
            "upvotes": 15,
            "discussionId": "67e250490487eeecfd9a599e",
            "githubRepo": "https://github.com/opendatalab/FakeVLM",
            "ai_keywords": [
                "large multimodal model",
                "FakeVLM",
                "DeepFake detection",
                "image artifacts",
                "natural language explanations",
                "FakeClue",
                "fine-grained artifact clues"
            ]
        },
        "publishedAt": "2025-03-19T01:14:44.000Z",
        "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
        "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "fullname": "Zichen Wen",
            "name": "zichenwen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.13964",
            "authors": [
                {
                    "_id": "67e20852c0c932395394dbb0",
                    "name": "Siwei Han",
                    "hidden": false
                },
                {
                    "_id": "67e20852c0c932395394dbb1",
                    "user": {
                        "_id": "643e9ee6f6bb3c31a26e7bc4",
                        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                        "isPro": false,
                        "fullname": "Peng Xia",
                        "user": "richardxp888",
                        "type": "user"
                    },
                    "name": "Peng Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:49:23.939Z",
                    "hidden": false
                },
                {
                    "_id": "67e20852c0c932395394dbb2",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e20852c0c932395394dbb3",
                    "name": "Tong Sun",
                    "hidden": false
                },
                {
                    "_id": "67e20852c0c932395394dbb4",
                    "name": "Yun Li",
                    "hidden": false
                },
                {
                    "_id": "67e20852c0c932395394dbb5",
                    "name": "Hongtu Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e20852c0c932395394dbb6",
                    "name": "Huaxiu Yao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
            ],
            "publishedAt": "2025-03-18T06:57:21.000Z",
            "submittedOnDailyAt": "2025-03-26T03:52:37.520Z",
            "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "643e9ee6f6bb3c31a26e7bc4",
                "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                "isPro": false,
                "fullname": "Peng Xia",
                "user": "richardxp888",
                "type": "user"
            },
            "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
            "upvotes": 13,
            "discussionId": "67e20858c0c932395394dde6",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Large Vision Language Models (LVLMs)",
                "Retrieval Augmented Generation (RAG)",
                "multi-modal reasoning",
                "multi-modal multi-agent framework",
                "general agent",
                "critical agent",
                "text agent",
                "image agent",
                "summarizing agent",
                "multi-modal context retrieval"
            ]
        },
        "publishedAt": "2025-03-18T02:57:21.000Z",
        "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
        "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643e9ee6f6bb3c31a26e7bc4",
            "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
            "fullname": "Peng Xia",
            "name": "richardxp888",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19855",
            "authors": [
                {
                    "_id": "67e36792a281c900d76a93c8",
                    "name": "Xiaoyu Tian",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93c9",
                    "name": "Sitong Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93ca",
                    "name": "Haotian Wang",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93cb",
                    "name": "Shuaiting Chen",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93cc",
                    "name": "Yunjie Ji",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93cd",
                    "name": "Yiping Peng",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93ce",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e36792a281c900d76a93cf",
                    "name": "Xiangang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T17:19:38.000Z",
            "submittedOnDailyAt": "2025-03-26T01:04:39.479Z",
            "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
            "upvotes": 12,
            "discussionId": "67e36793a281c900d76a9459",
            "ai_keywords": [
                "large language models",
                "OpenAI-o1",
                "DeepSeek-R1",
                "test-time scaling",
                "extended reasoning processes",
                "reinforcement learning",
                "Multi-round Thinking",
                "iterative refinement",
                "AIME 2024",
                "MATH-500",
                "GPQA-diamond",
                "LiveCodeBench",
                "accuracy",
                "stable enhancements",
                "test-time scaling techniques"
            ]
        },
        "publishedAt": "2025-03-25T13:19:38.000Z",
        "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
        "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6475
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19910",
            "authors": [
                {
                    "_id": "67e35e4cff080b9ee71e3295",
                    "user": {
                        "_id": "63a4d196cde2b28f82a56bd9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
                        "isPro": false,
                        "fullname": "Chuong Huynh",
                        "user": "chuonghm",
                        "type": "user"
                    },
                    "name": "Chuong Huynh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:39.247Z",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e3296",
                    "name": "Jinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e3297",
                    "name": "Ashish Tawari",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e3298",
                    "name": "Mubarak Shah",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e3299",
                    "name": "Son Tran",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e329a",
                    "name": "Raffay Hamid",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e329b",
                    "name": "Trishul Chilimbi",
                    "hidden": false
                },
                {
                    "_id": "67e35e4cff080b9ee71e329c",
                    "name": "Abhinav Shrivastava",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T17:59:50.000Z",
            "submittedOnDailyAt": "2025-03-26T00:26:00.764Z",
            "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
            "submittedOnDailyBy": {
                "_id": "63a4d196cde2b28f82a56bd9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
                "isPro": false,
                "fullname": "Chuong Huynh",
                "user": "chuonghm",
                "type": "user"
            },
            "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
            "upvotes": 9,
            "discussionId": "67e35e4eff080b9ee71e3353",
            "projectPage": "https://collm-cvpr25.github.io/",
            "ai_keywords": [
                "Composed Image Retrieval (CIR)",
                "multimodal query",
                "triplets",
                "reference image",
                "textual description",
                "target image",
                "zero-shot approaches",
                "synthetic triplets",
                "vision-language models (VLMs)",
                "web-crawled image-caption pairs",
                "joint embedding learning",
                "complex and nuanced modification texts",
                "multimodal fusion",
                "CoLLM",
                "Large Language Models (LLMs)",
                "Multi-Text CIR (MTCIR)",
                "CIRR benchmark",
                "Fashion-IQ benchmark",
                "state-of-the-art performance"
            ]
        },
        "publishedAt": "2025-03-25T13:59:50.000Z",
        "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
        "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4d196cde2b28f82a56bd9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
            "fullname": "Chuong Huynh",
            "name": "chuonghm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19470",
            "authors": [
                {
                    "_id": "67e365b0dcfc2aeae1bf3da2",
                    "name": "Mingyang Chen",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da3",
                    "name": "Tianpeng Li",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da4",
                    "name": "Haoze Sun",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da5",
                    "name": "Yijie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da6",
                    "name": "Chenzheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da7",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da8",
                    "name": "Zenan Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3da9",
                    "name": "Weipeng Chen",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3daa",
                    "name": "Haofen Wang",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3dab",
                    "name": "Jeff Z. Pan",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3dac",
                    "name": "Wen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e365b0dcfc2aeae1bf3dad",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T09:00:58.000Z",
            "submittedOnDailyAt": "2025-03-26T00:56:07.098Z",
            "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
            "upvotes": 8,
            "discussionId": "67e365b1dcfc2aeae1bf3df6",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "OpenAI-o1",
                "DeepSeek-R1",
                "complex multi-hop questions",
                "ReSearch",
                "reinforcement learning",
                "text-based thinking",
                "reflection",
                "self-correction",
                "Qwen2.5-7B(-Instruct)",
                "Qwen2.5-32B(-Instruct)"
            ]
        },
        "publishedAt": "2025-03-25T05:00:58.000Z",
        "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
        "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6475
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18446",
            "authors": [
                {
                    "_id": "67e367ee4363e3c4bbbaca3a",
                    "name": "Jinho Jeong",
                    "hidden": false
                },
                {
                    "_id": "67e367ee4363e3c4bbbaca3b",
                    "name": "Sangmin Han",
                    "hidden": false
                },
                {
                    "_id": "67e367ee4363e3c4bbbaca3c",
                    "name": "Jinwoo Kim",
                    "hidden": false
                },
                {
                    "_id": "67e367ee4363e3c4bbbaca3d",
                    "name": "Seon Joo Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T08:50:15.000Z",
            "submittedOnDailyAt": "2025-03-26T01:07:15.007Z",
            "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "66b5f733f0c16f37f307f35e",
                "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
                "isPro": false,
                "fullname": "JinHo Jeong",
                "user": "3587jjh",
                "type": "user"
            },
            "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
            "upvotes": 6,
            "discussionId": "67e367f14363e3c4bbbacae1",
            "ai_keywords": [
                "LSRNA",
                "diffusion models",
                "latent space",
                "super-resolution",
                "structural distortions",
                "content repetition",
                "reference-based methods",
                "manifold deviation",
                "RGB space",
                "manifold alignment",
                "Region-wise Noise Addition (RNA)",
                "high-frequency details"
            ]
        },
        "publishedAt": "2025-03-24T04:50:15.000Z",
        "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
        "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66b5f733f0c16f37f307f35e",
            "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
            "fullname": "JinHo Jeong",
            "name": "3587jjh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19065",
            "authors": [
                {
                    "_id": "67e3deabf2cb5de878fbf557",
                    "user": {
                        "_id": "654f99f74c8874c64d4e5664",
                        "avatarUrl": "/avatars/e9da0d688f91ae49db91d0ebebb3782a.svg",
                        "isPro": false,
                        "fullname": "杨忠裕",
                        "user": "yzzyu",
                        "type": "user"
                    },
                    "name": "Zhongyu Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:00.382Z",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf558",
                    "name": "Jun Chen",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf559",
                    "name": "Dannong Xu",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf55a",
                    "name": "Junjie Fei",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf55b",
                    "name": "Xiaoqian Shen",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf55c",
                    "name": "Liangbing Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf55d",
                    "name": "Chun-Mei Feng",
                    "hidden": false
                },
                {
                    "_id": "67e3deabf2cb5de878fbf55e",
                    "name": "Mohamed Elhoseiny",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T18:51:55.000Z",
            "submittedOnDailyAt": "2025-03-26T12:36:33.366Z",
            "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Knowledge discovery and collection are intelligence-intensive tasks that\ntraditionally require significant human effort to ensure high-quality outputs.\nRecent research has explored multi-agent frameworks for automating\nWikipedia-style article generation by retrieving and synthesizing information\nfrom the internet. However, these methods primarily focus on text-only\ngeneration, overlooking the importance of multimodal content in enhancing\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\nsystem for automated multimodal Wikipedia-style article generation. Unlike\nprior approaches, WikiAutoGen retrieves and integrates relevant images\nalongside text, enriching both the depth and visual appeal of generated\ncontent. To further improve factual accuracy and comprehensiveness, we propose\na multi-perspective self-reflection mechanism, which critically assesses\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\nWikipedia articles with topics paired with both textual and image-based\nrepresentations, designed to evaluate multimodal knowledge generation on more\nchallenging topics. Experimental results show that WikiAutoGen outperforms\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\ncoherent, and visually enriched Wikipedia-style articles. We show some of our\ngenerated examples in https://wikiautogen.github.io/ .",
            "upvotes": 5,
            "discussionId": "67e3deacf2cb5de878fbf5f8",
            "projectPage": "https://wikiautogen.github.io/",
            "githubRepo": "https://github.com/01yzzyu/wikiautogen",
            "ai_keywords": [
                "multimodal content",
                "WikiAutoGen",
                "image retrieval",
                "text retrieval",
                "multi-perspective self-reflection mechanism",
                "WikiSeek",
                "multimodal knowledge generation",
                "factual accuracy",
                "comprehensiveness",
                "depth",
                "visual appeal",
                "reliability",
                "coherence",
                "visual enrichment"
            ]
        },
        "publishedAt": "2025-03-24T14:51:55.000Z",
        "title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation",
        "summary": "Knowledge discovery and collection are intelligence-intensive tasks that\ntraditionally require significant human effort to ensure high-quality outputs.\nRecent research has explored multi-agent frameworks for automating\nWikipedia-style article generation by retrieving and synthesizing information\nfrom the internet. However, these methods primarily focus on text-only\ngeneration, overlooking the importance of multimodal content in enhancing\ninformativeness and engagement. In this work, we introduce WikiAutoGen, a novel\nsystem for automated multimodal Wikipedia-style article generation. Unlike\nprior approaches, WikiAutoGen retrieves and integrates relevant images\nalongside text, enriching both the depth and visual appeal of generated\ncontent. To further improve factual accuracy and comprehensiveness, we propose\na multi-perspective self-reflection mechanism, which critically assesses\nretrieved content from diverse viewpoints to enhance reliability, breadth, and\ncoherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising\nWikipedia articles with topics paired with both textual and image-based\nrepresentations, designed to evaluate multimodal knowledge generation on more\nchallenging topics. Experimental results show that WikiAutoGen outperforms\nprevious methods by 8%-29% on our WikiSeek benchmark, producing more accurate,\ncoherent, and visually enriched Wikipedia-style articles. We show some of our\ngenerated examples in https://wikiautogen.github.io/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19065.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6475
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19041",
            "authors": [
                {
                    "_id": "67e35da0b1b97cc3392024b1",
                    "name": "Kangwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b2",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b3",
                    "name": "Yujie Luo",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b4",
                    "name": "Lin Yuan",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b5",
                    "name": "Mengshu Sun",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b6",
                    "name": "Ningyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b7",
                    "name": "Lei Liang",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b8",
                    "name": "Zhiqiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024b9",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e35da0b1b97cc3392024ba",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
            ],
            "publishedAt": "2025-03-24T18:11:42.000Z",
            "submittedOnDailyAt": "2025-03-26T00:22:20.466Z",
            "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
            "upvotes": 4,
            "discussionId": "67e35da1b1b97cc339202525",
            "ai_keywords": [
                "LookAhead Tuning",
                "safety alignment",
                "data-driven methods",
                "partial answer prefixes",
                "token distributions",
                "robust performance",
                "downstream tasks"
            ]
        },
        "publishedAt": "2025-03-24T14:11:42.000Z",
        "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
        "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17361",
            "authors": [
                {
                    "_id": "67e35ca7363374850440d91d",
                    "name": "Sophia Tang",
                    "hidden": false
                },
                {
                    "_id": "67e35ca7363374850440d91e",
                    "name": "Yinuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e35ca7363374850440d91f",
                    "name": "Alexander Tong",
                    "hidden": false
                },
                {
                    "_id": "67e35ca7363374850440d920",
                    "user": {
                        "_id": "64cd5b3f0494187a9e8b7c69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
                        "isPro": false,
                        "fullname": "Pranam Chatterjee",
                        "user": "pranamanam",
                        "type": "user"
                    },
                    "name": "Pranam Chatterjee",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-26T01:57:51.167Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T17:59:43.000Z",
            "submittedOnDailyAt": "2025-03-26T00:18:51.908Z",
            "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
            "submittedOnDailyBy": {
                "_id": "64cd5b3f0494187a9e8b7c69",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
                "isPro": false,
                "fullname": "Pranam Chatterjee",
                "user": "pranamanam",
                "type": "user"
            },
            "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
            "upvotes": 4,
            "discussionId": "67e35caa363374850440d9df",
            "ai_keywords": [
                "Gumbel-Softmax Flow",
                "Score Matching",
                "simplex",
                "Gumbel-Softmax interpolant",
                "time-dependent temperature",
                "parameterized velocity field",
                "smooth categorical distributions",
                "Gumbel-Softmax Flow Matching",
                "Straight-Through Guided Flows",
                "STGFlow",
                "straight-through estimators",
                "classifiers",
                "de novo sequence generation",
                "conditional DNA promoter design",
                "sequence-only protein generation",
                "target-binding peptide design"
            ]
        },
        "publishedAt": "2025-03-21T13:59:43.000Z",
        "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
        "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "fullname": "Pranam Chatterjee",
            "name": "pranamanam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19907",
            "authors": [
                {
                    "_id": "67e419a8e3471e8d9fa0b303",
                    "name": "Xuan Ju",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b304",
                    "name": "Weicai Ye",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b305",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b306",
                    "name": "Qiulin Wang",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b307",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b308",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b309",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b30a",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "67e419a8e3471e8d9fa0b30b",
                    "name": "Qiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T17:59:06.000Z",
            "submittedOnDailyAt": "2025-03-26T13:43:59.359Z",
            "title": "FullDiT: Multi-Task Video Generative Foundation Model with Full\n  Attention",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Current video generative foundation models primarily focus on text-to-video\ntasks, providing limited control for fine-grained video content creation.\nAlthough adapter-based approaches (e.g., ControlNet) enable additional controls\nwith minimal fine-tuning, they encounter challenges when integrating multiple\nconditions, including: branch conflicts between independently trained adapters,\nparameter redundancy leading to increased computational cost, and suboptimal\nperformance compared to full fine-tuning. To address these challenges, we\nintroduce FullDiT, a unified foundation model for video generation that\nseamlessly integrates multiple conditions via unified full-attention\nmechanisms. By fusing multi-task conditions into a unified sequence\nrepresentation and leveraging the long-context learning ability of full\nself-attention to capture condition dynamics, FullDiT reduces parameter\noverhead, avoids conditions conflict, and shows scalability and emergent\nability. We further introduce FullBench for multi-task video generation\nevaluation. Experiments demonstrate that FullDiT achieves state-of-the-art\nresults, highlighting the efficacy of full-attention in complex multi-task\nvideo generation.",
            "upvotes": 3,
            "discussionId": "67e419aae3471e8d9fa0b3d4",
            "projectPage": "https://fulldit.github.io",
            "ai_keywords": [
                "full-attention mechanisms",
                "multi-task conditions",
                "sequence representation",
                "self-attention",
                "condition dynamics",
                "FullDiT",
                "FullBench",
                "multi-task video generation",
                "state-of-the-art results"
            ]
        },
        "publishedAt": "2025-03-25T13:59:06.000Z",
        "title": "FullDiT: Multi-Task Video Generative Foundation Model with Full\n  Attention",
        "summary": "Current video generative foundation models primarily focus on text-to-video\ntasks, providing limited control for fine-grained video content creation.\nAlthough adapter-based approaches (e.g., ControlNet) enable additional controls\nwith minimal fine-tuning, they encounter challenges when integrating multiple\nconditions, including: branch conflicts between independently trained adapters,\nparameter redundancy leading to increased computational cost, and suboptimal\nperformance compared to full fine-tuning. To address these challenges, we\nintroduce FullDiT, a unified foundation model for video generation that\nseamlessly integrates multiple conditions via unified full-attention\nmechanisms. By fusing multi-task conditions into a unified sequence\nrepresentation and leveraging the long-context learning ability of full\nself-attention to capture condition dynamics, FullDiT reduces parameter\noverhead, avoids conditions conflict, and shows scalability and emergent\nability. We further introduce FullBench for multi-task video generation\nevaluation. Experiments demonstrate that FullDiT achieves state-of-the-art\nresults, highlighting the efficacy of full-attention in complex multi-task\nvideo generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19907.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 35
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19881",
            "authors": [
                {
                    "_id": "67e3e1fc50e42a17c472be23",
                    "user": {
                        "_id": "6371b9bb3d1bd47a4ec73ec5",
                        "avatarUrl": "/avatars/ed92874a85efcd9ea487a5b59959eb46.svg",
                        "isPro": false,
                        "fullname": "Tianhao Qi",
                        "user": "qth",
                        "type": "user"
                    },
                    "name": "Tianhao Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:43:57.893Z",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be24",
                    "name": "Jianlong Yuan",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be25",
                    "name": "Wanquan Feng",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be26",
                    "name": "Shancheng Fang",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be27",
                    "name": "Jiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be28",
                    "name": "SiYu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be29",
                    "name": "Qian He",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be2a",
                    "name": "Hongtao Xie",
                    "hidden": false
                },
                {
                    "_id": "67e3e1fc50e42a17c472be2b",
                    "name": "Yongdong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T17:46:50.000Z",
            "submittedOnDailyAt": "2025-03-26T23:58:33.549Z",
            "title": "Mask^2DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long\n  Video Generation",
            "submittedOnDailyBy": {
                "_id": "6371b9bb3d1bd47a4ec73ec5",
                "avatarUrl": "/avatars/ed92874a85efcd9ea487a5b59959eb46.svg",
                "isPro": false,
                "fullname": "Tianhao Qi",
                "user": "qth",
                "type": "user"
            },
            "summary": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT)\narchitecture in single-scene video generation. However, the more challenging\ntask of multi-scene video generation, which offers broader applications,\nremains relatively underexplored. To bridge this gap, we propose Mask^2DiT, a\nnovel approach that establishes fine-grained, one-to-one alignment between\nvideo segments and their corresponding text annotations. Specifically, we\nintroduce a symmetric binary mask at each attention layer within the DiT\narchitecture, ensuring that each text annotation applies exclusively to its\nrespective video segment while preserving temporal coherence across visual\ntokens. This attention mechanism enables precise segment-level\ntextual-to-visual alignment, allowing the DiT architecture to effectively\nhandle video generation tasks with a fixed number of scenes. To further equip\nthe DiT architecture with the ability to generate additional scenes based on\nexisting ones, we incorporate a segment-level conditional mask, which\nconditions each newly generated segment on the preceding video segments,\nthereby enabling auto-regressive scene extension. Both qualitative and\nquantitative experiments confirm that Mask^2DiT excels in maintaining visual\nconsistency across segments while ensuring semantic alignment between each\nsegment and its corresponding text description. Our project page is\nhttps://tianhao-qi.github.io/Mask2DiTProject.",
            "upvotes": 3,
            "discussionId": "67e3e20150e42a17c472bf98",
            "ai_keywords": [
                "Diffusion Transformer (DiT)",
                "single-scene video generation",
                "multi-scene video generation",
                "text annotations",
                "symmetric binary mask",
                "attention layer",
                "temporal coherence",
                "visual tokens",
                "segment-level textual-to-visual alignment",
                "segment-level conditional mask",
                "auto-regressive scene extension",
                "visual consistency",
                "semantic alignment"
            ]
        },
        "publishedAt": "2025-03-25T13:46:50.000Z",
        "title": "Mask^2DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long\n  Video Generation",
        "summary": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT)\narchitecture in single-scene video generation. However, the more challenging\ntask of multi-scene video generation, which offers broader applications,\nremains relatively underexplored. To bridge this gap, we propose Mask^2DiT, a\nnovel approach that establishes fine-grained, one-to-one alignment between\nvideo segments and their corresponding text annotations. Specifically, we\nintroduce a symmetric binary mask at each attention layer within the DiT\narchitecture, ensuring that each text annotation applies exclusively to its\nrespective video segment while preserving temporal coherence across visual\ntokens. This attention mechanism enables precise segment-level\ntextual-to-visual alignment, allowing the DiT architecture to effectively\nhandle video generation tasks with a fixed number of scenes. To further equip\nthe DiT architecture with the ability to generate additional scenes based on\nexisting ones, we incorporate a segment-level conditional mask, which\nconditions each newly generated segment on the preceding video segments,\nthereby enabling auto-regressive scene extension. Both qualitative and\nquantitative experiments confirm that Mask^2DiT excels in maintaining visual\nconsistency across segments while ensuring semantic alignment between each\nsegment and its corresponding text description. Our project page is\nhttps://tianhao-qi.github.io/Mask2DiTProject.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19881.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6371b9bb3d1bd47a4ec73ec5",
            "avatarUrl": "/avatars/ed92874a85efcd9ea487a5b59959eb46.svg",
            "fullname": "Tianhao Qi",
            "name": "qth",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18893",
            "authors": [
                {
                    "_id": "67e4110756b46b70c6d49e2f",
                    "name": "Chi-Chih Chang",
                    "hidden": false
                },
                {
                    "_id": "67e4110756b46b70c6d49e30",
                    "name": "Chien-Yu Lin",
                    "hidden": false
                },
                {
                    "_id": "67e4110756b46b70c6d49e31",
                    "name": "Yash Akhauri",
                    "hidden": false
                },
                {
                    "_id": "67e4110756b46b70c6d49e32",
                    "name": "Wei-Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "67e4110756b46b70c6d49e33",
                    "name": "Kai-Chiang Wu",
                    "hidden": false
                },
                {
                    "_id": "67e4110756b46b70c6d49e34",
                    "name": "Luis Ceze",
                    "hidden": false
                },
                {
                    "_id": "67e4110756b46b70c6d49e35",
                    "name": "Mohamed S. Abdelfattah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T17:06:37.000Z",
            "submittedOnDailyAt": "2025-03-26T13:26:48.770Z",
            "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
            "submittedOnDailyBy": {
                "_id": "6589f238d1331d552b1b26f5",
                "avatarUrl": "/avatars/4a496d5791a79df9718e4a71845ae8eb.svg",
                "isPro": false,
                "fullname": "Chi-Chih Chang",
                "user": "shadowpa0327",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
            "upvotes": 3,
            "discussionId": "67e4110856b46b70c6d49e7d",
            "projectPage": "https://abdelfattah-lab.github.io/xKV/",
            "githubRepo": "https://github.com/abdelfattah-lab/xKV",
            "ai_keywords": [
                "Key and Value states (KV-Cache)",
                "post-training method",
                "Singular Value Decomposition (SVD)",
                "low-rank subspace",
                "RULER long-context benchmark",
                "Llama-3.1",
                "Qwen2.5",
                "compression rates",
                "Multi-Head Latent Attention (MLA)",
                "DeepSeek-Coder-V2"
            ]
        },
        "publishedAt": "2025-03-24T13:06:37.000Z",
        "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
        "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18893.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6589f238d1331d552b1b26f5",
            "avatarUrl": "/avatars/4a496d5791a79df9718e4a71845ae8eb.svg",
            "fullname": "Chi-Chih Chang",
            "name": "shadowpa0327",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17973",
            "authors": [
                {
                    "_id": "67e35159b3f0452257728c31",
                    "user": {
                        "_id": "63f2bb987ddf724fbcc57ae9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f2bb987ddf724fbcc57ae9/j81g8dBAMcJchjjsRQSrP.jpeg",
                        "isPro": false,
                        "fullname": "Hanxiao Jiang",
                        "user": "Jianghanxiao",
                        "type": "user"
                    },
                    "name": "Hanxiao Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:50.816Z",
                    "hidden": false
                },
                {
                    "_id": "67e35159b3f0452257728c32",
                    "name": "Hao-Yu Hsu",
                    "hidden": false
                },
                {
                    "_id": "67e35159b3f0452257728c33",
                    "name": "Kaifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e35159b3f0452257728c34",
                    "name": "Hsin-Ni Yu",
                    "hidden": false
                },
                {
                    "_id": "67e35159b3f0452257728c35",
                    "name": "Shenlong Wang",
                    "hidden": false
                },
                {
                    "_id": "67e35159b3f0452257728c36",
                    "name": "Yunzhu Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/652770b2c99126d30f939a19/kNNU-60lApT2jBIsL0KDV.mp4"
            ],
            "publishedAt": "2025-03-23T07:49:19.000Z",
            "submittedOnDailyAt": "2025-03-26T14:35:59.931Z",
            "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos",
            "submittedOnDailyBy": {
                "_id": "652770b2c99126d30f939a19",
                "avatarUrl": "/avatars/8f0bf756079ccb61c7cde323a4b19748.svg",
                "isPro": false,
                "fullname": "Hao-Yu Hsu",
                "user": "haoyuhsu",
                "type": "user"
            },
            "summary": "Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning.",
            "upvotes": 3,
            "discussionId": "67e3515bb3f0452257728cea",
            "ai_keywords": [
                "physics-informed representation",
                "spring-mass models",
                "generative shape models",
                "Gaussian splats",
                "multi-stage, optimization-based inverse modeling framework",
                "dense physical properties",
                "inverse physics framework",
                "visual perception cues",
                "high-fidelity reconstruction",
                "deformable objects",
                "interactive real-time simulation",
                "model-based robotic motion planning"
            ]
        },
        "publishedAt": "2025-03-23T03:49:19.000Z",
        "title": "PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos",
        "summary": "Creating a physical digital twin of a real-world object has immense potential\nin robotics, content creation, and XR. In this paper, we present PhysTwin, a\nnovel framework that uses sparse videos of dynamic objects under interaction to\nproduce a photo- and physically realistic, real-time interactive virtual\nreplica. Our approach centers on two key components: (1) a physics-informed\nrepresentation that combines spring-mass models for realistic physical\nsimulation, generative shape models for geometry, and Gaussian splats for\nrendering; and (2) a novel multi-stage, optimization-based inverse modeling\nframework that reconstructs complete geometry, infers dense physical\nproperties, and replicates realistic appearance from videos. Our method\nintegrates an inverse physics framework with visual perception cues, enabling\nhigh-fidelity reconstruction even from partial, occluded, and limited\nviewpoints. PhysTwin supports modeling various deformable objects, including\nropes, stuffed animals, cloth, and delivery packages. Experiments show that\nPhysTwin outperforms competing methods in reconstruction, rendering, future\nprediction, and simulation under novel interactions. We further demonstrate its\napplications in interactive real-time simulation and model-based robotic motion\nplanning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652770b2c99126d30f939a19/kNNU-60lApT2jBIsL0KDV.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17973.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652770b2c99126d30f939a19",
            "avatarUrl": "/avatars/8f0bf756079ccb61c7cde323a4b19748.svg",
            "fullname": "Hao-Yu Hsu",
            "name": "haoyuhsu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17237",
            "authors": [
                {
                    "_id": "67e2b68e08c6a250edda264a",
                    "user": {
                        "_id": "67e2063e1ee7f6db889849d6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
                        "isPro": false,
                        "fullname": "Yu-Hsi Chen",
                        "user": "wish44165",
                        "type": "user"
                    },
                    "name": "Yu-Hsi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T14:35:46.455Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
            ],
            "publishedAt": "2025-03-21T15:40:18.000Z",
            "submittedOnDailyAt": "2025-03-26T04:35:14.607Z",
            "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
            "submittedOnDailyBy": {
                "_id": "67e2063e1ee7f6db889849d6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
                "isPro": false,
                "fullname": "Yu-Hsi Chen",
                "user": "wish44165",
                "type": "user"
            },
            "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
            "upvotes": 3,
            "discussionId": "67e2b69108c6a250edda279f",
            "githubRepo": "https://github.com/wish44165/YOLOv12-BoT-SORT-ReID",
            "ai_keywords": [
                "YOLOv12",
                "BoT-SORT",
                "multi-UAV tracking",
                "thermal infrared video",
                "detection",
                "tracking",
                "tailored training",
                "inference strategies",
                "4th Anti-UAV Challenge",
                "contrast enhancement",
                "temporal information fusion",
                "UAV features",
                "Strong Baseline"
            ]
        },
        "publishedAt": "2025-03-21T11:40:18.000Z",
        "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
        "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67e2063e1ee7f6db889849d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
            "fullname": "Yu-Hsi Chen",
            "name": "wish44165",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16965",
            "authors": [
                {
                    "_id": "67e35c3bf049c252c672b824",
                    "name": "Zhe Hu",
                    "hidden": false
                },
                {
                    "_id": "67e35c3bf049c252c672b825",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "67e35c3bf049c252c672b826",
                    "name": "Yu Yin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T09:25:23.000Z",
            "submittedOnDailyAt": "2025-03-26T00:20:32.465Z",
            "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
            "submittedOnDailyBy": {
                "_id": "63999a6fe657365725d0d0a4",
                "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
                "isPro": false,
                "fullname": "Derek Zhe Hu",
                "user": "zhehuderek",
                "type": "user"
            },
            "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
            "upvotes": 3,
            "discussionId": "67e35c3cf049c252c672b859",
            "ai_keywords": [
                "Visual Language Models (VLMs)",
                "multimodal human-centered decision-making tasks",
                "Large Language Models (LLMs)",
                "textual descriptions",
                "visual alignment",
                "text-only training approach",
                "synthesized textual data",
                "self-improvement",
                "training data",
                "GPT-4",
                "human-centered decision-making capabilities"
            ]
        },
        "publishedAt": "2025-03-21T05:25:23.000Z",
        "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
        "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63999a6fe657365725d0d0a4",
            "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
            "fullname": "Derek Zhe Hu",
            "name": "zhehuderek",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.20110",
            "authors": [
                {
                    "_id": "67e49dc4376c1c8380394731",
                    "name": "Pin-Jie Lin",
                    "hidden": false
                },
                {
                    "_id": "67e49dc4376c1c8380394732",
                    "name": "Rishab Balasubramanian",
                    "hidden": false
                },
                {
                    "_id": "67e49dc4376c1c8380394733",
                    "name": "Fengyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67e49dc4376c1c8380394734",
                    "name": "Nikhil Kandpal",
                    "hidden": false
                },
                {
                    "_id": "67e49dc4376c1c8380394735",
                    "name": "Tu Vu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T23:24:43.000Z",
            "submittedOnDailyAt": "2025-03-26T23:08:43.281Z",
            "title": "Efficient Model Development through Fine-tuning Transfer",
            "submittedOnDailyBy": {
                "_id": "636828a644e19ccad213063a",
                "avatarUrl": "/avatars/e1551d4ee9ca94018103af3bf1249cf2.svg",
                "isPro": false,
                "fullname": "Tu Vu",
                "user": "tuvu",
                "type": "user"
            },
            "summary": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.",
            "upvotes": 2,
            "discussionId": "67e49dc5376c1c838039479f",
            "ai_keywords": [
                "diff vector",
                "weight changes",
                "fine-tuning updates",
                "open-weight model versions",
                "parameter space",
                "iterative recycling-then-finetuning"
            ]
        },
        "publishedAt": "2025-03-25T19:24:43.000Z",
        "title": "Efficient Model Development through Fine-tuning Transfer",
        "summary": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20110.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "636828a644e19ccad213063a",
            "avatarUrl": "/avatars/e1551d4ee9ca94018103af3bf1249cf2.svg",
            "fullname": "Tu Vu",
            "name": "tuvu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19207",
            "authors": [
                {
                    "_id": "67e3f27a0d1715115dcd5b28",
                    "name": "Rong Wang",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b29",
                    "name": "Fabian Prada",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b2a",
                    "name": "Ziyan Wang",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b2b",
                    "name": "Zhongshi Jiang",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b2c",
                    "name": "Chengxiang Yin",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b2d",
                    "name": "Junxuan Li",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b2e",
                    "name": "Shunsuke Saito",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b2f",
                    "name": "Igor Santesteban",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b30",
                    "name": "Javier Romero",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b31",
                    "name": "Rohan Joshi",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b32",
                    "name": "Hongdong Li",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b33",
                    "name": "Jason Saragih",
                    "hidden": false
                },
                {
                    "_id": "67e3f27a0d1715115dcd5b34",
                    "name": "Yaser Sheikh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T23:20:47.000Z",
            "submittedOnDailyAt": "2025-03-26T10:57:51.434Z",
            "title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images",
            "submittedOnDailyBy": {
                "_id": "6493306970d925ae80523a53",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nRCxbVng_PPBqKd-Z3KVc.jpeg",
                "isPro": false,
                "fullname": "Dmitry Ryumin",
                "user": "DmitryRyumin",
                "type": "user"
            },
            "summary": "We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps://github.com/rongakowang/FRESA.",
            "upvotes": 2,
            "discussionId": "67e3f2810d1715115dcd5d7a",
            "projectPage": "https://rongakowang.github.io/fresa/fresa.html",
            "githubRepo": "https://github.com/rongakowang/FRESA",
            "ai_keywords": [
                "personalized 3D human avatars",
                "realistic animation",
                "universal prior",
                "instant feedforward generation",
                "zero-shot generalization",
                "personalized avatar shape",
                "skinning weights",
                "pose-dependent deformations",
                "geometric fidelity",
                "deformation artifacts",
                "3D canonicalization",
                "pixel-aligned initial conditions",
                "fine-grained geometric details",
                "multi-frame feature aggregation",
                "canonicalization",
                "plausible avatar",
                "person-specific identities",
                "end-to-end framework",
                "large-scale capture dataset",
                "high-quality 3D scans",
                "authentic reconstruction",
                "casual phone photos"
            ]
        },
        "publishedAt": "2025-03-24T19:20:47.000Z",
        "title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images",
        "summary": "We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps://github.com/rongakowang/FRESA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19207.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6493306970d925ae80523a53",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nRCxbVng_PPBqKd-Z3KVc.jpeg",
            "fullname": "Dmitry Ryumin",
            "name": "DmitryRyumin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 396
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19123",
            "authors": [
                {
                    "_id": "67e3fcb76323be71222fce35",
                    "name": "Haebin Shin",
                    "hidden": false
                },
                {
                    "_id": "67e3fcb76323be71222fce36",
                    "user": {
                        "_id": "664bed934bea570e25a8dc8c",
                        "avatarUrl": "/avatars/4e5a69db9005759b8812a9b54faf14c1.svg",
                        "isPro": false,
                        "fullname": "Lei Ji",
                        "user": "jilei03111",
                        "type": "user"
                    },
                    "name": "Lei Ji",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-26T13:10:17.004Z",
                    "hidden": false
                },
                {
                    "_id": "67e3fcb76323be71222fce37",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-26T13:10:33.192Z",
                    "hidden": false
                },
                {
                    "_id": "67e3fcb76323be71222fce38",
                    "user": {
                        "_id": "643f615aa16cd6d1f4c581de",
                        "avatarUrl": "/avatars/47753a3e82b44f81881600c52e1e8495.svg",
                        "isPro": false,
                        "fullname": "Yeyun Gong",
                        "user": "yegong",
                        "type": "user"
                    },
                    "name": "Yeyun Gong",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-26T13:10:17.004Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T20:19:31.000Z",
            "submittedOnDailyAt": "2025-03-26T11:40:56.071Z",
            "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling",
            "submittedOnDailyBy": {
                "_id": "63fb6e281b4b1bd4e7ffc5be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                "isPro": false,
                "fullname": "Xiao Liu",
                "user": "lx865712528",
                "type": "user"
            },
            "summary": "Using large teacher models to guide the training of smaller student models\nhas become the prevailing paradigm for efficient and effective learning.\nHowever, vocabulary mismatches between teacher and student language models pose\nsignificant challenges in language modeling, resulting in divergent token\nsequences and output distributions. To overcome these limitations, we propose\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\napproach that bridges the gap caused by vocabulary mismatch through two key\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\nof teacher model to guide effective student training. We demonstrate its\neffectiveness in language modeling with 1B student model using various 7B\nteacher models with different vocabularies. Notably, with\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\nconsistently benefits from stronger teacher models, providing a robust solution\nto vocabulary mismatches in language modeling.",
            "upvotes": 2,
            "discussionId": "67e3fcb96323be71222fce9e",
            "ai_keywords": [
                "Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM)",
                "Token-level Lexical Alignment",
                "Teacher Guided Loss",
                "language modeling",
                "vocabulary mismatches",
                "token sequences",
                "output distributions",
                "vocabulary",
                "Qwen2.5-Math-Instruct",
                "TinyLlama"
            ]
        },
        "publishedAt": "2025-03-24T16:19:31.000Z",
        "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling",
        "summary": "Using large teacher models to guide the training of smaller student models\nhas become the prevailing paradigm for efficient and effective learning.\nHowever, vocabulary mismatches between teacher and student language models pose\nsignificant challenges in language modeling, resulting in divergent token\nsequences and output distributions. To overcome these limitations, we propose\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\napproach that bridges the gap caused by vocabulary mismatch through two key\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\nof teacher model to guide effective student training. We demonstrate its\neffectiveness in language modeling with 1B student model using various 7B\nteacher models with different vocabularies. Notably, with\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\nconsistently benefits from stronger teacher models, providing a robust solution\nto vocabulary mismatches in language modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19123.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
            "fullname": "Xiao Liu",
            "name": "lx865712528",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18783",
            "authors": [
                {
                    "_id": "67e2a43d5116df47da357eec",
                    "user": {
                        "_id": "642438eaa3adbc7142c3ca0f",
                        "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
                        "isPro": false,
                        "fullname": "CharlesChen",
                        "user": "CharlesChen2023",
                        "type": "user"
                    },
                    "name": "Linwei Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-25T14:36:22.430Z",
                    "hidden": false
                },
                {
                    "_id": "67e2a43d5116df47da357eed",
                    "name": "Lin Gu",
                    "hidden": false
                },
                {
                    "_id": "67e2a43d5116df47da357eee",
                    "name": "Liang Li",
                    "hidden": false
                },
                {
                    "_id": "67e2a43d5116df47da357eef",
                    "name": "Chenggang Yan",
                    "hidden": false
                },
                {
                    "_id": "67e2a43d5116df47da357ef0",
                    "name": "Ying Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T15:32:06.000Z",
            "submittedOnDailyAt": "2025-03-26T01:08:28.390Z",
            "title": "Frequency Dynamic Convolution for Dense Image Prediction",
            "submittedOnDailyBy": {
                "_id": "642438eaa3adbc7142c3ca0f",
                "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
                "isPro": false,
                "fullname": "CharlesChen",
                "user": "CharlesChen2023",
                "type": "user"
            },
            "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
            "upvotes": 2,
            "discussionId": "67e2a4405116df47da357ff7",
            "ai_keywords": [
                "Dynamic Convolution (DY-Conv)",
                "Frequency Dynamic Convolution (FDConv)",
                "attention mechanism",
                "parameter budget",
                "Fourier domain",
                "frequency-based groups",
                "disjoint Fourier indices",
                "frequency-diverse weights",
                "Kernel Spatial Modulation (KSM)",
                "Frequency Band Modulation (FBM)",
                "frequency response",
                "spatial level",
                "frequency bands",
                "local content",
                "object detection",
                "segmentation",
                "classification",
                "ResNet-50",
                "ConvNeXt",
                "Swin-Transformer",
                "parameter-efficient"
            ]
        },
        "publishedAt": "2025-03-24T11:32:06.000Z",
        "title": "Frequency Dynamic Convolution for Dense Image Prediction",
        "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642438eaa3adbc7142c3ca0f",
            "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
            "fullname": "CharlesChen",
            "name": "CharlesChen2023",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18712",
            "authors": [
                {
                    "_id": "67e47a92d762ffface602862",
                    "name": "Shaokai Ye",
                    "hidden": false
                },
                {
                    "_id": "67e47a92d762ffface602863",
                    "name": "Haozhe Qi",
                    "hidden": false
                },
                {
                    "_id": "67e47a92d762ffface602864",
                    "name": "Alexander Mathis",
                    "hidden": false
                },
                {
                    "_id": "67e47a92d762ffface602865",
                    "user": {
                        "_id": "62c2e4d672ff397f721ac6f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656944910507-62c2e4d672ff397f721ac6f7.jpeg",
                        "isPro": false,
                        "fullname": "Mackenzie Mathis",
                        "user": "mwmathis",
                        "type": "user"
                    },
                    "name": "Mackenzie W. Mathis",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-26T22:13:22.189Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T14:24:17.000Z",
            "submittedOnDailyAt": "2025-03-26T20:39:28.088Z",
            "title": "LLaVAction: evaluating and training multi-modal large language models\n  for action recognition",
            "submittedOnDailyBy": {
                "_id": "62c2e4d672ff397f721ac6f7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656944910507-62c2e4d672ff397f721ac6f7.jpeg",
                "isPro": false,
                "fullname": "Mackenzie Mathis",
                "user": "mwmathis",
                "type": "user"
            },
            "summary": "Understanding human behavior requires measuring behavioral actions. Due to\nits complexity, behavior is best mapped onto a rich, semantic structure such as\nlanguage. The recent development of multi-modal large language models (MLLMs)\nis a promising candidate for a wide range of action understanding tasks. In\nthis work, we focus on evaluating and then improving MLLMs to perform action\nrecognition. We reformulate EPIC-KITCHENS-100, one of the largest and most\nchallenging egocentric action datasets, to the form of video multiple question\nanswering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult\nincorrect answers as distractors, leading MLLMs struggle to recognize the\ncorrect actions. We propose a series of methods that greatly improve the MLLMs'\nability to perform action recognition, achieving state-of-the-art on both the\nEPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points\nin accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other\naction-related video benchmarks such as EgoSchema, PerceptionTest,\nLongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising\npath forward for complex action tasks. Code and models are available at:\nhttps://github.com/AdaptiveMotorControlLab/LLaVAction.",
            "upvotes": 2,
            "discussionId": "67e47a93d762ffface6028fa",
            "projectPage": "https://mmathislab.github.io/llavaction/",
            "githubRepo": "https://github.com/AdaptiveMotorControlLab/LLaVAction",
            "ai_keywords": [
                "multi-modal large language models (MLLMs)",
                "EPIC-KITCHENS-100",
                "video multiple question answering (EPIC-KITCHENS-100-MQA)",
                "action recognition",
                "GPT-4o",
                "EgoSchema",
                "PerceptionTest",
                "LongVideoBench",
                "VideoMME",
                "MVBench",
                "LLaVAction"
            ]
        },
        "publishedAt": "2025-03-24T10:24:17.000Z",
        "title": "LLaVAction: evaluating and training multi-modal large language models\n  for action recognition",
        "summary": "Understanding human behavior requires measuring behavioral actions. Due to\nits complexity, behavior is best mapped onto a rich, semantic structure such as\nlanguage. The recent development of multi-modal large language models (MLLMs)\nis a promising candidate for a wide range of action understanding tasks. In\nthis work, we focus on evaluating and then improving MLLMs to perform action\nrecognition. We reformulate EPIC-KITCHENS-100, one of the largest and most\nchallenging egocentric action datasets, to the form of video multiple question\nanswering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult\nincorrect answers as distractors, leading MLLMs struggle to recognize the\ncorrect actions. We propose a series of methods that greatly improve the MLLMs'\nability to perform action recognition, achieving state-of-the-art on both the\nEPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points\nin accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other\naction-related video benchmarks such as EgoSchema, PerceptionTest,\nLongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising\npath forward for complex action tasks. Code and models are available at:\nhttps://github.com/AdaptiveMotorControlLab/LLaVAction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18712.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c2e4d672ff397f721ac6f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656944910507-62c2e4d672ff397f721ac6f7.jpeg",
            "fullname": "Mackenzie Mathis",
            "name": "mwmathis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.11849",
            "authors": [
                {
                    "_id": "67e3d0ac304f166b665e4a67",
                    "user": {
                        "_id": "64cba974a81988d0734c9925",
                        "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
                        "isPro": false,
                        "fullname": "Yi Wang",
                        "user": "wangyi111",
                        "type": "user"
                    },
                    "name": "Yi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:12.292Z",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a68",
                    "name": "Zhitong Xiong",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a69",
                    "name": "Chenying Liu",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a6a",
                    "name": "Adam J. Stewart",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a6b",
                    "name": "Thomas Dujardin",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a6c",
                    "name": "Nikolaos Ioannis Bountos",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a6d",
                    "name": "Angelos Zavras",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a6e",
                    "name": "Franziska Gerken",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a6f",
                    "name": "Ioannis Papoutsis",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a70",
                    "name": "Laura Leal-Taixé",
                    "hidden": false
                },
                {
                    "_id": "67e3d0ac304f166b665e4a71",
                    "name": "Xiao Xiang Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-14T20:16:48.000Z",
            "submittedOnDailyAt": "2025-03-26T08:34:37.209Z",
            "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
            "submittedOnDailyBy": {
                "_id": "64cba974a81988d0734c9925",
                "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
                "isPro": false,
                "fullname": "Yi Wang",
                "user": "wangyi111",
                "type": "user"
            },
            "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.",
            "upvotes": 2,
            "discussionId": "67e3d0af304f166b665e4b68",
            "githubRepo": "https://github.com/zhu-xlab/Copernicus-FM",
            "ai_keywords": [
                "extended dynamic hypernetworks",
                "flexible metadata encoding"
            ]
        },
        "publishedAt": "2025-03-14T16:16:48.000Z",
        "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
        "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11849.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cba974a81988d0734c9925",
            "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
            "fullname": "Yi Wang",
            "name": "wangyi111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19777",
            "authors": [
                {
                    "_id": "67e3d807304f166b6660db63",
                    "name": "Vladan Stojnić",
                    "hidden": false
                },
                {
                    "_id": "67e3d807304f166b6660db64",
                    "name": "Yannis Kalantidis",
                    "hidden": false
                },
                {
                    "_id": "67e3d807304f166b6660db65",
                    "name": "Jiří Matas",
                    "hidden": false
                },
                {
                    "_id": "67e3d807304f166b6660db66",
                    "name": "Giorgos Tolias",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T15:47:13.000Z",
            "submittedOnDailyAt": "2025-03-26T09:04:59.080Z",
            "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation",
            "submittedOnDailyBy": {
                "_id": "66a3ae59f33ff23e1c027ccd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a3ae59f33ff23e1c027ccd/tZzpESNPnmhty62xhHszF.jpeg",
                "isPro": true,
                "fullname": "Vladan Stojnic",
                "user": "stojnvla",
                "type": "user"
            },
            "summary": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS",
            "upvotes": 1,
            "discussionId": "67e3d809304f166b6660dc45",
            "githubRepo": "https://github.com/vladan-stojnic/LPOSS",
            "ai_keywords": [
                "Vision-and-Language Models (VLMs)",
                "label propagation",
                "per-patch predictions",
                "patch-to-patch relationships",
                "Vision Model (VM)",
                "intra-modal similarity",
                "patch-based encoders",
                "pixel level",
                "segmentation accuracy",
                "class boundaries",
                "contextual interactions",
                "LPOSS+",
                "inference over the entire image",
                "window-based processing",
                "state-of-the-art performance",
                "datasets"
            ]
        },
        "publishedAt": "2025-03-25T11:47:13.000Z",
        "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation",
        "summary": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19777.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a3ae59f33ff23e1c027ccd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a3ae59f33ff23e1c027ccd/tZzpESNPnmhty62xhHszF.jpeg",
            "fullname": "Vladan Stojnic",
            "name": "stojnvla",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19356",
            "authors": [
                {
                    "_id": "67e376caa26895e9a90af7de",
                    "name": "Reza Pourreza",
                    "hidden": false
                },
                {
                    "_id": "67e376caa26895e9a90af7df",
                    "user": {
                        "_id": "60796959c59d9e1697fa2324",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
                        "isPro": false,
                        "fullname": "Rishit Dagli",
                        "user": "rishitdagli",
                        "type": "user"
                    },
                    "name": "Rishit Dagli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:21.889Z",
                    "hidden": false
                },
                {
                    "_id": "67e376caa26895e9a90af7e0",
                    "name": "Apratim Bhattacharyya",
                    "hidden": false
                },
                {
                    "_id": "67e376caa26895e9a90af7e1",
                    "name": "Sunny Panchal",
                    "hidden": false
                },
                {
                    "_id": "67e376caa26895e9a90af7e2",
                    "name": "Guillaume Berger",
                    "hidden": false
                },
                {
                    "_id": "67e376caa26895e9a90af7e3",
                    "name": "Roland Memisevic",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60796959c59d9e1697fa2324/XDGZe8OhURpl4fPftAwAb.jpeg"
            ],
            "publishedAt": "2025-03-25T05:13:12.000Z",
            "submittedOnDailyAt": "2025-03-26T12:47:21.215Z",
            "title": "Can Vision-Language Models Answer Face to Face Questions in the\n  Real-World?",
            "submittedOnDailyBy": {
                "_id": "60796959c59d9e1697fa2324",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
                "isPro": false,
                "fullname": "Rishit Dagli",
                "user": "rishitdagli",
                "type": "user"
            },
            "summary": "AI models have made significant strides in recent years in their ability to\ndescribe and answer questions about real-world images. They have also made\nprogress in the ability to converse with users in real-time using audio input.\nThis raises the question: have we reached the point where AI models, connected\nto a camera and microphone, can converse with users in real-time about scenes\nand events that are unfolding live in front of the camera? This has been a\nlong-standing goal in AI and is a prerequisite for real-world AI assistants and\nhumanoid robots to interact with humans in everyday situations. In this work,\nwe introduce a new dataset and benchmark, the Qualcomm Interactive Video\nDataset (IVD), which allows us to assess the extent to which existing models\ncan support these abilities, and to what degree these capabilities can be\ninstilled through fine-tuning. The dataset is based on a simple\nquestion-answering setup, where users ask questions that the system has to\nanswer, in real-time, based on the camera and audio input. We show that\nexisting models fall far behind human performance on this task, and we identify\nthe main sources for the performance gap. However, we also show that for many\nof the required perceptual skills, fine-tuning on this form of data can\nsignificantly reduce this gap.",
            "upvotes": 1,
            "discussionId": "67e376cea26895e9a90af923",
            "ai_keywords": [
                "Qualcomm Interactive Video Dataset (IVD)",
                "question-answering setup",
                "real-time",
                "camera and audio input",
                "perceptual skills",
                "fine-tuning"
            ]
        },
        "publishedAt": "2025-03-25T01:13:12.000Z",
        "title": "Can Vision-Language Models Answer Face to Face Questions in the\n  Real-World?",
        "summary": "AI models have made significant strides in recent years in their ability to\ndescribe and answer questions about real-world images. They have also made\nprogress in the ability to converse with users in real-time using audio input.\nThis raises the question: have we reached the point where AI models, connected\nto a camera and microphone, can converse with users in real-time about scenes\nand events that are unfolding live in front of the camera? This has been a\nlong-standing goal in AI and is a prerequisite for real-world AI assistants and\nhumanoid robots to interact with humans in everyday situations. In this work,\nwe introduce a new dataset and benchmark, the Qualcomm Interactive Video\nDataset (IVD), which allows us to assess the extent to which existing models\ncan support these abilities, and to what degree these capabilities can be\ninstilled through fine-tuning. The dataset is based on a simple\nquestion-answering setup, where users ask questions that the system has to\nanswer, in real-time, based on the camera and audio input. We show that\nexisting models fall far behind human performance on this task, and we identify\nthe main sources for the performance gap. However, we also show that for many\nof the required perceptual skills, fine-tuning on this form of data can\nsignificantly reduce this gap.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60796959c59d9e1697fa2324/XDGZe8OhURpl4fPftAwAb.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19356.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60796959c59d9e1697fa2324",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
            "fullname": "Rishit Dagli",
            "name": "rishitdagli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19355",
            "authors": [
                {
                    "_id": "67e46aeafe1f5acc68e9ee04",
                    "name": "Dohwan Ko",
                    "hidden": false
                },
                {
                    "_id": "67e46aeafe1f5acc68e9ee05",
                    "name": "Sihyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "67e46aeafe1f5acc68e9ee06",
                    "name": "Yumin Suh",
                    "hidden": false
                },
                {
                    "_id": "67e46aeafe1f5acc68e9ee07",
                    "name": "Vijay Kumar B. G",
                    "hidden": false
                },
                {
                    "_id": "67e46aeafe1f5acc68e9ee08",
                    "name": "Minseo Yoon",
                    "hidden": false
                },
                {
                    "_id": "67e46aeafe1f5acc68e9ee09",
                    "name": "Manmohan Chandraker",
                    "hidden": false
                },
                {
                    "_id": "67e46aeafe1f5acc68e9ee0a",
                    "name": "Hyunwoo J. Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T05:08:06.000Z",
            "submittedOnDailyAt": "2025-03-26T19:32:29.761Z",
            "title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "62bab5a69f045fd1fb9f760e",
                "avatarUrl": "/avatars/3f12a908faec1135e0221444f0769496.svg",
                "isPro": false,
                "fullname": "Dohwan Ko",
                "user": "ikodoh",
                "type": "user"
            },
            "summary": "Spatio-temporal reasoning is essential in understanding real-world\nenvironments in various fields, eg, autonomous driving and sports analytics.\nRecent advances have improved the spatial reasoning ability of Vision-Language\nModels (VLMs) by introducing large-scale data, but these models still struggle\nto analyze kinematic elements like traveled distance and speed of moving\nobjects. To bridge this gap, we construct a spatio-temporal reasoning dataset\nand benchmark involving kinematic instruction tuning, referred to as STKit and\nSTKit-Bench. They consist of real-world videos with 3D annotations, detailing\nobject motion dynamics: traveled distance, speed, movement direction,\ninter-object distance comparisons, and relative movement direction. To further\nscale such data construction to videos without 3D labels, we propose an\nautomatic pipeline to generate pseudo-labels using 4D reconstruction in\nreal-world scale. With our kinematic instruction tuning data for\nspatio-temporal reasoning, we present ST-VLM, a VLM enhanced for\nspatio-temporal reasoning, which exhibits outstanding performance on\nSTKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across\ndiverse domains and tasks, outperforming baselines on other spatio-temporal\nbenchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned\nspatio-temporal reasoning with existing abilities, ST-VLM enables complex\nmulti-step reasoning. Project page: https://ikodoh.github.io/ST-VLM.",
            "upvotes": 1,
            "discussionId": "67e46aeefe1f5acc68e9ef35",
            "projectPage": "https://ikodoh.github.io/ST-VLM",
            "githubRepo": "https://github.com/mlvlab/ST-VLM",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "spatio-temporal reasoning",
                "kinematic elements",
                "traveled distance",
                "speed",
                "moving objects",
                "spatio-temporal reasoning dataset",
                "benchmark",
                "kinematic instruction tuning",
                "STKit",
                "STKit-Bench",
                "real-world videos",
                "3D annotations",
                "object motion dynamics",
                "movement direction",
                "inter-object distance comparisons",
                "relative movement direction",
                "pseudo-labels",
                "4D reconstruction",
                "real-world scale",
                "ST-VLM",
                "spatio-temporal reasoning",
                "performance",
                "STKit-Bench",
                "generalizes robustly",
                "diverse domains",
                "tasks",
                "ActivityNet",
                "TVQA+",
                "complex multi-step reasoning"
            ]
        },
        "publishedAt": "2025-03-25T01:08:06.000Z",
        "title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in\n  Vision-Language Models",
        "summary": "Spatio-temporal reasoning is essential in understanding real-world\nenvironments in various fields, eg, autonomous driving and sports analytics.\nRecent advances have improved the spatial reasoning ability of Vision-Language\nModels (VLMs) by introducing large-scale data, but these models still struggle\nto analyze kinematic elements like traveled distance and speed of moving\nobjects. To bridge this gap, we construct a spatio-temporal reasoning dataset\nand benchmark involving kinematic instruction tuning, referred to as STKit and\nSTKit-Bench. They consist of real-world videos with 3D annotations, detailing\nobject motion dynamics: traveled distance, speed, movement direction,\ninter-object distance comparisons, and relative movement direction. To further\nscale such data construction to videos without 3D labels, we propose an\nautomatic pipeline to generate pseudo-labels using 4D reconstruction in\nreal-world scale. With our kinematic instruction tuning data for\nspatio-temporal reasoning, we present ST-VLM, a VLM enhanced for\nspatio-temporal reasoning, which exhibits outstanding performance on\nSTKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across\ndiverse domains and tasks, outperforming baselines on other spatio-temporal\nbenchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned\nspatio-temporal reasoning with existing abilities, ST-VLM enables complex\nmulti-step reasoning. Project page: https://ikodoh.github.io/ST-VLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19355.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62bab5a69f045fd1fb9f760e",
            "avatarUrl": "/avatars/3f12a908faec1135e0221444f0769496.svg",
            "fullname": "Dohwan Ko",
            "name": "ikodoh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18673",
            "authors": [
                {
                    "_id": "67e34bdb8aa257f5a3ebe346",
                    "user": {
                        "_id": "648283e7eb4befee37855c93",
                        "avatarUrl": "/avatars/9e83accbd2410b0bcb5332df2b7f3ced.svg",
                        "isPro": false,
                        "fullname": "Taeyeop Lee",
                        "user": "taeyeop",
                        "type": "user"
                    },
                    "name": "Taeyeop Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:45:09.497Z",
                    "hidden": false
                },
                {
                    "_id": "67e34bdb8aa257f5a3ebe347",
                    "name": "Bowen Wen",
                    "hidden": false
                },
                {
                    "_id": "67e34bdb8aa257f5a3ebe348",
                    "name": "Minjun Kang",
                    "hidden": false
                },
                {
                    "_id": "67e34bdb8aa257f5a3ebe349",
                    "name": "Gyuree Kang",
                    "hidden": false
                },
                {
                    "_id": "67e34bdb8aa257f5a3ebe34a",
                    "name": "In So Kweon",
                    "hidden": false
                },
                {
                    "_id": "67e34bdb8aa257f5a3ebe34b",
                    "name": "Kuk-Jin Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T13:46:21.000Z",
            "submittedOnDailyAt": "2025-03-26T22:42:02.554Z",
            "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects",
            "submittedOnDailyBy": {
                "_id": "648283e7eb4befee37855c93",
                "avatarUrl": "/avatars/9e83accbd2410b0bcb5332df2b7f3ced.svg",
                "isPro": false,
                "fullname": "Taeyeop Lee",
                "user": "taeyeop",
                "type": "user"
            },
            "summary": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d",
            "upvotes": 1,
            "discussionId": "67e34bdd8aa257f5a3ebe3d7",
            "ai_keywords": [
                "6D object pose estimation",
                "RGB-D",
                "joint object alignment",
                "2D-3D alignment",
                "metric scale estimation",
                "render-and-compare strategy",
                "pose hypotheses"
            ]
        },
        "publishedAt": "2025-03-24T09:46:21.000Z",
        "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects",
        "summary": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18673.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648283e7eb4befee37855c93",
            "avatarUrl": "/avatars/9e83accbd2410b0bcb5332df2b7f3ced.svg",
            "fullname": "Taeyeop Lee",
            "name": "taeyeop",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.04919",
            "authors": [
                {
                    "_id": "67e480296d47bc4578afc3a4",
                    "name": "Ian Huang",
                    "hidden": false
                },
                {
                    "_id": "67e480296d47bc4578afc3a5",
                    "name": "Yanan Bao",
                    "hidden": false
                },
                {
                    "_id": "67e480296d47bc4578afc3a6",
                    "name": "Karen Truong",
                    "hidden": false
                },
                {
                    "_id": "67e480296d47bc4578afc3a7",
                    "name": "Howard Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e480296d47bc4578afc3a8",
                    "name": "Cordelia Schmid",
                    "hidden": false
                },
                {
                    "_id": "67e480296d47bc4578afc3a9",
                    "name": "Leonidas Guibas",
                    "hidden": false
                },
                {
                    "_id": "67e480296d47bc4578afc3aa",
                    "name": "Alireza Fathi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-06T19:34:15.000Z",
            "submittedOnDailyAt": "2025-03-26T21:03:30.157Z",
            "title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D\n  Object Placement",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Scene generation with 3D assets presents a complex challenge, requiring both\nhigh-level semantic understanding and low-level geometric reasoning. While\nMultimodal Large Language Models (MLLMs) excel at semantic tasks, their\napplication to 3D scene generation is hindered by their limited grounding on 3D\ngeometry. In this paper, we investigate how to best work with MLLMs in an\nobject placement task. Towards this goal, we introduce a novel framework,\nFirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the\nextraction of relevant geometric details from the 3D scene, (2) constructing\nand solving geometric constraints on the extracted low-level geometry, and (3)\npruning for final placements that conform to common sense. By combining\ngeometric reasoning with real-world understanding of MLLMs, our method can\npropose object placements that satisfy both geometric constraints as well as\nhigh-level semantic common-sense considerations. Our experiments show that\nthese capabilities allow our method to place objects more effectively in\ncomplex scenes with intricate geometry, surpassing the quality of prior work.",
            "upvotes": 1,
            "discussionId": "67e480306d47bc4578afc68e",
            "projectPage": "https://fireplace3d.github.io/",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "3D geometric reasoning",
                "geometric constraints",
                "geometric details",
                "pruning",
                "common sense",
                "object placements"
            ]
        },
        "publishedAt": "2025-03-06T14:34:15.000Z",
        "title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D\n  Object Placement",
        "summary": "Scene generation with 3D assets presents a complex challenge, requiring both\nhigh-level semantic understanding and low-level geometric reasoning. While\nMultimodal Large Language Models (MLLMs) excel at semantic tasks, their\napplication to 3D scene generation is hindered by their limited grounding on 3D\ngeometry. In this paper, we investigate how to best work with MLLMs in an\nobject placement task. Towards this goal, we introduce a novel framework,\nFirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the\nextraction of relevant geometric details from the 3D scene, (2) constructing\nand solving geometric constraints on the extracted low-level geometry, and (3)\npruning for final placements that conform to common sense. By combining\ngeometric reasoning with real-world understanding of MLLMs, our method can\npropose object placements that satisfy both geometric constraints as well as\nhigh-level semantic common-sense considerations. Our experiments show that\nthese capabilities allow our method to place objects more effectively in\ncomplex scenes with intricate geometry, surpassing the quality of prior work.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04919.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": true,
            "isMod": false,
            "followerCount": 6475
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.17982",
            "authors": [
                {
                    "_id": "67e4276e59b2d29cb2b20276",
                    "user": {
                        "_id": "6780384de11224514e5f1059",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Rxkp1GnjCf-u5AlE1-4gi.png",
                        "isPro": false,
                        "fullname": "Yara AlaaEldin",
                        "user": "yaraalaa0",
                        "type": "user"
                    },
                    "name": "Yara AlaaEldin",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-03-26T16:41:18.789Z",
                    "hidden": false
                },
                {
                    "_id": "67e4276e59b2d29cb2b20277",
                    "name": "Francesca Odone",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-23T08:25:07.000Z",
            "submittedOnDailyAt": "2025-03-26T15:10:38.913Z",
            "title": "Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on\n  Aerial Images",
            "submittedOnDailyBy": {
                "_id": "6780384de11224514e5f1059",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Rxkp1GnjCf-u5AlE1-4gi.png",
                "isPro": false,
                "fullname": "Yara AlaaEldin",
                "user": "yaraalaa0",
                "type": "user"
            },
            "summary": "Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps://github.com/Malga-Vision/Co-SemDepth",
            "upvotes": 0,
            "discussionId": "67e4276f59b2d29cb2b202ef",
            "ai_keywords": [
                "joint deep-learning architecture",
                "depth prediction",
                "semantic segmentation",
                "monocular cameras",
                "low-altitude unstructured environments",
                "MidAir benchmark dataset",
                "Aeroscapes benchmark dataset",
                "CPU FPS",
                "memory footprint"
            ]
        },
        "publishedAt": "2025-03-23T04:25:07.000Z",
        "title": "Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on\n  Aerial Images",
        "summary": "Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps://github.com/Malga-Vision/Co-SemDepth",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6780384de11224514e5f1059",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Rxkp1GnjCf-u5AlE1-4gi.png",
            "fullname": "Yara AlaaEldin",
            "name": "yaraalaa0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16776",
            "authors": [
                {
                    "_id": "67e3d39948da72bf6f3d27f7",
                    "name": "Valentin Bieri",
                    "hidden": false
                },
                {
                    "_id": "67e3d39948da72bf6f3d27f8",
                    "name": "Marco Zamboni",
                    "hidden": false
                },
                {
                    "_id": "67e3d39948da72bf6f3d27f9",
                    "name": "Nicolas S. Blumer",
                    "hidden": false
                },
                {
                    "_id": "67e3d39948da72bf6f3d27fa",
                    "user": {
                        "_id": "64462645866784561a882802",
                        "avatarUrl": "/avatars/4c08f6f60a82b7292c91fccb2c8bea37.svg",
                        "isPro": false,
                        "fullname": "Qingxuan Chen",
                        "user": "LUC1O",
                        "type": "user"
                    },
                    "name": "Qingxuan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:10.272Z",
                    "hidden": false
                },
                {
                    "_id": "67e3d39948da72bf6f3d27fb",
                    "name": "Francis Engelmann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T01:11:21.000Z",
            "submittedOnDailyAt": "2025-03-26T20:05:54.189Z",
            "title": "OpenCity3D: What do Vision-Language Models know about Urban\n  Environments?",
            "submittedOnDailyBy": {
                "_id": "64462645866784561a882802",
                "avatarUrl": "/avatars/4c08f6f60a82b7292c91fccb2c8bea37.svg",
                "isPro": false,
                "fullname": "Qingxuan Chen",
                "user": "LUC1O",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) show great promise for 3D scene understanding\nbut are mainly applied to indoor spaces or autonomous driving, focusing on\nlow-level tasks like segmentation. This work expands their use to urban-scale\nenvironments by leveraging 3D reconstructions from multi-view aerial imagery.\nWe propose OpenCity3D, an approach that addresses high-level tasks, such as\npopulation density estimation, building age classification, property price\nprediction, crime rate assessment, and noise pollution evaluation. Our findings\nhighlight OpenCity3D's impressive zero-shot and few-shot capabilities,\nshowcasing adaptability to new contexts. This research establishes a new\nparadigm for language-driven urban analytics, enabling applications in\nplanning, policy, and environmental monitoring. See our project page:\nopencity3d.github.io",
            "upvotes": 0,
            "discussionId": "67e3d39a48da72bf6f3d28a3",
            "ai_keywords": [
                "Vision-language models (VLMs)",
                "3D scene understanding",
                "segmentation",
                "3D reconstructions",
                "multi-view aerial imagery",
                "zero-shot capabilities",
                "few-shot capabilities",
                "population density estimation",
                "building age classification",
                "property price prediction",
                "crime rate assessment",
                "noise pollution evaluation",
                "language-driven urban analytics"
            ]
        },
        "publishedAt": "2025-03-20T21:11:21.000Z",
        "title": "OpenCity3D: What do Vision-Language Models know about Urban\n  Environments?",
        "summary": "Vision-language models (VLMs) show great promise for 3D scene understanding\nbut are mainly applied to indoor spaces or autonomous driving, focusing on\nlow-level tasks like segmentation. This work expands their use to urban-scale\nenvironments by leveraging 3D reconstructions from multi-view aerial imagery.\nWe propose OpenCity3D, an approach that addresses high-level tasks, such as\npopulation density estimation, building age classification, property price\nprediction, crime rate assessment, and noise pollution evaluation. Our findings\nhighlight OpenCity3D's impressive zero-shot and few-shot capabilities,\nshowcasing adaptability to new contexts. This research establishes a new\nparadigm for language-driven urban analytics, enabling applications in\nplanning, policy, and environmental monitoring. See our project page:\nopencity3d.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16776.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64462645866784561a882802",
            "avatarUrl": "/avatars/4c08f6f60a82b7292c91fccb2c8bea37.svg",
            "fullname": "Qingxuan Chen",
            "name": "LUC1O",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.15667",
            "authors": [
                {
                    "_id": "67e358f8f16653f6dc99f162",
                    "user": {
                        "_id": "64d69b22bcab729cb4b0a6ee",
                        "avatarUrl": "/avatars/17604cb8577cb769e07dd40fc22a1cd5.svg",
                        "isPro": false,
                        "fullname": "Yuming Gu",
                        "user": "gym890",
                        "type": "user"
                    },
                    "name": "Yuming Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:41.513Z",
                    "hidden": false
                },
                {
                    "_id": "67e358f8f16653f6dc99f163",
                    "name": "Phong Tran",
                    "hidden": false
                },
                {
                    "_id": "67e358f8f16653f6dc99f164",
                    "name": "Yujian Zheng",
                    "hidden": false
                },
                {
                    "_id": "67e358f8f16653f6dc99f165",
                    "name": "Hongyi Xu",
                    "hidden": false
                },
                {
                    "_id": "67e358f8f16653f6dc99f166",
                    "name": "Heyuan Li",
                    "hidden": false
                },
                {
                    "_id": "67e358f8f16653f6dc99f167",
                    "name": "Adilbek Karmanov",
                    "hidden": false
                },
                {
                    "_id": "67e358f8f16653f6dc99f168",
                    "name": "Hao Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T19:47:04.000Z",
            "submittedOnDailyAt": "2025-03-26T20:32:24.095Z",
            "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis",
            "submittedOnDailyBy": {
                "_id": "64d69b22bcab729cb4b0a6ee",
                "avatarUrl": "/avatars/17604cb8577cb769e07dd40fc22a1cd5.svg",
                "isPro": false,
                "fullname": "Yuming Gu",
                "user": "gym890",
                "type": "user"
            },
            "summary": "Generating high-quality 360-degree views of human heads from single-view\nimages is essential for enabling accessible immersive telepresence applications\nand scalable personalized content creation. While cutting-edge methods for full\nhead generation are limited to modeling realistic human heads, the latest\ndiffusion-based approaches for style-omniscient head synthesis can produce only\nfrontal views and struggle with view consistency, preventing their conversion\ninto true 3D models for rendering from arbitrary angles. We introduce a novel\napproach that generates fully consistent 360-degree head views, accommodating\nhuman, stylized, and anthropomorphic forms, including accessories like glasses\nand hats. Our method builds on the DiffPortrait3D framework, incorporating a\ncustom ControlNet for back-of-head detail generation and a dual appearance\nmodule to ensure global front-back consistency. By training on continuous view\nsequences and integrating a back reference image, our approach achieves robust,\nlocally continuous view synthesis. Our model can be used to produce\nhigh-quality neural radiance fields (NeRFs) for real-time, free-viewpoint\nrendering, outperforming state-of-the-art methods in object synthesis and\n360-degree head generation for very challenging input portraits.",
            "upvotes": 0,
            "discussionId": "67e35902f16653f6dc99f45f",
            "projectPage": "https://freedomgu.github.io/DiffPortrait360/",
            "githubRepo": "https://github.com/FreedomGu/DiffPortrait360/",
            "ai_keywords": [
                "diffusion-based approaches",
                "style-omniscient head synthesis",
                "3D models",
                "rendering",
                "360-degree views",
                "realistic human heads",
                "human, stylized, and anthropomorphic forms",
                "accessories",
                "DiffPortrait3D framework",
                "custom ControlNet",
                "dual appearance module",
                "back-of-head detail generation",
                "continuous view sequences",
                "back reference image",
                "robust",
                "locally continuous view synthesis",
                "neural radiance fields (NeRFs)",
                "real-time",
                "free-viewpoint rendering",
                "object synthesis",
                "360-degree head generation",
                "challenging input portraits"
            ]
        },
        "publishedAt": "2025-03-19T15:47:04.000Z",
        "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis",
        "summary": "Generating high-quality 360-degree views of human heads from single-view\nimages is essential for enabling accessible immersive telepresence applications\nand scalable personalized content creation. While cutting-edge methods for full\nhead generation are limited to modeling realistic human heads, the latest\ndiffusion-based approaches for style-omniscient head synthesis can produce only\nfrontal views and struggle with view consistency, preventing their conversion\ninto true 3D models for rendering from arbitrary angles. We introduce a novel\napproach that generates fully consistent 360-degree head views, accommodating\nhuman, stylized, and anthropomorphic forms, including accessories like glasses\nand hats. Our method builds on the DiffPortrait3D framework, incorporating a\ncustom ControlNet for back-of-head detail generation and a dual appearance\nmodule to ensure global front-back consistency. By training on continuous view\nsequences and integrating a back reference image, our approach achieves robust,\nlocally continuous view synthesis. Our model can be used to produce\nhigh-quality neural radiance fields (NeRFs) for real-time, free-viewpoint\nrendering, outperforming state-of-the-art methods in object synthesis and\n360-degree head generation for very challenging input portraits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15667.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d69b22bcab729cb4b0a6ee",
            "avatarUrl": "/avatars/17604cb8577cb769e07dd40fc22a1cd5.svg",
            "fullname": "Yuming Gu",
            "name": "gym890",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]