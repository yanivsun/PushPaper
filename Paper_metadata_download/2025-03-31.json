[
    {
        "paper": {
            "id": "2503.19693",
            "authors": [
                {
                    "_id": "67ea363dd13d75fc156ec498",
                    "user": {
                        "_id": "671f8106d677d3a764a6f9a5",
                        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
                        "isPro": false,
                        "fullname": "itay nakash",
                        "user": "itaynakash",
                        "type": "user"
                    },
                    "name": "Itay Nakash",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:24:09.634Z",
                    "hidden": false
                },
                {
                    "_id": "67ea363dd13d75fc156ec499",
                    "user": {
                        "_id": "62d6a0c18faee0ac953c51fa",
                        "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
                        "isPro": false,
                        "fullname": "Nitay Calderon",
                        "user": "nitay",
                        "type": "user"
                    },
                    "name": "Nitay Calderon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:24:15.760Z",
                    "hidden": false
                },
                {
                    "_id": "67ea363dd13d75fc156ec49a",
                    "user": {
                        "_id": "6645fc650e6706053171ce51",
                        "avatarUrl": "/avatars/54b03ac6939d4b8943606b12b979ce52.svg",
                        "isPro": false,
                        "fullname": "Eyal Ben-David",
                        "user": "eyalbd",
                        "type": "user"
                    },
                    "name": "Eyal Ben David",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:24:21.914Z",
                    "hidden": false
                },
                {
                    "_id": "67ea363dd13d75fc156ec49b",
                    "user": {
                        "_id": "630480fa6dbbb80f16352ee3",
                        "avatarUrl": "/avatars/f39ce2fe96a578f42a57e3bfe3a2d137.svg",
                        "isPro": false,
                        "fullname": "Elad Hoffer",
                        "user": "ehoffer",
                        "type": "user"
                    },
                    "name": "Elad Hoffer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:24:27.895Z",
                    "hidden": false
                },
                {
                    "_id": "67ea363dd13d75fc156ec49c",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/Rq3iCgXutkz1jOx5krnk-.png"
            ],
            "publishedAt": "2025-03-25T14:18:21.000Z",
            "submittedOnDailyAt": "2025-03-31T05:02:48.696Z",
            "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
            "submittedOnDailyBy": {
                "_id": "671f8106d677d3a764a6f9a5",
                "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
                "isPro": false,
                "fullname": "itay nakash",
                "user": "itaynakash",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
            "upvotes": 52,
            "discussionId": "67ea363ed13d75fc156ec4e8",
            "projectPage": "https://itay-nakash.github.io/AdaptiVocab/",
            "ai_keywords": [
                "AdaptiVocab",
                "vocabulary adaptation",
                "n-gram-based tokens",
                "token embeddings",
                "lightweight fine-tuning"
            ]
        },
        "publishedAt": "2025-03-25T10:18:21.000Z",
        "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
        "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/Rq3iCgXutkz1jOx5krnk-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19693.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "671f8106d677d3a764a6f9a5",
            "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
            "fullname": "itay nakash",
            "name": "itaynakash",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22230",
            "authors": [
                {
                    "_id": "67e9fdd446d9dd867e9728d3",
                    "user": {
                        "_id": "6468823272d9180d4ac90bdf",
                        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                        "isPro": false,
                        "fullname": "Wei Shen",
                        "user": "Swtheking",
                        "type": "user"
                    },
                    "name": "Wei Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:24:54.522Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728d4",
                    "user": {
                        "_id": "67805c4a43a58ab7b52a05ea",
                        "avatarUrl": "/avatars/759d0466020b6f7c0207aaf62ad89eca.svg",
                        "isPro": false,
                        "fullname": "Guanlin Liu",
                        "user": "glnbyte",
                        "type": "user"
                    },
                    "name": "Guanlin Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-31T02:28:37.898Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728d5",
                    "user": {
                        "_id": "648223754de983d03190f4af",
                        "avatarUrl": "/avatars/36c70a6a3a1aa8a7cc0de106d5902a81.svg",
                        "isPro": false,
                        "fullname": "Zheng Wu",
                        "user": "zhengwu07",
                        "type": "user"
                    },
                    "name": "Zheng Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:25:03.076Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728d6",
                    "name": "Ruofei Zhu",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728d7",
                    "user": {
                        "_id": "64d20e1821aed29b2ffd2d99",
                        "avatarUrl": "/avatars/b0719319a74e8f51fc8a1404aca367e6.svg",
                        "isPro": false,
                        "fullname": "Qingping Yang",
                        "user": "qingping95",
                        "type": "user"
                    },
                    "name": "Qingping Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:25:15.803Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728d8",
                    "user": {
                        "_id": "661bca6576ac250a1106bfa6",
                        "avatarUrl": "/avatars/200327d87103f13f7cbbb40d11f2f188.svg",
                        "isPro": false,
                        "fullname": "Chao Xin",
                        "user": "amusingchao",
                        "type": "user"
                    },
                    "name": "Chao Xin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:25:23.410Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728d9",
                    "name": "Yu Yue",
                    "hidden": false
                },
                {
                    "_id": "67e9fdd446d9dd867e9728da",
                    "name": "Lin Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T08:26:41.000Z",
            "submittedOnDailyAt": "2025-03-31T00:59:21.502Z",
            "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
            "submittedOnDailyBy": {
                "_id": "6468823272d9180d4ac90bdf",
                "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                "isPro": false,
                "fullname": "Wei Shen",
                "user": "Swtheking",
                "type": "user"
            },
            "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nlarge language models with human preferences. While recent research has focused\non algorithmic improvements, the importance of prompt-data construction has\nbeen overlooked. This paper addresses this gap by exploring data-driven\nbottlenecks in RLHF performance scaling, particularly reward hacking and\ndecreasing response diversity. We introduce a hybrid reward system combining\nreasoning task verifiers (RTV) and a generative reward model (GenRM) to\nmitigate reward hacking. We also propose a novel prompt-selection method,\nPre-PPO, to maintain response diversity and enhance learning effectiveness.\nAdditionally, we find that prioritizing mathematical and coding tasks early in\nRLHF training significantly improves performance. Experiments across two model\nsizes validate our methods' effectiveness and scalability. Results show that\nRTV is most resistant to reward hacking, followed by GenRM with ground truth,\nand then GenRM with SFT Best-of-N responses. Our strategies enable rapid\ncapture of subtle task-specific distinctions, leading to substantial\nimprovements in overall RLHF performance. This work highlights the importance\nof careful data construction and provides practical methods to overcome\nperformance barriers in RLHF.",
            "upvotes": 28,
            "discussionId": "67e9fdd546d9dd867e97292c",
            "ai_keywords": [
                "Reinforcement Learning from Human Feedback (RLHF)",
                "reward hacking",
                "response diversity",
                "reasoning task verifiers (RTV)",
                "generative reward model (GenRM)",
                "Pre-PPO",
                "prompt-selection method",
                "mathematical tasks",
                "coding tasks",
                "GenRM with ground truth",
                "GenRM with SFT Best-of-N responses"
            ]
        },
        "publishedAt": "2025-03-28T04:26:41.000Z",
        "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nlarge language models with human preferences. While recent research has focused\non algorithmic improvements, the importance of prompt-data construction has\nbeen overlooked. This paper addresses this gap by exploring data-driven\nbottlenecks in RLHF performance scaling, particularly reward hacking and\ndecreasing response diversity. We introduce a hybrid reward system combining\nreasoning task verifiers (RTV) and a generative reward model (GenRM) to\nmitigate reward hacking. We also propose a novel prompt-selection method,\nPre-PPO, to maintain response diversity and enhance learning effectiveness.\nAdditionally, we find that prioritizing mathematical and coding tasks early in\nRLHF training significantly improves performance. Experiments across two model\nsizes validate our methods' effectiveness and scalability. Results show that\nRTV is most resistant to reward hacking, followed by GenRM with ground truth,\nand then GenRM with SFT Best-of-N responses. Our strategies enable rapid\ncapture of subtle task-specific distinctions, leading to substantial\nimprovements in overall RLHF performance. This work highlights the importance\nof careful data construction and provides practical methods to overcome\nperformance barriers in RLHF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22230.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6468823272d9180d4ac90bdf",
            "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
            "fullname": "Wei Shen",
            "name": "Swtheking",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22675",
            "authors": [
                {
                    "_id": "67e9f6b7d13d75fc155c7f2e",
                    "user": {
                        "_id": "65acfb3a14e6582c30b4ce76",
                        "avatarUrl": "/avatars/3402ba72fe2436a9c2c2f92e56b15deb.svg",
                        "isPro": false,
                        "fullname": "TangJiakai",
                        "user": "TangJiakai5704",
                        "type": "user"
                    },
                    "name": "Jiakai Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:47.198Z",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f2f",
                    "user": {
                        "_id": "64db88993725f8d9a908c077",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                        "isPro": false,
                        "fullname": "Sunhao Dai",
                        "user": "KID-22",
                        "type": "user"
                    },
                    "name": "Sunhao Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:23:02.371Z",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f30",
                    "user": {
                        "_id": "66152fbe1bcd61054402449b",
                        "avatarUrl": "/avatars/17cb2f997e7983d706d87cf7c8c5c3dd.svg",
                        "isPro": false,
                        "fullname": "Shi",
                        "user": "TengShi",
                        "type": "user"
                    },
                    "name": "Teng Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:23:10.725Z",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f31",
                    "name": "Jun Xu",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f32",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f33",
                    "name": "Wen Chen",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f34",
                    "name": "Wu Jian",
                    "hidden": false
                },
                {
                    "_id": "67e9f6b7d13d75fc155c7f35",
                    "name": "Yuning Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:59:03.000Z",
            "submittedOnDailyAt": "2025-03-31T00:29:30.669Z",
            "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
            "submittedOnDailyBy": {
                "_id": "64db88993725f8d9a908c077",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                "isPro": false,
                "fullname": "Sunhao Dai",
                "user": "KID-22",
                "type": "user"
            },
            "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose ReaRec, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
            "upvotes": 26,
            "discussionId": "67e9f6bdd13d75fc155c805e",
            "githubRepo": "https://github.com/TangJiakai/ReaRec",
            "ai_keywords": [
                "ReaRec",
                "reasoning position embeddings",
                "Ensemble Reasoning Learning (ERL)",
                "Progressive Reasoning Learning (PRL)",
                "sequential recommendation backbones",
                "autoregressive feeding"
            ]
        },
        "publishedAt": "2025-03-28T13:59:03.000Z",
        "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
        "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose ReaRec, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22675.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64db88993725f8d9a908c077",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
            "fullname": "Sunhao Dai",
            "name": "KID-22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21614",
            "authors": [
                {
                    "_id": "67ea331c1238e1aa16fc18b3",
                    "user": {
                        "_id": "64cb54da1af278541d663708",
                        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                        "isPro": false,
                        "fullname": "Xiaoye Qu",
                        "user": "Xiaoye08",
                        "type": "user"
                    },
                    "name": "Xiaoye Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:27:07.849Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18b4",
                    "user": {
                        "_id": "63f3502a520c14618925825a",
                        "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                        "isPro": false,
                        "fullname": "Yafu Li",
                        "user": "yaful",
                        "type": "user"
                    },
                    "name": "Yafu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:27:17.977Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18b5",
                    "user": {
                        "_id": "64264095ba51f8a2136946a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
                        "isPro": false,
                        "fullname": "Zhaochen Su",
                        "user": "Warrieryes",
                        "type": "user"
                    },
                    "name": "Zhaochen Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:31.516Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18b6",
                    "user": {
                        "_id": "6246bb33da617c00b48e4d92",
                        "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                        "isPro": false,
                        "fullname": "Weigao Sun",
                        "user": "weigao266",
                        "type": "user"
                    },
                    "name": "Weigao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:28:08.547Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18b7",
                    "user": {
                        "_id": "6086838b19137b3a6ba760e7",
                        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
                        "isPro": false,
                        "fullname": "Jianhao Yan",
                        "user": "Elliott",
                        "type": "user"
                    },
                    "name": "Jianhao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:28:21.561Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18b8",
                    "user": {
                        "_id": "657fe7a8504da7f6f30a2832",
                        "avatarUrl": "/avatars/65987e3cba449b5d250616510ee11f33.svg",
                        "isPro": false,
                        "fullname": "Dongrui Liu",
                        "user": "Max9803",
                        "type": "user"
                    },
                    "name": "Dongrui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:28:40.653Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18b9",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:28:47.399Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18ba",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18bb",
                    "user": {
                        "_id": "640052d5330a45b0360483aa",
                        "avatarUrl": "/avatars/0836247e9e0ecbf68b069eaa3c6edd47.svg",
                        "isPro": false,
                        "fullname": "Shuxian Liang",
                        "user": "liang4sx",
                        "type": "user"
                    },
                    "name": "Shuxian Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:29:01.763Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18bc",
                    "user": {
                        "_id": "615f34ec3f6d24d67c1b5c78",
                        "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
                        "isPro": false,
                        "fullname": "Junxian He",
                        "user": "jxhe",
                        "type": "user"
                    },
                    "name": "Junxian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:29:08.537Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18bd",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18be",
                    "name": "Wei Wei",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18bf",
                    "name": "Jing Shao",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18c0",
                    "name": "Chaochao Lu",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18c1",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18c2",
                    "name": "Xian-Sheng Hua",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18c3",
                    "user": {
                        "_id": "669f614b59adf5b56e05bce3",
                        "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
                        "isPro": false,
                        "fullname": "BowenZhou",
                        "user": "bowenZhou",
                        "type": "user"
                    },
                    "name": "Bowen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:29:49.460Z",
                    "hidden": false
                },
                {
                    "_id": "67ea331c1238e1aa16fc18c4",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T15:36:30.000Z",
            "submittedOnDailyAt": "2025-03-31T04:49:37.564Z",
            "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
            "submittedOnDailyBy": {
                "_id": "64cb54da1af278541d663708",
                "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                "isPro": false,
                "fullname": "Xiaoye Qu",
                "user": "Xiaoye08",
                "type": "user"
            },
            "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
            "upvotes": 21,
            "discussionId": "67ea331d1238e1aa16fc190f",
            "githubRepo": "https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning",
            "ai_keywords": [
                "Large Reasoning Models (LRMs)",
                "DeepSeek-R1",
                "OpenAI o1",
                "Chain-of-Thought (CoT) reasoning",
                "reasoning traces",
                "redundant content",
                "over-analysis",
                "superficial exploration",
                "reasoning efficiency",
                "token economy",
                "agent-based systems",
                "pretraining",
                "inference",
                "GitHub repository"
            ]
        },
        "publishedAt": "2025-03-27T11:36:30.000Z",
        "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
        "summary": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have\ndemonstrated strong performance gains by scaling up the length of\nChain-of-Thought (CoT) reasoning during inference. However, a growing concern\nlies in their tendency to produce excessively long reasoning traces, which are\noften filled with redundant content (e.g., repeated definitions), over-analysis\nof simple problems, and superficial exploration of multiple reasoning paths for\nharder tasks. This inefficiency introduces significant challenges for training,\ninference, and real-world deployment (e.g., in agent-based systems), where\ntoken economy is critical. In this survey, we provide a comprehensive overview\nof recent efforts aimed at improving reasoning efficiency in LRMs, with a\nparticular focus on the unique challenges that arise in this new paradigm. We\nidentify common patterns of inefficiency, examine methods proposed across the\nLRM lifecycle, i.e., from pretraining to inference, and discuss promising\nfuture directions for research. To support ongoing development, we also\nmaintain a real-time GitHub repository tracking recent progress in the field.\nWe hope this survey serves as a foundation for further exploration and inspires\ninnovation in this rapidly evolving area.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21614.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64cb54da1af278541d663708",
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "fullname": "Xiaoye Qu",
            "name": "Xiaoye08",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.16081",
            "authors": [
                {
                    "_id": "67ea70d128179c61bef64043",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67ea70d128179c61bef64044",
                    "name": "Yuting Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ea70d128179c61bef64045",
                    "name": "Feng Liu",
                    "hidden": false
                },
                {
                    "_id": "67ea70d128179c61bef64046",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ea70d128179c61bef64047",
                    "name": "Ying Sun",
                    "hidden": false
                },
                {
                    "_id": "67ea70d128179c61bef64048",
                    "user": {
                        "_id": "66da6c9e84f243eba3b49cf1",
                        "avatarUrl": "/avatars/d30d96025141d68f18a28005a5d6f5af.svg",
                        "isPro": false,
                        "fullname": "junwang.lu",
                        "user": "jwanglux",
                        "type": "user"
                    },
                    "name": "Jun Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-31T10:39:14.597Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-20T12:22:18.000Z",
            "submittedOnDailyAt": "2025-03-31T09:51:04.725Z",
            "title": "OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  via dynamic reinforcement learning",
            "submittedOnDailyBy": {
                "_id": "67163854c30d32112e7d0d66",
                "avatarUrl": "/avatars/c7c9b482e7a1b08f801275c7df956033.svg",
                "isPro": false,
                "fullname": "Yuting Zhang",
                "user": "Sonia755",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have gained significant traction for\ntheir ability to process diverse input data types and generate coherent,\ncontextually relevant outputs across various applications. While supervised\nfine-tuning (SFT) has been the predominant approach to enhance MLLM\ncapabilities in task-specific optimization, it often falls short in fostering\ncrucial generalized reasoning abilities. Although reinforcement learning (RL)\nholds great promise in overcoming these limitations, it encounters two\nsignificant challenges: (1) its generalized capacities in multimodal tasks\nremain largely unexplored, and (2) its training constraints, including the\nconstant Kullback-Leibler divergence or the clamp strategy, often result in\nsuboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an\nadvanced MLLM equipped with profound comprehension and reasoning capabilities\nacross multimodal tasks. Specifically, we introduce Group Relative Policy\nOptimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly\nenhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct,\nGRPO-D achieves a relative improvement of more than 5.72% over SFT and more\nthan 13.59% over GRPO in same-task evaluation on two adapted datasets.\nFurthermore, GRPO-D demonstrates remarkable cross-task generalization\ncapabilities, with an average relative improvement of more than 61.63% over SFT\nin cross-task evaluation. These results highlight that the MLLM trained with\nGRPO-D on one multimodal task can be effectively transferred to another task,\nunderscoring the superior generalized reasoning capabilities of our proposed\nOThink-MR1 model.",
            "upvotes": 19,
            "discussionId": "67ea70d228179c61bef6408e",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "supervised fine-tuning (SFT)",
                "reinforcement learning (RL)",
                "Kullback-Leibler divergence",
                "Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D)",
                "Qwen2-VL-2B-Instruct",
                "same-task evaluation",
                "cross-task evaluation",
                "cross-task generalization"
            ]
        },
        "publishedAt": "2025-03-20T08:22:18.000Z",
        "title": "OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  via dynamic reinforcement learning",
        "summary": "Multimodal Large Language Models (MLLMs) have gained significant traction for\ntheir ability to process diverse input data types and generate coherent,\ncontextually relevant outputs across various applications. While supervised\nfine-tuning (SFT) has been the predominant approach to enhance MLLM\ncapabilities in task-specific optimization, it often falls short in fostering\ncrucial generalized reasoning abilities. Although reinforcement learning (RL)\nholds great promise in overcoming these limitations, it encounters two\nsignificant challenges: (1) its generalized capacities in multimodal tasks\nremain largely unexplored, and (2) its training constraints, including the\nconstant Kullback-Leibler divergence or the clamp strategy, often result in\nsuboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an\nadvanced MLLM equipped with profound comprehension and reasoning capabilities\nacross multimodal tasks. Specifically, we introduce Group Relative Policy\nOptimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly\nenhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct,\nGRPO-D achieves a relative improvement of more than 5.72% over SFT and more\nthan 13.59% over GRPO in same-task evaluation on two adapted datasets.\nFurthermore, GRPO-D demonstrates remarkable cross-task generalization\ncapabilities, with an average relative improvement of more than 61.63% over SFT\nin cross-task evaluation. These results highlight that the MLLM trained with\nGRPO-D on one multimodal task can be effectively transferred to another task,\nunderscoring the superior generalized reasoning capabilities of our proposed\nOThink-MR1 model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16081.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67163854c30d32112e7d0d66",
            "avatarUrl": "/avatars/c7c9b482e7a1b08f801275c7df956033.svg",
            "fullname": "Yuting Zhang",
            "name": "Sonia755",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.22194",
            "authors": [
                {
                    "_id": "67e9eebe1f495035ca228ded",
                    "user": {
                        "_id": "66ee81b676a8038cb42c8caa",
                        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
                        "isPro": false,
                        "fullname": "Yunhong Min",
                        "user": "myhong",
                        "type": "user"
                    },
                    "name": "Yunhong Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:49.474Z",
                    "hidden": false
                },
                {
                    "_id": "67e9eebe1f495035ca228dee",
                    "user": {
                        "_id": "6616702547ea6347974667e5",
                        "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
                        "isPro": false,
                        "fullname": "Daehyeon Choi",
                        "user": "daehyeonchoi",
                        "type": "user"
                    },
                    "name": "Daehyeon Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:51.359Z",
                    "hidden": false
                },
                {
                    "_id": "67e9eebe1f495035ca228def",
                    "user": {
                        "_id": "659e42cfb65ee9ee1fd11e61",
                        "avatarUrl": "/avatars/a9220d099f32800fc43ae79bb519c1e9.svg",
                        "isPro": false,
                        "fullname": "Kyeongmin Yeo",
                        "user": "32V",
                        "type": "user"
                    },
                    "name": "Kyeongmin Yeo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:26:31.020Z",
                    "hidden": false
                },
                {
                    "_id": "67e9eebe1f495035ca228df0",
                    "name": "Jihyun Lee",
                    "hidden": false
                },
                {
                    "_id": "67e9eebe1f495035ca228df1",
                    "user": {
                        "_id": "631f432b5ba8c026340a7890",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg",
                        "isPro": false,
                        "fullname": "Minhyuk Sung",
                        "user": "Minhyuk",
                        "type": "user"
                    },
                    "name": "Minhyuk Sung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:26:40.432Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T07:23:12.000Z",
            "submittedOnDailyAt": "2025-03-31T00:39:49.117Z",
            "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "6616702547ea6347974667e5",
                "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
                "isPro": false,
                "fullname": "Daehyeon Choi",
                "user": "daehyeonchoi",
                "type": "user"
            },
            "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.",
            "upvotes": 17,
            "discussionId": "67e9eebf1f495035ca228e34",
            "projectPage": "https://origen2025.github.io/",
            "ai_keywords": [
                "zero-shot method",
                "3D orientation grounding",
                "text-to-image generation",
                "spatial grounding",
                "reward-guided sampling approach",
                "pretrained discriminative model",
                "3D orientation estimation",
                "one-step text-to-image generative flow model",
                "gradient-ascent-based optimization",
                "Langevin dynamics",
                "adaptive time rescaling"
            ]
        },
        "publishedAt": "2025-03-28T03:23:12.000Z",
        "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
        "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22194.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6616702547ea6347974667e5",
            "avatarUrl": "/avatars/6bf95e50ba19df163ef89867ed63fecc.svg",
            "fullname": "Daehyeon Choi",
            "name": "daehyeonchoi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21332",
            "authors": [
                {
                    "_id": "67e623f10aaa5e9f7cf8a179",
                    "user": {
                        "_id": "65642d7401de72cb63165d22",
                        "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
                        "isPro": true,
                        "fullname": "ytaewon",
                        "user": "hamzzi",
                        "type": "user"
                    },
                    "name": "Taewon Yun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-28T08:36:56.604Z",
                    "hidden": false
                },
                {
                    "_id": "67e623f10aaa5e9f7cf8a17a",
                    "name": "Jihwan Oh",
                    "hidden": false
                },
                {
                    "_id": "67e623f10aaa5e9f7cf8a17b",
                    "user": {
                        "_id": "6510c8ebf26dbb8827ee5e80",
                        "avatarUrl": "/avatars/cc49a2f176c951007006e0dae331bc50.svg",
                        "isPro": false,
                        "fullname": "Hyangsuk Min",
                        "user": "hyang0503",
                        "type": "user"
                    },
                    "name": "Hyangsuk Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:01:07.989Z",
                    "hidden": false
                },
                {
                    "_id": "67e623f10aaa5e9f7cf8a17c",
                    "user": {
                        "_id": "63f6eec4c96958470d207698",
                        "avatarUrl": "/avatars/7fba5e561b809a1623bf2228435f1aad.svg",
                        "isPro": false,
                        "fullname": "Yuho Lee",
                        "user": "Myyhlee",
                        "type": "user"
                    },
                    "name": "Yuho Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:01:13.949Z",
                    "hidden": false
                },
                {
                    "_id": "67e623f10aaa5e9f7cf8a17d",
                    "user": {
                        "_id": "644938d43def32791088b762",
                        "avatarUrl": "/avatars/1f17916b92ef13452151175cb8cafdf9.svg",
                        "isPro": false,
                        "fullname": "Jihwan Bang",
                        "user": "hwany-j",
                        "type": "user"
                    },
                    "name": "Jihwan Bang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:01:20.117Z",
                    "hidden": false
                },
                {
                    "_id": "67e623f10aaa5e9f7cf8a17e",
                    "user": {
                        "_id": "6463c26aa5af935cfe70f08d",
                        "avatarUrl": "/avatars/33b1210098891db54f57d1344b5110fb.svg",
                        "isPro": false,
                        "fullname": "Jinglun (Jason) Cai",
                        "user": "jasoncai",
                        "type": "user"
                    },
                    "name": "Jason Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:01:43.066Z",
                    "hidden": false
                },
                {
                    "_id": "67e623f10aaa5e9f7cf8a17f",
                    "name": "Hwanjun Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T10:11:41.000Z",
            "submittedOnDailyAt": "2025-03-31T05:50:46.001Z",
            "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback",
            "submittedOnDailyBy": {
                "_id": "65642d7401de72cb63165d22",
                "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
                "isPro": true,
                "fullname": "ytaewon",
                "user": "hamzzi",
                "type": "user"
            },
            "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
            "upvotes": 17,
            "discussionId": "67e623f20aaa5e9f7cf8a1dc",
            "ai_keywords": [
                "Long-CoT-based dataset",
                "reflective reasoning",
                "refinement performance",
                "ReFeed",
                "SumFeed-CoT"
            ]
        },
        "publishedAt": "2025-03-27T06:11:41.000Z",
        "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback",
        "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21332.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65642d7401de72cb63165d22",
            "avatarUrl": "/avatars/1f4417c4ac5e781ce73eae1060e3f7f2.svg",
            "fullname": "ytaewon",
            "name": "hamzzi",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.20785",
            "authors": [
                {
                    "_id": "67e7b4ba05d7355e476f4a10",
                    "user": {
                        "_id": "66d347eebb76fb26eedb256e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
                        "isPro": false,
                        "fullname": "tianqi liu",
                        "user": "tqliu",
                        "type": "user"
                    },
                    "name": "Tianqi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:12:50.975Z",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a11",
                    "user": {
                        "_id": "631b24f2f6bc4be4a64c4d43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631b24f2f6bc4be4a64c4d43/P9_tVF7SESmVxxGKVCgCk.jpeg",
                        "isPro": false,
                        "fullname": "Zihao Huang",
                        "user": "Inso",
                        "type": "user"
                    },
                    "name": "Zihao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:33:22.318Z",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a12",
                    "user": {
                        "_id": "62fc8cf7ee999004b5a8b982",
                        "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
                        "isPro": false,
                        "fullname": "Zhaoxi Chen",
                        "user": "FrozenBurning",
                        "type": "user"
                    },
                    "name": "Zhaoxi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:33:43.896Z",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a13",
                    "user": {
                        "_id": "62e893da40bd989bb71b8f89",
                        "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
                        "isPro": false,
                        "fullname": "Guangcong Wang",
                        "user": "GuangcongWang",
                        "type": "user"
                    },
                    "name": "Guangcong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:33:54.167Z",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a14",
                    "user": {
                        "_id": "6503be91a450492f84314af8",
                        "avatarUrl": "/avatars/ef94efdad0bc8a423262d25a8cf77e41.svg",
                        "isPro": false,
                        "fullname": "Shoukang Hu",
                        "user": "skhu101",
                        "type": "user"
                    },
                    "name": "Shoukang Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:34:21.036Z",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a15",
                    "name": "Liao Shen",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a16",
                    "name": "Huiqiang Sun",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a17",
                    "name": "Zhiguo Cao",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a18",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67e7b4ba05d7355e476f4a19",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:34:47.540Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-26T17:59:44.000Z",
            "submittedOnDailyAt": "2025-03-31T03:00:39.077Z",
            "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
            "submittedOnDailyBy": {
                "_id": "62fc8cf7ee999004b5a8b982",
                "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
                "isPro": false,
                "fullname": "Zhaoxi Chen",
                "user": "FrozenBurning",
                "type": "user"
            },
            "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.",
            "upvotes": 15,
            "discussionId": "67e7b4bb05d7355e476f4a74",
            "ai_keywords": [
                "diffusion models",
                "4D scene representation",
                "image-to-video",
                "adaptive guidance mechanism",
                "point-guided denoising",
                "latent replacement strategy",
                "temporal coherence",
                "modulation-based refinement"
            ]
        },
        "publishedAt": "2025-03-26T13:59:44.000Z",
        "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
        "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20785.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "fullname": "Zhaoxi Chen",
            "name": "FrozenBurning",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.20308",
            "authors": [
                {
                    "_id": "67ea2b9a676ae1ad3402eede",
                    "user": {
                        "_id": "67ea28b89f3eff13b78260ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9Azj8WDU1XAG2Ihrbc4Mb.jpeg",
                        "isPro": false,
                        "fullname": "Lee Chae-Yeon",
                        "user": "Chae-Yeon",
                        "type": "user"
                    },
                    "name": "Lee Chae-Yeon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:33.799Z",
                    "hidden": false
                },
                {
                    "_id": "67ea2b9a676ae1ad3402eedf",
                    "name": "Oh Hyun-Bin",
                    "hidden": false
                },
                {
                    "_id": "67ea2b9a676ae1ad3402eee0",
                    "user": {
                        "_id": "65067ef0d8d96e913b3213ee",
                        "avatarUrl": "/avatars/91732e9cead404fc18e11aa339641f6d.svg",
                        "isPro": false,
                        "fullname": "Han EunGi",
                        "user": "Han-EunGi",
                        "type": "user"
                    },
                    "name": "Han EunGi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:02:59.233Z",
                    "hidden": false
                },
                {
                    "_id": "67ea2b9a676ae1ad3402eee1",
                    "user": {
                        "_id": "66d0986b9678056278ce86f2",
                        "avatarUrl": "/avatars/816cd3fad6c11c7232ea10c9899fa016.svg",
                        "isPro": false,
                        "fullname": "KIM SUNGBIN",
                        "user": "backryun",
                        "type": "user"
                    },
                    "name": "Kim Sung-Bin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:03:07.568Z",
                    "hidden": true
                },
                {
                    "_id": "67ea2b9a676ae1ad3402eee2",
                    "user": {
                        "_id": "6760e12288be0baf4b1196f2",
                        "avatarUrl": "/avatars/487631d07b0ab439778836dfcd12dfe4.svg",
                        "isPro": false,
                        "fullname": "suekyeong nam",
                        "user": "akasha9890",
                        "type": "user"
                    },
                    "name": "Suekyeong Nam",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:03:13.804Z",
                    "hidden": false
                },
                {
                    "_id": "67ea2b9a676ae1ad3402eee3",
                    "user": {
                        "_id": "674622d01310ed05c6c5a5aa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0ga3pvGwd8oTEEWxIuPr6.png",
                        "isPro": false,
                        "fullname": "Tae-Hyun Oh",
                        "user": "taehyunoh",
                        "type": "user"
                    },
                    "name": "Tae-Hyun Oh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:03:19.362Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-26T08:18:57.000Z",
            "submittedOnDailyAt": "2025-03-31T06:59:13.941Z",
            "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics",
            "submittedOnDailyBy": {
                "_id": "67ea28b89f3eff13b78260ca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9Azj8WDU1XAG2Ihrbc4Mb.jpeg",
                "isPro": false,
                "fullname": "Lee Chae-Yeon",
                "user": "Chae-Yeon",
                "type": "user"
            },
            "summary": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.",
            "upvotes": 15,
            "discussionId": "67ea2b9b676ae1ad3402ef63",
            "projectPage": "https://perceptual-3d-talking-head.github.io/",
            "ai_keywords": [
                "speech-mesh synchronized representation",
                "perceptual loss",
                "perceptual metric",
                "lip synchronization metrics",
                "Temporal Synchronization",
                "Lip Readability",
                "Expressiveness",
                "3D talking head generation",
                "lip movements",
                "speech signals",
                "3D face meshes"
            ]
        },
        "publishedAt": "2025-03-26T04:18:57.000Z",
        "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics",
        "summary": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20308.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67ea28b89f3eff13b78260ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9Azj8WDU1XAG2Ihrbc4Mb.jpeg",
            "fullname": "Lee Chae-Yeon",
            "name": "Chae-Yeon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21821",
            "authors": [
                {
                    "_id": "67e9ffab6887b70da56d0de5",
                    "user": {
                        "_id": "668b6668cc2c0b4ae303bdb8",
                        "avatarUrl": "/avatars/2a4e30c0a5ee76b66232f425d5e62747.svg",
                        "isPro": false,
                        "fullname": "Kaiyue Feng",
                        "user": "Carrie777",
                        "type": "user"
                    },
                    "name": "Kaiyue Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:30:14.677Z",
                    "hidden": false
                },
                {
                    "_id": "67e9ffab6887b70da56d0de6",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:42.629Z",
                    "hidden": false
                },
                {
                    "_id": "67e9ffab6887b70da56d0de7",
                    "user": {
                        "_id": "6244de1c1c560fb11edfca44",
                        "avatarUrl": "/avatars/36558928bd04be7f49837d4c603681d7.svg",
                        "isPro": false,
                        "fullname": "Yixin Liu",
                        "user": "henryL7",
                        "type": "user"
                    },
                    "name": "Yixin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:30:33.598Z",
                    "hidden": false
                },
                {
                    "_id": "67e9ffab6887b70da56d0de8",
                    "name": "Tianyu Yang",
                    "hidden": false
                },
                {
                    "_id": "67e9ffab6887b70da56d0de9",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e9ffab6887b70da56d0dea",
                    "user": {
                        "_id": "6626e136cee7ea5738a8442b",
                        "avatarUrl": "/avatars/8c1537773e2c70f9c10b51a004380824.svg",
                        "isPro": false,
                        "fullname": "John Sous",
                        "user": "jsous",
                        "type": "user"
                    },
                    "name": "John Sous",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:30:58.653Z",
                    "hidden": false
                },
                {
                    "_id": "67e9ffab6887b70da56d0deb",
                    "user": {
                        "_id": "5f5ba21188f57f65f951f255",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
                        "isPro": false,
                        "fullname": "Arman Cohan",
                        "user": "armanc",
                        "type": "user"
                    },
                    "name": "Arman Cohan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:31:04.921Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-26T06:21:56.000Z",
            "submittedOnDailyAt": "2025-03-31T01:07:24.176Z",
            "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
            "submittedOnDailyBy": {
                "_id": "62f662bcc58915315c4eccea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                "isPro": true,
                "fullname": "Yilun",
                "user": "yilunzhao",
                "type": "user"
            },
            "summary": "We introduce PHYSICS, a comprehensive benchmark for university-level physics\nproblem solving. It contains 1297 expert-annotated problems covering six core\nareas: classical mechanics, quantum mechanics, thermodynamics and statistical\nmechanics, electromagnetism, atomic physics, and optics. Each problem requires\nadvanced physics knowledge and mathematical reasoning. We develop a robust\nautomated evaluation system for precise and reliable validation. Our evaluation\nof leading foundation models reveals substantial limitations. Even the most\nadvanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant\nchallenges in solving high-level scientific problems. Through comprehensive\nerror analysis, exploration of diverse prompting strategies, and\nRetrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify\nkey areas for improvement, laying the foundation for future advancements.",
            "upvotes": 15,
            "discussionId": "67e9ffac6887b70da56d0e15",
            "githubRepo": "https://github.com/yale-nlp/Physics"
        },
        "publishedAt": "2025-03-26T02:21:56.000Z",
        "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
        "summary": "We introduce PHYSICS, a comprehensive benchmark for university-level physics\nproblem solving. It contains 1297 expert-annotated problems covering six core\nareas: classical mechanics, quantum mechanics, thermodynamics and statistical\nmechanics, electromagnetism, atomic physics, and optics. Each problem requires\nadvanced physics knowledge and mathematical reasoning. We develop a robust\nautomated evaluation system for precise and reliable validation. Our evaluation\nof leading foundation models reveals substantial limitations. Even the most\nadvanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant\nchallenges in solving high-level scientific problems. Through comprehensive\nerror analysis, exploration of diverse prompting strategies, and\nRetrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify\nkey areas for improvement, laying the foundation for future advancements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "fullname": "Yilun",
            "name": "yilunzhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19108",
            "authors": [
                {
                    "_id": "67ea6d5a2d95c10a0d921c6b",
                    "user": {
                        "_id": "6368212544e19ccad212bbf2",
                        "avatarUrl": "/avatars/3ab0d1c6273a95f16b7b82c72db4a5e2.svg",
                        "isPro": false,
                        "fullname": "Tommie Kerssies",
                        "user": "tommiekerssies",
                        "type": "user"
                    },
                    "name": "Tommie Kerssies",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T12:57:35.964Z",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c6c",
                    "user": {
                        "_id": "67ea650e28179c61bef30406",
                        "avatarUrl": "/avatars/cad077d3185386fc2a0b49fbf724ca5e.svg",
                        "isPro": false,
                        "fullname": "Niccol Cavagnero",
                        "user": "neikos00",
                        "type": "user"
                    },
                    "name": "Niccol Cavagnero",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T12:57:32.351Z",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c6d",
                    "name": "Alexander Hermans",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c6e",
                    "name": "Narges Norouzi",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c6f",
                    "name": "Giuseppe Averta",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c70",
                    "name": "Bastian Leibe",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c71",
                    "name": "Gijs Dubbelman",
                    "hidden": false
                },
                {
                    "_id": "67ea6d5a2d95c10a0d921c72",
                    "name": "Daan de Geus",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T19:56:02.000Z",
            "submittedOnDailyAt": "2025-03-31T09:04:48.765Z",
            "title": "Your ViT is Secretly an Image Segmentation Model",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Vision Transformers (ViTs) have shown remarkable performance and scalability\nacross various computer vision tasks. To apply single-scale ViTs to image\nsegmentation, existing methods adopt a convolutional adapter to generate\nmulti-scale features, a pixel decoder to fuse these features, and a Transformer\ndecoder that uses the fused features to make predictions. In this paper, we\nshow that the inductive biases introduced by these task-specific components can\ninstead be learned by the ViT itself, given sufficiently large models and\nextensive pre-training. Based on these findings, we introduce the Encoder-only\nMask Transformer (EoMT), which repurposes the plain ViT architecture to conduct\nimage segmentation. With large-scale models and pre-training, EoMT obtains a\nsegmentation accuracy similar to state-of-the-art models that use task-specific\ncomponents. At the same time, EoMT is significantly faster than these methods\ndue to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a\nrange of model sizes, EoMT demonstrates an optimal balance between segmentation\naccuracy and prediction speed, suggesting that compute resources are better\nspent on scaling the ViT itself rather than adding architectural complexity.\nCode: https://www.tue-mps.org/eomt/.",
            "upvotes": 11,
            "discussionId": "67ea6d5c2d95c10a0d921cbf",
            "projectPage": "https://www.tue-mps.org/eomt/",
            "githubRepo": "https://github.com/tue-mps/eomt",
            "ai_keywords": [
                "Vision Transformers (ViTs)",
                "image segmentation",
                "convolutional adapter",
                "pixel decoder",
                "Transformer decoder",
                "inductive biases",
                "Encoder-only Mask Transformer (EoMT)",
                "architectural simplicity"
            ]
        },
        "publishedAt": "2025-03-24T15:56:02.000Z",
        "title": "Your ViT is Secretly an Image Segmentation Model",
        "summary": "Vision Transformers (ViTs) have shown remarkable performance and scalability\nacross various computer vision tasks. To apply single-scale ViTs to image\nsegmentation, existing methods adopt a convolutional adapter to generate\nmulti-scale features, a pixel decoder to fuse these features, and a Transformer\ndecoder that uses the fused features to make predictions. In this paper, we\nshow that the inductive biases introduced by these task-specific components can\ninstead be learned by the ViT itself, given sufficiently large models and\nextensive pre-training. Based on these findings, we introduce the Encoder-only\nMask Transformer (EoMT), which repurposes the plain ViT architecture to conduct\nimage segmentation. With large-scale models and pre-training, EoMT obtains a\nsegmentation accuracy similar to state-of-the-art models that use task-specific\ncomponents. At the same time, EoMT is significantly faster than these methods\ndue to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a\nrange of model sizes, EoMT demonstrates an optimal balance between segmentation\naccuracy and prediction speed, suggesting that compute resources are better\nspent on scaling the ViT itself rather than adding architectural complexity.\nCode: https://www.tue-mps.org/eomt/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19108.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 804
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.22268",
            "authors": [
                {
                    "_id": "67e9fec1564b123aa5d70388",
                    "name": "Nan Huang",
                    "hidden": false
                },
                {
                    "_id": "67e9fec1564b123aa5d70389",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67e9fec1564b123aa5d7038a",
                    "user": {
                        "_id": "654ca71d5255ee86711b52c5",
                        "avatarUrl": "/avatars/52bf00fd74c8db5643c4daa185c678e6.svg",
                        "isPro": false,
                        "fullname": "Chenfeng Xu",
                        "user": "chenfengx",
                        "type": "user"
                    },
                    "name": "Chenfeng Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:56:25.025Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fec1564b123aa5d7038b",
                    "user": {
                        "_id": "6251bf4b183aa4266924ad91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678041834400-6251bf4b183aa4266924ad91.jpeg",
                        "isPro": true,
                        "fullname": "Kurt Keutzer",
                        "user": "kurtkeutzer",
                        "type": "user"
                    },
                    "name": "Kurt Keutzer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:56:18.832Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fec1564b123aa5d7038c",
                    "name": "Shanghang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67e9fec1564b123aa5d7038d",
                    "user": {
                        "_id": "6478cf7150ff7001631679c3",
                        "avatarUrl": "/avatars/65ec385a9cc44c972e6caf952e759ff1.svg",
                        "isPro": false,
                        "fullname": "Angjoo Kanazawa",
                        "user": "akanazawa",
                        "type": "user"
                    },
                    "name": "Angjoo Kanazawa",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:56:03.262Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fec1564b123aa5d7038e",
                    "user": {
                        "_id": "6616f0c4c2e30710e607c2bf",
                        "avatarUrl": "/avatars/a5941e0ed940439f4c7c67747318cbfc.svg",
                        "isPro": false,
                        "fullname": "Qianqian Wang",
                        "user": "qianqian68",
                        "type": "user"
                    },
                    "name": "Qianqian Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:55:54.563Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T09:34:11.000Z",
            "submittedOnDailyAt": "2025-03-31T01:03:54.297Z",
            "title": "Segment Any Motion in Videos",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.",
            "upvotes": 10,
            "discussionId": "67e9fec2564b123aa5d70406",
            "ai_keywords": [
                "DINO-based",
                "SAM2",
                "Spatio-Temporal Trajectory Attention",
                "Motion-Semantic Decoupled Embedding"
            ]
        },
        "publishedAt": "2025-03-28T05:34:11.000Z",
        "title": "Segment Any Motion in Videos",
        "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22268.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6536
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.22236",
            "authors": [
                {
                    "_id": "67e9fe132a2d5e305e4e6b80",
                    "name": "Chongjie Ye",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b81",
                    "name": "Yushuang Wu",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b82",
                    "user": {
                        "_id": "6735f447eb4b9c3f36dea354",
                        "avatarUrl": "/avatars/396d007ba4d49ca62e604f3b5c227b42.svg",
                        "isPro": false,
                        "fullname": "",
                        "user": "LUZITENG",
                        "type": "user"
                    },
                    "name": "Ziteng Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T08:31:42.733Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b83",
                    "name": "Jiahao Chang",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b84",
                    "name": "Xiaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b85",
                    "name": "Jiaqing Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b86",
                    "name": "Hao Zhao",
                    "hidden": false
                },
                {
                    "_id": "67e9fe132a2d5e305e4e6b87",
                    "name": "Xiaoguang Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T08:39:20.000Z",
            "submittedOnDailyAt": "2025-03-31T01:00:02.027Z",
            "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.",
            "upvotes": 9,
            "discussionId": "67e9fe172a2d5e305e4e6ce6",
            "ai_keywords": [
                "image-to-normal estimator",
                "dual-stream training",
                "normal-regularized latent diffusion learning",
                "3D data synthesis pipeline",
                "normal maps"
            ]
        },
        "publishedAt": "2025-03-28T04:39:20.000Z",
        "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
        "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22236.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6536
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.22622",
            "authors": [
                {
                    "_id": "67e9fd61b13aed34b44ace5b",
                    "user": {
                        "_id": "67e9fc3797cd6860c81d5838",
                        "avatarUrl": "/avatars/6c37731156bf52c123bd390823890d28.svg",
                        "isPro": false,
                        "fullname": "Jangho Park",
                        "user": "jhpark96",
                        "type": "user"
                    },
                    "name": "Jangho Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:44.942Z",
                    "hidden": false
                },
                {
                    "_id": "67e9fd61b13aed34b44ace5c",
                    "name": "Taesung Kwon",
                    "hidden": false
                },
                {
                    "_id": "67e9fd61b13aed34b44ace5d",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:14:48.000Z",
            "submittedOnDailyAt": "2025-03-31T23:37:58.349Z",
            "title": "Zero4D: Training-Free 4D Video Generation From Single Video Using\n  Off-the-Shelf Video Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "67e9fc3797cd6860c81d5838",
                "avatarUrl": "/avatars/6c37731156bf52c123bd390823890d28.svg",
                "isPro": false,
                "fullname": "Jangho Park",
                "user": "jhpark96",
                "type": "user"
            },
            "summary": "Recently, multi-view or 4D video generation has emerged as a significant\nresearch topic. Nonetheless, recent approaches to 4D generation still struggle\nwith fundamental limitations, as they primarily rely on harnessing multiple\nvideo diffusion models with additional training or compute-intensive training\nof a full 4D diffusion model with limited real-world 4D data and large\ncomputational costs. To address these challenges, here we propose the first\ntraining-free 4D video generation method that leverages the off-the-shelf video\ndiffusion models to generate multi-view videos from a single input video. Our\napproach consists of two key steps: (1) By designating the edge frames in the\nspatio-temporal sampling grid as key frames, we first synthesize them using a\nvideo diffusion model, leveraging a depth-based warping technique for guidance.\nThis approach ensures structural consistency across the generated frames,\npreserving spatial and temporal coherence. (2) We then interpolate the\nremaining frames using a video diffusion model, constructing a fully populated\nand temporally coherent sampling grid while preserving spatial and temporal\nconsistency. Through this approach, we extend a single video into a multi-view\nvideo along novel camera trajectories while maintaining spatio-temporal\nconsistency. Our method is training-free and fully utilizes an off-the-shelf\nvideo diffusion model, offering a practical and effective solution for\nmulti-view video generation.",
            "upvotes": 7,
            "discussionId": "67e9fd64b13aed34b44acf48",
            "projectPage": "https://zero4dvid.github.io/",
            "ai_keywords": [
                "multi-view video generation",
                "4D video generation",
                "video diffusion models",
                "spatio-temporal sampling grid",
                "key frames",
                "depth-based warping technique",
                "structural consistency",
                "spatial coherence",
                "temporal coherence",
                "interpolation",
                "temporally coherent sampling grid",
                "novel camera trajectories",
                "training-free method",
                "off-the-shelf video diffusion model"
            ]
        },
        "publishedAt": "2025-03-28T13:14:48.000Z",
        "title": "Zero4D: Training-Free 4D Video Generation From Single Video Using\n  Off-the-Shelf Video Diffusion Model",
        "summary": "Recently, multi-view or 4D video generation has emerged as a significant\nresearch topic. Nonetheless, recent approaches to 4D generation still struggle\nwith fundamental limitations, as they primarily rely on harnessing multiple\nvideo diffusion models with additional training or compute-intensive training\nof a full 4D diffusion model with limited real-world 4D data and large\ncomputational costs. To address these challenges, here we propose the first\ntraining-free 4D video generation method that leverages the off-the-shelf video\ndiffusion models to generate multi-view videos from a single input video. Our\napproach consists of two key steps: (1) By designating the edge frames in the\nspatio-temporal sampling grid as key frames, we first synthesize them using a\nvideo diffusion model, leveraging a depth-based warping technique for guidance.\nThis approach ensures structural consistency across the generated frames,\npreserving spatial and temporal coherence. (2) We then interpolate the\nremaining frames using a video diffusion model, constructing a fully populated\nand temporally coherent sampling grid while preserving spatial and temporal\nconsistency. Through this approach, we extend a single video into a multi-view\nvideo along novel camera trajectories while maintaining spatio-temporal\nconsistency. Our method is training-free and fully utilizes an off-the-shelf\nvideo diffusion model, offering a practical and effective solution for\nmulti-view video generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22622.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67e9fc3797cd6860c81d5838",
            "avatarUrl": "/avatars/6c37731156bf52c123bd390823890d28.svg",
            "fullname": "Jangho Park",
            "name": "jhpark96",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22329",
            "authors": [
                {
                    "_id": "67ea01e3d13d75fc155fa69d",
                    "user": {
                        "_id": "6071c4b270e11b30cfcfd7a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
                        "isPro": false,
                        "fullname": "Louis Owen",
                        "user": "louisowen6",
                        "type": "user"
                    },
                    "name": "Louis Owen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:39.976Z",
                    "hidden": false
                },
                {
                    "_id": "67ea01e3d13d75fc155fa69e",
                    "user": {
                        "_id": "645a0d3dd6648853107c5fdc",
                        "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
                        "isPro": false,
                        "fullname": "Nilabhra Roy Chowdhury",
                        "user": "nilabhra",
                        "type": "user"
                    },
                    "name": "Nilabhra Roy Chowdhury",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:37.938Z",
                    "hidden": false
                },
                {
                    "_id": "67ea01e3d13d75fc155fa69f",
                    "user": {
                        "_id": "62cd4b03c5cc157be82f0b56",
                        "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
                        "isPro": false,
                        "fullname": "Abhay kumar",
                        "user": "akanyaani",
                        "type": "user"
                    },
                    "name": "Abhay Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:35.757Z",
                    "hidden": false
                },
                {
                    "_id": "67ea01e3d13d75fc155fa6a0",
                    "name": "Fabian Gra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T11:08:34.000Z",
            "submittedOnDailyAt": "2025-03-31T01:17:56.852Z",
            "title": "A Refined Analysis of Massive Activations in LLMs",
            "submittedOnDailyBy": {
                "_id": "6071c4b270e11b30cfcfd7a3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
                "isPro": false,
                "fullname": "Louis Owen",
                "user": "louisowen6",
                "type": "user"
            },
            "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
            "upvotes": 7,
            "discussionId": "67ea01e4d13d75fc155fa6d2",
            "githubRepo": "https://github.com/bluorion-com/refine_massive_activations",
            "ai_keywords": [
                "massive activations",
                "low-precision training",
                "quantization",
                "large language models (LLMs)",
                "GLU-based architectures",
                "Attention KV bias",
                "Target Variance Rescaling (TVR)",
                "Dynamic Tanh (DyT)",
                "perplexity",
                "downstream task performance"
            ]
        },
        "publishedAt": "2025-03-28T07:08:34.000Z",
        "title": "A Refined Analysis of Massive Activations in LLMs",
        "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22329.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "fullname": "Louis Owen",
            "name": "louisowen6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.17827",
            "authors": [
                {
                    "_id": "67e2b7beb408962c5815c52d",
                    "user": {
                        "_id": "658167e4499fbe1b9541adb9",
                        "avatarUrl": "/avatars/0cec32b67c31b2d17b86f5a498400a17.svg",
                        "isPro": false,
                        "fullname": "Wenxuan Zhu",
                        "user": "vxuanz",
                        "type": "user"
                    },
                    "name": "Wenxuan Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-28T14:22:33.321Z",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c52e",
                    "user": {
                        "_id": "666ddb45c0f3d5afc27e85ba",
                        "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
                        "isPro": false,
                        "fullname": "Bing Li",
                        "user": "bing-li-ai",
                        "type": "user"
                    },
                    "name": "Bing Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:15:06.874Z",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c52f",
                    "name": "Cheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c530",
                    "name": "Jinjie Mai",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c531",
                    "name": "Jun Chen",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c532",
                    "user": {
                        "_id": "67ea63f26da1353351989746",
                        "avatarUrl": "/avatars/c7766b43f082bc71623a8fc1a23768ff.svg",
                        "isPro": false,
                        "fullname": "Letian Jiang",
                        "user": "TonNew",
                        "type": "user"
                    },
                    "name": "Letian Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:57:38.643Z",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c533",
                    "user": {
                        "_id": "62fe3442e9061c0170d06e0b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660827186084-62fe3442e9061c0170d06e0b.png",
                        "isPro": false,
                        "fullname": "Abdullah Hamdi",
                        "user": "ajhamdi",
                        "type": "user"
                    },
                    "name": "Abdullah Hamdi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:57:44.416Z",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c534",
                    "name": "Sara Rojas Martinez",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c535",
                    "name": "Chia-Wen Lin",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c536",
                    "user": {
                        "_id": "64a27d39649b0d08ca0a4ca6",
                        "avatarUrl": "/avatars/e4446a875506c10de9ae28411dc6416d.svg",
                        "isPro": false,
                        "fullname": "Mohamed Elhoseiny",
                        "user": "mhelhoseiny",
                        "type": "user"
                    },
                    "name": "Mohamed Elhoseiny",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:58:09.462Z",
                    "hidden": false
                },
                {
                    "_id": "67e2b7beb408962c5815c537",
                    "name": "Bernard Ghanem",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-22T17:55:53.000Z",
            "submittedOnDailyAt": "2025-03-31T07:19:16.539Z",
            "title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "666ddb45c0f3d5afc27e85ba",
                "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
                "isPro": false,
                "fullname": "Bing Li",
                "user": "bing-li-ai",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D\nimage/video understanding capabilities. However, there are no publicly\nstandardized benchmarks to assess the abilities of MLLMs in understanding the\n4D objects (3D objects with temporal evolution over time). In this paper, we\nintroduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs\nin 4D object understanding, featuring tasks in 4D object Question Answering (4D\nobject QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse\ncategories, high-quality annotations, and tasks necessitating multi-view\nspatial-temporal understanding, different from existing 2D image/video-based\nbenchmarks. With 4D-Bench, we evaluate a wide range of open-source and\nclosed-source MLLMs. The results from the 4D object captioning experiment\nindicate that MLLMs generally exhibit weaker temporal understanding compared to\ntheir appearance understanding, notably, while open-source models approach\nclosed-source performance in appearance understanding, they show larger\nperformance gaps in temporal understanding. 4D object QA yields surprising\nfindings: even with simple single-object videos, MLLMs perform poorly, with\nstate-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human\nbaseline of 91\\%. These findings highlight a substantial gap in 4D object\nunderstanding and the need for further advancements in MLLMs.",
            "upvotes": 7,
            "discussionId": "67e2b7c1b408962c5815c671",
            "projectPage": "https://wenxuanzhu1103.github.io/4dbench.github.io/",
            "githubRepo": "https://github.com/WenxuanZhu1103/4D-Bench",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "4D objects",
                "4D-Bench",
                "4D object Question Answering (4D object QA)",
                "4D object captioning",
                "multi-view",
                "spatial-temporal understanding",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-03-22T13:55:53.000Z",
        "title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D\nimage/video understanding capabilities. However, there are no publicly\nstandardized benchmarks to assess the abilities of MLLMs in understanding the\n4D objects (3D objects with temporal evolution over time). In this paper, we\nintroduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs\nin 4D object understanding, featuring tasks in 4D object Question Answering (4D\nobject QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse\ncategories, high-quality annotations, and tasks necessitating multi-view\nspatial-temporal understanding, different from existing 2D image/video-based\nbenchmarks. With 4D-Bench, we evaluate a wide range of open-source and\nclosed-source MLLMs. The results from the 4D object captioning experiment\nindicate that MLLMs generally exhibit weaker temporal understanding compared to\ntheir appearance understanding, notably, while open-source models approach\nclosed-source performance in appearance understanding, they show larger\nperformance gaps in temporal understanding. 4D object QA yields surprising\nfindings: even with simple single-object videos, MLLMs perform poorly, with\nstate-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human\nbaseline of 91\\%. These findings highlight a substantial gap in 4D object\nunderstanding and the need for further advancements in MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17827.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "666ddb45c0f3d5afc27e85ba",
            "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
            "fullname": "Bing Li",
            "name": "bing-li-ai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21732",
            "authors": [
                {
                    "_id": "67e620f77203bed82eb944e9",
                    "user": {
                        "_id": "66744b514f3d4b3327cd228d",
                        "avatarUrl": "/avatars/9768587af7442fbb140f6b3d58100f91.svg",
                        "isPro": false,
                        "fullname": "XianglongHe",
                        "user": "XianglongHe",
                        "type": "user"
                    },
                    "name": "Xianglong He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-28T10:51:50.659Z",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944ea",
                    "user": {
                        "_id": "644dbf6453ad80c6593bf748",
                        "avatarUrl": "/avatars/0e170cf2aa8d7f0f3f83e36f06f023f8.svg",
                        "isPro": false,
                        "fullname": "Zixin Zou",
                        "user": "zouzx",
                        "type": "user"
                    },
                    "name": "Zi-Xin Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-28T10:52:17.598Z",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944eb",
                    "name": "Chia-Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944ec",
                    "user": {
                        "_id": "6346aaa3f06b237ba4e297b0",
                        "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
                        "isPro": false,
                        "fullname": "Yuan-Chen Guo",
                        "user": "bennyguo",
                        "type": "user"
                    },
                    "name": "Yuan-Chen Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-28T10:52:33.753Z",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944ed",
                    "name": "Ding Liang",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944ee",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944ef",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944f0",
                    "user": {
                        "_id": "638066faf022c8a5803f7eb8",
                        "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
                        "isPro": false,
                        "fullname": "Yanpei Cao",
                        "user": "pookiefoof",
                        "type": "user"
                    },
                    "name": "Yan-Pei Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-28T10:52:50.823Z",
                    "hidden": false
                },
                {
                    "_id": "67e620f77203bed82eb944f1",
                    "user": {
                        "_id": "64d71083a787c9bc7b9f1238",
                        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                        "isPro": false,
                        "fullname": "Yangguang Li",
                        "user": "Lp256",
                        "type": "user"
                    },
                    "name": "Yangguang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-28T10:52:59.331Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T17:46:42.000Z",
            "submittedOnDailyAt": "2025-03-31T01:22:51.212Z",
            "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
            "submittedOnDailyBy": {
                "_id": "64d71083a787c9bc7b9f1238",
                "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                "isPro": false,
                "fullname": "Yangguang Li",
                "user": "Lp256",
                "type": "user"
            },
            "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to 1024^3\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.",
            "upvotes": 6,
            "discussionId": "67e620fb7203bed82eb945e8",
            "projectPage": "https://xianglonghe.github.io/TripoSF/index.html",
            "githubRepo": "https://github.com/VAST-AI-Research/TripoSF",
            "ai_keywords": [
                "SparseFlex",
                "isosurface representation",
                "differentiable mesh reconstruction",
                "Flexicubes",
                "sparse voxel structure",
                "frustum-aware",
                "sectional voxel training strategy",
                "memory consumption",
                "variational autoencoder (VAE)",
                "rectified flow transformer",
                "high-quality 3D shape generation",
                "Chamfer Distance",
                "F-score",
                "high-resolution, differentiable mesh reconstruction",
                "3D shape representation",
                "3D shape modeling"
            ]
        },
        "publishedAt": "2025-03-27T13:46:42.000Z",
        "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
        "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to 1024^3\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21732.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "fullname": "Yangguang Li",
            "name": "Lp256",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21751",
            "authors": [
                {
                    "_id": "67e6d76a3394f1ed9c9d804b",
                    "user": {
                        "_id": "66e1103cde9aca0f831f05d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1103cde9aca0f831f05d8/tkdrZgBH9Kyxv_VrGK05R.png",
                        "isPro": false,
                        "fullname": "Yan XIA",
                        "user": "IsshikiHugh",
                        "type": "user"
                    },
                    "name": "Yan Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:14:23.016Z",
                    "hidden": false
                },
                {
                    "_id": "67e6d76a3394f1ed9c9d804c",
                    "name": "Xiaowei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67e6d76a3394f1ed9c9d804d",
                    "name": "Etienne Vouga",
                    "hidden": false
                },
                {
                    "_id": "67e6d76a3394f1ed9c9d804e",
                    "name": "Qixing Huang",
                    "hidden": false
                },
                {
                    "_id": "67e6d76a3394f1ed9c9d804f",
                    "user": {
                        "_id": "6478d3433b7f8b1f6249b469",
                        "avatarUrl": "/avatars/11e7d7a94ae26500c1c2ad62e760726f.svg",
                        "isPro": false,
                        "fullname": "Georgios Pavlakos",
                        "user": "geopavlakos",
                        "type": "user"
                    },
                    "name": "Georgios Pavlakos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:03:56.262Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T17:56:24.000Z",
            "submittedOnDailyAt": "2025-03-31T08:28:45.743Z",
            "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
            "submittedOnDailyBy": {
                "_id": "66e1103cde9aca0f831f05d8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1103cde9aca0f831f05d8/tkdrZgBH9Kyxv_VrGK05R.png",
                "isPro": false,
                "fullname": "Yan XIA",
                "user": "IsshikiHugh",
                "type": "user"
            },
            "summary": "In this paper, we introduce a method for reconstructing 3D humans from a\nsingle image using a biomechanically accurate skeleton model. To achieve this,\nwe train a transformer that takes an image as input and estimates the\nparameters of the model. Due to the lack of training data for this task, we\nbuild a pipeline to produce pseudo ground truth model parameters for single\nimages and implement a training procedure that iteratively refines these pseudo\nlabels. Compared to state-of-the-art methods for 3D human mesh recovery, our\nmodel achieves competitive performance on standard benchmarks, while it\nsignificantly outperforms them in settings with extreme 3D poses and\nviewpoints. Additionally, we show that previous reconstruction methods\nfrequently violate joint angle limits, leading to unnatural rotations. In\ncontrast, our approach leverages the biomechanically plausible degrees of\nfreedom making more realistic joint rotation estimates. We validate our\napproach across multiple human pose estimation benchmarks. We make the code,\nmodels and data available at: https://isshikihugh.github.io/HSMR/",
            "upvotes": 4,
            "discussionId": "67e6d76c3394f1ed9c9d80c4",
            "projectPage": "https://isshikihugh.github.io/HSMR/",
            "githubRepo": "https://github.com/IsshikiHugh/HSMR",
            "ai_keywords": [
                "transformer",
                "biomechanically accurate skeleton model",
                "pseudo ground truth",
                "iterative refinement",
                "state-of-the-art methods",
                "3D human mesh recovery",
                "Standard benchmarks",
                "extreme 3D poses",
                "viewpoints",
                "joint angle limits",
                "biomechanically plausible degrees of freedom",
                "human pose estimation benchmarks"
            ]
        },
        "publishedAt": "2025-03-27T13:56:24.000Z",
        "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
        "summary": "In this paper, we introduce a method for reconstructing 3D humans from a\nsingle image using a biomechanically accurate skeleton model. To achieve this,\nwe train a transformer that takes an image as input and estimates the\nparameters of the model. Due to the lack of training data for this task, we\nbuild a pipeline to produce pseudo ground truth model parameters for single\nimages and implement a training procedure that iteratively refines these pseudo\nlabels. Compared to state-of-the-art methods for 3D human mesh recovery, our\nmodel achieves competitive performance on standard benchmarks, while it\nsignificantly outperforms them in settings with extreme 3D poses and\nviewpoints. Additionally, we show that previous reconstruction methods\nfrequently violate joint angle limits, leading to unnatural rotations. In\ncontrast, our approach leverages the biomechanically plausible degrees of\nfreedom making more realistic joint rotation estimates. We validate our\napproach across multiple human pose estimation benchmarks. We make the code,\nmodels and data available at: https://isshikihugh.github.io/HSMR/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e1103cde9aca0f831f05d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e1103cde9aca0f831f05d8/tkdrZgBH9Kyxv_VrGK05R.png",
            "fullname": "Yan XIA",
            "name": "IsshikiHugh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18968",
            "authors": [
                {
                    "_id": "67ea0ede7b856e8fa8ff50d0",
                    "user": {
                        "_id": "65be4d7d5e342a230dc19a54",
                        "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
                        "isPro": false,
                        "fullname": "Ziyue Wang",
                        "user": "ZiyueWang",
                        "type": "user"
                    },
                    "name": "Ziyue Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:59:13.929Z",
                    "hidden": false
                },
                {
                    "_id": "67ea0ede7b856e8fa8ff50d1",
                    "user": {
                        "_id": "6317257fc92fd6fee317ff7c",
                        "avatarUrl": "/avatars/2f460a2f28562c987becb2acad8d93e7.svg",
                        "isPro": false,
                        "fullname": "Junde Wu",
                        "user": "morson",
                        "type": "user"
                    },
                    "name": "Junde Wu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-03-31T03:41:20.293Z",
                    "hidden": false
                },
                {
                    "_id": "67ea0ede7b856e8fa8ff50d2",
                    "name": "Chang Han Low",
                    "hidden": false
                },
                {
                    "_id": "67ea0ede7b856e8fa8ff50d3",
                    "name": "Yueming Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-21T14:04:18.000Z",
            "submittedOnDailyAt": "2025-03-31T02:13:07.501Z",
            "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
            "submittedOnDailyBy": {
                "_id": "65be4d7d5e342a230dc19a54",
                "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
                "isPro": false,
                "fullname": "Ziyue Wang",
                "user": "ZiyueWang",
                "type": "user"
            },
            "summary": "Developing reliable AI systems to assist human clinicians in multi-modal\nmedical diagnosis has long been a key objective for researchers. Recently,\nMulti-modal Large Language Models (MLLMs) have gained significant attention and\nachieved success across various domains. With strong reasoning capabilities and\nthe ability to perform diverse tasks based on user instructions, they hold\ngreat potential for enhancing medical diagnosis. However, directly applying\nMLLMs to the medical domain still presents challenges. They lack detailed\nperception of visual inputs, limiting their ability to perform quantitative\nimage analysis, which is crucial for medical diagnostics. Additionally, MLLMs\noften exhibit hallucinations and inconsistencies in reasoning, whereas clinical\ndiagnoses must adhere strictly to established criteria. To address these\nchallenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system\ndesigned to achieve reliable, explainable, and precise medical diagnoses. This\nis accomplished through a hierarchical workflow: at the task level,\nknowledge-based reasoning generate reliable diagnostic plans for specific\ndiseases following retrieved clinical criteria. While at the case level,\nmultiple tool agents process multi-modal inputs, analyze different indicators\naccording to the plan, and provide a final diagnosis based on both quantitative\nand qualitative evidence. Comprehensive experiments on both 2D and 3D medical\ndiagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro,\nwhile case studies further highlight its reliability and interpretability. The\ncode is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
            "upvotes": 4,
            "discussionId": "67ea0ee07b856e8fa8ff514f",
            "ai_keywords": [
                "Multi-modal Large Language Models (MLLMs)",
                "knowledge-based reasoning",
                "task level",
                "case level",
                "tool agents",
                "multi-modal inputs",
                "diagnostic plans",
                "quantitative analysis",
                "qualitative evidence",
                "hierarchical workflow",
                "evidence-based reasoning",
                "explainable",
                "precise medical diagnoses",
                "superior",
                "effective",
                "reliability",
                "interpretability",
                "clinical criteria"
            ]
        },
        "publishedAt": "2025-03-21T10:04:18.000Z",
        "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
        "summary": "Developing reliable AI systems to assist human clinicians in multi-modal\nmedical diagnosis has long been a key objective for researchers. Recently,\nMulti-modal Large Language Models (MLLMs) have gained significant attention and\nachieved success across various domains. With strong reasoning capabilities and\nthe ability to perform diverse tasks based on user instructions, they hold\ngreat potential for enhancing medical diagnosis. However, directly applying\nMLLMs to the medical domain still presents challenges. They lack detailed\nperception of visual inputs, limiting their ability to perform quantitative\nimage analysis, which is crucial for medical diagnostics. Additionally, MLLMs\noften exhibit hallucinations and inconsistencies in reasoning, whereas clinical\ndiagnoses must adhere strictly to established criteria. To address these\nchallenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system\ndesigned to achieve reliable, explainable, and precise medical diagnoses. This\nis accomplished through a hierarchical workflow: at the task level,\nknowledge-based reasoning generate reliable diagnostic plans for specific\ndiseases following retrieved clinical criteria. While at the case level,\nmultiple tool agents process multi-modal inputs, analyze different indicators\naccording to the plan, and provide a final diagnosis based on both quantitative\nand qualitative evidence. Comprehensive experiments on both 2D and 3D medical\ndiagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro,\nwhile case studies further highlight its reliability and interpretability. The\ncode is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18968.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65be4d7d5e342a230dc19a54",
            "avatarUrl": "/avatars/04ba16980da94954d811032f0091212f.svg",
            "fullname": "Ziyue Wang",
            "name": "ZiyueWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21851",
            "authors": [
                {
                    "_id": "67ea45e0cdd38e64b1134ec3",
                    "user": {
                        "_id": "633f243c13e836a0fc507388",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
                        "isPro": false,
                        "fullname": "Alessandro Conti",
                        "user": "altndrr",
                        "type": "user"
                    },
                    "name": "Alessandro Conti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T08:11:29.043Z",
                    "hidden": false
                },
                {
                    "_id": "67ea45e0cdd38e64b1134ec4",
                    "user": {
                        "_id": "62cf293d3200bfd438e81f1f",
                        "avatarUrl": "/avatars/608c19ee375ef091ca77d7cfbc40e76e.svg",
                        "isPro": false,
                        "fullname": "Massimiliano Mancini",
                        "user": "massimilianom",
                        "type": "user"
                    },
                    "name": "Massimiliano Mancini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:01:57.391Z",
                    "hidden": false
                },
                {
                    "_id": "67ea45e0cdd38e64b1134ec5",
                    "name": "Enrico Fini",
                    "hidden": false
                },
                {
                    "_id": "67ea45e0cdd38e64b1134ec6",
                    "name": "Yiming Wang",
                    "hidden": false
                },
                {
                    "_id": "67ea45e0cdd38e64b1134ec7",
                    "user": {
                        "_id": "62f3b1ea81861bd9bc5c5538",
                        "avatarUrl": "/avatars/0aef9ac5bfa91b9894166fe3c29925da.svg",
                        "isPro": false,
                        "fullname": "Paolo Rota",
                        "user": "paolorota",
                        "type": "user"
                    },
                    "name": "Paolo Rota",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:02:22.544Z",
                    "hidden": false
                },
                {
                    "_id": "67ea45e0cdd38e64b1134ec8",
                    "name": "Elisa Ricci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T17:03:18.000Z",
            "submittedOnDailyAt": "2025-03-31T06:13:51.445Z",
            "title": "On Large Multimodal Models as Open-World Image Classifiers",
            "submittedOnDailyBy": {
                "_id": "633f243c13e836a0fc507388",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
                "isPro": false,
                "fullname": "Alessandro Conti",
                "user": "altndrr",
                "type": "user"
            },
            "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.",
            "upvotes": 3,
            "discussionId": "67ea45e1cdd38e64b1134f35",
            "githubRepo": "https://github.com/altndrr/lmms-owc",
            "ai_keywords": [
                "Large Multimodal Models (LMMs)",
                "open-world setting",
                "evaluation protocol",
                "metrics",
                "alignment between predicted and ground truth classes",
                "prototypical",
                "non-prototypical",
                "fine-grained",
                "very fine-grained classes",
                "granularity",
                "fine-grained capabilities",
                "tailored prompting",
                "reasoning"
            ]
        },
        "publishedAt": "2025-03-27T13:03:18.000Z",
        "title": "On Large Multimodal Models as Open-World Image Classifiers",
        "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21851.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633f243c13e836a0fc507388",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f243c13e836a0fc507388/Td8TWn1q2L78sZN9098AO.jpeg",
            "fullname": "Alessandro Conti",
            "name": "altndrr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21779",
            "authors": [
                {
                    "_id": "67e69b75113f7c9e552bea69",
                    "user": {
                        "_id": "660b9dfc8b022f13fdc8db83",
                        "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
                        "isPro": false,
                        "fullname": "vortexyu",
                        "user": "vortex778",
                        "type": "user"
                    },
                    "name": "Weihao Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-28T14:22:23.190Z",
                    "hidden": false
                },
                {
                    "_id": "67e69b75113f7c9e552bea6a",
                    "user": {
                        "_id": "673969726c12c4b98b6ab29f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C2elfn7L68jAt4dtHzDAW.png",
                        "isPro": false,
                        "fullname": "Yuanhao Cai",
                        "user": "CaiYuanhao",
                        "type": "user"
                    },
                    "name": "Yuanhao Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:59:39.476Z",
                    "hidden": false
                },
                {
                    "_id": "67e69b75113f7c9e552bea6b",
                    "name": "Ruyi Zha",
                    "hidden": false
                },
                {
                    "_id": "67e69b75113f7c9e552bea6c",
                    "user": {
                        "_id": "6526386e1c6a09292d8d0a22",
                        "avatarUrl": "/avatars/471de830de2d775d35368678c1579f87.svg",
                        "isPro": false,
                        "fullname": "fan",
                        "user": "Fanzhiwen",
                        "type": "user"
                    },
                    "name": "Zhiwen Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T09:59:58.337Z",
                    "hidden": false
                },
                {
                    "_id": "67e69b75113f7c9e552bea6d",
                    "user": {
                        "_id": "6421c1cdeaad1bcb28b0e903",
                        "avatarUrl": "/avatars/7c720d0e39536a7e49340052f464a80d.svg",
                        "isPro": false,
                        "fullname": "Chenxin Li",
                        "user": "XGGNet",
                        "type": "user"
                    },
                    "name": "Chenxin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:00:05.746Z",
                    "hidden": false
                },
                {
                    "_id": "67e69b75113f7c9e552bea6e",
                    "user": {
                        "_id": "640fdfc9f2d7c41a1ea112ef",
                        "avatarUrl": "/avatars/780328c388ac4bc9acdf063e7833259d.svg",
                        "isPro": false,
                        "fullname": "yxyuan",
                        "user": "yixuanyuan",
                        "type": "user"
                    },
                    "name": "Yixuan Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-03-31T10:00:15.694Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/660b9dfc8b022f13fdc8db83/Em05evtcueTrruwgHTLlo.gif"
            ],
            "publishedAt": "2025-03-27T17:59:57.000Z",
            "submittedOnDailyAt": "2025-03-31T05:01:34.606Z",
            "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
            "submittedOnDailyBy": {
                "_id": "660b9dfc8b022f13fdc8db83",
                "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
                "isPro": false,
                "fullname": "vortexyu",
                "user": "vortex778",
                "type": "user"
            },
            "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
            "upvotes": 2,
            "discussionId": "67e69b76113f7c9e552beaa1",
            "projectPage": "https://x2-gaussian.github.io/",
            "githubRepo": "https://github.com/yuyouxixi/x2-gaussian",
            "ai_keywords": [
                "X$^2$-Gaussian",
                "continuous-time 4D-CT",
                "dynamic radiative Gaussian splatting",
                "self-supervised respiratory motion learning",
                "spatiotemporal encoder-decoder architecture",
                "time-varying Gaussian deformations",
                "physiology-driven periodic consistency loss",
                "differentiable optimization",
                "PSNR gain",
                "hardware-free period learning",
                "high-fidelity 4D CT reconstruction"
            ]
        },
        "publishedAt": "2025-03-27T13:59:57.000Z",
        "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
        "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/660b9dfc8b022f13fdc8db83/Em05evtcueTrruwgHTLlo.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21779.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "660b9dfc8b022f13fdc8db83",
            "avatarUrl": "/avatars/62c68259c8a4d53f121c41a2831cb89a.svg",
            "fullname": "vortexyu",
            "name": "vortex778",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21544",
            "authors": [
                {
                    "_id": "67e6069f19f7dbaac899dc4f",
                    "user": {
                        "_id": "64510a21f800611f94f0d9f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
                        "isPro": false,
                        "fullname": "Yuwei Yin",
                        "user": "yuweiyin",
                        "type": "user"
                    },
                    "name": "Yuwei Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-28T08:37:44.456Z",
                    "hidden": false
                },
                {
                    "_id": "67e6069f19f7dbaac899dc50",
                    "name": "EunJeong Hwang",
                    "hidden": false
                },
                {
                    "_id": "67e6069f19f7dbaac899dc51",
                    "name": "Giuseppe Carenini",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T14:34:28.000Z",
            "submittedOnDailyAt": "2025-03-31T14:32:17.285Z",
            "title": "SWI: Speaking with Intent in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64510a21f800611f94f0d9f8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
                "isPro": false,
                "fullname": "Yuwei Yin",
                "user": "yuweiyin",
                "type": "user"
            },
            "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
            "upvotes": 2,
            "discussionId": "67e606a019f7dbaac899dc90",
            "projectPage": "https://www.yuweiyin.com/publication/2025-03-27-swi/",
            "githubRepo": "https://github.com/YuweiYin/SWI",
            "ai_keywords": [
                "Speaking with Intent (SWI)",
                "large language models (LLMs)",
                "deliberate and purposeful thoughts",
                "reason-intensive question answering (QA)"
            ]
        },
        "publishedAt": "2025-03-27T10:34:28.000Z",
        "title": "SWI: Speaking with Intent in Large Language Models",
        "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21544.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64510a21f800611f94f0d9f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
            "fullname": "Yuwei Yin",
            "name": "yuweiyin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22625",
            "authors": [
                {
                    "_id": "67eb322f3d9d1396663d33ef",
                    "user": {
                        "_id": "6179abf8ec6ce4dc2e5f2376",
                        "avatarUrl": "/avatars/d10c6a1b350146b36949a24220471295.svg",
                        "isPro": false,
                        "fullname": "Alex Gu",
                        "user": "minimario",
                        "type": "user"
                    },
                    "name": "Alex Gu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-01T00:24:16.159Z",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f0",
                    "name": "Naman Jain",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f1",
                    "name": "Wen-Ding Li",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f2",
                    "user": {
                        "_id": "637bdc36e073a16fb7886923",
                        "avatarUrl": "/avatars/5f26408ef5f848091b25397defa1107a.svg",
                        "isPro": false,
                        "fullname": "Manish Shetty",
                        "user": "manishs",
                        "type": "user"
                    },
                    "name": "Manish Shetty",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-01T00:25:05.638Z",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f3",
                    "name": "Yijia Shao",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f4",
                    "name": "Ziyang Li",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f5",
                    "name": "Diyi Yang",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f6",
                    "name": "Kevin Ellis",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f7",
                    "name": "Koushik Sen",
                    "hidden": false
                },
                {
                    "_id": "67eb322f3d9d1396663d33f8",
                    "name": "Armando Solar-Lezama",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:17:57.000Z",
            "submittedOnDailyAt": "2025-03-31T23:17:28.461Z",
            "title": "Challenges and Paths Towards AI for Software Engineering",
            "submittedOnDailyBy": {
                "_id": "6179abf8ec6ce4dc2e5f2376",
                "avatarUrl": "/avatars/d10c6a1b350146b36949a24220471295.svg",
                "isPro": false,
                "fullname": "Alex Gu",
                "user": "minimario",
                "type": "user"
            },
            "summary": "AI for software engineering has made remarkable progress recently, becoming a\nnotable success within generative AI. Despite this, there are still many\nchallenges that need to be addressed before automated software engineering\nreaches its full potential. It should be possible to reach high levels of\nautomation where humans can focus on the critical decisions of what to build\nand how to balance difficult tradeoffs while most routine development effort is\nautomated away. Reaching this level of automation will require substantial\nresearch and engineering efforts across academia and industry. In this paper,\nwe aim to discuss progress towards this in a threefold manner. First, we\nprovide a structured taxonomy of concrete tasks in AI for software engineering,\nemphasizing the many other tasks in software engineering beyond code generation\nand completion. Second, we outline several key bottlenecks that limit current\napproaches. Finally, we provide an opinionated list of promising research\ndirections toward making progress on these bottlenecks, hoping to inspire\nfuture research in this rapidly maturing field.",
            "upvotes": 1,
            "discussionId": "67eb32303d9d1396663d3425"
        },
        "publishedAt": "2025-03-28T13:17:57.000Z",
        "title": "Challenges and Paths Towards AI for Software Engineering",
        "summary": "AI for software engineering has made remarkable progress recently, becoming a\nnotable success within generative AI. Despite this, there are still many\nchallenges that need to be addressed before automated software engineering\nreaches its full potential. It should be possible to reach high levels of\nautomation where humans can focus on the critical decisions of what to build\nand how to balance difficult tradeoffs while most routine development effort is\nautomated away. Reaching this level of automation will require substantial\nresearch and engineering efforts across academia and industry. In this paper,\nwe aim to discuss progress towards this in a threefold manner. First, we\nprovide a structured taxonomy of concrete tasks in AI for software engineering,\nemphasizing the many other tasks in software engineering beyond code generation\nand completion. Second, we outline several key bottlenecks that limit current\napproaches. Finally, we provide an opinionated list of promising research\ndirections toward making progress on these bottlenecks, hoping to inspire\nfuture research in this rapidly maturing field.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22625.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6179abf8ec6ce4dc2e5f2376",
            "avatarUrl": "/avatars/d10c6a1b350146b36949a24220471295.svg",
            "fullname": "Alex Gu",
            "name": "minimario",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    }
]