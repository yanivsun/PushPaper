[
    {
        "paper": {
            "id": "2503.23461",
            "authors": [
                {
                    "_id": "67eb594988a08fae617242f1",
                    "name": "Nikai Du",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f2",
                    "user": {
                        "_id": "66449e619ff401732687f013",
                        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "zhen-nan",
                        "type": "user"
                    },
                    "name": "Zhennan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:46.364Z",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f3",
                    "user": {
                        "_id": "637c22183d8e2e9c40c09fcf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669079538761-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zhizhou Chen",
                        "user": "Chenzzzzzz",
                        "type": "user"
                    },
                    "name": "Zhizhou Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:57:44.605Z",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f4",
                    "name": "Shan Gao",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f5",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f6",
                    "user": {
                        "_id": "67593dd0f522f4409e614ba0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
                        "isPro": false,
                        "fullname": "Jiang Zhengkai",
                        "user": "jzzzzk",
                        "type": "user"
                    },
                    "name": "Zhengkai Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:58:12.614Z",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f7",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "67eb594988a08fae617242f8",
                    "user": {
                        "_id": "65734004769f3ee9bde1af10",
                        "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
                        "isPro": false,
                        "fullname": "Ying Tai",
                        "user": "yingtai",
                        "type": "user"
                    },
                    "name": "Ying Tai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:44.350Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T14:36:55.000Z",
            "submittedOnDailyAt": "2025-04-01T01:44:26.275Z",
            "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
            "submittedOnDailyBy": {
                "_id": "66449e619ff401732687f013",
                "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
                "isPro": false,
                "fullname": "chen",
                "user": "zhen-nan",
                "type": "user"
            },
            "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
            "upvotes": 60,
            "discussionId": "67eb594b88a08fae617243ac",
            "projectPage": "https://dnknju.github.io/textcrafter-vue/",
            "githubRepo": "https://github.com/NJU-PCALab/TextCrafter",
            "ai_keywords": [
                "complex visual text",
                "TextCrafter",
                "multi-visual text rendering",
                "progressive strategy",
                "token focus enhancement",
                "CVTG-2K",
                "generative models",
                "CVTG tasks",
                "state-of-the-art approaches"
            ]
        },
        "publishedAt": "2025-03-30T10:36:55.000Z",
        "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
        "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23461.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66449e619ff401732687f013",
            "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
            "fullname": "chen",
            "name": "zhen-nan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23307",
            "authors": [
                {
                    "_id": "67eb4bd0eca57c4eebbb343a",
                    "user": {
                        "_id": "64f8e358766ff9f3d2b0de84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
                        "isPro": true,
                        "fullname": "Cong Wei",
                        "user": "lim142857",
                        "type": "user"
                    },
                    "name": "Cong Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:47:21.554Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb343b",
                    "name": "Bo Sun",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb343c",
                    "user": {
                        "_id": "650a8979c19e5b4c8a6ff062",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg",
                        "isPro": false,
                        "fullname": "Haoyu Ma",
                        "user": "haoyum1997",
                        "type": "user"
                    },
                    "name": "Haoyu Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:54:47.847Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb343d",
                    "name": "Ji Hou",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb343e",
                    "user": {
                        "_id": "6444e8911cfc9ae6bb3ad216",
                        "avatarUrl": "/avatars/8c06e064cf24789e4131f7af06dac86b.svg",
                        "isPro": false,
                        "fullname": "Xu",
                        "user": "FelixXu",
                        "type": "user"
                    },
                    "name": "Felix Juefei-Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:55:03.326Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb343f",
                    "name": "Zecheng He",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3440",
                    "user": {
                        "_id": "6549417b3ce45eb764faf993",
                        "avatarUrl": "/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg",
                        "isPro": false,
                        "fullname": "Xiaoliang Dai",
                        "user": "daixl1992",
                        "type": "user"
                    },
                    "name": "Xiaoliang Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:54:01.490Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3441",
                    "user": {
                        "_id": "65a4fa7d2548c41ad9d9b710",
                        "avatarUrl": "/avatars/3cace2d2f11f7194d8eca4b95b0b57cc.svg",
                        "isPro": false,
                        "fullname": "Luxin Zhang",
                        "user": "Luczzz",
                        "type": "user"
                    },
                    "name": "Luxin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:53:55.152Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3442",
                    "name": "Kunpeng Li",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3443",
                    "user": {
                        "_id": "655846d7ed8df83128f5826a",
                        "avatarUrl": "/avatars/d7ce174d7d1b8614d5f6f071225c0057.svg",
                        "isPro": false,
                        "fullname": "Hou",
                        "user": "Tingbo",
                        "type": "user"
                    },
                    "name": "Tingbo Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:53:10.214Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3444",
                    "name": "Animesh Sinha",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3445",
                    "name": "Peter Vajda",
                    "hidden": false
                },
                {
                    "_id": "67eb4bd0eca57c4eebbb3446",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:52:50.248Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T04:22:09.000Z",
            "submittedOnDailyAt": "2025-04-01T00:46:45.446Z",
            "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
            "submittedOnDailyBy": {
                "_id": "64f8e358766ff9f3d2b0de84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
                "isPro": true,
                "fullname": "Cong Wei",
                "user": "lim142857",
                "type": "user"
            },
            "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
            "upvotes": 46,
            "discussionId": "67eb4bd3eca57c4eebbb34c7",
            "projectPage": "https://congwei1230.github.io/MoCha/",
            "ai_keywords": [
                "speech-video window attention mechanism",
                "speech-labeled video datasets",
                "text-labeled video data",
                "structured prompt templates",
                "character tags",
                "multi-character conversation",
                "turn-based dialogue",
                "context-aware conversations",
                "cinematic coherence"
            ]
        },
        "publishedAt": "2025-03-30T00:22:09.000Z",
        "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
        "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23307.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
            "fullname": "Cong Wei",
            "name": "lim142857",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24235",
            "authors": [
                {
                    "_id": "67eb57023475e7b135788500",
                    "user": {
                        "_id": "62a42f22c683d02f5b63320c",
                        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
                        "isPro": false,
                        "fullname": "Qiyuan Zhang",
                        "user": "DonJoey",
                        "type": "user"
                    },
                    "name": "Qiyuan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:57:25.339Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788501",
                    "user": {
                        "_id": "65d2bb5c6130ef7be012d235",
                        "avatarUrl": "/avatars/1c1e3bbb2c683a5c9d1f792a2c13fc4a.svg",
                        "isPro": false,
                        "fullname": "Fuyuan Lyu",
                        "user": "silentspring2",
                        "type": "user"
                    },
                    "name": "Fuyuan Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:56:19.295Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788502",
                    "user": {
                        "_id": "65d1b42f3da87ce21e33261a",
                        "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
                        "isPro": false,
                        "fullname": "RubinSun",
                        "user": "RubinSun",
                        "type": "user"
                    },
                    "name": "Zexu Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:51.229Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788503",
                    "user": {
                        "_id": "646def60df618b303b419323",
                        "avatarUrl": "/avatars/97aa761d5255abf230304cfeade87835.svg",
                        "isPro": false,
                        "fullname": "Lei Wang",
                        "user": "demolei",
                        "type": "user"
                    },
                    "name": "Lei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T08:01:21.676Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788504",
                    "user": {
                        "_id": "63414659c5565a4b8d41bc42",
                        "avatarUrl": "/avatars/25b4ca3002edf4c35cded0902c26632a.svg",
                        "isPro": false,
                        "fullname": "Weixu Zhang",
                        "user": "nancy-zwx",
                        "type": "user"
                    },
                    "name": "Weixu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:56:26.532Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788505",
                    "user": {
                        "_id": "63c0c2497f52541dfc7d7567",
                        "avatarUrl": "/avatars/16c174e2803ef86d09815b36a666ee0e.svg",
                        "isPro": false,
                        "fullname": "ZhihanGUO",
                        "user": "ZhihanGUO",
                        "type": "user"
                    },
                    "name": "Zhihan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:56:31.979Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788506",
                    "name": "Yufei Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788507",
                    "name": "Irwin King",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788508",
                    "name": "Xue Liu",
                    "hidden": false
                },
                {
                    "_id": "67eb57023475e7b135788509",
                    "name": "Chen Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T15:46:15.000Z",
            "submittedOnDailyAt": "2025-04-01T01:37:27.268Z",
            "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "62a42f22c683d02f5b63320c",
                "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
                "isPro": false,
                "fullname": "Qiyuan Zhang",
                "user": "DonJoey",
                "type": "user"
            },
            "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
            "upvotes": 36,
            "discussionId": "67eb57053475e7b135788624",
            "ai_keywords": [
                "test-time scaling",
                "test-time computing",
                "large language models",
                "specialized reasoning tasks",
                "open-ended Q&A",
                "multidimensional framework",
                "what to scale",
                "how to scale",
                "where to scale",
                "how well to scale",
                "assessment aspects",
                "functional roles",
                "developmental trajectories",
                "practical deployment",
                "open challenges",
                "attributions"
            ]
        },
        "publishedAt": "2025-03-31T11:46:15.000Z",
        "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
        "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24235.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "fullname": "Qiyuan Zhang",
            "name": "DonJoey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24290",
            "authors": [
                {
                    "_id": "67eb762381e530baa56dc830",
                    "user": {
                        "_id": "625026b7d2d191ac43320c5e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg",
                        "isPro": false,
                        "fullname": "Jingcheng Hu",
                        "user": "reign12",
                        "type": "user"
                    },
                    "name": "Jingcheng Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:09:02.123Z",
                    "hidden": false
                },
                {
                    "_id": "67eb762381e530baa56dc831",
                    "user": {
                        "_id": "664ae39ab5e5f95dc6209365",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg",
                        "isPro": false,
                        "fullname": "Yinmin Zhang",
                        "user": "YinminZhang",
                        "type": "user"
                    },
                    "name": "Yinmin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:09:09.884Z",
                    "hidden": false
                },
                {
                    "_id": "67eb762381e530baa56dc832",
                    "name": "Qi Han",
                    "hidden": false
                },
                {
                    "_id": "67eb762381e530baa56dc833",
                    "user": {
                        "_id": "60d4440fe648443279aaffd8",
                        "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
                        "isPro": false,
                        "fullname": "Daxin Jiang",
                        "user": "djiang",
                        "type": "user"
                    },
                    "name": "Daxin Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:09:22.914Z",
                    "hidden": false
                },
                {
                    "_id": "67eb762381e530baa56dc834",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67eb762381e530baa56dc835",
                    "name": "Heung-Yeung Shum",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T16:36:05.000Z",
            "submittedOnDailyAt": "2025-04-01T03:44:53.609Z",
            "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
            "upvotes": 35,
            "discussionId": "67eb762481e530baa56dc872",
            "projectPage": "https://huggingface.co/Open-Reasoner-Zero",
            "githubRepo": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "vanilla PPO",
                "GAE ($\\lambda=1$, $\\gamma=1$)",
                "rule-based rewards",
                "KL regularization",
                "response length",
                "benchmark performance",
                "AIME2024",
                "MATH500",
                "GPQA Diamond benchmark",
                "training steps"
            ]
        },
        "publishedAt": "2025-03-31T12:36:05.000Z",
        "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
        "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24290.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6560
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.24388",
            "authors": [
                {
                    "_id": "67eb544113ca8dcb9ccb991b",
                    "name": "Zhonghan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67eb544113ca8dcb9ccb991c",
                    "user": {
                        "_id": "64e8505321540e1da3226b54",
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:59:04.743Z",
                    "hidden": false
                },
                {
                    "_id": "67eb544113ca8dcb9ccb991d",
                    "name": "Haian Huang",
                    "hidden": false
                },
                {
                    "_id": "67eb544113ca8dcb9ccb991e",
                    "name": "Kuikun Liu",
                    "hidden": false
                },
                {
                    "_id": "67eb544113ca8dcb9ccb991f",
                    "user": {
                        "_id": "64070c5c4dc5f2846c925e93",
                        "avatarUrl": "/avatars/ac2d7c1cd4ecccd6a88b85767c963ec7.svg",
                        "isPro": false,
                        "fullname": "Gao Jianfei",
                        "user": "pppppM",
                        "type": "user"
                    },
                    "name": "Jianfei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:59:34.182Z",
                    "hidden": false
                },
                {
                    "_id": "67eb544113ca8dcb9ccb9920",
                    "user": {
                        "_id": "64c9033c5381684d3eaac7f1",
                        "avatarUrl": "/avatars/07d36ca193826044b0df04e3602b9ef8.svg",
                        "isPro": false,
                        "fullname": "Gaoang Wang",
                        "user": "GaoangWang",
                        "type": "user"
                    },
                    "name": "Gaoang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T07:59:40.156Z",
                    "hidden": false
                },
                {
                    "_id": "67eb544113ca8dcb9ccb9921",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:59:52.000Z",
            "submittedOnDailyAt": "2025-04-01T01:27:12.837Z",
            "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
            "submittedOnDailyBy": {
                "_id": "6601196cc91ba4c08ad6e270",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
                "isPro": false,
                "fullname": "yuzhe gu",
                "user": "vanilla1116",
                "type": "user"
            },
            "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
            "upvotes": 22,
            "discussionId": "67eb544213ca8dcb9ccb9963"
        },
        "publishedAt": "2025-03-31T13:59:52.000Z",
        "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
        "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24388.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "fullname": "yuzhe gu",
            "name": "vanilla1116",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.24370",
            "authors": [
                {
                    "_id": "67eb4fff13ca8dcb9cca5f9b",
                    "user": {
                        "_id": "62fae9328e137d7c4b896498",
                        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
                        "isPro": false,
                        "fullname": "Tong Wu",
                        "user": "tongwu2020",
                        "type": "user"
                    },
                    "name": "Tong Wu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-01T02:31:28.689Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4fff13ca8dcb9cca5f9c",
                    "user": {
                        "_id": "653319c135ad8b9e58a6b874",
                        "avatarUrl": "/avatars/755efb5829f3b6d3cef886fee26e1ba9.svg",
                        "isPro": false,
                        "fullname": "Chong Xiang",
                        "user": "cxiang",
                        "type": "user"
                    },
                    "name": "Chong Xiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:00:19.547Z",
                    "hidden": false
                },
                {
                    "_id": "67eb4fff13ca8dcb9cca5f9d",
                    "name": "Jiachen T. Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb4fff13ca8dcb9cca5f9e",
                    "name": "Prateek Mittal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:50:13.000Z",
            "submittedOnDailyAt": "2025-04-01T01:02:34.304Z",
            "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
            "submittedOnDailyBy": {
                "_id": "62fae9328e137d7c4b896498",
                "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
                "isPro": false,
                "fullname": "Tong Wu",
                "user": "tongwu2020",
                "type": "user"
            },
            "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
            "upvotes": 14,
            "discussionId": "67eb500013ca8dcb9cca5fe0",
            "ai_keywords": [
                "Reasoning-enhanced large language models (LLMs)",
                "intermediate reasoning steps",
                "Thinking Intervention",
                "thinking tokens",
                "instruction following",
                "IFEval",
                "instruction hierarchy",
                "SEP",
                "safety alignment",
                "XSTest",
                "SORRY-Bench",
                "open-source DeepSeek R1 models"
            ]
        },
        "publishedAt": "2025-03-31T13:50:13.000Z",
        "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
        "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24370.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fae9328e137d7c4b896498",
            "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
            "fullname": "Tong Wu",
            "name": "tongwu2020",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23284",
            "authors": [
                {
                    "_id": "67eb5280aeab4ce97de07134",
                    "user": {
                        "_id": "6424538b9f9e65b42389920e",
                        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
                        "isPro": false,
                        "fullname": "Feng-Lin Liu",
                        "user": "Okrin",
                        "type": "user"
                    },
                    "name": "Feng-Lin Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:47:05.907Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5280aeab4ce97de07135",
                    "user": {
                        "_id": "662cd8b9322afcbae53fb06e",
                        "avatarUrl": "/avatars/9847f5c2282d49e61e76a0a303e0b2b1.svg",
                        "isPro": false,
                        "fullname": "fuhongbo",
                        "user": "fuhongbo",
                        "type": "user"
                    },
                    "name": "Hongbo Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:07:01.616Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5280aeab4ce97de07136",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:07:09.910Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5280aeab4ce97de07137",
                    "user": {
                        "_id": "6360d9f0472131c3bc4f61df",
                        "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
                        "isPro": false,
                        "fullname": "WeicaiYe",
                        "user": "WeicaiYe",
                        "type": "user"
                    },
                    "name": "Weicai Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:07:16.323Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5280aeab4ce97de07138",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67eb5280aeab4ce97de07139",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:07:30.236Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5280aeab4ce97de0713a",
                    "name": "Lin Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T02:44:09.000Z",
            "submittedOnDailyAt": "2025-04-01T02:19:10.110Z",
            "title": "SketchVideo: Sketch-based Video Generation and Editing",
            "submittedOnDailyBy": {
                "_id": "6424538b9f9e65b42389920e",
                "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
                "isPro": false,
                "fullname": "Feng-Lin Liu",
                "user": "Okrin",
                "type": "user"
            },
            "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
            "upvotes": 14,
            "discussionId": "67eb5286aeab4ce97de07320",
            "githubRepo": "https://github.com/IGLICT/SketchVideo",
            "ai_keywords": [
                "DiT video generation model",
                "memory-efficient control structure",
                "sketch control blocks",
                "residual features",
                "skipped DiT blocks",
                "temporally sparse sketch conditions",
                "inter-frame attention mechanism",
                "keyframes",
                "video insertion module",
                "spatial feature",
                "dynamic motion",
                "latent fusion",
                "SketchVideo"
            ]
        },
        "publishedAt": "2025-03-29T22:44:09.000Z",
        "title": "SketchVideo: Sketch-based Video Generation and Editing",
        "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23284.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "fullname": "Feng-Lin Liu",
            "name": "Okrin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23077",
            "authors": [
                {
                    "_id": "67eb58c71e23a7499b683cce",
                    "user": {
                        "_id": "6650c77a74664a42ddfb9187",
                        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                        "isPro": false,
                        "fullname": "yueliu1999",
                        "user": "yueliu1999",
                        "type": "user"
                    },
                    "name": "Yue Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:11:01.812Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683ccf",
                    "name": "Jiaying Wu",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd0",
                    "name": "Yufei He",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd1",
                    "user": {
                        "_id": "62728f4f6253fe2068da1021",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                        "isPro": false,
                        "fullname": "Hongcheng Gao",
                        "user": "HongchengGao",
                        "type": "user"
                    },
                    "name": "Hongcheng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:11:54.944Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd2",
                    "user": {
                        "_id": "67c7a9fb27c2e81cf8660375",
                        "avatarUrl": "/avatars/a5129cca93a31d4b730af4c543051d8e.svg",
                        "isPro": false,
                        "fullname": "Hongyu Chen",
                        "user": "HongyuChen",
                        "type": "user"
                    },
                    "name": "Hongyu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:12:05.159Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd3",
                    "user": {
                        "_id": "642577e06d0f0f5f1dc68904",
                        "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
                        "isPro": false,
                        "fullname": "Bibaolong",
                        "user": "Bibaolong",
                        "type": "user"
                    },
                    "name": "Baolong Bi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:12:18.251Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd4",
                    "user": {
                        "_id": "669e19e5dac1eb34c0f5f505",
                        "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Zhang",
                        "user": "jiaheng233",
                        "type": "user"
                    },
                    "name": "Jiaheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:12:23.776Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd5",
                    "user": {
                        "_id": "66221f1a90f3fd333c4ec52e",
                        "avatarUrl": "/avatars/a3173d9603a69020ec24170831c97c2f.svg",
                        "isPro": false,
                        "fullname": "Zhiqi Huang",
                        "user": "Angelalilyer",
                        "type": "user"
                    },
                    "name": "Zhiqi Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:12:36.398Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58c71e23a7499b683cd6",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:12:42.880Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T13:27:46.000Z",
            "submittedOnDailyAt": "2025-04-01T01:39:12.154Z",
            "title": "Efficient Inference for Large Reasoning Models: A Survey",
            "submittedOnDailyBy": {
                "_id": "6650c77a74664a42ddfb9187",
                "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                "isPro": false,
                "fullname": "yueliu1999",
                "user": "yueliu1999",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
            "upvotes": 14,
            "discussionId": "67eb58c81e23a7499b683d12",
            "ai_keywords": [
                "Chain-of-Thought (CoT)",
                "explicit compact Chain-of-Thought (CoT)",
                "implicit latent CoT",
                "model merging",
                "agent routers"
            ]
        },
        "publishedAt": "2025-03-29T09:27:46.000Z",
        "title": "Efficient Inference for Large Reasoning Models: A Survey",
        "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23077.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "fullname": "yueliu1999",
            "name": "yueliu1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23829",
            "authors": [
                {
                    "_id": "67eb759cb9fa8908e1934f21",
                    "name": "Yi Su",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f22",
                    "user": {
                        "_id": "62d58fd53bf5e059f7cc3245",
                        "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
                        "isPro": false,
                        "fullname": "Dian Yu",
                        "user": "yudian",
                        "type": "user"
                    },
                    "name": "Dian Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:13:47.119Z",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f23",
                    "user": {
                        "_id": "64c94eddcb2f1bf0e7db5a4d",
                        "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
                        "isPro": false,
                        "fullname": "Linfeng Song",
                        "user": "freesunshine0316",
                        "type": "user"
                    },
                    "name": "Linfeng Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:13:54.065Z",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f24",
                    "user": {
                        "_id": "6670e285b0c03c4e9d6e0985",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
                        "isPro": false,
                        "fullname": "Juntao Li",
                        "user": "douvleplus",
                        "type": "user"
                    },
                    "name": "Juntao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:14:03.120Z",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f25",
                    "user": {
                        "_id": "65147a1426fbd558dbd08f1b",
                        "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
                        "isPro": false,
                        "fullname": "Haitao Mi",
                        "user": "haitaominlp",
                        "type": "user"
                    },
                    "name": "Haitao Mi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:14:10.594Z",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f26",
                    "user": {
                        "_id": "67485743561b1e6f9579389f",
                        "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
                        "isPro": false,
                        "fullname": "Zhaopeng Tu",
                        "user": "zptu",
                        "type": "user"
                    },
                    "name": "Zhaopeng Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:14:16.978Z",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f27",
                    "name": "Min Zhang",
                    "hidden": false
                },
                {
                    "_id": "67eb759cb9fa8908e1934f28",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T08:22:49.000Z",
            "submittedOnDailyAt": "2025-04-01T03:42:19.595Z",
            "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
            "upvotes": 13,
            "discussionId": "67eb759db9fa8908e1934f62",
            "projectPage": "https://huggingface.co/collections/virtuoussy/rlvr-67ea349b086e3511f86d1c1f",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "verifiable rewards (RLVR)",
                "mathematical reasoning",
                "coding tasks",
                "well-structured reference answers",
                "diverse domains",
                "medicine",
                "chemistry",
                "psychology",
                "economics",
                "large language models (LLMs)",
                "binary judgments",
                "domain-specific reward models",
                "model-based soft scoring",
                "distilled generative reward model",
                "effective cross-domain verifier",
                "reward signals",
                "fine-tuning",
                "base 7B model",
                "RL algorithms",
                "state-of-the-art open-source aligned LLMs",
                "Qwen2.5-72B-Instruct",
                "DeepSeek-R1-Distill-Qwen-32B",
                "free-form answer settings",
                "robustness",
                "scalability",
                "real-world applications",
                "noisy labels",
                "weak labels"
            ]
        },
        "publishedAt": "2025-03-31T04:22:49.000Z",
        "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
        "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23829.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6560
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19901",
            "authors": [
                {
                    "_id": "67eac6433755a17e3cbff585",
                    "user": {
                        "_id": "6630cc7e9ee8861dd0b9bdbd",
                        "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
                        "isPro": false,
                        "fullname": "Liang Pan",
                        "user": "lianganimation",
                        "type": "user"
                    },
                    "name": "Liang Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:47:34.754Z",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff586",
                    "user": {
                        "_id": "649c178950b4be74229d680f",
                        "avatarUrl": "/avatars/941dec90fdfc46b9ae23378e3a3113f4.svg",
                        "isPro": false,
                        "fullname": "Zeshi Yang",
                        "user": "Zeshi209",
                        "type": "user"
                    },
                    "name": "Zeshi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:21:03.851Z",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff587",
                    "user": {
                        "_id": "645223fb01d7bd9555ea399a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
                        "isPro": false,
                        "fullname": "Zhiyang Dou",
                        "user": "frankzydou",
                        "type": "user"
                    },
                    "name": "Zhiyang Dou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:21:09.328Z",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff588",
                    "user": {
                        "_id": "6437a813fac5ea753f1c72d2",
                        "avatarUrl": "/avatars/69e60e60497e404149a1dad46649dad4.svg",
                        "isPro": false,
                        "fullname": "wenjia Wang",
                        "user": "WenjiaWang",
                        "type": "user"
                    },
                    "name": "Wenjia Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:21:22.428Z",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff589",
                    "name": "Buzhen Huang",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff58a",
                    "user": {
                        "_id": "635f93577c05eb9f59966209",
                        "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
                        "isPro": false,
                        "fullname": "Intelligent Digital Creation",
                        "user": "BoDai",
                        "type": "user"
                    },
                    "name": "Bo Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:21:59.939Z",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff58b",
                    "name": "Taku Komura",
                    "hidden": false
                },
                {
                    "_id": "67eac6433755a17e3cbff58c",
                    "user": {
                        "_id": "669cb638666901f41dae51bf",
                        "avatarUrl": "/avatars/a7cc19e3db84bd86bee3eb6fd4897959.svg",
                        "isPro": false,
                        "fullname": "Jingbo Wang",
                        "user": "jingbocuhk",
                        "type": "user"
                    },
                    "name": "Jingbo Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:21:43.455Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
            ],
            "publishedAt": "2025-03-25T17:57:46.000Z",
            "submittedOnDailyAt": "2025-04-01T06:24:17.315Z",
            "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization",
            "submittedOnDailyBy": {
                "_id": "6630cc7e9ee8861dd0b9bdbd",
                "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
                "isPro": false,
                "fullname": "Liang Pan",
                "user": "lianganimation",
                "type": "user"
            },
            "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/",
            "upvotes": 13,
            "discussionId": "67eac6443755a17e3cbff5cf",
            "projectPage": "https://liangpan99.github.io/TokenHSI/",
            "githubRepo": "https://github.com/liangpan99/TokenHSI",
            "ai_keywords": [
                "transformer-based policy",
                "proprioception",
                "shared token",
                "task tokens",
                "masking mechanism",
                "multi-task training",
                "variable length inputs",
                "task tokenizers"
            ]
        },
        "publishedAt": "2025-03-25T13:57:46.000Z",
        "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization",
        "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19901.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6630cc7e9ee8861dd0b9bdbd",
            "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
            "fullname": "Liang Pan",
            "name": "lianganimation",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24364",
            "authors": [
                {
                    "_id": "67eb6e6088a08fae617860f3",
                    "user": {
                        "_id": "600b381d3cc3b87db94bc0ce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
                        "isPro": false,
                        "fullname": "ukasz Borchmann",
                        "user": "Borchmann",
                        "type": "user"
                    },
                    "name": "ukasz Borchmann",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-01T05:01:23.216Z",
                    "hidden": false
                },
                {
                    "_id": "67eb6e6088a08fae617860f4",
                    "user": {
                        "_id": "66c5e93e8f14c260be9d9f63",
                        "avatarUrl": "/avatars/f4bf15e23923ef3256d3f01a3278d8bc.svg",
                        "isPro": false,
                        "fullname": "Marek Wydmuch",
                        "user": "sfc-mwydmuch",
                        "type": "user"
                    },
                    "name": "Marek Wydmuch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:01:57.075Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
            ],
            "publishedAt": "2025-03-31T17:43:36.000Z",
            "submittedOnDailyAt": "2025-04-01T03:14:34.239Z",
            "title": "Query and Conquer: Execution-Guided SQL Generation",
            "submittedOnDailyBy": {
                "_id": "600b381d3cc3b87db94bc0ce",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
                "isPro": false,
                "fullname": "ukasz Borchmann",
                "user": "Borchmann",
                "type": "user"
            },
            "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
            "upvotes": 12,
            "discussionId": "67eb6e6188a08fae6178613f",
            "ai_keywords": [
                "text-to-SQL",
                "execution results",
                "semantically consistent",
                "query",
                "candidates",
                "models",
                "reasoning methods",
                "o1",
                "o3-mini",
                "DeepSeek R1",
                "inference cost",
                "SQL generation"
            ]
        },
        "publishedAt": "2025-03-31T13:43:36.000Z",
        "title": "Query and Conquer: Execution-Guided SQL Generation",
        "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24364.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "fullname": "ukasz Borchmann",
            "name": "Borchmann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24115",
            "authors": [
                {
                    "_id": "67eb5116d3a707c0a5b02bd1",
                    "user": {
                        "_id": "64a0ed5ed5374ca472cfb0ac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
                        "isPro": false,
                        "fullname": "ZhimingMa",
                        "user": "JimmyMa99",
                        "type": "user"
                    },
                    "name": "Zhiming Ma",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd2",
                    "user": {
                        "_id": "6385f7b969634850f8ddd541",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669723465271-noauth.png",
                        "isPro": false,
                        "fullname": "Peidong Wang",
                        "user": "WDong",
                        "type": "user"
                    },
                    "name": "Peidong Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd3",
                    "user": {
                        "_id": "6466d57bf3e78d1d6be0505c",
                        "avatarUrl": "/avatars/9659b7d0f6fa51efc127afb7a1ba14b1.svg",
                        "isPro": false,
                        "fullname": "HuangMinhua",
                        "user": "HuangMinhua",
                        "type": "user"
                    },
                    "name": "Minhua Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:02:15.224Z",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd4",
                    "name": "Jingpeng Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd5",
                    "name": "Kai Wu",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd6",
                    "name": "Xiangzhao Lv",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd7",
                    "name": "Yachun Pang",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd8",
                    "name": "Yin Yang",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bd9",
                    "name": "Wenjie Tang",
                    "hidden": false
                },
                {
                    "_id": "67eb5116d3a707c0a5b02bda",
                    "name": "Yuchen Kang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
            ],
            "publishedAt": "2025-03-31T14:06:17.000Z",
            "submittedOnDailyAt": "2025-04-01T01:08:09.201Z",
            "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
            "submittedOnDailyBy": {
                "_id": "64a0ed5ed5374ca472cfb0ac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
                "isPro": false,
                "fullname": "ZhimingMa",
                "user": "JimmyMa99",
                "type": "user"
            },
            "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
            "upvotes": 9,
            "discussionId": "67eb5117d3a707c0a5b02c4c",
            "ai_keywords": [
                "automatically speech recognition (ASR)",
                "text-to-speech (TTS)",
                "large language model (LLM)",
                "self-instruction sampling",
                "multi-agent adversarial synthesis",
                "supervised fine-tuning (SFT)",
                "hybrid real/synthetic data"
            ]
        },
        "publishedAt": "2025-03-31T10:06:17.000Z",
        "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
        "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24115.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "fullname": "ZhimingMa",
            "name": "JimmyMa99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22673",
            "authors": [
                {
                    "_id": "67ec2f2882007f02949fdbed",
                    "name": "Jianguo Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbee",
                    "name": "Thai Hoang",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbef",
                    "name": "Ming Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf0",
                    "name": "Zuxin Liu",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf1",
                    "name": "Shiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf2",
                    "name": "Tulika Awalgaonkar",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf3",
                    "name": "Akshara Prabhakar",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf4",
                    "name": "Haolin Chen",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf5",
                    "name": "Weiran Yao",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf6",
                    "name": "Zhiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf7",
                    "name": "Juntao Tan",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf8",
                    "name": "Juan Carlos Niebles",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbf9",
                    "name": "Shelby Heinecke",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbfa",
                    "name": "Huan Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbfb",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "67ec2f2882007f02949fdbfc",
                    "name": "Caiming Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:58:33.000Z",
            "submittedOnDailyAt": "2025-04-01T16:56:01.961Z",
            "title": "ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models",
            "submittedOnDailyBy": {
                "_id": "630fcbd758d83e8f64d82777",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630fcbd758d83e8f64d82777/0jwTa1MzeXZbDtJ-uXGo6.jpeg",
                "isPro": false,
                "fullname": "Jianguo Zhang",
                "user": "jianguozhang",
                "type": "user"
            },
            "summary": "Action models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We present ActionStudio, a lightweight\nand extensible data and training framework designed for large action models.\nActionStudio unifies heterogeneous agent trajectories through a standardized\nformat, supports diverse training paradigms including LoRA, full fine-tuning,\nand distributed setups, and integrates robust preprocessing and verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to\nfacilitate research in the community.",
            "upvotes": 9,
            "discussionId": "67ec2f2982007f02949fdc5d",
            "githubRepo": "https://github.com/SalesforceAIResearch/xLAM",
            "ai_keywords": [
                "Action models",
                "autonomous agents",
                "agentic data",
                "scalable",
                "agent-specific fine-tuning",
                "ActionStudio",
                "heterogeneous agent trajectories",
                "standardized format",
                "LoRA",
                "full fine-tuning",
                "distributed setups",
                "preprocessing",
                "verification tools",
                "public benchmarks",
                "realistic industry benchmarks",
                "performance",
                "practical scalability"
            ]
        },
        "publishedAt": "2025-03-28T13:58:33.000Z",
        "title": "ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models",
        "summary": "Action models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We present ActionStudio, a lightweight\nand extensible data and training framework designed for large action models.\nActionStudio unifies heterogeneous agent trajectories through a standardized\nformat, supports diverse training paradigms including LoRA, full fine-tuning,\nand distributed setups, and integrates robust preprocessing and verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to\nfacilitate research in the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22673.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630fcbd758d83e8f64d82777",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630fcbd758d83e8f64d82777/0jwTa1MzeXZbDtJ-uXGo6.jpeg",
            "fullname": "Jianguo Zhang",
            "name": "jianguozhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18809",
            "authors": [
                {
                    "_id": "67eaa0f83ace6eb46745a9fe",
                    "user": {
                        "_id": "674f43d6df6fa102409f6d1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
                        "isPro": false,
                        "fullname": "Augusto B. Corra",
                        "user": "abcorrea",
                        "type": "user"
                    },
                    "name": "Augusto B. Corra",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T15:15:12.565Z",
                    "hidden": false
                },
                {
                    "_id": "67eaa0f83ace6eb46745a9ff",
                    "user": {
                        "_id": "662fb9c891587703a677856e",
                        "avatarUrl": "/avatars/9cb7f035a513279532fc205ce9c5902c.svg",
                        "isPro": false,
                        "fullname": "Andre Grahl Pereira",
                        "user": "andregrahl",
                        "type": "user"
                    },
                    "name": "Andr G. Pereira",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T15:15:13.942Z",
                    "hidden": false
                },
                {
                    "_id": "67eaa0f83ace6eb46745aa00",
                    "user": {
                        "_id": "66f3dfd4b8703dde248f6d26",
                        "avatarUrl": "/avatars/c199c91d422500cc7c7556569291644d.svg",
                        "isPro": false,
                        "fullname": "Jendrik Seipp",
                        "user": "jendrikseipp",
                        "type": "user"
                    },
                    "name": "Jendrik Seipp",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:47:39.862Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T15:50:20.000Z",
            "submittedOnDailyAt": "2025-04-01T00:45:14.321Z",
            "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
            "submittedOnDailyBy": {
                "_id": "674f43d6df6fa102409f6d1a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
                "isPro": false,
                "fullname": "Augusto B. Corra",
                "user": "abcorrea",
                "type": "user"
            },
            "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
            "upvotes": 9,
            "discussionId": "67eaa0f93ace6eb46745aa3e",
            "ai_keywords": [
                "large language models (LLMs)",
                "chain-of-thought prompting",
                "fine-tuning",
                "reasoning",
                "planning domain",
                "domain-dependent heuristic functions",
                "Python code",
                "greedy best-first search",
                "state-of-the-art domain-independent heuristics",
                "domain-dependent planning",
                "unoptimized Python planner",
                "highly optimized C++ code",
                "planning heuristic function programs"
            ]
        },
        "publishedAt": "2025-03-24T11:50:20.000Z",
        "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
        "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18809.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "674f43d6df6fa102409f6d1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
            "fullname": "Augusto B. Corra",
            "name": "abcorrea",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21694",
            "authors": [
                {
                    "_id": "67eb92defa85fe030e2db9e2",
                    "user": {
                        "_id": "64295d1f4e073875f6a605ac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
                        "isPro": true,
                        "fullname": "Zhiyuan Ma",
                        "user": "ZhiyuanthePony",
                        "type": "user"
                    },
                    "name": "Zhiyuan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:14:51.486Z",
                    "hidden": false
                },
                {
                    "_id": "67eb92defa85fe030e2db9e3",
                    "user": {
                        "_id": "672111333ced358bdac2925d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qHNDI3zYRkJZmhCHkSZwK.png",
                        "isPro": false,
                        "fullname": "Xinyue Liang",
                        "user": "DarklordLeto",
                        "type": "user"
                    },
                    "name": "Xinyue Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:14:58.992Z",
                    "hidden": false
                },
                {
                    "_id": "67eb92defa85fe030e2db9e4",
                    "name": "Rongyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "67eb92defa85fe030e2db9e5",
                    "name": "Xiangyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "67eb92defa85fe030e2db9e6",
                    "name": "Zhen Lei",
                    "hidden": false
                },
                {
                    "_id": "67eb92defa85fe030e2db9e7",
                    "name": "Lei Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
            ],
            "publishedAt": "2025-03-27T16:59:15.000Z",
            "submittedOnDailyAt": "2025-04-01T06:05:21.846Z",
            "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
            "submittedOnDailyBy": {
                "_id": "64295d1f4e073875f6a605ac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
                "isPro": true,
                "fullname": "Zhiyuan Ma",
                "user": "ZhiyuanthePony",
                "type": "user"
            },
            "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
            "upvotes": 8,
            "discussionId": "67eb92e2fa85fe030e2dbc04",
            "projectPage": "https://theericma.github.io/TriplaneTurbo/",
            "githubRepo": "https://github.com/theEricMa/TriplaneTurbo",
            "ai_keywords": [
                "diffusion models",
                "Stable Diffusion (SD)",
                "3D representations",
                "Progressive Rendering Distillation (PRD)",
                "U-Net",
                "latent from random noise",
                "denoise the latent",
                "3D output",
                "Multi-view diffusion models",
                "MVDream",
                "RichDreamer",
                "score distillation",
                "text-consistent textures",
                "geometries",
                "Triplane generator",
                "TriplaneTurbo",
                "high-quality 3D meshes"
            ]
        },
        "publishedAt": "2025-03-27T12:59:15.000Z",
        "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
        "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64295d1f4e073875f6a605ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
            "fullname": "Zhiyuan Ma",
            "name": "ZhiyuanthePony",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24391",
            "authors": [
                {
                    "_id": "67eb72a2291b56e50b66a063",
                    "user": {
                        "_id": "66606a13fc6c0816442bd161",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
                        "isPro": false,
                        "fullname": "Xingyu Chen",
                        "user": "rover-xingyu",
                        "type": "user"
                    },
                    "name": "Xingyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:33.467Z",
                    "hidden": false
                },
                {
                    "_id": "67eb72a2291b56e50b66a064",
                    "user": {
                        "_id": "66f80281d88dc2ad510663e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5HV3mTu-cxPCnWlCi_2wB.jpeg",
                        "isPro": false,
                        "fullname": "Yue Chen",
                        "user": "faneggg",
                        "type": "user"
                    },
                    "name": "Yue Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:31.158Z",
                    "hidden": false
                },
                {
                    "_id": "67eb72a2291b56e50b66a065",
                    "name": "Yuliang Xiu",
                    "hidden": false
                },
                {
                    "_id": "67eb72a2291b56e50b66a066",
                    "name": "Andreas Geiger",
                    "hidden": false
                },
                {
                    "_id": "67eb72a2291b56e50b66a067",
                    "name": "Anpei Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:59:58.000Z",
            "submittedOnDailyAt": "2025-04-01T07:01:19.970Z",
            "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
            "submittedOnDailyBy": {
                "_id": "66606a13fc6c0816442bd161",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
                "isPro": false,
                "fullname": "Xingyu Chen",
                "user": "rover-xingyu",
                "type": "user"
            },
            "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
            "upvotes": 3,
            "discussionId": "67eb72a5291b56e50b66a152",
            "ai_keywords": [
                "Transformer network architectures",
                "dense point clouds",
                "camera parameters",
                "direct supervision",
                "3D datasets",
                "4D datasets",
                "4D model",
                "fine-tune",
                "dynamic video data",
                "geometric priors",
                "optical flow",
                "depths",
                "training-free method",
                "4D reconstruction",
                "attention adaptation",
                "inference",
                "attention layers",
                "dynamic region segmentation",
                "camera pose estimation",
                "4D dense point map reconstruction"
            ]
        },
        "publishedAt": "2025-03-31T13:59:58.000Z",
        "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
        "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24391.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66606a13fc6c0816442bd161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
            "fullname": "Xingyu Chen",
            "name": "rover-xingyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23730",
            "authors": [
                {
                    "_id": "67eb567141abf40cd86e0e15",
                    "user": {
                        "_id": "67038a66eb760972bcb62c70",
                        "avatarUrl": "/avatars/8cc82af8f11cae994bc83f4bd99b51bc.svg",
                        "isPro": false,
                        "fullname": "",
                        "user": "yoonshik1205",
                        "type": "user"
                    },
                    "name": "Yoonshik Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:53.729Z",
                    "hidden": false
                },
                {
                    "_id": "67eb567141abf40cd86e0e16",
                    "user": {
                        "_id": "646484cfb90150b2706df03b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                        "isPro": true,
                        "fullname": "Jaeyoon Jung",
                        "user": "lastdefiance20",
                        "type": "user"
                    },
                    "name": "Jaeyoon Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:56.151Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T05:04:25.000Z",
            "submittedOnDailyAt": "2025-04-01T01:43:40.701Z",
            "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
            "submittedOnDailyBy": {
                "_id": "646484cfb90150b2706df03b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
                "isPro": true,
                "fullname": "Jaeyoon Jung",
                "user": "lastdefiance20",
                "type": "user"
            },
            "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
            "upvotes": 3,
            "discussionId": "67eb567341abf40cd86e0e63",
            "githubRepo": "https://github.com/maum-ai/KOFFVQA",
            "ai_keywords": [
                "Large Vision-Language Models (VLMs)",
                "visual question answering benchmark",
                "grading criteria"
            ]
        },
        "publishedAt": "2025-03-31T01:04:25.000Z",
        "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
        "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23730.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "fullname": "Jaeyoon Jung",
            "name": "lastdefiance20",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23022",
            "authors": [
                {
                    "_id": "67ebadecf9f9390b4cd1c6d9",
                    "name": "Xianglong He",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6da",
                    "name": "Junyi Chen",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6db",
                    "name": "Di Huang",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6dc",
                    "name": "Zexiang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6dd",
                    "name": "Xiaoshui Huang",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6de",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6df",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "67ebadecf9f9390b4cd1c6e0",
                    "name": "Yangguang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T09:21:50.000Z",
            "submittedOnDailyAt": "2025-04-01T07:42:31.137Z",
            "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs",
            "submittedOnDailyBy": {
                "_id": "64d71083a787c9bc7b9f1238",
                "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                "isPro": false,
                "fullname": "Yangguang Li",
                "user": "Lp256",
                "type": "user"
            },
            "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35times faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.",
            "upvotes": 3,
            "discussionId": "67ebadeef9f9390b4cd1c7b3",
            "ai_keywords": [
                "mesh auto-regressive techniques",
                "continuous spatial diffusion",
                "transformer-based VAE",
                "flow-based diffusion transformer",
                "high-fidelity mesh generation",
                "ShapeNet dataset",
                "Objaverse dataset",
                "conditional guidance"
            ]
        },
        "publishedAt": "2025-03-29T05:21:50.000Z",
        "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs",
        "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35times faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23022.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "fullname": "Yangguang Li",
            "name": "Lp256",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.20286",
            "authors": [
                {
                    "_id": "67eaa88c40bebc3127ade04c",
                    "user": {
                        "_id": "67e77099284080c98d8c9bfc",
                        "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
                        "isPro": false,
                        "fullname": "Zhenyu Liang",
                        "user": "ZhenyuLiang",
                        "type": "user"
                    },
                    "name": "Zhenyu Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T15:15:16.646Z",
                    "hidden": false
                },
                {
                    "_id": "67eaa88c40bebc3127ade04d",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "67eaa88c40bebc3127ade04e",
                    "name": "Naiwei Yu",
                    "hidden": false
                },
                {
                    "_id": "67eaa88c40bebc3127ade04f",
                    "name": "Kebin Sun",
                    "hidden": false
                },
                {
                    "_id": "67eaa88c40bebc3127ade050",
                    "name": "Ran Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-26T07:30:23.000Z",
            "submittedOnDailyAt": "2025-04-01T00:34:02.763Z",
            "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
            "submittedOnDailyBy": {
                "_id": "67e77099284080c98d8c9bfc",
                "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
                "isPro": false,
                "fullname": "Zhenyu Liang",
                "user": "ZhenyuLiang",
                "type": "user"
            },
            "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
            "upvotes": 3,
            "discussionId": "67eaa88e40bebc3127ade0eb",
            "githubRepo": "https://github.com/EMI-Group/evomo",
            "ai_keywords": [
                "evolutionary multiobjective optimization (EMO)",
                "parallelism",
                "scalability",
                "GPU",
                "tensorization",
                "tensor representations",
                "NSGA-III",
                "MOEA/D",
                "HypE",
                "GPU-accelerated physics engine",
                "multiobjective robot control benchmark",
                "population sizes",
                "high-quality solutions",
                "diverse behaviors"
            ]
        },
        "publishedAt": "2025-03-26T03:30:23.000Z",
        "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
        "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20286.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67e77099284080c98d8c9bfc",
            "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
            "fullname": "Zhenyu Liang",
            "name": "ZhenyuLiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.14941",
            "authors": [
                {
                    "_id": "67eb932522a341478ae86cb6",
                    "user": {
                        "_id": "67a99d1fef1439e285c4cbec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
                        "isPro": false,
                        "fullname": "Qihui Zhang",
                        "user": "77Hui",
                        "type": "user"
                    },
                    "name": "Qihui Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:00.373Z",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cb7",
                    "user": {
                        "_id": "65e14c28b1a6de8a71e70172",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e14c28b1a6de8a71e70172/D097SILGsqoufpp3sG8tV.jpeg",
                        "isPro": false,
                        "fullname": "Munan Ning",
                        "user": "MunanNing",
                        "type": "user"
                    },
                    "name": "Munan Ning",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:18:39.336Z",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cb8",
                    "name": "Zheyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cb9",
                    "name": "Yanbo Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cba",
                    "name": "Jiayi Ye",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cbb",
                    "user": {
                        "_id": "6637443ecd9097ac3c996d3c",
                        "avatarUrl": "/avatars/d1c38bf03c2517ba0a7004b2f9f9bc96.svg",
                        "isPro": false,
                        "fullname": "yue",
                        "user": "yuehuang",
                        "type": "user"
                    },
                    "name": "Yue Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:19:26.627Z",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cbc",
                    "name": "Shuo Yang",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cbd",
                    "name": "Xiao Chen",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cbe",
                    "user": {
                        "_id": "62c51800cb7033fd49b8efb7",
                        "avatarUrl": "/avatars/06c2be0015f8022f9912f2279f2b3597.svg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "Yibing",
                        "type": "user"
                    },
                    "name": "Yibing Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-01T08:19:07.616Z",
                    "hidden": false
                },
                {
                    "_id": "67eb932522a341478ae86cbf",
                    "name": "Li Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-19T07:15:41.000Z",
            "submittedOnDailyAt": "2025-04-01T05:48:16.581Z",
            "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
            "submittedOnDailyBy": {
                "_id": "67a99d1fef1439e285c4cbec",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
                "isPro": false,
                "fullname": "Qihui Zhang",
                "user": "77Hui",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
            "upvotes": 3,
            "discussionId": "67eb932622a341478ae86d15",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "Visual Question Answering (VQA)",
                "Q&A pairs",
                "MLLM-as-judge",
                "Unsupervised Peer review MLLM Evaluation (UPME)",
                "vision-language scoring system",
                "response correctness",
                "visual understanding and reasoning",
                "image-text correlation",
                "Pearson correlation",
                "MMstar dataset",
                "ScienceQA dataset"
            ]
        },
        "publishedAt": "2025-03-19T03:15:41.000Z",
        "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
        "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a99d1fef1439e285c4cbec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
            "fullname": "Qihui Zhang",
            "name": "77Hui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22655",
            "authors": [
                {
                    "_id": "67ebe24d386032554c9e84fc",
                    "name": "Xiaomin Yu",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e84fd",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e84fe",
                    "name": "Wenjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e84ff",
                    "name": "Siteng Huang",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e8500",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e8501",
                    "name": "Chengwei Qin",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e8502",
                    "name": "Kejian Wu",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e8503",
                    "name": "Zhaoxin Fan",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e8504",
                    "name": "Ziyue Qiao",
                    "hidden": false
                },
                {
                    "_id": "67ebe24d386032554c9e8505",
                    "name": "Donglin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:43:00.000Z",
            "submittedOnDailyAt": "2025-04-01T11:27:23.936Z",
            "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
            "submittedOnDailyBy": {
                "_id": "650ab54e23196fb2d86b486b",
                "avatarUrl": "/avatars/e0506393589695b553ec9ee3fe99b93a.svg",
                "isPro": false,
                "fullname": "SongYang Gao",
                "user": "Wizardcoast",
                "type": "user"
            },
            "summary": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.",
            "upvotes": 2,
            "discussionId": "67ebe24f386032554c9e8558",
            "ai_keywords": [
                "large language models (LLMs)",
                "cross-integrated three-stage multimodal data synthesis framework",
                "Unicorn-1.2M",
                "Unicorn-471K-Instruction",
                "Diverse Caption Data Synthesis",
                "Instruction-Tuning Data Generation",
                "multi-turn instruction-tuning tasks",
                "Modality Representation Transfer",
                "semantically diverse high-quality captions",
                "pretraining",
                "instruction-tuning"
            ]
        },
        "publishedAt": "2025-03-28T13:43:00.000Z",
        "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training",
        "summary": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22655.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650ab54e23196fb2d86b486b",
            "avatarUrl": "/avatars/e0506393589695b553ec9ee3fe99b93a.svg",
            "fullname": "SongYang Gao",
            "name": "Wizardcoast",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.23913",
            "authors": [
                {
                    "_id": "67ebaea15baac6e5085afcb9",
                    "name": "Xiaoxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67ebaea15baac6e5085afcba",
                    "name": "Yihe Deng",
                    "hidden": false
                },
                {
                    "_id": "67ebaea15baac6e5085afcbb",
                    "name": "Mingyu Derek Ma",
                    "hidden": false
                },
                {
                    "_id": "67ebaea15baac6e5085afcbc",
                    "name": "Wei Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T10:04:35.000Z",
            "submittedOnDailyAt": "2025-04-01T07:47:10.184Z",
            "title": "Entropy-Based Adaptive Weighting for Self-Training",
            "submittedOnDailyBy": {
                "_id": "64ba5946c0f19c9025665a3c",
                "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
                "isPro": false,
                "fullname": "Xiaoxuan Wang",
                "user": "xw27",
                "type": "user"
            },
            "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method.",
            "upvotes": 1,
            "discussionId": "67ebaea25baac6e5085afcfe",
            "ai_keywords": [
                "Entropy-Based Adaptive Weighting for Self-Training (EAST)",
                "mapping function",
                "tunable parameter",
                "weighting strategy",
                "uncertainty",
                "informative examples",
                "challenging examples"
            ]
        },
        "publishedAt": "2025-03-31T06:04:35.000Z",
        "title": "Entropy-Based Adaptive Weighting for Self-Training",
        "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23913.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ba5946c0f19c9025665a3c",
            "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
            "fullname": "Xiaoxuan Wang",
            "name": "xw27",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.19906",
            "authors": [
                {
                    "_id": "67e3c2afc12b767434674a01",
                    "user": {
                        "_id": "640d96783c82bd463ee1fe6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ct2OPR83Vt6VTrzpu79BE.png",
                        "isPro": true,
                        "fullname": "Hongyu LIU",
                        "user": "KumaPower",
                        "type": "user"
                    },
                    "name": "Hongyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:14.206Z",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a02",
                    "name": "Xuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a03",
                    "name": "Ziyu Wan",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a04",
                    "name": "Yue Ma",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a05",
                    "name": "Jingye Chen",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a06",
                    "name": "Yanbo Fan",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a07",
                    "name": "Yujun Shen",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a08",
                    "name": "Yibing Song",
                    "hidden": false
                },
                {
                    "_id": "67e3c2afc12b767434674a09",
                    "name": "Qifeng Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/640d96783c82bd463ee1fe6b/FSLaLwws70oQ2I5K8niQF.mp4"
            ],
            "publishedAt": "2025-03-25T17:59:03.000Z",
            "submittedOnDailyAt": "2025-04-01T18:07:37.417Z",
            "title": "AvatarArtist: Open-Domain 4D Avatarization",
            "submittedOnDailyBy": {
                "_id": "640d96783c82bd463ee1fe6b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ct2OPR83Vt6VTrzpu79BE.png",
                "isPro": true,
                "fullname": "Hongyu LIU",
                "user": "KumaPower",
                "type": "user"
            },
            "summary": "This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies..",
            "upvotes": 1,
            "discussionId": "67e3c2b2c12b767434674abe",
            "ai_keywords": [
                "parametric triplanes",
                "generative adversarial networks (GANs)",
                "diffusion models",
                "4D GANs",
                "2D diffusion prior",
                "multi-domain image-triplane dataset",
                "4D avatar creator"
            ]
        },
        "publishedAt": "2025-03-25T13:59:03.000Z",
        "title": "AvatarArtist: Open-Domain 4D Avatarization",
        "summary": "This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies..",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/640d96783c82bd463ee1fe6b/FSLaLwws70oQ2I5K8niQF.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19906.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d96783c82bd463ee1fe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Ct2OPR83Vt6VTrzpu79BE.png",
            "fullname": "Hongyu LIU",
            "name": "KumaPower",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.19794",
            "authors": [
                {
                    "_id": "67e372f9988d6c18d8204f48",
                    "user": {
                        "_id": "666f2534a119281ee0e39783",
                        "avatarUrl": "/avatars/1de737da5cdf68d9bb1f98db8d5ad8b9.svg",
                        "isPro": false,
                        "fullname": "Zhuoming Liu",
                        "user": "zhuomingliu",
                        "type": "user"
                    },
                    "name": "Zhuoming Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-26T20:44:26.546Z",
                    "hidden": false
                },
                {
                    "_id": "67e372f9988d6c18d8204f49",
                    "name": "Yiquan Li",
                    "hidden": false
                },
                {
                    "_id": "67e372f9988d6c18d8204f4a",
                    "name": "Khoi Duc Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67e372f9988d6c18d8204f4b",
                    "name": "Yiwu Zhong",
                    "hidden": false
                },
                {
                    "_id": "67e372f9988d6c18d8204f4c",
                    "name": "Yin Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-25T16:02:37.000Z",
            "submittedOnDailyAt": "2025-04-01T14:17:10.391Z",
            "title": "PAVE: Patching and Adapting Video Large Language Models",
            "submittedOnDailyBy": {
                "_id": "666f2534a119281ee0e39783",
                "avatarUrl": "/avatars/1de737da5cdf68d9bb1f98db8d5ad8b9.svg",
                "isPro": false,
                "fullname": "Zhuoming Liu",
                "user": "zhuomingliu",
                "type": "user"
            },
            "summary": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
            "upvotes": 1,
            "discussionId": "67e372fa988d6c18d8204fb6",
            "projectPage": "https://github.com/dragonlzm/PAVE",
            "githubRepo": "https://github.com/dragonlzm/PAVE",
            "ai_keywords": [
                "Video LLMs",
                "reasoning capabilities",
                "side-channel signals",
                "adapters",
                "patches",
                "parameters",
                "operations",
                "audio-visual question answering",
                "3D reasoning",
                "multi-view video recognition",
                "high frame rate video understanding",
                "FLOPs",
                "multi-task learning"
            ]
        },
        "publishedAt": "2025-03-25T12:02:37.000Z",
        "title": "PAVE: Patching and Adapting Video Large Language Models",
        "summary": "Pre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we present PAVE, a flexible framework for adapting\npre-trained Video LLMs to downstream tasks with side-channel signals, such as\naudio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so, PAVE can effectively adapt the pre-trained base model to support\ndiverse downstream tasks, including audio-visual question answering, 3D\nreasoning, multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks, PAVE significantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE\nsupports multi-task learning and generalizes well across different Video LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19794.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666f2534a119281ee0e39783",
            "avatarUrl": "/avatars/1de737da5cdf68d9bb1f98db8d5ad8b9.svg",
            "fullname": "Zhuoming Liu",
            "name": "zhuomingliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18225",
            "authors": [
                {
                    "_id": "67ebabe3c545cab686735182",
                    "user": {
                        "_id": "63f62ee3b29015adc33aafa0",
                        "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
                        "isPro": false,
                        "fullname": "Massimo Bini",
                        "user": "mwbini",
                        "type": "user"
                    },
                    "name": "Massimo Bini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T15:59:29.143Z",
                    "hidden": false
                },
                {
                    "_id": "67ebabe3c545cab686735183",
                    "name": "Leander Girrbach",
                    "hidden": false
                },
                {
                    "_id": "67ebabe3c545cab686735184",
                    "name": "Zeynep Akata",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-23T22:00:56.000Z",
            "submittedOnDailyAt": "2025-04-01T07:38:06.739Z",
            "title": "Decoupling Angles and Strength in Low-rank Adaptation",
            "submittedOnDailyBy": {
                "_id": "63f62ee3b29015adc33aafa0",
                "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
                "isPro": false,
                "fullname": "Massimo Bini",
                "user": "mwbini",
                "type": "user"
            },
            "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.",
            "upvotes": 1,
            "discussionId": "67ebabe5c545cab68673521b",
            "githubRepo": "https://github.com/ExplainableML/DeLoRA",
            "ai_keywords": [
                "Parameter-Efficient FineTuning (PEFT)",
                "LoRA",
                "bounded approaches",
                "ETHER",
                "Decoupled Low-rank Adaptation (DeLoRA)",
                "learnable low-rank matrices",
                "angular learning",
                "adaptation strength",
                "subject-driven image generation",
                "natural language understanding",
                "instruction tuning"
            ]
        },
        "publishedAt": "2025-03-23T18:00:56.000Z",
        "title": "Decoupling Angles and Strength in Low-rank Adaptation",
        "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18225.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f62ee3b29015adc33aafa0",
            "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
            "fullname": "Massimo Bini",
            "name": "mwbini",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22677",
            "authors": [
                {
                    "_id": "67ec5daf3dc5c42c327e161c",
                    "name": "Ruining Li",
                    "hidden": false
                },
                {
                    "_id": "67ec5daf3dc5c42c327e161d",
                    "name": "Chuanxia Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ec5daf3dc5c42c327e161e",
                    "name": "Christian Rupprecht",
                    "hidden": false
                },
                {
                    "_id": "67ec5daf3dc5c42c327e161f",
                    "name": "Andrea Vedaldi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:59:53.000Z",
            "submittedOnDailyAt": "2025-04-01T20:12:30.684Z",
            "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.",
            "upvotes": 0,
            "discussionId": "67ec5db43dc5c42c327e1739",
            "projectPage": "https://ruiningli.com/dso",
            "ai_keywords": [
                "differentiable physics simulators",
                "self-supporting",
                "stability score",
                "non-differentiable simulator",
                "Direct Simulation Optimization (DSO)",
                "direct preference optimization (DPO)",
                "direct reward optimization (DRO)",
                "diffusion models",
                "fine-tuning",
                "alignment metric",
                "ground-truth 3D objects"
            ]
        },
        "publishedAt": "2025-03-28T13:59:53.000Z",
        "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical\n  Soundness",
        "summary": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6560
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.22668",
            "authors": [
                {
                    "_id": "67ea65dafdfe44514f1eef1c",
                    "user": {
                        "_id": "6329b2dabdb6242b42b8e3ce",
                        "avatarUrl": "/avatars/99c68616b3191862f3ef504851e3fb05.svg",
                        "isPro": false,
                        "fullname": "Sindhu Hegde",
                        "user": "sindhuhegde",
                        "type": "user"
                    },
                    "name": "Sindhu B Hegde",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-03-31T12:57:30.227Z",
                    "hidden": false
                },
                {
                    "_id": "67ea65dafdfe44514f1eef1d",
                    "name": "K R Prajwal",
                    "hidden": false
                },
                {
                    "_id": "67ea65dafdfe44514f1eef1e",
                    "name": "Taein Kwon",
                    "hidden": false
                },
                {
                    "_id": "67ea65dafdfe44514f1eef1f",
                    "name": "Andrew Zisserman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T17:55:52.000Z",
            "submittedOnDailyAt": "2025-04-01T13:07:57.620Z",
            "title": "Understanding Co-speech Gestures in-the-wild",
            "submittedOnDailyBy": {
                "_id": "6329b2dabdb6242b42b8e3ce",
                "avatarUrl": "/avatars/99c68616b3191862f3ef504851e3fb05.svg",
                "isPro": false,
                "fullname": "Sindhu Hegde",
                "user": "sindhuhegde",
                "type": "user"
            },
            "summary": "Co-speech gestures play a vital role in non-verbal communication. In this\npaper, we introduce a new framework for co-speech gesture understanding in the\nwild. Specifically, we propose three new tasks and benchmarks to evaluate a\nmodel's capability to comprehend gesture-text-speech associations: (i)\ngesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker\ndetection using gestures. We present a new approach that learns a tri-modal\nspeech-text-video-gesture representation to solve these tasks. By leveraging a\ncombination of global phrase contrastive loss and local gesture-word coupling\nloss, we demonstrate that a strong gesture representation can be learned in a\nweakly supervised manner from videos in the wild. Our learned representations\noutperform previous methods, including large vision-language models (VLMs),\nacross all three tasks. Further analysis reveals that speech and text\nmodalities capture distinct gesture-related signals, underscoring the\nadvantages of learning a shared tri-modal embedding space. The dataset, model,\nand code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal",
            "upvotes": 0,
            "discussionId": "67ea65dcfdfe44514f1eef82",
            "projectPage": "https://www.robots.ox.ac.uk/~vgg/research/jegal/",
            "githubRepo": "https://github.com/Sindhu-Hegde/jegal",
            "ai_keywords": [
                "co-speech gestures",
                "tri-modal speech-text-video-gesture representation",
                "global phrase contrastive loss",
                "local gesture-word coupling loss",
                "weakly supervised",
                "vision-language models (VLMs)",
                "tri-modal embedding space"
            ]
        },
        "publishedAt": "2025-03-28T13:55:52.000Z",
        "title": "Understanding Co-speech Gestures in-the-wild",
        "summary": "Co-speech gestures play a vital role in non-verbal communication. In this\npaper, we introduce a new framework for co-speech gesture understanding in the\nwild. Specifically, we propose three new tasks and benchmarks to evaluate a\nmodel's capability to comprehend gesture-text-speech associations: (i)\ngesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker\ndetection using gestures. We present a new approach that learns a tri-modal\nspeech-text-video-gesture representation to solve these tasks. By leveraging a\ncombination of global phrase contrastive loss and local gesture-word coupling\nloss, we demonstrate that a strong gesture representation can be learned in a\nweakly supervised manner from videos in the wild. Our learned representations\noutperform previous methods, including large vision-language models (VLMs),\nacross all three tasks. Further analysis reveals that speech and text\nmodalities capture distinct gesture-related signals, underscoring the\nadvantages of learning a shared tri-modal embedding space. The dataset, model,\nand code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6329b2dabdb6242b42b8e3ce",
            "avatarUrl": "/avatars/99c68616b3191862f3ef504851e3fb05.svg",
            "fullname": "Sindhu Hegde",
            "name": "sindhuhegde",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]