[
    {
        "paper": {
            "id": "2503.24379",
            "authors": [
                {
                    "_id": "67ec0a7262144ec35d0e571d",
                    "user": {
                        "_id": "64c139d867eff857ea51caa8",
                        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                        "isPro": false,
                        "fullname": "Shengqiong Wu",
                        "user": "ChocoWu",
                        "type": "user"
                    },
                    "name": "Shengqiong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:35.392Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e571e",
                    "user": {
                        "_id": "6360d9f0472131c3bc4f61df",
                        "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
                        "isPro": false,
                        "fullname": "WeicaiYe",
                        "user": "WeicaiYe",
                        "type": "user"
                    },
                    "name": "Weicai Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:36:51.426Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e571f",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5720",
                    "name": "Quande Liu",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5721",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:38:20.890Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5722",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5723",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:38:50.116Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5724",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5725",
                    "user": {
                        "_id": "67eaa070b9fa8908e151fd7d",
                        "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
                        "isPro": false,
                        "fullname": "shuicheng yan",
                        "user": "shuicheng",
                        "type": "user"
                    },
                    "name": "Shuicheng Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:39:18.884Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5726",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:38.087Z",
                    "hidden": false
                },
                {
                    "_id": "67ec0a7262144ec35d0e5727",
                    "user": {
                        "_id": "6570ae84c4993b8fb96f41a8",
                        "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
                        "isPro": false,
                        "fullname": "Tat-Seng Chua",
                        "user": "chuats",
                        "type": "user"
                    },
                    "name": "Tat-Seng Chua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:39:25.537Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:59:01.000Z",
            "submittedOnDailyAt": "2025-04-02T01:29:41.083Z",
            "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "64c139d867eff857ea51caa8",
                "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                "isPro": false,
                "fullname": "Shengqiong Wu",
                "user": "ChocoWu",
                "type": "user"
            },
            "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
            "upvotes": 44,
            "discussionId": "67ec0a7562144ec35d0e57fc",
            "projectPage": "https://sqwu.top/Any2Cap/",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "dense, structured captions",
                "region",
                "motion",
                "camera poses",
                "any-condition-to-caption instruction tuning"
            ]
        },
        "publishedAt": "2025-03-31T13:59:01.000Z",
        "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
        "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24379.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "fullname": "Shengqiong Wu",
            "name": "ChocoWu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00050",
            "authors": [
                {
                    "_id": "67ec9bf3e58745dc7d652587",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d652588",
                    "user": {
                        "_id": "64351475901c5734bcb64248",
                        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Hu",
                        "user": "zhiyuanhucs",
                        "type": "user"
                    },
                    "name": "Zhiyuan Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:44:43.089Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d652589",
                    "user": {
                        "_id": "6730a1fed66bf1b6378cd451",
                        "avatarUrl": "/avatars/5ec9b7313213a951b7c325d35ca26692.svg",
                        "isPro": false,
                        "fullname": "qy",
                        "user": "qingyunzou",
                        "type": "user"
                    },
                    "name": "Qingyun Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:44:49.663Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258a",
                    "name": "Jiaying Wu",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258b",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258c",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:45:24.844Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9bf3e58745dc7d65258d",
                    "name": "Bingsheng He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T02:18:51.000Z",
            "submittedOnDailyAt": "2025-04-02T04:52:20.239Z",
            "title": "JudgeLRM: Large Reasoning Models as a Judge",
            "submittedOnDailyBy": {
                "_id": "64351475901c5734bcb64248",
                "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
                "isPro": false,
                "fullname": "Zhiyuan Hu",
                "user": "zhiyuanhucs",
                "type": "user"
            },
            "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
            "upvotes": 33,
            "discussionId": "67ec9bf4e58745dc7d6525c1",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Supervised Fine-Tuning (SFT)",
                "reasoning capabilities",
                "reinforcement learning (RL)",
                "judge-wise, outcome-driven rewards",
                "JudgeLRM",
                "GPT-4",
                "DeepSeek-R1"
            ]
        },
        "publishedAt": "2025-03-30T22:18:51.000Z",
        "title": "JudgeLRM: Large Reasoning Models as a Judge",
        "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00050.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64351475901c5734bcb64248",
            "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
            "fullname": "Zhiyuan Hu",
            "name": "zhiyuanhucs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23145",
            "authors": [
                {
                    "_id": "67eb710f1f8a09c48b2e3ba1",
                    "user": {
                        "_id": "674286496efe2b931f7ce354",
                        "avatarUrl": "/avatars/8f920618b777dbff5f2a117ebd9e9caa.svg",
                        "isPro": false,
                        "fullname": "Anjiang Wei",
                        "user": "anjiangwei",
                        "type": "user"
                    },
                    "name": "Anjiang Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:42.109Z",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba2",
                    "user": {
                        "_id": "65e7bb35e5e78134ab049942",
                        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
                        "isPro": false,
                        "fullname": "Tarun Suresh",
                        "user": "tarsur909",
                        "type": "user"
                    },
                    "name": "Tarun Suresh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:41.048Z",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba3",
                    "name": "Jiannan Cao",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba4",
                    "name": "Naveen Kannan",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba5",
                    "name": "Yuheng Wu",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba6",
                    "user": {
                        "_id": "65de7628deee79773f0f46f6",
                        "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
                        "isPro": false,
                        "fullname": "Kai Yan",
                        "user": "kaiyan289",
                        "type": "user"
                    },
                    "name": "Kai Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:43:08.150Z",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba7",
                    "name": "Thiago S. F. X. Teixeira",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba8",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb710f1f8a09c48b2e3ba9",
                    "name": "Alex Aiken",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T16:50:39.000Z",
            "submittedOnDailyAt": "2025-04-02T04:28:51.051Z",
            "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
            "submittedOnDailyBy": {
                "_id": "65e7bb35e5e78134ab049942",
                "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
                "isPro": false,
                "fullname": "Tarun Suresh",
                "user": "tarsur909",
                "type": "user"
            },
            "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
            "upvotes": 28,
            "discussionId": "67eb71101f8a09c48b2e3bee",
            "ai_keywords": [
                "inductive program synthesis",
                "programming by example",
                "function synthesis",
                "input-output examples",
                "large language model agents",
                "natural language",
                "evaluation protocols",
                "feedback mechanism",
                "real-world scenarios",
                "reverse engineering",
                "CodeARC",
                "Code Abstraction and Reasoning Challenge",
                "hidden target function",
                "querying",
                "candidate functions",
                "differential testing oracle",
                "interactive setting",
                "function calls",
                "self-correction",
                "large-scale benchmark",
                "general-purpose inductive program synthesis",
                "fine-tuning",
                "LLaMA-3.1-8B-Instruct",
                "synthesis traces",
                "relative performance gain"
            ]
        },
        "publishedAt": "2025-03-29T12:50:39.000Z",
        "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
        "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23145.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e7bb35e5e78134ab049942",
            "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
            "fullname": "Tarun Suresh",
            "name": "tarsur909",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24376",
            "authors": [
                {
                    "_id": "67eca2b8351721d62aa537df",
                    "user": {
                        "_id": "60d045c4778bafd0fbcfa3f5",
                        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
                        "isPro": false,
                        "fullname": "Yi Chen",
                        "user": "ChenYi99",
                        "type": "user"
                    },
                    "name": "Yi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:30.143Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e0",
                    "user": {
                        "_id": "6455cc8f654d8bccae50e4d4",
                        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
                        "isPro": false,
                        "fullname": "Yuying Ge",
                        "user": "tttoaster",
                        "type": "user"
                    },
                    "name": "Yuying Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:05.288Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e1",
                    "user": {
                        "_id": "62e0f1314db2175cd270ad08",
                        "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
                        "isPro": false,
                        "fullname": "Rui Wang",
                        "user": "ruiwang",
                        "type": "user"
                    },
                    "name": "Rui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:12.320Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e2",
                    "user": {
                        "_id": "640e9762b03f4cd29f58d982",
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:19.876Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e3",
                    "name": "Lu Qiu",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e4",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:26.551Z",
                    "hidden": false
                },
                {
                    "_id": "67eca2b8351721d62aa537e5",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:40:32.831Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:55:23.000Z",
            "submittedOnDailyAt": "2025-04-02T01:06:48.435Z",
            "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
            "submittedOnDailyBy": {
                "_id": "60d045c4778bafd0fbcfa3f5",
                "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
                "isPro": false,
                "fullname": "Yi Chen",
                "user": "ChenYi99",
                "type": "user"
            },
            "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
            "upvotes": 27,
            "discussionId": "67eca2b9351721d62aa53822",
            "githubRepo": "https://github.com/TencentARC/SEED-Bench-R1",
            "ai_keywords": [
                "Chain of Thought (COT)",
                "Large Language Models (LLMs)",
                "reinforcement learning (RL)",
                "Multimodal Large Language Models (MLLMs)",
                "SEED-Bench-R1",
                "video understanding",
                "multiple-choice questions",
                "in-distribution",
                "cross-environment",
                "cross-environment-task scenarios",
                "Qwen2-VL-Instruct-7B",
                "supervised fine-tuning (SFT)",
                "LongVideoBench",
                "visual perception",
                "reasoning chains",
                "inconsistent reasoning",
                "reward modeling"
            ]
        },
        "publishedAt": "2025-03-31T13:55:23.000Z",
        "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
        "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24376.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60d045c4778bafd0fbcfa3f5",
            "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
            "fullname": "Yi Chen",
            "name": "ChenYi99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00595",
            "authors": [
                {
                    "_id": "67ecaf516560da48c5c34106",
                    "user": {
                        "_id": "63d34004b734eaa4d4faeccf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
                        "isPro": false,
                        "fullname": "Weizhi Wang",
                        "user": "weizhiwang",
                        "type": "user"
                    },
                    "name": "Weizhi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:20.594Z",
                    "hidden": false
                },
                {
                    "_id": "67ecaf516560da48c5c34107",
                    "name": "Yu Tian",
                    "hidden": false
                },
                {
                    "_id": "67ecaf516560da48c5c34108",
                    "user": {
                        "_id": "67cd0b291580ba5d5ee65ffd",
                        "avatarUrl": "/avatars/9584a55473868e5ca1fa09b1536ca546.svg",
                        "isPro": false,
                        "fullname": "yanglinjie",
                        "user": "yanglj55",
                        "type": "user"
                    },
                    "name": "Linjie Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:55:04.382Z",
                    "hidden": false
                },
                {
                    "_id": "67ecaf516560da48c5c34109",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecaf516560da48c5c3410a",
                    "user": {
                        "_id": "65cd4785c40ab294321d610e",
                        "avatarUrl": "/avatars/3f0053aa2b3d90a10b60ab24cf575fd5.svg",
                        "isPro": false,
                        "fullname": "Xifeng Yan",
                        "user": "windwest",
                        "type": "user"
                    },
                    "name": "Xifeng Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:54:38.935Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T09:54:00.000Z",
            "submittedOnDailyAt": "2025-04-02T02:00:49.375Z",
            "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
            "submittedOnDailyBy": {
                "_id": "63d34004b734eaa4d4faeccf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
                "isPro": false,
                "fullname": "Weizhi Wang",
                "user": "weizhiwang",
                "type": "user"
            },
            "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
            "upvotes": 19,
            "discussionId": "67ecaf546560da48c5c341dc",
            "projectPage": "https://victorwz.github.io/Open-Qwen2VL/",
            "githubRepo": "https://github.com/Victorwz/Open-Qwen2VL",
            "ai_keywords": [
                "multimodal LLM",
                "pre-training",
                "high-quality data filtering",
                "multimodal data mixture strategies",
                "sequence packing techniques",
                "training frameworks",
                "Open-Qwen2VL",
                "fully open-source",
                "2B-parameter",
                "image-text pairs",
                "A100-40G GPU hours",
                "low-to-high dynamic image resolution",
                "multimodal sequence packing",
                "data quality",
                "academic level 8xA100-40G GPUs",
                "UCSB",
                "packed multimodal tokens",
                "Qwen2-VL",
                "instruction-tuned",
                "MMBench",
                "SEEDBench",
                "MMstar",
                "MathVista",
                "compute-efficient",
                "FSDP-based training",
                "WebDataset format",
                "pre-training data",
                "training codebase",
                "data filtering techniques",
                "sequence packing scripts",
                "pre-training data in WebDataset format",
                "FSDP-based training codebase",
                "base and instruction"
            ]
        },
        "publishedAt": "2025-04-01T05:54:00.000Z",
        "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
        "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00595.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
            "fullname": "Weizhi Wang",
            "name": "weizhiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00810",
            "authors": [
                {
                    "_id": "67eca0c4cfce948cbbbcff9a",
                    "user": {
                        "_id": "646f3443c261dc413383b8a4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/hEJd8wLyR5HTdMzApaloN.png",
                        "isPro": false,
                        "fullname": "Zhaojian Yu",
                        "user": "zjy2001",
                        "type": "user"
                    },
                    "name": "Zhaojian Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:38.129Z",
                    "hidden": false
                },
                {
                    "_id": "67eca0c4cfce948cbbbcff9b",
                    "user": {
                        "_id": "650ed552dc509ae7d7bb1ccc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ed552dc509ae7d7bb1ccc/CPR6gaIjPfnBGfr-D_rpO.jpeg",
                        "isPro": false,
                        "fullname": "Yinghao Wu",
                        "user": "yh1567",
                        "type": "user"
                    },
                    "name": "Yinghao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:50:27.577Z",
                    "hidden": false
                },
                {
                    "_id": "67eca0c4cfce948cbbbcff9c",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:34.689Z",
                    "hidden": false
                },
                {
                    "_id": "67eca0c4cfce948cbbbcff9d",
                    "user": {
                        "_id": "5f5ba21188f57f65f951f255",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
                        "isPro": false,
                        "fullname": "Arman Cohan",
                        "user": "armanc",
                        "type": "user"
                    },
                    "name": "Arman Cohan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:50:33.530Z",
                    "hidden": false
                },
                {
                    "_id": "67eca0c4cfce948cbbbcff9e",
                    "name": "Xiao-Ping Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T14:01:50.000Z",
            "submittedOnDailyAt": "2025-04-02T01:59:40.340Z",
            "title": "Z1: Efficient Test-time Scaling with Code",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.",
            "upvotes": 17,
            "discussionId": "67eca0c6cfce948cbbbd0042",
            "githubRepo": "https://github.com/efficientscaling/Z1",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "test-time computing scaling",
                "thinking tokens",
                "Z1-Code-Reasoning-107K",
                "solution trajectories",
                "Shifted Thinking Window",
                "context-delimiting tags",
                "efficient test-time scaling",
                "R1-Distill-Qwen-7B",
                "GPQA Diamond",
                "efficient reasoning elicitation"
            ]
        },
        "publishedAt": "2025-04-01T10:01:50.000Z",
        "title": "Z1: Efficient Test-time Scaling with Code",
        "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00810.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00698",
            "authors": [
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a2",
                    "name": "Team Cohere",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a3",
                    "user": {
                        "_id": "6268981926476157a9262a73",
                        "avatarUrl": "/avatars/a6e4d81580039e8680c194d89a716535.svg",
                        "isPro": false,
                        "fullname": "Aakanksha Naik",
                        "user": "aakanksha",
                        "type": "user"
                    },
                    "name": "Aakanksha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:00.167Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a4",
                    "user": {
                        "_id": "661b21f8ea926e8f86729e61",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b21f8ea926e8f86729e61/gM59piqaqjEu2IPbDvCbZ.jpeg",
                        "isPro": false,
                        "fullname": "Arash Ahmadian",
                        "user": "ArashAhmadian",
                        "type": "user"
                    },
                    "name": "Arash Ahmadian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:06.057Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a5",
                    "name": "Marwan Ahmed",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a6",
                    "user": {
                        "_id": "615cb5915db25883c685c109",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615cb5915db25883c685c109/8FV3c22N4MmvgfYqZS2xQ.jpeg",
                        "isPro": false,
                        "fullname": "Jay Alammar",
                        "user": "jayalammar",
                        "type": "user"
                    },
                    "name": "Jay Alammar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:27.857Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a7",
                    "user": {
                        "_id": "6534fa8199d8bba294382a1e",
                        "avatarUrl": "/avatars/87e47d2313c2f9215297f3df5edf8ecb.svg",
                        "isPro": false,
                        "fullname": "Yazeed Alnumay",
                        "user": "yazeed7",
                        "type": "user"
                    },
                    "name": "Yazeed Alnumay",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:34.082Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a8",
                    "user": {
                        "_id": "642c574c53e76b4c22842843",
                        "avatarUrl": "/avatars/09095a843c5f0dc8af998c893e41cd64.svg",
                        "isPro": false,
                        "fullname": "Sophia Althammer",
                        "user": "salthammer",
                        "type": "user"
                    },
                    "name": "Sophia Althammer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:40.018Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1a9",
                    "user": {
                        "_id": "65327ba4c7530aa27f690d32",
                        "avatarUrl": "/avatars/3baa31fbc9d1b1b2b7ead56ea42eea0b.svg",
                        "isPro": false,
                        "fullname": "Arkady Arkhangorodsky",
                        "user": "arkadyark-cohere",
                        "type": "user"
                    },
                    "name": "Arkady Arkhangorodsky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:51.225Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1aa",
                    "user": {
                        "_id": "633e9b78acd351fc37d69560",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e9b78acd351fc37d69560/rCk8z-C4oOlrksp-QE2lV.jpeg",
                        "isPro": false,
                        "fullname": "Viraat Aryabumi",
                        "user": "viraat",
                        "type": "user"
                    },
                    "name": "Viraat Aryabumi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:57:58.229Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ab",
                    "user": {
                        "_id": "5e844d5992fa4e3c26ddb32a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e844d5992fa4e3c26ddb32a/qnVLOiJchVUxDBd8F-wb_.jpeg",
                        "isPro": false,
                        "fullname": "Dennis Aumiller",
                        "user": "dennlinger",
                        "type": "user"
                    },
                    "name": "Dennis Aumiller",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:05.106Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ac",
                    "user": {
                        "_id": "67482e734a001d5b1fd9af75",
                        "avatarUrl": "/avatars/410b20f037416cd03a876d5eee7fdf5c.svg",
                        "isPro": false,
                        "fullname": "Raphael Avalos",
                        "user": "raphaelavalos-cohere",
                        "type": "user"
                    },
                    "name": "Raphaël Avalos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:14.298Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ad",
                    "user": {
                        "_id": "66edca3e2e2c3d8539faccd6",
                        "avatarUrl": "/avatars/9009b5c33a7864de282f59322fd9dd8b.svg",
                        "isPro": false,
                        "fullname": "Zahara Aviv",
                        "user": "zaharaviv",
                        "type": "user"
                    },
                    "name": "Zahara Aviv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:25.225Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ae",
                    "user": {
                        "_id": "664cf5ba7f808af99f6292cc",
                        "avatarUrl": "/avatars/86d02365e39c0483ddbdd322e013c3c9.svg",
                        "isPro": false,
                        "fullname": "Bae",
                        "user": "sammiebae",
                        "type": "user"
                    },
                    "name": "Sammie Bae",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:32.554Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1af",
                    "name": "Saurabh Baji",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b0",
                    "user": {
                        "_id": "61843bf62d33f05d928f9ec3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61843bf62d33f05d928f9ec3/BXDvDeFfMxIBpsii_ZF6o.jpeg",
                        "isPro": false,
                        "fullname": "Alexandre Barbet",
                        "user": "barbet",
                        "type": "user"
                    },
                    "name": "Alexandre Barbet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:43.721Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b1",
                    "user": {
                        "_id": "604f8c093050a33ebb17ef66",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615825896626-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Max Bartolo",
                        "user": "mbartolo",
                        "type": "user"
                    },
                    "name": "Max Bartolo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:50.489Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b2",
                    "user": {
                        "_id": "645c85505ebf379fd6d5e1cf",
                        "avatarUrl": "/avatars/e34d70b297528eadfa568abfb20f1668.svg",
                        "isPro": false,
                        "fullname": "Björn Bebensee",
                        "user": "bebensee",
                        "type": "user"
                    },
                    "name": "Björn Bebensee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:58:57.913Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b3",
                    "user": {
                        "_id": "668dfdd7e7cc32ad4db2f8fb",
                        "avatarUrl": "/avatars/21f2bade9e4b01b1c68af4de98e727aa.svg",
                        "isPro": false,
                        "fullname": "Neeral Beladia",
                        "user": "seeker-hf",
                        "type": "user"
                    },
                    "name": "Neeral Beladia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:59:08.341Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b4",
                    "user": {
                        "_id": "65724a4de724eeab559902ef",
                        "avatarUrl": "/avatars/1f907f0088ad21f0df6597bed297927d.svg",
                        "isPro": false,
                        "fullname": "Walter Beller-Morales",
                        "user": "walterbm-cohere",
                        "type": "user"
                    },
                    "name": "Walter Beller-Morales",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:59:18.723Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b5",
                    "user": {
                        "_id": "64837c9865e5f20d526d3826",
                        "avatarUrl": "/avatars/9d06b9e354d7b6e487881b1367e76845.svg",
                        "isPro": false,
                        "fullname": "Alexandre Berard",
                        "user": "alex-berard",
                        "type": "user"
                    },
                    "name": "Alexandre Bérard",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:59:24.698Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b6",
                    "name": "Andrew Berneshawi",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b7",
                    "name": "Anna Bialas",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b8",
                    "name": "Phil Blunsom",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1b9",
                    "name": "Matt Bobkin",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ba",
                    "name": "Adi Bongale",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1bb",
                    "user": {
                        "_id": "666b72693814e415473a23f3",
                        "avatarUrl": "/avatars/34c20a67557d52621f7908b18be44876.svg",
                        "isPro": false,
                        "fullname": "Sam Braun",
                        "user": "satchrocks",
                        "type": "user"
                    },
                    "name": "Sam Braun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:00:13.292Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1bc",
                    "user": {
                        "_id": "6713fff64d206518ae9a210b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dl8udMBejX9T0PaShARk_.jpeg",
                        "isPro": false,
                        "fullname": "Maxime Brunet",
                        "user": "maximebrunet",
                        "type": "user"
                    },
                    "name": "Maxime Brunet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:00:33.827Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1bd",
                    "user": {
                        "_id": "5f5c4d49e56d546cd623309b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599884443706-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Samuel Cahyawijaya",
                        "user": "samuelcahyawijaya",
                        "type": "user"
                    },
                    "name": "Samuel Cahyawijaya",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:00:42.662Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1be",
                    "user": {
                        "_id": "64674f1a8334813a7ae33023",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64674f1a8334813a7ae33023/VjxQYzaXW7zlH7ktPBsEX.jpeg",
                        "isPro": false,
                        "fullname": "David Cairuz",
                        "user": "davidcairuz",
                        "type": "user"
                    },
                    "name": "David Cairuz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:00:50.594Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1bf",
                    "name": "Jon Ander Campos",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c0",
                    "user": {
                        "_id": "6761cc33c2b5790c9b7fe361",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NdLf7Niw8ss1t85Sks08a.png",
                        "isPro": false,
                        "fullname": "Cassie Cao",
                        "user": "iceyipink",
                        "type": "user"
                    },
                    "name": "Cassie Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:01:01.464Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c1",
                    "name": "Kris Cao",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c2",
                    "user": {
                        "_id": "61efc5853bc9016395076dd7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662972114904-61efc5853bc9016395076dd7.png",
                        "isPro": false,
                        "fullname": "Roman Castagné",
                        "user": "RomanCast",
                        "type": "user"
                    },
                    "name": "Roman Castagné",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:01:11.902Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c3",
                    "user": {
                        "_id": "621fd566c7f47c5eb5df0012",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648914356867-621fd566c7f47c5eb5df0012.jpeg",
                        "isPro": false,
                        "fullname": "Julián Cendrero",
                        "user": "jucendrero",
                        "type": "user"
                    },
                    "name": "Julián Cendrero",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:01:19.248Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c4",
                    "name": "Leila Chan Currie",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c5",
                    "user": {
                        "_id": "65f14510cd326863999d371f",
                        "avatarUrl": "/avatars/30bc6ffadd458c1b2bf9a55a574fdc75.svg",
                        "isPro": false,
                        "fullname": "Yash chandak",
                        "user": "Yash016",
                        "type": "user"
                    },
                    "name": "Yash Chandak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:01:32.856Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c6",
                    "user": {
                        "_id": "6612ce3c6504d9bed52614fe",
                        "avatarUrl": "/avatars/ce5fbe681ffdea2f7d79a3d174d8403b.svg",
                        "isPro": false,
                        "fullname": "Diane Chang",
                        "user": "diamazon",
                        "type": "user"
                    },
                    "name": "Diane Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:01:41.351Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c7",
                    "user": {
                        "_id": "653bba49ea8b6b5fe3a6bfa8",
                        "avatarUrl": "/avatars/905b09ad7834dfc8dc7b6d0afd0045db.svg",
                        "isPro": false,
                        "fullname": "Giannis Chatziveroglou",
                        "user": "giannis22",
                        "type": "user"
                    },
                    "name": "Giannis Chatziveroglou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:01:48.691Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c8",
                    "name": "Hongyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1c9",
                    "name": "Claire Cheng",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ca",
                    "name": "Alexis Chevalier",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1cb",
                    "name": "Justin T. Chiu",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1cc",
                    "name": "Eugene Cho",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1cd",
                    "user": {
                        "_id": "60c341c722a9d3ec68102981",
                        "avatarUrl": "/avatars/bac7c3ce9f12d0c16c63ddd31a21e71f.svg",
                        "isPro": false,
                        "fullname": "Eugene Choi",
                        "user": "eugenechoi",
                        "type": "user"
                    },
                    "name": "Eugene Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:02:49.539Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ce",
                    "user": {
                        "_id": "660ba79bfb554841ab76ddd0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ba79bfb554841ab76ddd0/JfC2Z42DqbS4tz0VS6hcP.jpeg",
                        "isPro": false,
                        "fullname": "Eujeong Choi",
                        "user": "EujeongChoi",
                        "type": "user"
                    },
                    "name": "Eujeong Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:03:00.458Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1cf",
                    "name": "Tim Chung",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d0",
                    "name": "Volkan Cirik",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d1",
                    "user": {
                        "_id": "667b01fbcef9658a02f5af74",
                        "avatarUrl": "/avatars/ce5eb78ed7ae48d815b01a3ce60120a2.svg",
                        "isPro": false,
                        "fullname": "Ana Cismaru",
                        "user": "anacis-c",
                        "type": "user"
                    },
                    "name": "Ana Cismaru",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:02:04.831Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d2",
                    "name": "Pierre Clavier",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d3",
                    "name": "Henry Conklin",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d4",
                    "user": {
                        "_id": "67364c3cdbfc9d965f79a0cf",
                        "avatarUrl": "/avatars/ddf81949fb793315933fdbfc526c11b5.svg",
                        "isPro": false,
                        "fullname": "Lucas Crawhall-Stein",
                        "user": "colucas",
                        "type": "user"
                    },
                    "name": "Lucas Crawhall-Stein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:03:24.254Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d5",
                    "name": "Devon Crouse",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d6",
                    "name": "Andres Felipe Cruz-Salinas",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d7",
                    "name": "Ben Cyrus",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d8",
                    "name": "Daniel D'souza",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1d9",
                    "user": {
                        "_id": "6426908d7a0d3f02acd4cfb8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426908d7a0d3f02acd4cfb8/Qv-oeJQ6MaOmmRqzknH3G.png",
                        "isPro": false,
                        "fullname": "Hugo Dalla-torre",
                        "user": "hdallatorre",
                        "type": "user"
                    },
                    "name": "Hugo Dalla-Torre",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:04:17.807Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1da",
                    "user": {
                        "_id": "65c581dfc3fa039f843991f6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c581dfc3fa039f843991f6/O4ywGgQQrzps5JYeR3o0y.jpeg",
                        "isPro": false,
                        "fullname": "John Dang",
                        "user": "johndang-cohere",
                        "type": "user"
                    },
                    "name": "John Dang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:04:27.950Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1db",
                    "user": {
                        "_id": "67b32ace74016a1276489603",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bCMJggXAS89Pj0vFFFgpq.png",
                        "isPro": false,
                        "fullname": "William Darling",
                        "user": "williamdarling",
                        "type": "user"
                    },
                    "name": "William Darling",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:04:37.174Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1dc",
                    "name": "Omar Darwiche Domingues",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1dd",
                    "user": {
                        "_id": "63adeb79a31e9ea8298ad4ac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63adeb79a31e9ea8298ad4ac/a_kEJXPLkfaHDUN3XcTYA.jpeg",
                        "isPro": false,
                        "fullname": "Saurabh Dash",
                        "user": "saurabhdash",
                        "type": "user"
                    },
                    "name": "Saurabh Dash",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:04:52.952Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1de",
                    "name": "Antoine Debugne",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1df",
                    "user": {
                        "_id": "65ef658061b468467070ac6c",
                        "avatarUrl": "/avatars/579bf111c0f5dcebee045434bc7e1d69.svg",
                        "isPro": false,
                        "fullname": "Theo Dehaze",
                        "user": "Theodhz",
                        "type": "user"
                    },
                    "name": "Théo Dehaze",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:05:07.745Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e0",
                    "user": {
                        "_id": "647f456135bc6d6aa5fb4a06",
                        "avatarUrl": "/avatars/32154ae5db7cd4adaf2c28b1167a8e9f.svg",
                        "isPro": false,
                        "fullname": "Shaan Desai",
                        "user": "shaandesai",
                        "type": "user"
                    },
                    "name": "Shaan Desai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:05:15.218Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e1",
                    "name": "Joan Devassy",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e2",
                    "user": {
                        "_id": "65ef79fcd08d92badae9d4d4",
                        "avatarUrl": "/avatars/59243ff440cce01606f16dc678f9b931.svg",
                        "isPro": false,
                        "fullname": "Rishit DHolakia",
                        "user": "Rishit97",
                        "type": "user"
                    },
                    "name": "Rishit Dholakia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:05:27.514Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e3",
                    "user": {
                        "_id": "665de02a1d30854dbbd78b9f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HGpTNtb89TBmJA5xOyJwo.png",
                        "isPro": false,
                        "fullname": "Kyle Duffy",
                        "user": "kyle-cohere",
                        "type": "user"
                    },
                    "name": "Kyle Duffy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:05:37.222Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e4",
                    "name": "Ali Edalati",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e5",
                    "user": {
                        "_id": "66043fa65186899ca9c48ccd",
                        "avatarUrl": "/avatars/17c310802103c6b32c354acd9957cca0.svg",
                        "isPro": false,
                        "fullname": "Ace Eldeib",
                        "user": "ace-cohere",
                        "type": "user"
                    },
                    "name": "Ace Eldeib",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:06:02.364Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e6",
                    "name": "Abdullah Elkady",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e7",
                    "user": {
                        "_id": "6512cff997bb5c96ae6b30d9",
                        "avatarUrl": "/avatars/83cfc5f391cf515c8011e27618ac23e8.svg",
                        "isPro": false,
                        "fullname": "Sarah",
                        "user": "sarahelsharkawy",
                        "type": "user"
                    },
                    "name": "Sarah Elsharkawy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:06:13.645Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e8",
                    "user": {
                        "_id": "65c74cb0e2ba059be0024f6e",
                        "avatarUrl": "/avatars/a052488893789bc47f773f866a300ed8.svg",
                        "isPro": false,
                        "fullname": "Irem Ergun",
                        "user": "irombie",
                        "type": "user"
                    },
                    "name": "Irem Ergün",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:06:21.468Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1e9",
                    "user": {
                        "_id": "6634e8c750ea73bfbc59223e",
                        "avatarUrl": "/avatars/9f0a7bcdfd82d5344932437d458fce9f.svg",
                        "isPro": false,
                        "fullname": "Beyza Ermis",
                        "user": "beyzaermis",
                        "type": "user"
                    },
                    "name": "Beyza Ermis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:06:34.643Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ea",
                    "user": {
                        "_id": "6441042d5d600fb0951a5f99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6441042d5d600fb0951a5f99/4CbOaYcEz99BtVAQvnGTn.jpeg",
                        "isPro": false,
                        "fullname": "Marzieh Fadaee",
                        "user": "MarziehFadaee",
                        "type": "user"
                    },
                    "name": "Marzieh Fadaee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:06:42.634Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1eb",
                    "name": "Boyu Fan",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ec",
                    "name": "Lucas Fayoux",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ed",
                    "user": {
                        "_id": "64a5623d3e255e5c57dd5efb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/0ZtKZi2UevQ1AJlw73hq4.jpeg",
                        "isPro": false,
                        "fullname": "Yannis Flet-Berliac",
                        "user": "yfletberliac",
                        "type": "user"
                    },
                    "name": "Yannis Flet-Berliac",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T13:56:06.017Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ee",
                    "name": "Nick Frosst",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ef",
                    "name": "Matthias Gallé",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f0",
                    "name": "Wojciech Galuba",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f1",
                    "name": "Utsav Garg",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f2",
                    "name": "Matthieu Geist",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f3",
                    "user": {
                        "_id": "676a706791b7aeb8c48ce1be",
                        "avatarUrl": "/avatars/d08346049cd7972d18f66587f509a9af.svg",
                        "isPro": false,
                        "fullname": "MOHAMMAD GHESHLAGHI AZAR",
                        "user": "mgazar1981",
                        "type": "user"
                    },
                    "name": "Mohammad Gheshlaghi Azar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:11:56.631Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f4",
                    "user": {
                        "_id": "651ead3409474d05d0ccd999",
                        "avatarUrl": "/avatars/fe3f28408aed5024fa114c924a995048.svg",
                        "isPro": false,
                        "fullname": "Seraphina Goldfarb-Tarrant",
                        "user": "seraphinatarrant",
                        "type": "user"
                    },
                    "name": "Seraphina Goldfarb-Tarrant",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:10:36.798Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f5",
                    "user": {
                        "_id": "62975846efec96a3787652a9",
                        "avatarUrl": "/avatars/812edb4000fe8039a5b5bd978e55a831.svg",
                        "isPro": false,
                        "fullname": "Tomas Goldsack",
                        "user": "tomasg25",
                        "type": "user"
                    },
                    "name": "Tomas Goldsack",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:11:00.015Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f6",
                    "name": "Aidan Gomez",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f7",
                    "user": {
                        "_id": "63076ccacada14cdcede0639",
                        "avatarUrl": "/avatars/5e1482d4ad0ec4d78932f26bed56f14a.svg",
                        "isPro": false,
                        "fullname": "Victor Machado Gonzaga",
                        "user": "machadoprx",
                        "type": "user"
                    },
                    "name": "Victor Machado Gonzaga",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:11:13.905Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f8",
                    "name": "Nithya Govindarajan",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1f9",
                    "user": {
                        "_id": "65d7eaa2468a1810d4bc9c24",
                        "avatarUrl": "/avatars/4e00f98e7bb4fe4b35860b778ed6eec2.svg",
                        "isPro": false,
                        "fullname": "Manoj Govindassamy",
                        "user": "manojhf",
                        "type": "user"
                    },
                    "name": "Manoj Govindassamy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:11:26.029Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1fa",
                    "user": {
                        "_id": "64f3591b082e1bb944222d66",
                        "avatarUrl": "/avatars/704e903feacb92148a69005b310af448.svg",
                        "isPro": false,
                        "fullname": "Nathan Grinsztajn",
                        "user": "Ngrinsztajn",
                        "type": "user"
                    },
                    "name": "Nathan Grinsztajn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:11:33.344Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1fb",
                    "user": {
                        "_id": "664d0cb74b870dd1671283e4",
                        "avatarUrl": "/avatars/1ceb4821101455a386abbe1417e94ad9.svg",
                        "isPro": false,
                        "fullname": "Nikolas Gritsch",
                        "user": "ngritsch",
                        "type": "user"
                    },
                    "name": "Nikolas Gritsch",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:11:40.801Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1fc",
                    "name": "Patrick Gu",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1fd",
                    "name": "Shangmin Guo",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1fe",
                    "user": {
                        "_id": "664bb6dd07171c594ce65974",
                        "avatarUrl": "/avatars/7215dcdd62f8e90b44775efa1a2c2844.svg",
                        "isPro": false,
                        "fullname": "kilian haefeli",
                        "user": "kilianhaefeli",
                        "type": "user"
                    },
                    "name": "Kilian Haefeli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:12:24.681Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba1ff",
                    "name": "Rod Hajjar",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba200",
                    "name": "Tim Hawes",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba201",
                    "name": "Jingyi He",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba202",
                    "name": "Sebastian Hofstätter",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba203",
                    "name": "Sungjin Hong",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba204",
                    "name": "Sara Hooker",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba205",
                    "user": {
                        "_id": "6269415057cd241f40d6565c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6269415057cd241f40d6565c/ZJkTQDjU1dCejPu9BPfsB.jpeg",
                        "isPro": false,
                        "fullname": "Tom Hosking",
                        "user": "tomhosking",
                        "type": "user"
                    },
                    "name": "Tom Hosking",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T19:55:29.423Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba206",
                    "user": {
                        "_id": "67b78c5f6f352a33db67a50d",
                        "avatarUrl": "/avatars/89a10969f89c2cbcf74753d3d3384269.svg",
                        "isPro": false,
                        "fullname": "Stephanie Howe",
                        "user": "stephaniehowe",
                        "type": "user"
                    },
                    "name": "Stephanie Howe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:12:44.899Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba207",
                    "name": "Eric Hu",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba208",
                    "name": "Renjie Huang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba209",
                    "name": "Hemant Jain",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba20a",
                    "name": "Ritika Jain",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba20b",
                    "name": "Nick Jakobi",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba20c",
                    "name": "Madeline Jenkins",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba20d",
                    "name": "JJ Jordan",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba20e",
                    "user": {
                        "_id": "65f445a62e0dc61324fdc1dd",
                        "avatarUrl": "/avatars/93d4566aa2f2a886c7537281dd2801f5.svg",
                        "isPro": false,
                        "fullname": "Dhruti Joshi",
                        "user": "JoDhru",
                        "type": "user"
                    },
                    "name": "Dhruti Joshi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:12:54.955Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba20f",
                    "name": "Jason Jung",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba210",
                    "name": "Trushant Kalyanpur",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba211",
                    "name": "Siddhartha Rao Kamalakara",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba212",
                    "name": "Julia Kedrzycki",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba213",
                    "name": "Gokce Keskin",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba214",
                    "name": "Edward Kim",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba215",
                    "name": "Joon Kim",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba216",
                    "name": "Wei-Yin Ko",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba217",
                    "name": "Tom Kocmi",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba218",
                    "name": "Michael Kozakov",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba219",
                    "name": "Wojciech Kryściński",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba21a",
                    "name": "Arnav Kumar Jain",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba21b",
                    "user": {
                        "_id": "66e209b6f1af7fb64aed29f2",
                        "avatarUrl": "/avatars/18b674d24283f1a01a32fe4e3aaa6133.svg",
                        "isPro": false,
                        "fullname": "Komal Kumar Teru",
                        "user": "co-kkteru",
                        "type": "user"
                    },
                    "name": "Komal Kumar Teru",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:13:55.811Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba21c",
                    "user": {
                        "_id": "626f896cda2765b2f11b221b",
                        "avatarUrl": "/avatars/38d738b4c7b89ab44ad6c0f7d56fb2e4.svg",
                        "isPro": false,
                        "fullname": "Sander Land",
                        "user": "sanderland",
                        "type": "user"
                    },
                    "name": "Sander Land",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:14:12.456Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba21d",
                    "name": "Michael Lasby",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba21e",
                    "name": "Olivia Lasche",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba21f",
                    "name": "Justin Lee",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba220",
                    "name": "Patrick Lewis",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba221",
                    "name": "Jeffrey Li",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba222",
                    "name": "Jonathan Li",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba223",
                    "name": "Hangyu Lin",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba224",
                    "name": "Acyr Locatelli",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba225",
                    "name": "Kevin Luong",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba226",
                    "user": {
                        "_id": "659e9fa37014714c57f7ec78",
                        "avatarUrl": "/avatars/976b4e4212d5bba58550427d8efea4eb.svg",
                        "isPro": false,
                        "fullname": "Raymond Ma",
                        "user": "RaymondMa",
                        "type": "user"
                    },
                    "name": "Raymond Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:13:41.577Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba227",
                    "name": "Lukas Mach",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba228",
                    "name": "Marina Machado",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba229",
                    "name": "Joanne Magbitang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba22a",
                    "name": "Brenda Malacara Lopez",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba22b",
                    "name": "Aryan Mann",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba22c",
                    "name": "Kelly Marchisio",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba22d",
                    "name": "Olivia Markham",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba22e",
                    "user": {
                        "_id": "67ba63c7039a172a71fea9ad",
                        "avatarUrl": "/avatars/c7e23ce043126c41d0296f4e4d92333c.svg",
                        "isPro": false,
                        "fullname": "Alexandre Matton",
                        "user": "alex-matton-wf",
                        "type": "user"
                    },
                    "name": "Alexandre Matton",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:14:28.386Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba22f",
                    "user": {
                        "_id": "6079e94c88160e14e4e2e4b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666128776959-6079e94c88160e14e4e2e4b1.jpeg",
                        "isPro": false,
                        "fullname": "Alex McKinney",
                        "user": "afmck",
                        "type": "user"
                    },
                    "name": "Alex McKinney",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:17:48.376Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba230",
                    "user": {
                        "_id": "65ef9339b3be8515fbd0b3eb",
                        "avatarUrl": "/avatars/1d0c31ac86b3d66a5debd8eadb93ebbf.svg",
                        "isPro": false,
                        "fullname": "Dominic McLoughlin",
                        "user": "dom-mcloughlin",
                        "type": "user"
                    },
                    "name": "Dominic McLoughlin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T11:54:21.952Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba231",
                    "user": {
                        "_id": "66bdc372198f9d79f2b59a5a",
                        "avatarUrl": "/avatars/febcd63f0546149b0ab241d828ff5158.svg",
                        "isPro": false,
                        "fullname": "Jozef Mokry",
                        "user": "jozefcohere",
                        "type": "user"
                    },
                    "name": "Jozef Mokry",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:14:37.914Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba232",
                    "name": "Adrien Morisot",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba233",
                    "name": "Autumn Moulder",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba234",
                    "name": "Harry Moynehan",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba235",
                    "user": {
                        "_id": "660fc0931a6de3d8d2e04d0c",
                        "avatarUrl": "/avatars/43afbaf6ac9b9925b98b7359c7166a06.svg",
                        "isPro": false,
                        "fullname": "Maximilian Mozes",
                        "user": "mmozes",
                        "type": "user"
                    },
                    "name": "Maximilian Mozes",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:18:05.388Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba236",
                    "name": "Vivek Muppalla",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba237",
                    "name": "Lidiya Murakhovska",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba238",
                    "user": {
                        "_id": "65d933eee4d11d592e200ded",
                        "avatarUrl": "/avatars/8a70559c4eef72b10817c0f9d2d4b2e8.svg",
                        "isPro": false,
                        "fullname": "Hemangani Nagarajan",
                        "user": "Hemangani",
                        "type": "user"
                    },
                    "name": "Hemangani Nagarajan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:18:14.754Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba239",
                    "user": {
                        "_id": "657a40789ea9d52e5cb29ed8",
                        "avatarUrl": "/avatars/9b146fdcf57073a47725d8145600dc6f.svg",
                        "isPro": false,
                        "fullname": "Alekhya Nandula",
                        "user": "alunaan",
                        "type": "user"
                    },
                    "name": "Alekhya Nandula",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:18:34.733Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba23a",
                    "name": "Hisham Nasir",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba23b",
                    "name": "Shauna Nehra",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba23c",
                    "name": "Josh Netto-Rosen",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba23d",
                    "name": "Daniel Ohashi",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba23e",
                    "name": "James Owers-Bardsley",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba23f",
                    "name": "Jason Ozuzu",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba240",
                    "name": "Dennis Padilla",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba241",
                    "name": "Gloria Park",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba242",
                    "name": "Sam Passaglia",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba243",
                    "user": {
                        "_id": "60b5fb5cd34bdbe2cdbe7c8f",
                        "avatarUrl": "/avatars/28e95755107f8735ddf0232dfb284f37.svg",
                        "isPro": false,
                        "fullname": "Jeremy Pekmez",
                        "user": "jpekmez",
                        "type": "user"
                    },
                    "name": "Jeremy Pekmez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:19:46.629Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba244",
                    "name": "Laura Penstone",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba245",
                    "user": {
                        "_id": "5f5f4f093c67af20d9945a6c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1600081648206-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Aleksandra Piktus",
                        "user": "ola13",
                        "type": "user"
                    },
                    "name": "Aleksandra Piktus",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T19:55:27.087Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba246",
                    "name": "Case Ploeg",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba247",
                    "name": "Andrew Poulton",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba248",
                    "name": "Youran Qi",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba249",
                    "name": "Shubha Raghvendra",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba24a",
                    "name": "Miguel Ramos",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba24b",
                    "name": "Ekagra Ranjan",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba24c",
                    "user": {
                        "_id": "636eeea187545ca5a13bedbc",
                        "avatarUrl": "/avatars/cdbb5e0a700b60a43c99f93f8e9ef2b4.svg",
                        "isPro": false,
                        "fullname": "Pierre Richemond",
                        "user": "PierreRichemond",
                        "type": "user"
                    },
                    "name": "Pierre Richemond",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:21:09.039Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba24d",
                    "user": {
                        "_id": "660eccc9a556f0bb2e5e9c05",
                        "avatarUrl": "/avatars/f5868adef63adcb9a6cb5a26822e671b.svg",
                        "isPro": false,
                        "fullname": "Cécile Robert-Michon",
                        "user": "cecilerm",
                        "type": "user"
                    },
                    "name": "Cécile Robert-Michon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:19:23.073Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba24e",
                    "name": "Aurélien Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba24f",
                    "name": "Sudip Roy",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba250",
                    "name": "Laura Ruis",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba251",
                    "name": "Louise Rust",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba252",
                    "user": {
                        "_id": "679bfb534378dbc3579e2cb2",
                        "avatarUrl": "/avatars/a756b04aab0db710911bdfe676c9a5f9.svg",
                        "isPro": false,
                        "fullname": "Anubhav Sachan",
                        "user": "anubhav-cohere",
                        "type": "user"
                    },
                    "name": "Anubhav Sachan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:19:38.510Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba253",
                    "name": "Alejandro Salamanca",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba254",
                    "name": "Kailash Karthik Saravanakumar",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba255",
                    "name": "Isha Satyakam",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba256",
                    "user": {
                        "_id": "6553549b2c9a069eee144f71",
                        "avatarUrl": "/avatars/d3af587755659e8714334f6d81fcbccc.svg",
                        "isPro": false,
                        "fullname": "Alice Schoenauer Sebag",
                        "user": "aschose-c",
                        "type": "user"
                    },
                    "name": "Alice Schoenauer Sebag",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:18:58.962Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba257",
                    "name": "Priyanka Sen",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba258",
                    "name": "Sholeh Sepehri",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba259",
                    "name": "Preethi Seshadri",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba25a",
                    "name": "Ye Shen",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba25b",
                    "user": {
                        "_id": "5fae7fb3b8423e1d80b8a98f",
                        "avatarUrl": "/avatars/1c0575701529ae0f15a6fed6834a473c.svg",
                        "isPro": false,
                        "fullname": "Tom Sherborne",
                        "user": "tomsherborne",
                        "type": "user"
                    },
                    "name": "Tom Sherborne",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:20:42.073Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba25c",
                    "name": "Sylvie Chang Shi",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba25d",
                    "name": "Sanal Shivaprasad",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba25e",
                    "name": "Vladyslav Shmyhlo",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba25f",
                    "name": "Anirudh Shrinivason",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba260",
                    "name": "Inna Shteinbuk",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba261",
                    "name": "Amir Shukayev",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba262",
                    "name": "Mathieu Simard",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba263",
                    "name": "Ella Snyder",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba264",
                    "name": "Ava Spataru",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba265",
                    "name": "Victoria Spooner",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba266",
                    "name": "Trisha Starostina",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba267",
                    "name": "Florian Strub",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba268",
                    "name": "Yixuan Su",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba269",
                    "name": "Jimin Sun",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba26a",
                    "name": "Dwarak Talupuru",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba26b",
                    "name": "Eugene Tarassov",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba26c",
                    "name": "Elena Tommasone",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba26d",
                    "user": {
                        "_id": "65ef666b1c862f3aa0b242fb",
                        "avatarUrl": "/avatars/a1227b68ce38e9dfcaaf4e9f60de8831.svg",
                        "isPro": false,
                        "fullname": "Jennifer Tracey",
                        "user": "jt-cohere",
                        "type": "user"
                    },
                    "name": "Jennifer Tracey",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:20:15.888Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba26e",
                    "name": "Billy Trend",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba26f",
                    "name": "Evren Tumer",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba270",
                    "name": "Ahmet Üstün",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba271",
                    "name": "Bharat Venkitesh",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba272",
                    "name": "David Venuto",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba273",
                    "name": "Pat Verga",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba274",
                    "name": "Maxime Voisin",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba275",
                    "name": "Alex Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba276",
                    "name": "Donglu Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba277",
                    "name": "Shijian Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba278",
                    "name": "Edmond Wen",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba279",
                    "name": "Naomi White",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba27a",
                    "name": "Jesse Willman",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba27b",
                    "user": {
                        "_id": "620ce62d06a4320dbf3b50e2",
                        "avatarUrl": "/avatars/983c3a9b21e3841aac4957010b5c66ac.svg",
                        "isPro": false,
                        "fullname": "Marysia Winkels",
                        "user": "marysiaw",
                        "type": "user"
                    },
                    "name": "Marysia Winkels",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:21:30.519Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba27c",
                    "name": "Chen Xia",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba27d",
                    "name": "Jessica Xie",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba27e",
                    "name": "Minjie Xu",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba27f",
                    "name": "Bowen Yang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba280",
                    "name": "Tan Yi-Chern",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba281",
                    "name": "Ivan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba282",
                    "name": "Zhenyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ecc2eb077a9bc63e7ba283",
                    "name": "Zhoujie Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T12:08:07.000Z",
            "submittedOnDailyAt": "2025-04-02T03:24:20.179Z",
            "title": "Command A: An Enterprise-Ready Large Language Model",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
            "upvotes": 17,
            "discussionId": "67ecc2ec077a9bc63e7ba2bd",
            "ai_keywords": [
                "agent-optimised",
                "multilingual-capable",
                "hybrid architecture",
                "Retrieval Augmented Generation (RAG)",
                "grounding",
                "tool use",
                "decentralised training",
                "self-refinement algorithms",
                "model merging techniques"
            ]
        },
        "publishedAt": "2025-04-01T08:08:07.000Z",
        "title": "Command A: An Enterprise-Ready Large Language Model",
        "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00698.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01016",
            "authors": [
                {
                    "_id": "67ec958ebb1d6dd924f94a31",
                    "user": {
                        "_id": "65f8e4778dc7bb5b4db97f92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gBmQdovmANmjV72k3gaW8.png",
                        "isPro": false,
                        "fullname": "Tian-Xing Xu",
                        "user": "slothfulxtx",
                        "type": "user"
                    },
                    "name": "Tian-Xing Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:51:06.226Z",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a32",
                    "user": {
                        "_id": "64c0953a8137192a1e2474dc",
                        "avatarUrl": "/avatars/546405a7eaf2f60ad108ceaa0dda7d08.svg",
                        "isPro": false,
                        "fullname": "xiangjun gao",
                        "user": "xiangjun0211",
                        "type": "user"
                    },
                    "name": "Xiangjun Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:51:14.978Z",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a33",
                    "user": {
                        "_id": "657a7458afbb0117ba15c59f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                        "isPro": false,
                        "fullname": "Wenbo Hu",
                        "user": "wbhu-tc",
                        "type": "user"
                    },
                    "name": "Wenbo Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:52.313Z",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a34",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a35",
                    "name": "Song-Hai Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ec958ebb1d6dd924f94a36",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:51:45.869Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
            ],
            "publishedAt": "2025-04-01T17:58:03.000Z",
            "submittedOnDailyAt": "2025-04-02T00:15:17.585Z",
            "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
            "submittedOnDailyBy": {
                "_id": "657a7458afbb0117ba15c59f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                "isPro": false,
                "fullname": "Wenbo Hu",
                "user": "wbhu-tc",
                "type": "user"
            },
            "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
            "upvotes": 15,
            "discussionId": "67ec9593bb1d6dd924f94b3e",
            "projectPage": "https://geometrycrafter.github.io/",
            "githubRepo": "https://github.com/TencentARC/GeometryCrafter",
            "ai_keywords": [
                "GeometryCrafter",
                "point map Variational Autoencoder (VAE)",
                "latent space",
                "video latent distributions",
                "point map encoding",
                "point map decoding",
                "video diffusion model",
                "point map sequences",
                "3D accuracy",
                "temporal consistency",
                "generalization capability"
            ]
        },
        "publishedAt": "2025-04-01T13:58:03.000Z",
        "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
        "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01016.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "fullname": "Wenbo Hu",
            "name": "wbhu-tc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00906",
            "authors": [
                {
                    "_id": "67ec9340913c072638c16bbf",
                    "user": {
                        "_id": "627837664f2afdc41bbd622a",
                        "avatarUrl": "/avatars/b7bfc4fb77830bba71839c04a4aeea64.svg",
                        "isPro": false,
                        "fullname": "Saaket Agashe",
                        "user": "saa1605",
                        "type": "user"
                    },
                    "name": "Saaket Agashe",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:49:55.477Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9340913c072638c16bc0",
                    "name": "Kyle Wong",
                    "hidden": false
                },
                {
                    "_id": "67ec9340913c072638c16bc1",
                    "name": "Vincent Tu",
                    "hidden": false
                },
                {
                    "_id": "67ec9340913c072638c16bc2",
                    "user": {
                        "_id": "65edff6233c279253952e0bd",
                        "avatarUrl": "/avatars/3130f6f9873ae0a943631e56f6d8d341.svg",
                        "isPro": false,
                        "fullname": "Jiachen Yang",
                        "user": "jc-y42",
                        "type": "user"
                    },
                    "name": "Jiachen Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:46:55.173Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9340913c072638c16bc3",
                    "name": "Ang Li",
                    "hidden": false
                },
                {
                    "_id": "67ec9340913c072638c16bc4",
                    "user": {
                        "_id": "64679a226192d39142245e5e",
                        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                        "isPro": false,
                        "fullname": "Xin Eric Wang",
                        "user": "xw-eric",
                        "type": "user"
                    },
                    "name": "Xin Eric Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:46:30.920Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T15:40:27.000Z",
            "submittedOnDailyAt": "2025-04-02T00:02:12.706Z",
            "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
            "submittedOnDailyBy": {
                "_id": "64679a226192d39142245e5e",
                "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
                "isPro": false,
                "fullname": "Xin Eric Wang",
                "user": "xw-eric",
                "type": "user"
            },
            "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
            "upvotes": 15,
            "discussionId": "67ec9343913c072638c16c5f",
            "projectPage": "https://www.simular.ai/articles/agent-s2-technical-review",
            "githubRepo": "https://github.com/simular-ai/Agent-S",
            "ai_keywords": [
                "Mixture-of-Grounding",
                "Proactive Hierarchical Planning",
                "compositional framework",
                "GUI localization",
                "action plans",
                "state-of-the-art (SOTA) performance",
                "OSWorld",
                "WindowsAgentArena",
                "AndroidWorld"
            ]
        },
        "publishedAt": "2025-04-01T11:40:27.000Z",
        "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
        "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00906.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64679a226192d39142245e5e",
            "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
            "fullname": "Xin Eric Wang",
            "name": "xw-eric",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23434",
            "authors": [
                {
                    "_id": "67ecf47dd0f4d6684e0fb7e4",
                    "user": {
                        "_id": "64beb6b6140491ca9f803ebf",
                        "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
                        "isPro": false,
                        "fullname": "Yucheng SHi",
                        "user": "YuchengShi",
                        "type": "user"
                    },
                    "name": "Yucheng Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:28:32.523Z",
                    "hidden": false
                },
                {
                    "_id": "67ecf47dd0f4d6684e0fb7e5",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "67ecf47dd0f4d6684e0fb7e6",
                    "user": {
                        "_id": "634f18e4aae4bde2c8e2adca",
                        "avatarUrl": "/avatars/40549a59fc5ba04a4baa5a1d5dba0847.svg",
                        "isPro": false,
                        "fullname": "Wenlin Yao",
                        "user": "wenlinyao",
                        "type": "user"
                    },
                    "name": "Wenlin Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:28:50.560Z",
                    "hidden": false
                },
                {
                    "_id": "67ecf47dd0f4d6684e0fb7e7",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:28:56.387Z",
                    "hidden": false
                },
                {
                    "_id": "67ecf47dd0f4d6684e0fb7e8",
                    "user": {
                        "_id": "67baa6c519e9dba50ece56b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/F3ZmRr7nCzfSVbpS4SY1E.png",
                        "isPro": false,
                        "fullname": "Ninghao Liu",
                        "user": "NeoLiu43",
                        "type": "user"
                    },
                    "name": "Ninghao Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:29:04.216Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T13:26:00.000Z",
            "submittedOnDailyAt": "2025-04-02T06:59:17.200Z",
            "title": "Towards Trustworthy GUI Agents: A Survey",
            "submittedOnDailyBy": {
                "_id": "64beb6b6140491ca9f803ebf",
                "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
                "isPro": false,
                "fullname": "Yucheng SHi",
                "user": "YuchengShi",
                "type": "user"
            },
            "summary": "GUI agents, powered by large foundation models, can interact with digital\ninterfaces, enabling various applications in web automation, mobile navigation,\nand software testing. However, their increasing autonomy has raised critical\nconcerns about their security, privacy, and safety. This survey examines the\ntrustworthiness of GUI agents in five critical dimensions: security\nvulnerabilities, reliability in dynamic environments, transparency and\nexplainability, ethical considerations, and evaluation methodologies. We also\nidentify major challenges such as vulnerability to adversarial attacks,\ncascading failure modes in sequential decision-making, and a lack of realistic\nevaluation benchmarks. These issues not only hinder real-world deployment but\nalso call for comprehensive mitigation strategies beyond task success. As GUI\nagents become more widespread, establishing robust safety standards and\nresponsible development practices is essential. This survey provides a\nfoundation for advancing trustworthy GUI agents through systematic\nunderstanding and future research.",
            "upvotes": 15,
            "discussionId": "67ecf480d0f4d6684e0fb855",
            "githubRepo": "https://github.com/sycny/Awesome-Trustworthy-GUI-Agents",
            "ai_keywords": [
                "foundation models",
                "GUI agents",
                "web automation",
                "mobile navigation",
                "software testing",
                "security vulnerabilities",
                "reliability in dynamic environments",
                "transparency and explainability",
                "ethical considerations",
                "evaluation methodologies",
                "adversarial attacks",
                "cascading failure modes",
                "sequential decision-making",
                "realistic evaluation benchmarks"
            ]
        },
        "publishedAt": "2025-03-30T09:26:00.000Z",
        "title": "Towards Trustworthy GUI Agents: A Survey",
        "summary": "GUI agents, powered by large foundation models, can interact with digital\ninterfaces, enabling various applications in web automation, mobile navigation,\nand software testing. However, their increasing autonomy has raised critical\nconcerns about their security, privacy, and safety. This survey examines the\ntrustworthiness of GUI agents in five critical dimensions: security\nvulnerabilities, reliability in dynamic environments, transparency and\nexplainability, ethical considerations, and evaluation methodologies. We also\nidentify major challenges such as vulnerability to adversarial attacks,\ncascading failure modes in sequential decision-making, and a lack of realistic\nevaluation benchmarks. These issues not only hinder real-world deployment but\nalso call for comprehensive mitigation strategies beyond task success. As GUI\nagents become more widespread, establishing robust safety standards and\nresponsible development practices is essential. This survey provides a\nfoundation for advancing trustworthy GUI agents through systematic\nunderstanding and future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23434.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64beb6b6140491ca9f803ebf",
            "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
            "fullname": "Yucheng SHi",
            "name": "YuchengShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00927",
            "authors": [
                {
                    "_id": "67ecc23b6a2bc6abdc2e9183",
                    "user": {
                        "_id": "6318b1e06e6bd039440871d6",
                        "avatarUrl": "/avatars/2d9f96eb4092a9d1734dcc09303b259a.svg",
                        "isPro": false,
                        "fullname": "Olga Golovneva",
                        "user": "Golovneva",
                        "type": "user"
                    },
                    "name": "Olga Golovneva",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:56:07.426Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc23b6a2bc6abdc2e9184",
                    "user": {
                        "_id": "63f6a0e6b4c9a104f4b95d86",
                        "avatarUrl": "/avatars/660c3e44bea976f3e0560bf95466b03c.svg",
                        "isPro": false,
                        "fullname": "Tianlu Wang",
                        "user": "Tianlu",
                        "type": "user"
                    },
                    "name": "Tianlu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:56:14.148Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc23b6a2bc6abdc2e9185",
                    "user": {
                        "_id": "62f023a36a027498eaa2f9cc",
                        "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                        "isPro": false,
                        "fullname": "Jason Weston",
                        "user": "spermwhale",
                        "type": "user"
                    },
                    "name": "Jason Weston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:56:21.002Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc23b6a2bc6abdc2e9186",
                    "user": {
                        "_id": "66a8611eb51510d82ed54231",
                        "avatarUrl": "/avatars/ad559e774fee4914091b82c9831ae2a2.svg",
                        "isPro": false,
                        "fullname": "Sainbayar Sukhbaatar",
                        "user": "sainbar",
                        "type": "user"
                    },
                    "name": "Sainbayar Sukhbaatar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:56:26.631Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T15:59:32.000Z",
            "submittedOnDailyAt": "2025-04-02T03:21:25.809Z",
            "title": "Multi-Token Attention",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
            "upvotes": 14,
            "discussionId": "67ecc23c6a2bc6abdc2e91c2",
            "ai_keywords": [
                "Soft attention",
                "LLMs (Large Language Models)",
                "single token attention",
                "Multi-Token Attention (MTA)",
                "convolution operations",
                "queries",
                "keys",
                "heads",
                "attention weights"
            ]
        },
        "publishedAt": "2025-04-01T11:59:32.000Z",
        "title": "Multi-Token Attention",
        "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00927.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.01019",
            "authors": [
                {
                    "_id": "67ecd893553eb1c01a311882",
                    "user": {
                        "_id": "62ace9bc717ee4c12b72e275",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
                        "isPro": false,
                        "fullname": "Pablo Ruiz-Ponce",
                        "user": "pabloruizponce",
                        "type": "user"
                    },
                    "name": "Pablo Ruiz-Ponce",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:21:16.654Z",
                    "hidden": false
                },
                {
                    "_id": "67ecd893553eb1c01a311883",
                    "user": {
                        "_id": "64fac0f3b961d0d12c756b59",
                        "avatarUrl": "/avatars/27cc7dbad927df818ba2f91a5b6942f9.svg",
                        "isPro": false,
                        "fullname": "German Barquero",
                        "user": "Germs96",
                        "type": "user"
                    },
                    "name": "German Barquero",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:53:49.237Z",
                    "hidden": false
                },
                {
                    "_id": "67ecd893553eb1c01a311884",
                    "name": "Cristina Palmero",
                    "hidden": false
                },
                {
                    "_id": "67ecd893553eb1c01a311885",
                    "name": "Sergio Escalera",
                    "hidden": false
                },
                {
                    "_id": "67ecd893553eb1c01a311886",
                    "name": "José García-Rodríguez",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
            ],
            "publishedAt": "2025-04-01T17:59:44.000Z",
            "submittedOnDailyAt": "2025-04-02T05:06:30.372Z",
            "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "62ace9bc717ee4c12b72e275",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
                "isPro": false,
                "fullname": "Pablo Ruiz-Ponce",
                "user": "pabloruizponce",
                "type": "user"
            },
            "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.",
            "upvotes": 13,
            "discussionId": "67ecd894553eb1c01a3118df",
            "projectPage": "https://www.pabloruizponce.com/papers/MixerMDM",
            "githubRepo": "https://github.com/pabloruizponce/MixerMDM",
            "ai_keywords": [
                "motion diffusion models",
                "text-conditioned",
                "learnable model composition",
                "dynamic mixing strategy",
                "adversarial fashion",
                "denoising process",
                "fine-grained control",
                "single-person",
                "multi-person",
                "interaction",
                "evaluation technique",
                "alignment between generated motions and conditions"
            ]
        },
        "publishedAt": "2025-04-01T13:59:44.000Z",
        "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
        "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01019.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ace9bc717ee4c12b72e275",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
            "fullname": "Pablo Ruiz-Ponce",
            "name": "pabloruizponce",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00509",
            "authors": [
                {
                    "_id": "67ecae49a34baf018ca3c4cd",
                    "user": {
                        "_id": "65de7628deee79773f0f46f6",
                        "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
                        "isPro": false,
                        "fullname": "Kai Yan",
                        "user": "kaiyan289",
                        "type": "user"
                    },
                    "name": "Kai Yan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-02T03:26:02.443Z",
                    "hidden": false
                },
                {
                    "_id": "67ecae49a34baf018ca3c4ce",
                    "user": {
                        "_id": "642957104e073875f6a5ddd0",
                        "avatarUrl": "/avatars/32b706d35c5ff52932c5029b94caa7b9.svg",
                        "isPro": false,
                        "fullname": "Yufei Xu",
                        "user": "yfxu",
                        "type": "user"
                    },
                    "name": "Yufei Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:23.307Z",
                    "hidden": false
                },
                {
                    "_id": "67ecae49a34baf018ca3c4cf",
                    "user": {
                        "_id": "67978efb9d331acb4022b3c9",
                        "avatarUrl": "/avatars/9b6f3f75b10820284ada99ab970d6e96.svg",
                        "isPro": false,
                        "fullname": "DuzHENGYIN",
                        "user": "ALEXoDu",
                        "type": "user"
                    },
                    "name": "Zhengyin Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T10:17:39.401Z",
                    "hidden": false
                },
                {
                    "_id": "67ecae49a34baf018ca3c4d0",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "67ecae49a34baf018ca3c4d1",
                    "name": "Zheyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecae49a34baf018ca3c4d2",
                    "name": "Xiaowen Guo",
                    "hidden": false
                },
                {
                    "_id": "67ecae49a34baf018ca3c4d3",
                    "user": {
                        "_id": "66df70fe5a0c5910d663160d",
                        "avatarUrl": "/avatars/980ca32bd0049ef5bbf002e7dc9f911c.svg",
                        "isPro": false,
                        "fullname": "jiecao.chen",
                        "user": "xmerge123",
                        "type": "user"
                    },
                    "name": "Jiecao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T10:16:06.617Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T07:57:58.000Z",
            "submittedOnDailyAt": "2025-04-02T01:56:35.333Z",
            "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
            "upvotes": 12,
            "discussionId": "67ecae4aa34baf018ca3c506",
            "projectPage": "https://team.doubao.com/zh/publication/recitation-over-reasoning-how-cutting-edge-language-models-can-fail-on-elementary-school-level-reasoning-problems?view_from=homepage_recommend",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "RoR-Bench",
                "multi-modal benchmark",
                "recitation behavior",
                "elementary school-level arithmetic",
                "reasoning problems",
                "OpenAI-o1",
                "DeepSeek-R1"
            ]
        },
        "publishedAt": "2025-04-01T03:57:58.000Z",
        "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
        "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00509.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24377",
            "authors": [
                {
                    "_id": "67eca439a62c82ed64354e36",
                    "user": {
                        "_id": "67298b338c66e235932ca088",
                        "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
                        "isPro": false,
                        "fullname": "WANG Rui",
                        "user": "Ray121381",
                        "type": "user"
                    },
                    "name": "Rui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:52:11.104Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e37",
                    "user": {
                        "_id": "65f906e5c3dbdcae83ff7aac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                        "isPro": false,
                        "fullname": "Hongru Wang",
                        "user": "Merlin-Hongru",
                        "type": "user"
                    },
                    "name": "Hongru Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:52:28.243Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e38",
                    "user": {
                        "_id": "66ab39295558689cb8676559",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/La5iNOHELvZ2peByiakqe.png",
                        "isPro": false,
                        "fullname": "XUE Boyang",
                        "user": "BeyondHsueh",
                        "type": "user"
                    },
                    "name": "Boyang Xue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:52:39.163Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e39",
                    "user": {
                        "_id": "64a3d40815655921915b8ce2",
                        "avatarUrl": "/avatars/6b6b550d96be4a6473e2ccf74df438f7.svg",
                        "isPro": false,
                        "fullname": "Jianhuipang",
                        "user": "pangjh3",
                        "type": "user"
                    },
                    "name": "Jianhui Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:52:45.274Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e3a",
                    "user": {
                        "_id": "654ce87af0b05673196a9f45",
                        "avatarUrl": "/avatars/7b9c854eb98e487e3057479b1c7860ac.svg",
                        "isPro": false,
                        "fullname": "Shudong Liu",
                        "user": "Sudanl",
                        "type": "user"
                    },
                    "name": "Shudong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:52:51.029Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e3b",
                    "user": {
                        "_id": "609fee945ad152dff7bb3b77",
                        "avatarUrl": "/avatars/3694f5affe50ace501df7191e1b952d4.svg",
                        "isPro": false,
                        "fullname": "Yichen",
                        "user": "Yichen",
                        "type": "user"
                    },
                    "name": "Yi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:52:58.486Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e3c",
                    "name": "Jiahao Qiu",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e3d",
                    "user": {
                        "_id": "648bd523805e4bcc541ec320",
                        "avatarUrl": "/avatars/443ca0c7cbeda3a08eb4af6a0e2da8bc.svg",
                        "isPro": false,
                        "fullname": "Derek Wong",
                        "user": "derekfw",
                        "type": "user"
                    },
                    "name": "Derek Fai Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T09:53:28.254Z",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e3e",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "67eca439a62c82ed64354e3f",
                    "name": "Kam-Fai Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:58:07.000Z",
            "submittedOnDailyAt": "2025-04-02T01:17:01.812Z",
            "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "67298b338c66e235932ca088",
                "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
                "isPro": false,
                "fullname": "WANG Rui",
                "user": "Ray121381",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
            "upvotes": 11,
            "discussionId": "67eca43aa62c82ed64354e82",
            "githubRepo": "https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers"
        },
        "publishedAt": "2025-03-31T13:58:07.000Z",
        "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
        "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24377.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67298b338c66e235932ca088",
            "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
            "fullname": "WANG Rui",
            "name": "Ray121381",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22952",
            "authors": [
                {
                    "_id": "67eb58e5969b278277adb831",
                    "user": {
                        "_id": "60b9e6837946aff342f734ae",
                        "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
                        "isPro": false,
                        "fullname": "Yuxuan Wang",
                        "user": "ColorfulAI",
                        "type": "user"
                    },
                    "name": "Yuxuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T09:42:25.151Z",
                    "hidden": false
                },
                {
                    "_id": "67eb58e5969b278277adb832",
                    "name": "Yueqian Wang",
                    "hidden": false
                },
                {
                    "_id": "67eb58e5969b278277adb833",
                    "name": "Bo Chen",
                    "hidden": false
                },
                {
                    "_id": "67eb58e5969b278277adb834",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "67eb58e5969b278277adb835",
                    "name": "Dongyan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67eb58e5969b278277adb836",
                    "user": {
                        "_id": "63a95a6a7930fa8c7dd63d4e",
                        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
                        "isPro": false,
                        "fullname": "Zilong Zheng",
                        "user": "zlzheng",
                        "type": "user"
                    },
                    "name": "Zilong Zheng",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-01T03:09:26.871Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T02:46:58.000Z",
            "submittedOnDailyAt": "2025-04-02T07:32:05.470Z",
            "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts",
            "submittedOnDailyBy": {
                "_id": "60b9e6837946aff342f734ae",
                "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
                "isPro": false,
                "fullname": "Yuxuan Wang",
                "user": "ColorfulAI",
                "type": "user"
            },
            "summary": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating.",
            "upvotes": 11,
            "discussionId": "67eb58e6969b278277adb887",
            "ai_keywords": [
                "multi-modal language models",
                "GPT-4o",
                "Omni language models",
                "streaming video contexts",
                "OmniMMI",
                "multi-modal interaction benchmark",
                "streaming video understanding",
                "proactive reasoning",
                "Multi-modal Multiplexing Modeling",
                "inference-efficient streaming model"
            ]
        },
        "publishedAt": "2025-03-28T22:46:58.000Z",
        "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts",
        "summary": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22952.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60b9e6837946aff342f734ae",
            "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
            "fullname": "Yuxuan Wang",
            "name": "ColorfulAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01017",
            "authors": [
                {
                    "_id": "67ecebc51f669fb5591616cd",
                    "user": {
                        "_id": "66e9df8c38f5bae5a91785ca",
                        "avatarUrl": "/avatars/6f49429fc34553903309872ec6f122b8.svg",
                        "isPro": false,
                        "fullname": "David Fan",
                        "user": "davidfan97",
                        "type": "user"
                    },
                    "name": "David Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:18:13.746Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616ce",
                    "user": {
                        "_id": "6374cbb7255276f3a22b4b35",
                        "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
                        "isPro": true,
                        "fullname": "Peter Tong",
                        "user": "tsbpp",
                        "type": "user"
                    },
                    "name": "Shengbang Tong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:18:31.173Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616cf",
                    "user": {
                        "_id": "6552126dd8a8835b66653767",
                        "avatarUrl": "/avatars/0b1dad9ebaeada8f5e7ebe453123960b.svg",
                        "isPro": false,
                        "fullname": "Jiachen Zhu",
                        "user": "JiachenZhu",
                        "type": "user"
                    },
                    "name": "Jiachen Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:54:32.112Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d0",
                    "user": {
                        "_id": "622b93067b9143726fbedc37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg",
                        "isPro": false,
                        "fullname": "Koustuv Sinha",
                        "user": "koustuvs",
                        "type": "user"
                    },
                    "name": "Koustuv Sinha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:54:39.909Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d1",
                    "user": {
                        "_id": "647d3a9383cdb93baf76dce2",
                        "avatarUrl": "/avatars/56efca912a66ef9d4f2f3679aac31bae.svg",
                        "isPro": false,
                        "fullname": "Zhuang Liu",
                        "user": "liuzhuang13",
                        "type": "user"
                    },
                    "name": "Zhuang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:55:01.118Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d2",
                    "user": {
                        "_id": "63e58e3a006a775275e59e41",
                        "avatarUrl": "/avatars/75262a35b27a2ae1939df9118120d99e.svg",
                        "isPro": false,
                        "fullname": "Xinlei Chen",
                        "user": "endernewton",
                        "type": "user"
                    },
                    "name": "Xinlei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:55:07.362Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d3",
                    "name": "Michael Rabbat",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d4",
                    "name": "Nicolas Ballas",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d5",
                    "user": {
                        "_id": "64ed0b8c2203a126eb1a5b9a",
                        "avatarUrl": "/avatars/9156dc406ed3f9ee62b73657ac20f5ed.svg",
                        "isPro": false,
                        "fullname": "Yann LeCun",
                        "user": "ylecun",
                        "type": "user"
                    },
                    "name": "Yann LeCun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:55:22.112Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d6",
                    "user": {
                        "_id": "632bad473690fb57e70c9d42",
                        "avatarUrl": "/avatars/8c09bc614a7e62a7bd30c4cbb97d4122.svg",
                        "isPro": false,
                        "fullname": "Amir Bar",
                        "user": "amirbar1",
                        "type": "user"
                    },
                    "name": "Amir Bar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:55:47.451Z",
                    "hidden": false
                },
                {
                    "_id": "67ecebc51f669fb5591616d7",
                    "user": {
                        "_id": "6596422646624a86ff3b3bda",
                        "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
                        "isPro": false,
                        "fullname": "Saining Xie",
                        "user": "sainx",
                        "type": "user"
                    },
                    "name": "Saining Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:55:54.586Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T17:59:15.000Z",
            "submittedOnDailyAt": "2025-04-02T06:19:35.566Z",
            "title": "Scaling Language-Free Visual Representation Learning",
            "submittedOnDailyBy": {
                "_id": "6374cbb7255276f3a22b4b35",
                "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
                "isPro": true,
                "fullname": "Peter Tong",
                "user": "tsbpp",
                "type": "user"
            },
            "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL and CLIP\nmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behind CLIP due to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL and CLIP models on the same MetaCLIP data,\nand leveraging VQA as a diverse testbed for vision encoders. In this controlled\nsetup, visual SSL models scale better than CLIP models in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieve\nCLIP-level performance on a wide range of VQA and classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning.",
            "upvotes": 10,
            "discussionId": "67ecebc61f669fb559161742",
            "ai_keywords": [
                "Visual Self-Supervised Learning (SSL)",
                "Contrastive Language-Image Pretraining (CLIP)",
                "Visual Question Answering (VQA)",
                "MetaCLIP data",
                "vision encoders",
                "data capacity",
                "CLIP-level performance",
                "vision-centric representation learning"
            ]
        },
        "publishedAt": "2025-04-01T13:59:15.000Z",
        "title": "Scaling Language-Free Visual Representation Learning",
        "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL and CLIP\nmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behind CLIP due to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL and CLIP models on the same MetaCLIP data,\nand leveraging VQA as a diverse testbed for vision encoders. In this controlled\nsetup, visual SSL models scale better than CLIP models in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieve\nCLIP-level performance on a wide range of VQA and classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01017.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6374cbb7255276f3a22b4b35",
            "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
            "fullname": "Peter Tong",
            "name": "tsbpp",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01005",
            "authors": [
                {
                    "_id": "67ec95ea3d267d26663ea34b",
                    "user": {
                        "_id": "641f72d8eefe94aff6e21068",
                        "avatarUrl": "/avatars/0f07cffb49698232c4bd912898abc920.svg",
                        "isPro": false,
                        "fullname": "Nishad Singhi",
                        "user": "nishadsinghi",
                        "type": "user"
                    },
                    "name": "Nishad Singhi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T13:56:07.949Z",
                    "hidden": false
                },
                {
                    "_id": "67ec95ea3d267d26663ea34c",
                    "user": {
                        "_id": "61c5c25705aa54027c52f7b3",
                        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
                        "isPro": false,
                        "fullname": "Hritik Bansal",
                        "user": "hbXNov",
                        "type": "user"
                    },
                    "name": "Hritik Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:16:19.164Z",
                    "hidden": false
                },
                {
                    "_id": "67ec95ea3d267d26663ea34d",
                    "user": {
                        "_id": "62e447e9640206c5d21e5ff6",
                        "avatarUrl": "/avatars/42d8247c2ffa98a4c6255ce84051f1b9.svg",
                        "isPro": false,
                        "fullname": "Arian Hosseini",
                        "user": "arianhosseini",
                        "type": "user"
                    },
                    "name": "Arian Hosseini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:17:10.400Z",
                    "hidden": false
                },
                {
                    "_id": "67ec95ea3d267d26663ea34e",
                    "user": {
                        "_id": "65d38c1e5d5749637491679b",
                        "avatarUrl": "/avatars/010a6588f382d6d4394216549a2385e3.svg",
                        "isPro": false,
                        "fullname": "Aditya Grover",
                        "user": "adityagrover",
                        "type": "user"
                    },
                    "name": "Aditya Grover",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:17:22.994Z",
                    "hidden": false
                },
                {
                    "_id": "67ec95ea3d267d26663ea34f",
                    "user": {
                        "_id": "60b7b9d71b90c5d07c23fbd0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1622653364258-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai-Wei Chang",
                        "user": "kaiweichang",
                        "type": "user"
                    },
                    "name": "Kai-Wei Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:17:29.569Z",
                    "hidden": false
                },
                {
                    "_id": "67ec95ea3d267d26663ea350",
                    "name": "Marcus Rohrbach",
                    "hidden": false
                },
                {
                    "_id": "67ec95ea3d267d26663ea351",
                    "user": {
                        "_id": "6397ca710bdcc7e3216f3c49",
                        "avatarUrl": "/avatars/51ad23a000eaf817c2e47149810b0156.svg",
                        "isPro": false,
                        "fullname": "Anna Rohrbach",
                        "user": "anna-rohrbach",
                        "type": "user"
                    },
                    "name": "Anna Rohrbach",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T11:16:35.830Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T17:41:57.000Z",
            "submittedOnDailyAt": "2025-04-02T00:12:49.083Z",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "61c5c25705aa54027c52f7b3",
                "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
                "isPro": false,
                "fullname": "Hritik Bansal",
                "user": "hbXNov",
                "type": "user"
            },
            "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "upvotes": 10,
            "discussionId": "67ec95eb3d267d26663ea38b",
            "ai_keywords": [
                "Large language models (LLMs)",
                "Mathematical problem-solving",
                "Self-Consistency (SC)",
                "Reward model (verifier)",
                "Generative Reward Models (GenRM)",
                "Next-token prediction task",
                "Chains-of-thought",
                "Inference budget",
                "Compute-efficient",
                "Inference scaling laws",
                "Compute-optimal inference"
            ]
        },
        "publishedAt": "2025-04-01T13:41:57.000Z",
        "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
        "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01005.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61c5c25705aa54027c52f7b3",
            "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
            "fullname": "Hritik Bansal",
            "name": "hbXNov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.00557",
            "authors": [
                {
                    "_id": "67eca9a501a4d1c29e4e70f1",
                    "user": {
                        "_id": "642d395784bf892b8fa91816",
                        "avatarUrl": "/avatars/2db2202641ad7ed9aeaea7e8bfc3acca.svg",
                        "isPro": false,
                        "fullname": "Jewon Lee",
                        "user": "je1lee",
                        "type": "user"
                    },
                    "name": "Jewon Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:22:32.687Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f2",
                    "user": {
                        "_id": "645f6407b5a2706bce578eed",
                        "avatarUrl": "/avatars/8f576ebd8b7c89578e083ccabf6ebc33.svg",
                        "isPro": false,
                        "fullname": "Ki-Ung song",
                        "user": "sk851",
                        "type": "user"
                    },
                    "name": "Ki-Ung Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:22:38.520Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f3",
                    "user": {
                        "_id": "679cedca09f47bf5c1ca251f",
                        "avatarUrl": "/avatars/aa8d6cc5f2993849b7f7c16d2cb1c8f6.svg",
                        "isPro": false,
                        "fullname": "Seungmin Yang",
                        "user": "ainoobchobo",
                        "type": "user"
                    },
                    "name": "Seungmin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:22:46.031Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f4",
                    "user": {
                        "_id": "62e7773c870b23dac1b944e2",
                        "avatarUrl": "/avatars/919ad4150b0b0aa58bc49651b4911c53.svg",
                        "isPro": false,
                        "fullname": "Lim",
                        "user": "DongUK",
                        "type": "user"
                    },
                    "name": "Donguk Lim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:22:55.363Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f5",
                    "user": {
                        "_id": "6548c84e10d47c1f771f183a",
                        "avatarUrl": "/avatars/37d759063d870e2fe0df926efebff750.svg",
                        "isPro": false,
                        "fullname": "Jaeyeon Kim",
                        "user": "jaeyeonkim99",
                        "type": "user"
                    },
                    "name": "Jaeyeon Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:23:09.888Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f6",
                    "user": {
                        "_id": "67a0a0d48f047b67c3172e9e",
                        "avatarUrl": "/avatars/fa2ee986071f3642c8c7cd6ff693dbfd.svg",
                        "isPro": false,
                        "fullname": "Wooksu Shin",
                        "user": "dnrtn1101",
                        "type": "user"
                    },
                    "name": "Wooksu Shin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:23:19.454Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f7",
                    "user": {
                        "_id": "64374f10cd93f4c9a34c1239",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64374f10cd93f4c9a34c1239/tdkdcgl18rKXLvyTEhfnG.jpeg",
                        "isPro": false,
                        "fullname": "Bo-Kyeong Kim",
                        "user": "bokyeong1015",
                        "type": "user"
                    },
                    "name": "Bo-Kyeong Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:23:24.647Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f8",
                    "user": {
                        "_id": "649f41ee70a478f8b36b2984",
                        "avatarUrl": "/avatars/1c9a76717a450ac4aeb62a1e823d2e4a.svg",
                        "isPro": false,
                        "fullname": "Yong Jae Lee",
                        "user": "yjlee0222",
                        "type": "user"
                    },
                    "name": "Yong Jae Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:23:30.906Z",
                    "hidden": false
                },
                {
                    "_id": "67eca9a501a4d1c29e4e70f9",
                    "user": {
                        "_id": "67a701f26b44b5a72af20c9e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a701f26b44b5a72af20c9e/vriqmqgRG3f1mMIjj3tce.png",
                        "isPro": false,
                        "fullname": "Tae-ho Kim",
                        "user": "sabioguru",
                        "type": "user"
                    },
                    "name": "Tae-Ho Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:23:44.160Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
            ],
            "publishedAt": "2025-04-01T09:10:32.000Z",
            "submittedOnDailyAt": "2025-04-02T01:41:42.016Z",
            "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
            "submittedOnDailyBy": {
                "_id": "61f44bab7eba274ea80b74ce",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
                "isPro": false,
                "fullname": "Hyoung-Kyu Song",
                "user": "deepkyu",
                "type": "user"
            },
            "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
            "upvotes": 9,
            "discussionId": "67eca9a601a4d1c29e4e7131",
            "ai_keywords": [
                "visual token reduction",
                "inference costs",
                "image features",
                "large vision-language models (LVLMs)",
                "self-attention-only LVLMs",
                "cross-attention-based models",
                "key-value (KV) cache size",
                "self-attention layers",
                "cross-attention layers",
                "sparse nature",
                "cross-attention maps",
                "redundant visual features",
                "Trimmed Llama",
                "KV cache demands",
                "inference latency",
                "memory usage",
                "benchmark parity"
            ]
        },
        "publishedAt": "2025-04-01T05:10:32.000Z",
        "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
        "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00557.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61f44bab7eba274ea80b74ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
            "fullname": "Hyoung-Kyu Song",
            "name": "deepkyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.23733",
            "authors": [
                {
                    "_id": "67ec99788088196efd062021",
                    "name": "Yiyang Du",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062022",
                    "name": "Xiaochen Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062023",
                    "user": {
                        "_id": "642086ed290342c5df85662d",
                        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                        "isPro": false,
                        "fullname": "Chi Chen",
                        "user": "carboncoo",
                        "type": "user"
                    },
                    "name": "Chi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:49.411Z",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062024",
                    "name": "Jiabo Ye",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062025",
                    "name": "Yiru Wang",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062026",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062027",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062028",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd062029",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd06202a",
                    "name": "Zhifang Sui",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd06202b",
                    "name": "Maosong Sun",
                    "hidden": false
                },
                {
                    "_id": "67ec99788088196efd06202c",
                    "name": "Yang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T05:13:02.000Z",
            "submittedOnDailyAt": "2025-04-02T00:32:49.651Z",
            "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
            "submittedOnDailyBy": {
                "_id": "642086ed290342c5df85662d",
                "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                "isPro": false,
                "fullname": "Chi Chen",
                "user": "carboncoo",
                "type": "user"
            },
            "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
            "upvotes": 9,
            "discussionId": "67ec99798088196efd062081",
            "ai_keywords": [
                "model merging",
                "Large Language Models (LLMs)",
                "Multimodal Large Language Models (MLLMs)",
                "heterogeneous property",
                "model architecture",
                "parameter space",
                "AdaMMS",
                "mapping function",
                "linear interpolation",
                "hyper-parameter searching",
                "unsupervised hyper-parameter selection",
                "vision-language benchmarks"
            ]
        },
        "publishedAt": "2025-03-31T01:13:02.000Z",
        "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
        "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23733.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "fullname": "Chi Chen",
            "name": "carboncoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22165",
            "authors": [
                {
                    "_id": "67ecc00ea0e64cb1b27da958",
                    "user": {
                        "_id": "652b86ffa2d97e682b425b29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b86ffa2d97e682b425b29/W_LzRdzndvKxqbw0ZP_BV.jpeg",
                        "isPro": false,
                        "fullname": "(Andrew) Zhanke Zhou",
                        "user": "AndrewZhou924",
                        "type": "user"
                    },
                    "name": "Zhanke Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:21:26.334Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da959",
                    "name": "Zhaocheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da95a",
                    "user": {
                        "_id": "649842ada9a46fd69e1d5121",
                        "avatarUrl": "/avatars/35372b654678df75a2e8f4e8d20c5dcd.svg",
                        "isPro": false,
                        "fullname": "XuanLi",
                        "user": "GazeEzio",
                        "type": "user"
                    },
                    "name": "Xuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:21:22.992Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da95b",
                    "user": {
                        "_id": "63c09599dd793d5a62890e7d",
                        "avatarUrl": "/avatars/fed51ddd492b98e7cd4c3d1f82998635.svg",
                        "isPro": false,
                        "fullname": "Michael Galkin",
                        "user": "mgalkin",
                        "type": "user"
                    },
                    "name": "Mikhail Galkin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T19:55:33.633Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da95c",
                    "user": {
                        "_id": "67bc2858b24be20e87b11c8e",
                        "avatarUrl": "/avatars/a4cc2c8735fbcbeb3b182740749e521f.svg",
                        "isPro": false,
                        "fullname": "Xiao Feng",
                        "user": "KyrieXiao",
                        "type": "user"
                    },
                    "name": "Xiao Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:21:20.033Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da95d",
                    "name": "Sanmi Koyejo",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da95e",
                    "name": "Jian Tang",
                    "hidden": false
                },
                {
                    "_id": "67ecc00ea0e64cb1b27da95f",
                    "name": "Bo Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T06:09:51.000Z",
            "submittedOnDailyAt": "2025-04-02T17:07:46.753Z",
            "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "652b86ffa2d97e682b425b29",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b86ffa2d97e682b425b29/W_LzRdzndvKxqbw0ZP_BV.jpeg",
                "isPro": false,
                "fullname": "(Andrew) Zhanke Zhou",
                "user": "AndrewZhou924",
                "type": "user"
            },
            "summary": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.",
            "upvotes": 9,
            "discussionId": "67ecc014a0e64cb1b27dab22",
            "ai_keywords": [
                "landscape of thoughts",
                "chain-of-thought",
                "multi-choice dataset",
                "feature vectors",
                "t-SNE",
                "qualitative and quantitative analysis",
                "reasoning paths",
                "lightweight verifier"
            ]
        },
        "publishedAt": "2025-03-28T02:09:51.000Z",
        "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
        "summary": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts-the first\nvisualization tool for users to inspect the reasoning paths of chain-of-thought\nand its derivatives on any multi-choice dataset. Specifically, we represent the\nstates in a reasoning path as feature vectors that quantify their distances to\nall answer choices. These features are then visualized in two-dimensional plots\nusing t-SNE. Qualitative and quantitative analysis with the landscape of\nthoughts effectively distinguishes between strong and weak models, correct and\nincorrect answers, as well as different reasoning tasks. It also uncovers\nundesirable reasoning patterns, such as low consistency and high uncertainty.\nAdditionally, users can adapt our tool to a model that predicts the property\nthey observe. We showcase this advantage by adapting our tool to a lightweight\nverifier that evaluates the correctness of reasoning paths. The code is\npublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22165.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652b86ffa2d97e682b425b29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652b86ffa2d97e682b425b29/W_LzRdzndvKxqbw0ZP_BV.jpeg",
            "fullname": "(Andrew) Zhanke Zhou",
            "name": "AndrewZhou924",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00294",
            "authors": [
                {
                    "_id": "67ecadb199892db0bda56561",
                    "name": "Vidhisha Balachandran",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56562",
                    "name": "Jingya Chen",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56563",
                    "name": "Lingjiao Chen",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56564",
                    "name": "Shivam Garg",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56565",
                    "name": "Neel Joshi",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56566",
                    "user": {
                        "_id": "66c7745fb05bb4dedd0d4097",
                        "avatarUrl": "/avatars/46386b2e0f1ade6852819195e1a680cb.svg",
                        "isPro": false,
                        "fullname": "Yash Lara",
                        "user": "yashlara",
                        "type": "user"
                    },
                    "name": "Yash Lara",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:25:10.416Z",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56567",
                    "user": {
                        "_id": "631fd3375ba8c026340f0384",
                        "avatarUrl": "/avatars/3ecf171bcd5a9eea72127c0c9cea1c6f.svg",
                        "isPro": false,
                        "fullname": "John Langford",
                        "user": "johnmech",
                        "type": "user"
                    },
                    "name": "John Langford",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:25:29.551Z",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56568",
                    "user": {
                        "_id": "638ee2ed848625f9fad11034",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670309301345-638ee2ed848625f9fad11034.jpeg",
                        "isPro": false,
                        "fullname": "Besmira Nushi",
                        "user": "nushib",
                        "type": "user"
                    },
                    "name": "Besmira Nushi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:25:35.619Z",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda56569",
                    "user": {
                        "_id": "63c8527becdb7c9fdd9cacc6",
                        "avatarUrl": "/avatars/c8a3f5e1e5159ae5ead41bd9fc2b9b34.svg",
                        "isPro": false,
                        "fullname": "Vibhav Vineet",
                        "user": "vibhav-vineet",
                        "type": "user"
                    },
                    "name": "Vibhav Vineet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:25:41.418Z",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda5656a",
                    "user": {
                        "_id": "62b177c408a32205a7083b65",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659927891538-62b177c408a32205a7083b65.jpeg",
                        "isPro": false,
                        "fullname": "Yue Wu",
                        "user": "yuewu",
                        "type": "user"
                    },
                    "name": "Yue Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:25:48.285Z",
                    "hidden": false
                },
                {
                    "_id": "67ecadb199892db0bda5656b",
                    "user": {
                        "_id": "632aadf1a968c34257d9f506",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663741551623-632aadf1a968c34257d9f506.jpeg",
                        "isPro": false,
                        "fullname": "Safoora Yousefi",
                        "user": "safooray",
                        "type": "user"
                    },
                    "name": "Safoora Yousefi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:25:59.919Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T23:40:28.000Z",
            "submittedOnDailyAt": "2025-04-02T01:53:41.171Z",
            "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.",
            "upvotes": 6,
            "discussionId": "67ecadb299892db0bda565aa",
            "ai_keywords": [
                "inference-time scaling",
                "reasoning capabilities",
                "large language models (LLMs)",
                "step-by-step problem solving",
                "generated scratchpads",
                "state-of-the-art models",
                "math and STEM reasoning",
                "calendar planning",
                "NP-hard problems",
                "navigation",
                "spatial reasoning",
                "conventional models",
                "fine-tuned models",
                "model calls",
                "performance bounds",
                "multi-model inference systems",
                "empirical analysis",
                "token usage",
                "accuracy",
                "perfect verifiers",
                "performance gap"
            ]
        },
        "publishedAt": "2025-03-31T19:40:28.000Z",
        "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
        "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00294.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.00869",
            "authors": [
                {
                    "_id": "67ecb60276900f68cd1df503",
                    "user": {
                        "_id": "63318b2349a9563915469f3b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
                        "isPro": true,
                        "fullname": "Xiaoke Huang",
                        "user": "xk-huang",
                        "type": "user"
                    },
                    "name": "Xiaoke Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:27:31.825Z",
                    "hidden": false
                },
                {
                    "_id": "67ecb60276900f68cd1df504",
                    "user": {
                        "_id": "641a58d791e3376a0579f011",
                        "avatarUrl": "/avatars/bb4745bb83ac0ee094b43e1e5562d743.svg",
                        "isPro": false,
                        "fullname": "Juncheng Wu",
                        "user": "JunchengWu",
                        "type": "user"
                    },
                    "name": "Juncheng Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:27:39.042Z",
                    "hidden": false
                },
                {
                    "_id": "67ecb60276900f68cd1df505",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "67ecb60276900f68cd1df506",
                    "user": {
                        "_id": "6465f6467ff8fcbef7d22513",
                        "avatarUrl": "/avatars/07992835c235fbb07016a0ea4f1d61cb.svg",
                        "isPro": false,
                        "fullname": "Xianfeng Tang",
                        "user": "xianft",
                        "type": "user"
                    },
                    "name": "Xianfeng Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:27:48.508Z",
                    "hidden": false
                },
                {
                    "_id": "67ecb60276900f68cd1df507",
                    "user": {
                        "_id": "66c7fb4ce2c92fe5b132f314",
                        "avatarUrl": "/avatars/22d915fa339a70803c5c748255250256.svg",
                        "isPro": false,
                        "fullname": "Yuyin Zhou",
                        "user": "RitaCoding",
                        "type": "user"
                    },
                    "name": "Yuyin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:27:54.857Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T14:57:43.000Z",
            "submittedOnDailyAt": "2025-04-02T02:30:28.553Z",
            "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
            "submittedOnDailyBy": {
                "_id": "63318b2349a9563915469f3b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
                "isPro": true,
                "fullname": "Xiaoke Huang",
                "user": "xk-huang",
                "type": "user"
            },
            "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
            "upvotes": 5,
            "discussionId": "67ecb60376900f68cd1df550",
            "githubRepo": "https://github.com/UCSC-VLAA/m1",
            "ai_keywords": [
                "test-time scaling",
                "large language models",
                "medical reasoning",
                "knowledge representation",
                "decision-making processes",
                "lightweight fine-tuned models",
                "state-of-the-art performance",
                "reasoning token budget",
                "budget forcing",
                "iterative prompts",
                "medical QA performance",
                "medical knowledge",
                "data scale",
                "data quality",
                "model capacity",
                "medical knowledge grounding",
                "challenging medical benchmarks",
                "reasoning depth"
            ]
        },
        "publishedAt": "2025-04-01T10:57:43.000Z",
        "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
        "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00869.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63318b2349a9563915469f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
            "fullname": "Xiaoke Huang",
            "name": "xk-huang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00072",
            "authors": [
                {
                    "_id": "67eccbca1006da75eca94d24",
                    "user": {
                        "_id": "64dbcedf3e51c5123392219e",
                        "avatarUrl": "/avatars/e939ea34b4ddb877e436421f03a7b1e9.svg",
                        "isPro": false,
                        "fullname": "Ventura",
                        "user": "LucasVenturaEng",
                        "type": "user"
                    },
                    "name": "Lucas Ventura",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:28:08.348Z",
                    "hidden": false
                },
                {
                    "_id": "67eccbca1006da75eca94d25",
                    "user": {
                        "_id": "6512da122f79557a90b1717f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6512da122f79557a90b1717f/-rL2Iu_HHPEi1zWOL4cZi.jpeg",
                        "isPro": false,
                        "fullname": "Antoine Yang",
                        "user": "antoineyang",
                        "type": "user"
                    },
                    "name": "Antoine Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:28:15.216Z",
                    "hidden": false
                },
                {
                    "_id": "67eccbca1006da75eca94d26",
                    "name": "Cordelia Schmid",
                    "hidden": false
                },
                {
                    "_id": "67eccbca1006da75eca94d27",
                    "name": "Gül Varol",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T17:41:29.000Z",
            "submittedOnDailyAt": "2025-04-02T04:02:28.798Z",
            "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page.",
            "upvotes": 5,
            "discussionId": "67eccbce1006da75eca94e68",
            "ai_keywords": [
                "large language model (LLM)",
                "context window",
                "speech transcripts",
                "captions",
                "timestamps",
                "frame selection strategy",
                "chapter boundaries",
                "F1 score"
            ]
        },
        "publishedAt": "2025-03-31T13:41:29.000Z",
        "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
        "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00072.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.23361",
            "authors": [
                {
                    "_id": "67eb57a56522661171fb4725",
                    "user": {
                        "_id": "64d660308ebc40443813f014",
                        "avatarUrl": "/avatars/516bb2d2383be99794e366dfb41636b6.svg",
                        "isPro": false,
                        "fullname": "Linxin Song",
                        "user": "linxinso",
                        "type": "user"
                    },
                    "name": "Linxin Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:23:45.180Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb4726",
                    "user": {
                        "_id": "676f6c0ccb094bb8d8a0948f",
                        "avatarUrl": "/avatars/78d26c39e1108ee4029639e4202d0ec9.svg",
                        "isPro": false,
                        "fullname": "Xuwei Ding",
                        "user": "Xuwei04",
                        "type": "user"
                    },
                    "name": "Xuwei Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:26:16.488Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb4727",
                    "user": {
                        "_id": "640131b08ba76abe4b71b5d0",
                        "avatarUrl": "/avatars/2288b96a9a0ae8f584768f54e098def1.svg",
                        "isPro": false,
                        "fullname": "Jieyu Zhang",
                        "user": "jieyuz2",
                        "type": "user"
                    },
                    "name": "Jieyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:26:35.397Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb4728",
                    "user": {
                        "_id": "62e1b3cb3eb0730f621a83f6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Taiwei Shi",
                        "user": "MaksimSTW",
                        "type": "user"
                    },
                    "name": "Taiwei Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-01T07:46:48.349Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb4729",
                    "user": {
                        "_id": "65641c8ec683336b76fecb7f",
                        "avatarUrl": "/avatars/3dcf51fc55f7e5ce2c9e84eaee270277.svg",
                        "isPro": false,
                        "fullname": "Ryotaro Shimizu",
                        "user": "latataro",
                        "type": "user"
                    },
                    "name": "Ryotaro Shimizu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:26:41.560Z",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb472a",
                    "name": "Rahul Gupta",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb472b",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb472c",
                    "name": "Jian Kang",
                    "hidden": false
                },
                {
                    "_id": "67eb57a56522661171fb472d",
                    "user": {
                        "_id": "64ba0230c9a9894feb0cd32b",
                        "avatarUrl": "/avatars/1a9e6eb47c14fe278b8e1d907d518f46.svg",
                        "isPro": false,
                        "fullname": "Jieyu Zhao",
                        "user": "jieyuz",
                        "type": "user"
                    },
                    "name": "Jieyu Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:27:18.161Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T08:33:56.000Z",
            "submittedOnDailyAt": "2025-04-02T00:04:15.369Z",
            "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
            "submittedOnDailyBy": {
                "_id": "62e1b3cb3eb0730f621a83f6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
                "isPro": false,
                "fullname": "Taiwei Shi",
                "user": "MaksimSTW",
                "type": "user"
            },
            "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.",
            "upvotes": 4,
            "discussionId": "67eb57a66522661171fb476a",
            "githubRepo": "https://github.com/uscnlp-lime/SEA",
            "ai_keywords": [
                "stochastic error ascent (SEA)",
                "knowledge deficiencies (errors)",
                "closed-weight LLMs",
                "stochastic optimization process",
                "semantic similarity",
                "hierarchical retrieval",
                "document level",
                "paragraph level",
                "relation directed acyclic graph (DAG)",
                "error propagation",
                "systematic failure modes",
                "Automated Capability Discovery",
                "AutoBencher",
                "cost-per-error",
                "human evaluation",
                "ablation analysis",
                "convergence analysis",
                "correlated failure patterns",
                "LLM families",
                "data coverage",
                "targeted fine-tuning"
            ]
        },
        "publishedAt": "2025-03-30T04:33:56.000Z",
        "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
        "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23361.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e1b3cb3eb0730f621a83f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
            "fullname": "Taiwei Shi",
            "name": "MaksimSTW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23157",
            "authors": [
                {
                    "_id": "67ed3cd4ec3d31f7c7591f56",
                    "user": {
                        "_id": "61c9d9574534e41f9b58b5c8",
                        "avatarUrl": "/avatars/dd7dd0e57782217c726348abbd4fa9c6.svg",
                        "isPro": false,
                        "fullname": "Mohammadreza Pourreza",
                        "user": "MrezaPRZ",
                        "type": "user"
                    },
                    "name": "Mohammadreza Pourreza",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T13:55:55.210Z",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f57",
                    "user": {
                        "_id": "65cd45d1b4b92671346f2d3d",
                        "avatarUrl": "/avatars/2814478bbd2e4ac4c19d437f265b247d.svg",
                        "isPro": false,
                        "fullname": "Shayan Talaei",
                        "user": "stalaei",
                        "type": "user"
                    },
                    "name": "Shayan Talaei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T13:56:45.205Z",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f58",
                    "user": {
                        "_id": "653093ad471261a41241e048",
                        "avatarUrl": "/avatars/54263ce75b102b81211d27b696920870.svg",
                        "isPro": false,
                        "fullname": "Ruoxi Sun",
                        "user": "brycesun",
                        "type": "user"
                    },
                    "name": "Ruoxi Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T13:57:00.441Z",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f59",
                    "name": "Xingchen Wan",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f5a",
                    "name": "Hailong Li",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f5b",
                    "user": {
                        "_id": "66ba4e72c9b2ab14b3707be0",
                        "avatarUrl": "/avatars/97ef14d683ed2d0115a9c4694b9763dc.svg",
                        "isPro": false,
                        "fullname": "Azalia Mirhoseini",
                        "user": "am34",
                        "type": "user"
                    },
                    "name": "Azalia Mirhoseini",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-02T13:34:14.113Z",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f5c",
                    "name": "Amin Saberi",
                    "hidden": false
                },
                {
                    "_id": "67ed3cd4ec3d31f7c7591f5d",
                    "name": "Sercan \"O. Arik",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T17:29:30.000Z",
            "submittedOnDailyAt": "2025-04-02T12:06:18.243Z",
            "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL",
            "submittedOnDailyBy": {
                "_id": "61c9d9574534e41f9b58b5c8",
                "avatarUrl": "/avatars/dd7dd0e57782217c726348abbd4fa9c6.svg",
                "isPro": false,
                "fullname": "Mohammadreza Pourreza",
                "user": "MrezaPRZ",
                "type": "user"
            },
            "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks.",
            "upvotes": 3,
            "discussionId": "67ed3cd6ec3d31f7c7591fa7",
            "ai_keywords": [
                "reasoning-enhanced models",
                "reward-driven self-exploration",
                "schema-linking",
                "AI feedback",
                "n-gram similarity",
                "syntax check",
                "reward sparsity",
                "reinforcement learning (RL)",
                "group relative policy optimization (GRPO)",
                "large language models (LLMs)",
                "supervised fine-tuning (SFT)",
                "BIRD benchmark"
            ]
        },
        "publishedAt": "2025-03-29T13:29:30.000Z",
        "title": "Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards\n  for Reasoning-Enhanced Text-to-SQL",
        "summary": "Text-to-SQL is a challenging task involving multiple reasoning-intensive\nsubtasks, including natural language understanding, database schema\ncomprehension, and precise SQL query formulation. Existing approaches often\nrely on handcrafted reasoning paths with inductive biases that can limit their\noverall effectiveness. Motivated by the recent success of reasoning-enhanced\nmodels such as DeepSeek R1 and OpenAI o1, which effectively leverage\nreward-driven self-exploration to enhance reasoning capabilities and\ngeneralization, we propose a novel set of partial rewards tailored specifically\nfor the Text-to-SQL task. Our reward set includes schema-linking, AI feedback,\nn-gram similarity, and syntax check, explicitly designed to address the reward\nsparsity issue prevalent in reinforcement learning (RL). Leveraging group\nrelative policy optimization (GRPO), our approach explicitly encourages large\nlanguage models (LLMs) to develop intrinsic reasoning skills necessary for\naccurate SQL query generation. With models of different sizes, we demonstrate\nthat RL-only training with our proposed rewards consistently achieves higher\naccuracy and superior generalization compared to supervised fine-tuning (SFT).\nRemarkably, our RL-trained 14B-parameter model significantly outperforms larger\nproprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD\nbenchmark. These highlight the efficacy of our proposed RL-training framework\nwith partial rewards for enhancing both accuracy and reasoning capabilities in\nText-to-SQL tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23157.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61c9d9574534e41f9b58b5c8",
            "avatarUrl": "/avatars/dd7dd0e57782217c726348abbd4fa9c6.svg",
            "fullname": "Mohammadreza Pourreza",
            "name": "MrezaPRZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01833",
            "authors": [
                {
                    "_id": "67eddf485224e5e2c1a39491",
                    "user": {
                        "_id": "64676c81e7a6a374fd181110",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64676c81e7a6a374fd181110/HEDlAT9FYhJF9Nw__LLNI.jpeg",
                        "isPro": true,
                        "fullname": "Sumuk Shashidhar",
                        "user": "sumuks",
                        "type": "user"
                    },
                    "name": "Sumuk Shashidhar",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-03T01:07:21.792Z",
                    "hidden": false
                },
                {
                    "_id": "67eddf485224e5e2c1a39492",
                    "user": {
                        "_id": "6202a599216215a22221dea9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644340617257-noauth.png",
                        "isPro": false,
                        "fullname": "Clémentine Fourrier",
                        "user": "clefourrier",
                        "type": "user"
                    },
                    "name": "Clémentine Fourrier",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-03T01:07:21.792Z",
                    "hidden": false
                },
                {
                    "_id": "67eddf485224e5e2c1a39493",
                    "name": "Alina Lozovskia",
                    "hidden": false
                },
                {
                    "_id": "67eddf485224e5e2c1a39494",
                    "name": "Thomas Wolf",
                    "hidden": false
                },
                {
                    "_id": "67eddf485224e5e2c1a39495",
                    "name": "Gokhan Tur",
                    "hidden": false
                },
                {
                    "_id": "67eddf485224e5e2c1a39496",
                    "name": "Dilek Hakkani-Tür",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T15:40:24.000Z",
            "submittedOnDailyAt": "2025-04-02T23:41:06.369Z",
            "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
            "submittedOnDailyBy": {
                "_id": "64676c81e7a6a374fd181110",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64676c81e7a6a374fd181110/HEDlAT9FYhJF9Nw__LLNI.jpeg",
                "isPro": true,
                "fullname": "Sumuk Shashidhar",
                "user": "sumuks",
                "type": "user"
            },
            "summary": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation.",
            "upvotes": 2,
            "discussionId": "67eddf495224e5e2c1a394eb",
            "ai_keywords": [
                "large language models (LLMs)",
                "static benchmarks",
                "human evaluations",
                "dynamic",
                "automated generation",
                "domain-tailored benchmarks",
                "YourBench",
                "MMLU subsets",
                "Tempora-0325",
                "SoTA models",
                "citation grounding"
            ]
        },
        "publishedAt": "2025-04-02T11:40:24.000Z",
        "title": "YourBench: Easy Custom Evaluation Sets for Everyone",
        "summary": "Evaluating large language models (LLMs) effectively remains a critical\nbottleneck, as traditional static benchmarks suffer from saturation and\ncontamination, while human evaluations are costly and slow. This hinders timely\nor domain-specific assessment, crucial for real-world applications. We\nintroduce YourBench, a novel, open-source framework that addresses these\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\nand domain-tailored benchmarks cheaply and without manual annotation, directly\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\nin total inference costs while perfectly preserving the relative model\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\nensure that YourBench generates data grounded in provided input instead of\nrelying on posterior parametric knowledge in models, we also introduce\nTempora-0325, a novel dataset of over 7K diverse documents, published\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\nfrom 7 major families across varying scales (3-671B parameters) to validate the\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\ncitation grounding) and human assessments. We release the YourBench library,\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\nevaluation and inference traces to facilitate reproducible research and empower\nthe community to generate bespoke benchmarks on demand, fostering more relevant\nand trustworthy LLM evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01833.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64676c81e7a6a374fd181110",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64676c81e7a6a374fd181110/HEDlAT9FYhJF9Nw__LLNI.jpeg",
            "fullname": "Sumuk Shashidhar",
            "name": "sumuks",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24210",
            "authors": [
                {
                    "_id": "67ece4b12511c2aab09b84ca",
                    "user": {
                        "_id": "662f5b1e5aeedf55d209741a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
                        "isPro": false,
                        "fullname": "Seungjun Lee",
                        "user": "onandon",
                        "type": "user"
                    },
                    "name": "Seungjun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T11:16:54.586Z",
                    "hidden": false
                },
                {
                    "_id": "67ece4b12511c2aab09b84cb",
                    "user": {
                        "_id": "64bc7568b7375f6b84503ce2",
                        "avatarUrl": "/avatars/d970c9990d9d84ac51ff225c1b26e304.svg",
                        "isPro": false,
                        "fullname": "Gim Hee Lee",
                        "user": "gimhee-lee",
                        "type": "user"
                    },
                    "name": "Gim Hee Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:29:18.073Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
            ],
            "publishedAt": "2025-03-31T15:27:07.000Z",
            "submittedOnDailyAt": "2025-04-02T05:49:14.777Z",
            "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "662f5b1e5aeedf55d209741a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
                "isPro": false,
                "fullname": "Seungjun Lee",
                "user": "onandon",
                "type": "user"
            },
            "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
            "upvotes": 2,
            "discussionId": "67ece4b62511c2aab09b8660",
            "projectPage": "https://diet-gs.github.io",
            "githubRepo": "https://github.com/DiET-GS/DiET-GS",
            "ai_keywords": [
                "diffusion prior",
                "event stream",
                "motion deblurring",
                "3DGS (3D Geometry Synthesis)",
                "blur-free event streams",
                "event double integral",
                "edge details",
                "novel view synthesis"
            ]
        },
        "publishedAt": "2025-03-31T11:27:07.000Z",
        "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
        "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24210.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662f5b1e5aeedf55d209741a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
            "fullname": "Seungjun Lee",
            "name": "onandon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.21860",
            "authors": [
                {
                    "_id": "67ec97f65d9c75ff46de2974",
                    "name": "Kailin Li",
                    "hidden": false
                },
                {
                    "_id": "67ec97f65d9c75ff46de2975",
                    "user": {
                        "_id": "63d4e83913ae45b7807acbb1",
                        "avatarUrl": "/avatars/4b8778ed239b64de5f8781f0ba57168b.svg",
                        "isPro": false,
                        "fullname": "Puhao Li",
                        "user": "puhao",
                        "type": "user"
                    },
                    "name": "Puhao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:29:34.965Z",
                    "hidden": false
                },
                {
                    "_id": "67ec97f65d9c75ff46de2976",
                    "user": {
                        "_id": "634379a81bdd3dfa55dcbe82",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665366425318-noauth.png",
                        "isPro": false,
                        "fullname": "Tengyu Liu",
                        "user": "oxrider",
                        "type": "user"
                    },
                    "name": "Tengyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:29:41.351Z",
                    "hidden": false
                },
                {
                    "_id": "67ec97f65d9c75ff46de2977",
                    "user": {
                        "_id": "637cef70b8e573d75beb5748",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637cef70b8e573d75beb5748/hMl_pKUO3BanR47RQqN6o.png",
                        "isPro": false,
                        "fullname": "Yuyang Li",
                        "user": "aidenli",
                        "type": "user"
                    },
                    "name": "Yuyang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:29:48.257Z",
                    "hidden": false
                },
                {
                    "_id": "67ec97f65d9c75ff46de2978",
                    "user": {
                        "_id": "63c7a33121bd95f80ed74652",
                        "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
                        "isPro": false,
                        "fullname": "Siyuan Huang",
                        "user": "thuhsy",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T12:29:56.926Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-27T17:50:30.000Z",
            "submittedOnDailyAt": "2025-04-02T00:21:41.585Z",
            "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.",
            "upvotes": 2,
            "discussionId": "67ec97f85d9c75ff46de29f0",
            "projectPage": "https://maniptrans.github.io/",
            "ai_keywords": [
                "dexterous robotic manipulation",
                "data-driven embodied AI",
                "trajectory imitator",
                "bimanual skills",
                "interaction constraints",
                "fine-tuning",
                "parameter space",
                "DexManipNet",
                "pen capping",
                "bottle unscrewing",
                "episodes",
                "policy training"
            ]
        },
        "publishedAt": "2025-03-27T13:50:30.000Z",
        "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
        "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21860.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.24219",
            "authors": [
                {
                    "_id": "67ed2033d170f0444d5d3b64",
                    "user": {
                        "_id": "665a137c8528df6b00414572",
                        "avatarUrl": "/avatars/c7b5b2a9baf634a5279f9f1d24f70c23.svg",
                        "isPro": false,
                        "fullname": "Karim Radouane",
                        "user": "rdkarim",
                        "type": "user"
                    },
                    "name": "Karim Radouane",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T11:54:20.276Z",
                    "hidden": false
                },
                {
                    "_id": "67ed2033d170f0444d5d3b65",
                    "name": "Hanane Azzag",
                    "hidden": false
                },
                {
                    "_id": "67ed2033d170f0444d5d3b66",
                    "name": "Mustapha lebbah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T15:36:41.000Z",
            "submittedOnDailyAt": "2025-04-02T10:03:44.404Z",
            "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing",
            "submittedOnDailyBy": {
                "_id": "665a137c8528df6b00414572",
                "avatarUrl": "/avatars/c7b5b2a9baf634a5279f9f1d24f70c23.svg",
                "isPro": false,
                "fullname": "Karim Radouane",
                "user": "rdkarim",
                "type": "user"
            },
            "summary": "We propose a unified framework that integrates object detection (OD) and\nvisual grounding (VG) for remote sensing (RS) imagery. To support conventional\nOD and establish an intuitive prior for VG task, we fine-tune an open-set\nobject detector using referring expression data, framing it as a partially\nsupervised OD task. In the first stage, we construct a graph representation of\neach image, comprising object queries, class embeddings, and proposal\nlocations. Then, our task-aware architecture processes this graph to perform\nthe VG task. The model consists of: (i) a multi-branch network that integrates\nspatial, visual, and categorical features to generate task-aware proposals, and\n(ii) an object reasoning network that assigns probabilities across proposals,\nfollowed by a soft selection mechanism for final referring object localization.\nOur model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG\ndatasets, achieving significant improvements over state-of-the-art methods\nwhile retaining classical OD capabilities. The code will be available in our\nrepository: https://github.com/rd20karim/MB-ORES.",
            "upvotes": 1,
            "discussionId": "67ed2038d170f0444d5d3c8d",
            "githubRepo": "https://github.com/rd20karim/MB-ORES",
            "ai_keywords": [
                "open-set object detector",
                "referring expression data",
                "partially supervised OD task",
                "graph representation",
                "object queries",
                "class embeddings",
                "proposal locations",
                "multi-branch network",
                "task-aware architecture",
                "spatial features",
                "visual features",
                "categorical features",
                "task-aware proposals",
                "object reasoning network",
                "soft selection mechanism",
                "referring object localization",
                "OPT-RSVG dataset",
                "DIOR-RSVG dataset"
            ]
        },
        "publishedAt": "2025-03-31T11:36:41.000Z",
        "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote\n  Sensing",
        "summary": "We propose a unified framework that integrates object detection (OD) and\nvisual grounding (VG) for remote sensing (RS) imagery. To support conventional\nOD and establish an intuitive prior for VG task, we fine-tune an open-set\nobject detector using referring expression data, framing it as a partially\nsupervised OD task. In the first stage, we construct a graph representation of\neach image, comprising object queries, class embeddings, and proposal\nlocations. Then, our task-aware architecture processes this graph to perform\nthe VG task. The model consists of: (i) a multi-branch network that integrates\nspatial, visual, and categorical features to generate task-aware proposals, and\n(ii) an object reasoning network that assigns probabilities across proposals,\nfollowed by a soft selection mechanism for final referring object localization.\nOur model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG\ndatasets, achieving significant improvements over state-of-the-art methods\nwhile retaining classical OD capabilities. The code will be available in our\nrepository: https://github.com/rd20karim/MB-ORES.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24219.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665a137c8528df6b00414572",
            "avatarUrl": "/avatars/c7b5b2a9baf634a5279f9f1d24f70c23.svg",
            "fullname": "Karim Radouane",
            "name": "rdkarim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]