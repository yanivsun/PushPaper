[
    {
        "paper": {
            "id": "2504.00999",
            "authors": [
                {
                    "_id": "67ecc3973d267d266649e075",
                    "user": {
                        "_id": "640f7083208821a59b74c757",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
                        "isPro": false,
                        "fullname": "Siyuan Li",
                        "user": "Lupin1998",
                        "type": "user"
                    },
                    "name": "Siyuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:38.819Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e076",
                    "user": {
                        "_id": "671b4781d2f774c5ec9ebd62",
                        "avatarUrl": "/avatars/b4f1cbaa6e092eda005f81f199a35e19.svg",
                        "isPro": false,
                        "fullname": "Luyuan Zhang",
                        "user": "LuyuanZhang01",
                        "type": "user"
                    },
                    "name": "Luyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:41.242Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e077",
                    "user": {
                        "_id": "6594d390674349122ce6f368",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/luDBiSMX_9l8QEpAQu3HJ.jpeg",
                        "isPro": false,
                        "fullname": "Zedong Wang",
                        "user": "ZedongWangAI",
                        "type": "user"
                    },
                    "name": "Zedong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:39.150Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e078",
                    "user": {
                        "_id": "670880950e79a8b46f7ff9dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
                        "isPro": false,
                        "fullname": "Juanxi Tian",
                        "user": "Juanxi",
                        "type": "user"
                    },
                    "name": "Juanxi Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:43.436Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e079",
                    "user": {
                        "_id": "64be296a46cc3cdfbb057f7e",
                        "avatarUrl": "/avatars/f9f27567b3d59085bf32dd1c708dc90c.svg",
                        "isPro": false,
                        "fullname": "Cheng Tan",
                        "user": "chengtan9907",
                        "type": "user"
                    },
                    "name": "Cheng Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T19:21:03.903Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07a",
                    "name": "Zicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07b",
                    "user": {
                        "_id": "63578f79a1f8ad1c31bd2148",
                        "avatarUrl": "/avatars/e91cf0a7c71a9533556267e67bf0610f.svg",
                        "isPro": false,
                        "fullname": "Y",
                        "user": "CY-7",
                        "type": "user"
                    },
                    "name": "Chang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T13:33:08.711Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07c",
                    "user": {
                        "_id": "64184911db24526c7c9ca035",
                        "avatarUrl": "/avatars/2be15485bdd9e00857d4bff107d1577e.svg",
                        "isPro": false,
                        "fullname": "qingsong xie",
                        "user": "kongming11",
                        "type": "user"
                    },
                    "name": "Qingsong Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:34:47.666Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07d",
                    "user": {
                        "_id": "603864181b642aef9350af81",
                        "avatarUrl": "/avatars/27ec0b885cbedef007876c8c98c69d15.svg",
                        "isPro": false,
                        "fullname": "Haonan Lu",
                        "user": "luhaonan20",
                        "type": "user"
                    },
                    "name": "Haonan Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:34:57.853Z",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07e",
                    "name": "Haoqian Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecc3973d267d266649e07f",
                    "name": "Zhen Lei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T17:39:19.000Z",
            "submittedOnDailyAt": "2025-04-03T00:15:32.614Z",
            "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
            "submittedOnDailyBy": {
                "_id": "670880950e79a8b46f7ff9dd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
                "isPro": false,
                "fullname": "Juanxi Tian",
                "user": "Juanxi",
                "type": "user"
            },
            "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
            "upvotes": 56,
            "discussionId": "67ecc3993d267d266649e10c",
            "projectPage": "https://apexgen-x.github.io/MergeVQ/",
            "githubRepo": "https://github.com/ApexGen-X/MergeVQ",
            "ai_keywords": [
                "Masked Image Modeling (MIM)",
                "Vector Quantization (VQ)",
                "shared latent space",
                "generation quality",
                "representation learning",
                "token merging",
                "generative models",
                "token merge module",
                "self-attention blocks",
                "encoder",
                "Look-up Free Quantization (LFQ)",
                "global alignment",
                "cross-attention",
                "decoder",
                "reconstruction",
                "MergeAR",
                "KV Cache compression",
                "raster-order prediction",
                "AR generative model",
                "ImageNet",
                "token efficiency",
                "inference speed"
            ]
        },
        "publishedAt": "2025-04-01T13:39:19.000Z",
        "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
        "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00999.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "fullname": "Juanxi Tian",
            "name": "Juanxi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00883",
            "authors": [
                {
                    "_id": "67edf28e042e8ba3e95d1960",
                    "user": {
                        "_id": "67ed94169723a841a7128c6f",
                        "avatarUrl": "/avatars/950397084cc48c0885cf85bd3e5260b1.svg",
                        "isPro": false,
                        "fullname": "ZhenYi Liao",
                        "user": "ZhenYiao",
                        "type": "user"
                    },
                    "name": "Zhenyi Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:44:50.157Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1961",
                    "user": {
                        "_id": "64184911db24526c7c9ca035",
                        "avatarUrl": "/avatars/2be15485bdd9e00857d4bff107d1577e.svg",
                        "isPro": false,
                        "fullname": "qingsong xie",
                        "user": "kongming11",
                        "type": "user"
                    },
                    "name": "Qingsong Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:08.446Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1962",
                    "user": {
                        "_id": "6527849a5fc08c60dc6a3120",
                        "avatarUrl": "/avatars/54651dc48acd119c012520a616285e7a.svg",
                        "isPro": false,
                        "fullname": "Yanhao Zhang",
                        "user": "YanhaoZhang",
                        "type": "user"
                    },
                    "name": "Yanhao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:23.767Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1963",
                    "name": "Zijian Kong",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1964",
                    "user": {
                        "_id": "603864181b642aef9350af81",
                        "avatarUrl": "/avatars/27ec0b885cbedef007876c8c98c69d15.svg",
                        "isPro": false,
                        "fullname": "Haonan Lu",
                        "user": "luhaonan20",
                        "type": "user"
                    },
                    "name": "Haonan Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:39.183Z",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1965",
                    "name": "Zhenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "67edf28e042e8ba3e95d1966",
                    "user": {
                        "_id": "64bba541da140e461924dfed",
                        "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
                        "isPro": false,
                        "fullname": "zhijie deng",
                        "user": "zhijie3",
                        "type": "user"
                    },
                    "name": "Zhijie Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:14.204Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T15:11:11.000Z",
            "submittedOnDailyAt": "2025-04-03T01:03:18.798Z",
            "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
            "submittedOnDailyBy": {
                "_id": "64bba541da140e461924dfed",
                "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
                "isPro": false,
                "fullname": "zhijie deng",
                "user": "zhijie3",
                "type": "user"
            },
            "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
            "upvotes": 42,
            "discussionId": "67edf28f042e8ba3e95d1a60",
            "githubRepo": "https://github.com/zhijie-group/R1-Zero-VSI",
            "ai_keywords": [
                "multi-modal large language models (MLLMs)",
                "video-based visual-spatial intelligence (VSI)",
                "Chain of Thought (CoT)",
                "GRPO training",
                "VSI-100k dataset",
                "DeepSeek-R1-Zero",
                "KL penalty",
                "vsGRPO-2B model",
                "Qwen2-VL-2B",
                "vsGRPO-7B model",
                "Qwen2-VL-7B",
                "LLaVA-NeXT-Video-72B",
                "supervised fine-tuning",
                "direct preference optimization"
            ]
        },
        "publishedAt": "2025-04-01T11:11:11.000Z",
        "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
        "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00883.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "fullname": "zhijie deng",
            "name": "zhijie3",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01014",
            "authors": [
                {
                    "_id": "67eca389e14049f5ff064ea6",
                    "user": {
                        "_id": "6506b77a773ceaa8d52ecea1",
                        "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
                        "isPro": false,
                        "fullname": "CJH",
                        "user": "Howe666",
                        "type": "user"
                    },
                    "name": "Junhao Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:27.228Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064ea7",
                    "user": {
                        "_id": "6455cc8f654d8bccae50e4d4",
                        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
                        "isPro": false,
                        "fullname": "Yuying Ge",
                        "user": "tttoaster",
                        "type": "user"
                    },
                    "name": "Yuying Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:45:57.084Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064ea8",
                    "user": {
                        "_id": "640e9762b03f4cd29f58d982",
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:03.701Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064ea9",
                    "user": {
                        "_id": "65e77726767bfc7d109c45bf",
                        "avatarUrl": "/avatars/24e68c86e06055ea1209598ba49ce8b9.svg",
                        "isPro": false,
                        "fullname": "Jing Liao",
                        "user": "CeciliaJL",
                        "type": "user"
                    },
                    "name": "Jing Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:09.694Z",
                    "hidden": false
                },
                {
                    "_id": "67eca389e14049f5ff064eaa",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:15.314Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T17:57:18.000Z",
            "submittedOnDailyAt": "2025-04-03T01:15:35.152Z",
            "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
            "submittedOnDailyBy": {
                "_id": "6506b77a773ceaa8d52ecea1",
                "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
                "isPro": false,
                "fullname": "CJH",
                "user": "Howe666",
                "type": "user"
            },
            "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
            "upvotes": 30,
            "discussionId": "67eca39ce14049f5ff06535b",
            "projectPage": "https://howe125.github.io/AnimeGamer.github.io/",
            "githubRepo": "https://github.com/TencentARC/AnimeGamer",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "video diffusion model",
                "action-aware multimodal representations",
                "automated metrics",
                "human evaluations"
            ]
        },
        "publishedAt": "2025-04-01T13:57:18.000Z",
        "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
        "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01014.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6506b77a773ceaa8d52ecea1",
            "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
            "fullname": "CJH",
            "name": "Howe666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23368",
            "authors": [
                {
                    "_id": "67ee6cee153d50e65e63d024",
                    "user": {
                        "_id": "6760e5fcf3591a93356e9a00",
                        "avatarUrl": "/avatars/e4ab60ee6fa498170c901b3c4599ef4d.svg",
                        "isPro": false,
                        "fullname": "Xindi Yang",
                        "user": "Madaoer-Otaku",
                        "type": "user"
                    },
                    "name": "Xindi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:55:35.772Z",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d025",
                    "user": {
                        "_id": "652cb3e1c6857682d30d4c2b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JEQW6WZBcmjAWwODiDkYA.jpeg",
                        "isPro": false,
                        "fullname": "Libaolu",
                        "user": "8ruceLi",
                        "type": "user"
                    },
                    "name": "Baolu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T13:32:52.747Z",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d026",
                    "name": "Yiming Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d027",
                    "user": {
                        "_id": "64e314ad24809d7fa0f20fbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg",
                        "isPro": false,
                        "fullname": "Zhenfei Yin",
                        "user": "JeremyYin",
                        "type": "user"
                    },
                    "name": "Zhenfei Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:56:12.459Z",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d028",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d029",
                    "user": {
                        "_id": "645b3f63f9d4ec91fdd5129f",
                        "avatarUrl": "/avatars/e31b57294cd6622b42a96d5f83e95495.svg",
                        "isPro": false,
                        "fullname": "Ma Liqian",
                        "user": "Joyhunter9",
                        "type": "user"
                    },
                    "name": "Liqian Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:56:36.706Z",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d02a",
                    "name": "Zhiyong Wang",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d02b",
                    "user": {
                        "_id": "656fda082f058b368c27d1b9",
                        "avatarUrl": "/avatars/cba33f6b20fc93ac9d3d579cbd7f839a.svg",
                        "isPro": false,
                        "fullname": "cai jianfei",
                        "user": "genify",
                        "type": "user"
                    },
                    "name": "Jianfei Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:56:25.898Z",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d02c",
                    "user": {
                        "_id": "65574f0fc4865c852d5eec15",
                        "avatarUrl": "/avatars/1e03db4f2de4959dee620c577fbbb063.svg",
                        "isPro": false,
                        "fullname": "Tien-Tsin Wong",
                        "user": "ttwong",
                        "type": "user"
                    },
                    "name": "Tien-Tsin Wong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:56:52.247Z",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d02d",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "67ee6cee153d50e65e63d02e",
                    "name": "Xu Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T09:03:09.000Z",
            "submittedOnDailyAt": "2025-04-03T09:42:46.004Z",
            "title": "Towards Physically Plausible Video Generation via VLM Planning",
            "submittedOnDailyBy": {
                "_id": "652cb3e1c6857682d30d4c2b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JEQW6WZBcmjAWwODiDkYA.jpeg",
                "isPro": false,
                "fullname": "Libaolu",
                "user": "8ruceLi",
                "type": "user"
            },
            "summary": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics. In\nthe first stage, we employ a Vision Language Model (VLM) as a coarse-grained\nmotion planner, integrating chain-of-thought and physics-aware reasoning to\npredict a rough motion trajectories/changes that approximate real-world\nphysical dynamics while ensuring the inter-frame consistency. In the second\nstage, we use the predicted motion trajectories/changes to guide the video\ngeneration of a VDM. As the predicted motion trajectories/changes are rough,\nnoise is added during inference to provide freedom to the VDM in generating\nmotion with more fine details. Extensive experimental results demonstrate that\nour framework can produce physically plausible motion, and comparative\nevaluations highlight the notable superiority of our approach over existing\nmethods. More video results are available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation.",
            "upvotes": 25,
            "discussionId": "67ee6cf0153d50e65e63d093",
            "projectPage": "https://madaoer.github.io/projects/physically_plausible_video_generation/",
            "ai_keywords": [
                "Video diffusion models (VDMs)",
                "Vision Language Model (VLM)",
                "chain-of-thought",
                "physics-aware reasoning",
                "motion trajectories/changes",
                "inter-frame consistency",
                "physically plausible motion"
            ]
        },
        "publishedAt": "2025-03-30T05:03:09.000Z",
        "title": "Towards Physically Plausible Video Generation via VLM Planning",
        "summary": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics. In\nthe first stage, we employ a Vision Language Model (VLM) as a coarse-grained\nmotion planner, integrating chain-of-thought and physics-aware reasoning to\npredict a rough motion trajectories/changes that approximate real-world\nphysical dynamics while ensuring the inter-frame consistency. In the second\nstage, we use the predicted motion trajectories/changes to guide the video\ngeneration of a VDM. As the predicted motion trajectories/changes are rough,\nnoise is added during inference to provide freedom to the VDM in generating\nmotion with more fine details. Extensive experimental results demonstrate that\nour framework can produce physically plausible motion, and comparative\nevaluations highlight the notable superiority of our approach over existing\nmethods. More video results are available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23368.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "652cb3e1c6857682d30d4c2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JEQW6WZBcmjAWwODiDkYA.jpeg",
            "fullname": "Libaolu",
            "name": "8ruceLi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.20783",
            "authors": [
                {
                    "_id": "67e97f581cb6fc648f642a05",
                    "user": {
                        "_id": "65f5392c68b8e0cb3c9977a2",
                        "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
                        "isPro": false,
                        "fullname": "Zichen",
                        "user": "lkevinzc",
                        "type": "user"
                    },
                    "name": "Zichen Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:29:10.035Z",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a06",
                    "user": {
                        "_id": "64e416dc54e18f390ef79ba4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5n01J00ZaVRrebsON8iYA.jpeg",
                        "isPro": true,
                        "fullname": "Changyu Chen",
                        "user": "Cameron-Chen",
                        "type": "user"
                    },
                    "name": "Changyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:29:07.988Z",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a07",
                    "name": "Wenjun Li",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a08",
                    "user": {
                        "_id": "63885f1d0bebb233d8ad6e5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Penghui Qi",
                        "user": "QPHutu",
                        "type": "user"
                    },
                    "name": "Penghui Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:29:05.240Z",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a09",
                    "user": {
                        "_id": "63d91b6d255ef6add20e1b38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "P2333",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:50:22.635Z",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a0a",
                    "user": {
                        "_id": "632407c892e07e3ca20aca28",
                        "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
                        "isPro": false,
                        "fullname": "Chao Du",
                        "user": "duchao",
                        "type": "user"
                    },
                    "name": "Chao Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:50:46.967Z",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a0b",
                    "name": "Wee Sun Lee",
                    "hidden": false
                },
                {
                    "_id": "67e97f581cb6fc648f642a0c",
                    "name": "Min Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-26T17:59:14.000Z",
            "submittedOnDailyAt": "2025-04-03T03:47:54.547Z",
            "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
            "submittedOnDailyBy": {
                "_id": "65f5392c68b8e0cb3c9977a2",
                "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
                "isPro": false,
                "fullname": "Zichen",
                "user": "lkevinzc",
                "type": "user"
            },
            "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
            "upvotes": 25,
            "discussionId": "67e97f591cb6fc648f642a38",
            "githubRepo": "https://github.com/sail-sg/understand-r1-zero",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "reasoning capabilities",
                "LLMs",
                "base models",
                "DeepSeek-V3-Base",
                "pretraining characteristics",
                "Qwen2.5",
                "prompt templates",
                "pretraining biases",
                "Group Relative Policy Optimization (GRPO)",
                "optimization bias",
                "response length",
                "Dr. GRPO",
                "token efficiency",
                "minimalist R1-Zero recipe",
                "AIME 2024",
                "7B base model"
            ]
        },
        "publishedAt": "2025-03-26T13:59:14.000Z",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
        "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20783.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "fullname": "Zichen",
            "name": "lkevinzc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01724",
            "authors": [
                {
                    "_id": "67edf7b6d277de0ec2aa5b6b",
                    "name": "Yuxuan Luo",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6c",
                    "name": "Zhengkun Rong",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6d",
                    "user": {
                        "_id": "63451d2dfeba4bdba59cf9b1",
                        "avatarUrl": "/avatars/ff5dd2f3b502c54e7c3e2512c3e98b28.svg",
                        "isPro": false,
                        "fullname": "Lizhen Wang",
                        "user": "wanglz14",
                        "type": "user"
                    },
                    "name": "Lizhen Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:47:42.725Z",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6e",
                    "user": {
                        "_id": "602e78801f993496bc14d9c9",
                        "avatarUrl": "/avatars/eff73f60e6652d107cebeda3b2b87fe1.svg",
                        "isPro": false,
                        "fullname": "Longhao Zhang",
                        "user": "zhanglonghao",
                        "type": "user"
                    },
                    "name": "Longhao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:47:48.593Z",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b6f",
                    "name": "Tianshu Hu",
                    "hidden": false
                },
                {
                    "_id": "67edf7b6d277de0ec2aa5b70",
                    "user": {
                        "_id": "668f66c56575435392165c25",
                        "avatarUrl": "/avatars/4ebe10abc29229e7862ada6891d1ccbe.svg",
                        "isPro": false,
                        "fullname": "Zhu",
                        "user": "YongmingZhu",
                        "type": "user"
                    },
                    "name": "Yongming Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:48:03.944Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T13:30:32.000Z",
            "submittedOnDailyAt": "2025-04-03T01:22:04.548Z",
            "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
            "upvotes": 24,
            "discussionId": "67edf7bcd277de0ec2aa5d7b",
            "ai_keywords": [
                "diffusion transformer (DiT)",
                "hybrid guidance",
                "implicit facial representations",
                "3D head spheres",
                "3D body skeletons",
                "facial expressions",
                "body movements",
                "expressive animations",
                "identity-preserving animations",
                "progressive training strategy",
                "varying resolutions",
                "varying scales",
                "motion patterns",
                "sequential frames",
                "visual references",
                "long-term temporal coherence",
                "long-term consistency",
                "expressive results",
                "upper-body generation",
                "full-body generation"
            ]
        },
        "publishedAt": "2025-04-02T09:30:32.000Z",
        "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
        "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01724.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01956",
            "authors": [
                {
                    "_id": "67ee01265839c8a023344aee",
                    "user": {
                        "_id": "65c38f6c137aba2aee524989",
                        "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
                        "isPro": false,
                        "fullname": "Hanyang Wang",
                        "user": "hanyang-21",
                        "type": "user"
                    },
                    "name": "Hanyang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:06.284Z",
                    "hidden": false
                },
                {
                    "_id": "67ee01265839c8a023344aef",
                    "user": {
                        "_id": "6505a02f9310ce8c400edc63",
                        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                        "isPro": false,
                        "fullname": "Fangfu Liu",
                        "user": "Liuff23",
                        "type": "user"
                    },
                    "name": "Fangfu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:26.790Z",
                    "hidden": false
                },
                {
                    "_id": "67ee01265839c8a023344af0",
                    "user": {
                        "_id": "66e14cf2fb009ef598305fe5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/etohfcD9GKaRPir1HRP-5.jpeg",
                        "isPro": false,
                        "fullname": "Jiawei Chi",
                        "user": "isaac158",
                        "type": "user"
                    },
                    "name": "Jiawei Chi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:47.647Z",
                    "hidden": false
                },
                {
                    "_id": "67ee01265839c8a023344af1",
                    "user": {
                        "_id": "66c8131afafc0fc87ca99650",
                        "avatarUrl": "/avatars/a6eeba2ccf011d5c9964fd38f85bd671.svg",
                        "isPro": false,
                        "fullname": "Yueqi Duan",
                        "user": "duanyueqi",
                        "type": "user"
                    },
                    "name": "Yueqi Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:46:55.497Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
            ],
            "publishedAt": "2025-04-02T17:59:21.000Z",
            "submittedOnDailyAt": "2025-04-03T02:07:36.716Z",
            "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
            "submittedOnDailyBy": {
                "_id": "65c38f6c137aba2aee524989",
                "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
                "isPro": false,
                "fullname": "Hanyang Wang",
                "user": "hanyang-21",
                "type": "user"
            },
            "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
            "upvotes": 22,
            "discussionId": "67ee012a5839c8a023344bdb",
            "projectPage": "https://hanyang-21.github.io/VideoScene",
            "githubRepo": "https://github.com/hanyang-21/VideoScene",
            "ai_keywords": [
                "video generative models",
                "video diffusion models",
                "3D scenes",
                "sparse views",
                "geometry regularization",
                "feed-forward model",
                "video generative prior",
                "inference time",
                "3D constraint",
                "reconstruction artifacts",
                "VideoScene",
                "3D-aware leap flow distillation",
                "dynamic denoising policy network"
            ]
        },
        "publishedAt": "2025-04-02T13:59:21.000Z",
        "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
        "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01956.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c38f6c137aba2aee524989",
            "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
            "fullname": "Hanyang Wang",
            "name": "hanyang-21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01848",
            "authors": [
                {
                    "_id": "67edf3d579018bf61e050435",
                    "user": {
                        "_id": "627b8bc83974b0ed6b28db67",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665158475637-627b8bc83974b0ed6b28db67.jpeg",
                        "isPro": false,
                        "fullname": "Giulio Starace",
                        "user": "thesofakillers",
                        "type": "user"
                    },
                    "name": "Giulio Starace",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:57:27.629Z",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e050436",
                    "name": "Oliver Jaffe",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e050437",
                    "name": "Dane Sherburn",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e050438",
                    "user": {
                        "_id": "6316bd602fd6e930e429b362",
                        "avatarUrl": "/avatars/28abc3ad3a00811da404f5d88dd25a69.svg",
                        "isPro": false,
                        "fullname": "James Aung",
                        "user": "james-aung",
                        "type": "user"
                    },
                    "name": "James Aung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:57:49.748Z",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e050439",
                    "user": {
                        "_id": "60db178afb288e95534e7909",
                        "avatarUrl": "/avatars/54a656506a66d06aeb3556c88a9a072a.svg",
                        "isPro": false,
                        "fullname": "Chan Jun Shern",
                        "user": "junshern",
                        "type": "user"
                    },
                    "name": "Jun Shern Chan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:58:00.906Z",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e05043a",
                    "user": {
                        "_id": "632c9a1c1d303f5f9ad0685c",
                        "avatarUrl": "/avatars/d8828064fe0d89efed7594c5c9747078.svg",
                        "isPro": false,
                        "fullname": "M",
                        "user": "LeonM",
                        "type": "user"
                    },
                    "name": "Leon Maksin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:58:13.107Z",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e05043b",
                    "name": "Rachel Dias",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e05043c",
                    "user": {
                        "_id": "636d9548da13cd91f922e0d0",
                        "avatarUrl": "/avatars/488f6a4b12f20809d3a4c3d40ae82b53.svg",
                        "isPro": false,
                        "fullname": "Evan Mays",
                        "user": "evanmays",
                        "type": "user"
                    },
                    "name": "Evan Mays",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:58:23.365Z",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e05043d",
                    "user": {
                        "_id": "65397cb82a101a151da4836b",
                        "avatarUrl": "/avatars/dd9e03be4e06fd41b82cce97c9ca2d21.svg",
                        "isPro": false,
                        "fullname": "Benjamin Kinsella",
                        "user": "bjk127",
                        "type": "user"
                    },
                    "name": "Benjamin Kinsella",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:58:29.583Z",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e05043e",
                    "name": "Wyatt Thompson",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e05043f",
                    "name": "Johannes Heidecke",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e050440",
                    "name": "Amelia Glaese",
                    "hidden": false
                },
                {
                    "_id": "67edf3d579018bf61e050441",
                    "user": {
                        "_id": "64d4f504887f55fb6eedec74",
                        "avatarUrl": "/avatars/054fb826890adcb330f0e4cbca3ef7c4.svg",
                        "isPro": false,
                        "fullname": "Tejal Patwardhan",
                        "user": "tejalp",
                        "type": "user"
                    },
                    "name": "Tejal Patwardhan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:58:46.470Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T15:55:24.000Z",
            "submittedOnDailyAt": "2025-04-03T01:05:22.442Z",
            "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\nhttps://github.com/openai/preparedness{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
            "upvotes": 20,
            "discussionId": "67edf3d679018bf61e0504c0",
            "ai_keywords": [
                "anLM-based judge",
                "replication attempts"
            ]
        },
        "publishedAt": "2025-04-02T11:55:24.000Z",
        "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
        "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\nhttps://github.com/openai/preparedness{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01848.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.00824",
            "authors": [
                {
                    "_id": "67ede79d21d7e74ee3e2832a",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e2832b",
                    "user": {
                        "_id": "5ec82854968f6028e0559f70",
                        "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
                        "isPro": true,
                        "fullname": "Xueguang Ma",
                        "user": "MrLight",
                        "type": "user"
                    },
                    "name": "Xueguang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:59:15.484Z",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e2832c",
                    "user": {
                        "_id": "65358802a920f38780b3248a",
                        "avatarUrl": "/avatars/9415510b598079973c2b0436ad12db9c.svg",
                        "isPro": false,
                        "fullname": "Ping Nie",
                        "user": "pingnieuk",
                        "type": "user"
                    },
                    "name": "Ping Nie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:59:44.386Z",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e2832d",
                    "name": "Huaye Zeng",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e2832e",
                    "user": {
                        "_id": "6114ea319ae90b69cb29fc92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6114ea319ae90b69cb29fc92/uw2n4iiXGD5v5iUocmbxE.jpeg",
                        "isPro": false,
                        "fullname": "Zhiheng Lyu",
                        "user": "cogito233",
                        "type": "user"
                    },
                    "name": "Zhiheng Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:59:50.957Z",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e2832f",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e28330",
                    "user": {
                        "_id": "6669ffeecf27eed0f2f2581e",
                        "avatarUrl": "/avatars/6a1012cce15da12fa3bd6f9e0f87d100.svg",
                        "isPro": false,
                        "fullname": "Benjamin Schneider",
                        "user": "BenSchneider",
                        "type": "user"
                    },
                    "name": "Benjamin Schneider",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:00:40.436Z",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e28331",
                    "name": "Yi Lu",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e28332",
                    "user": {
                        "_id": "6230d750d93e84e233882dbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Yue",
                        "user": "yuexiang96",
                        "type": "user"
                    },
                    "name": "Xiang Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:00:53.880Z",
                    "hidden": false
                },
                {
                    "_id": "67ede79d21d7e74ee3e28333",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T13:59:08.344Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
            ],
            "publishedAt": "2025-04-01T14:12:14.000Z",
            "submittedOnDailyAt": "2025-04-03T00:13:20.491Z",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
            "upvotes": 19,
            "discussionId": "67ede79e21d7e74ee3e2838c",
            "githubRepo": "https://github.com/TIGER-AI-Lab/ScholarCopilot",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "ScholarCopilot",
                "large language models",
                "retrieval token [RET]",
                "scholarly references",
                "top-1 retrieval accuracy",
                "arXiv",
                "generation quality",
                "relevance",
                "coherence",
                "academic rigor",
                "completeness",
                "innovation",
                "citation recall",
                "writing efficiency",
                "user experience"
            ]
        },
        "publishedAt": "2025-04-01T10:12:14.000Z",
        "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
        "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00824.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 36
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01934",
            "authors": [
                {
                    "_id": "67edfe07f5d1509d1a990178",
                    "user": {
                        "_id": "630f0542cc8ed75decb03b68",
                        "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
                        "isPro": false,
                        "fullname": "huangrh9",
                        "user": "huangrh9",
                        "type": "user"
                    },
                    "name": "Runhui Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T13:49:29.196Z",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a990179",
                    "user": {
                        "_id": "644131e5666947ad561349c1",
                        "avatarUrl": "/avatars/8a2d9be81cef30cb3bc71a695c802d2f.svg",
                        "isPro": false,
                        "fullname": "Wangchunwei",
                        "user": "17day",
                        "type": "user"
                    },
                    "name": "Chunwei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:01:13.452Z",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a99017a",
                    "name": "Junwei Yang",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a99017b",
                    "name": "Guansong Lu",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a99017c",
                    "name": "Yunlong Yuan",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a99017d",
                    "user": {
                        "_id": "6357c8afa43d8d1bbe64efc9",
                        "avatarUrl": "/avatars/901a656e7327d5d949f10812f35f19b8.svg",
                        "isPro": false,
                        "fullname": "Jianhua Han",
                        "user": "hanjianhua44",
                        "type": "user"
                    },
                    "name": "Jianhua Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:02:30.946Z",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a99017e",
                    "user": {
                        "_id": "5f86882fee5616341bc51da1",
                        "avatarUrl": "/avatars/b0c5883d6253ef67639d1cea591fa337.svg",
                        "isPro": false,
                        "fullname": "Lu Hou",
                        "user": "houlu369",
                        "type": "user"
                    },
                    "name": "Lu Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:02:38.473Z",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a99017f",
                    "name": "Wei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a990180",
                    "user": {
                        "_id": "671207b95550f581fcce3cfd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wqvv-QQ59dNT1KSpOUQLa.png",
                        "isPro": false,
                        "fullname": "Lanqing HONG",
                        "user": "racheltechie",
                        "type": "user"
                    },
                    "name": "Lanqing Hong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:02:44.952Z",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a990181",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67edfe07f5d1509d1a990182",
                    "name": "Hang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T17:45:00.000Z",
            "submittedOnDailyAt": "2025-04-03T01:58:02.658Z",
            "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
            "submittedOnDailyBy": {
                "_id": "630f0542cc8ed75decb03b68",
                "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
                "isPro": false,
                "fullname": "huangrh9",
                "user": "huangrh9",
                "type": "user"
            },
            "summary": "We present ILLUME+ that leverages dual visual tokenization and a diffusion\ndecoder to improve both deep semantic understanding and high-fidelity image\ngeneration. Existing unified models have struggled to simultaneously handle the\nthree fundamental capabilities in a unified model: understanding, generation,\nand editing. Models like Chameleon and EMU3 utilize VQGAN for image\ndiscretization, due to the lack of deep semantic interaction, they lag behind\nspecialist models like LLaVA in visual understanding tasks. To mitigate this,\nLaViT and ILLUME employ semantic encoders for tokenization, but they struggle\nwith image editing due to poor texture preservation. Meanwhile, Janus series\ndecouples the input and output image representation, limiting their abilities\nto seamlessly handle interleaved image-text understanding and generation. In\ncontrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which\npreserves both fine-grained textures and text-aligned semantics while enabling\na coarse-to-fine image representation strategy for multimodal understanding and\ngeneration. Additionally, we employ a diffusion model as the image detokenizer\nfor enhanced generation quality and efficient super-resolution. ILLUME+ follows\na continuous-input, discrete-output scheme within the unified MLLM and adopts a\nprogressive training procedure that supports dynamic resolution across the\nvision tokenizer, MLLM, and diffusion decoder. This design allows for flexible\nand efficient context-aware image editing and generation across diverse tasks.\nILLUME+ (3B) exhibits competitive performance against existing unified MLLMs\nand specialized models across multimodal understanding, generation, and editing\nbenchmarks. With its strong performance, ILLUME+ provides a scalable and\nversatile foundation for future multimodal applications. Project Page:\nhttps://illume-unified-mllm.github.io/.",
            "upvotes": 16,
            "discussionId": "67edfe09f5d1509d1a990214",
            "projectPage": "https://illume-unified-mllm.github.io/",
            "githubRepo": "https://github.com/illume-unified-mllm/ILLUME_plus",
            "ai_keywords": [
                "dual visual tokenization",
                "diffusion decoder",
                "deep semantic understanding",
                "high-fidelity image generation",
                "VQGAN",
                "LaViT",
                "semantic encoders",
                "DualViTok",
                "texture preservation",
                "multimodal understanding",
                "continuous-input, discrete-output scheme",
                "MLLM",
                "progressive training procedure",
                "dynamic resolution",
                "context-aware image editing"
            ]
        },
        "publishedAt": "2025-04-02T13:45:00.000Z",
        "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
        "summary": "We present ILLUME+ that leverages dual visual tokenization and a diffusion\ndecoder to improve both deep semantic understanding and high-fidelity image\ngeneration. Existing unified models have struggled to simultaneously handle the\nthree fundamental capabilities in a unified model: understanding, generation,\nand editing. Models like Chameleon and EMU3 utilize VQGAN for image\ndiscretization, due to the lack of deep semantic interaction, they lag behind\nspecialist models like LLaVA in visual understanding tasks. To mitigate this,\nLaViT and ILLUME employ semantic encoders for tokenization, but they struggle\nwith image editing due to poor texture preservation. Meanwhile, Janus series\ndecouples the input and output image representation, limiting their abilities\nto seamlessly handle interleaved image-text understanding and generation. In\ncontrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which\npreserves both fine-grained textures and text-aligned semantics while enabling\na coarse-to-fine image representation strategy for multimodal understanding and\ngeneration. Additionally, we employ a diffusion model as the image detokenizer\nfor enhanced generation quality and efficient super-resolution. ILLUME+ follows\na continuous-input, discrete-output scheme within the unified MLLM and adopts a\nprogressive training procedure that supports dynamic resolution across the\nvision tokenizer, MLLM, and diffusion decoder. This design allows for flexible\nand efficient context-aware image editing and generation across diverse tasks.\nILLUME+ (3B) exhibits competitive performance against existing unified MLLMs\nand specialized models across multimodal understanding, generation, and editing\nbenchmarks. With its strong performance, ILLUME+ provides a scalable and\nversatile foundation for future multimodal applications. Project Page:\nhttps://illume-unified-mllm.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01934.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "630f0542cc8ed75decb03b68",
            "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
            "fullname": "huangrh9",
            "name": "huangrh9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01204",
            "authors": [
                {
                    "_id": "67edf4bf5e87fcaa485a0ad9",
                    "user": {
                        "_id": "6387064a2ee2261893d85c0e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6387064a2ee2261893d85c0e/LznJi69uQwm1Le5ma6Zb_.jpeg",
                        "isPro": false,
                        "fullname": "xuan li",
                        "user": "xuanli",
                        "type": "user"
                    },
                    "name": "Xuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:04:06.298Z",
                    "hidden": false
                },
                {
                    "_id": "67edf4bf5e87fcaa485a0ada",
                    "name": "Qianli Ma",
                    "hidden": true
                },
                {
                    "_id": "67edf4bf5e87fcaa485a0adb",
                    "user": {
                        "_id": "63b738acbd2d1535227daa4c",
                        "avatarUrl": "/avatars/23300548e5c50f44e95d63568221f47b.svg",
                        "isPro": false,
                        "fullname": "Tsung-Yi Lin",
                        "user": "tsungyi",
                        "type": "user"
                    },
                    "name": "Tsung-Yi Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:03:42.992Z",
                    "hidden": false
                },
                {
                    "_id": "67edf4bf5e87fcaa485a0adc",
                    "user": {
                        "_id": "66f4cf1a03b5ba8a7f1f6522",
                        "avatarUrl": "/avatars/2768d6e37d3f280194cfb8ed274f6015.svg",
                        "isPro": false,
                        "fullname": "Yongxin Chen",
                        "user": "Ema11",
                        "type": "user"
                    },
                    "name": "Yongxin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:03:36.411Z",
                    "hidden": false
                },
                {
                    "_id": "67edf4bf5e87fcaa485a0add",
                    "user": {
                        "_id": "655683727be68c0961673f45",
                        "avatarUrl": "/avatars/cddca36c041fa04860a4d42c0feaa07f.svg",
                        "isPro": false,
                        "fullname": "Chenfanfu Jiang",
                        "user": "cffjiang",
                        "type": "user"
                    },
                    "name": "Chenfanfu Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:03:29.907Z",
                    "hidden": false
                },
                {
                    "_id": "67edf4bf5e87fcaa485a0ade",
                    "user": {
                        "_id": "62f049afdf4b93aad5c7f2d6",
                        "avatarUrl": "/avatars/e272e58ad996733d7098e50248e5b57e.svg",
                        "isPro": false,
                        "fullname": "Ming-Yu Liu",
                        "user": "mingyuliutw",
                        "type": "user"
                    },
                    "name": "Ming-Yu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:03:21.297Z",
                    "hidden": false
                },
                {
                    "_id": "67edf4bf5e87fcaa485a0adf",
                    "name": "Donglai Xiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T21:37:57.000Z",
            "submittedOnDailyAt": "2025-04-03T01:09:40.312Z",
            "title": "Articulated Kinematics Distillation from Video Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/",
            "upvotes": 13,
            "discussionId": "67edf4c65e87fcaa485a0cb7",
            "ai_keywords": [
                "skeleton-based representation",
                "Degrees of Freedom (DoFs)",
                "joint-level control",
                "Score Distillation Sampling (SDS)",
                "video diffusion models",
                "articulated motions",
                "structural integrity",
                "physics-based simulation",
                "text-to-4D generation"
            ]
        },
        "publishedAt": "2025-04-01T17:37:57.000Z",
        "title": "Articulated Kinematics Distillation from Video Diffusion Models",
        "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01204.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6571
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2405.20216",
            "authors": [
                {
                    "_id": "666e9b96bc840e67481f20f3",
                    "user": {
                        "_id": "6662b3ee280fb71780b85ef8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wDr9CDlL40_gp-NBRm7gB.png",
                        "isPro": false,
                        "fullname": "Sanghyeon Na",
                        "user": "sanghyeonna",
                        "type": "user"
                    },
                    "name": "Sanghyeon Na",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-02T11:59:46.873Z",
                    "hidden": false
                },
                {
                    "_id": "666e9b96bc840e67481f20f4",
                    "user": {
                        "_id": "6523df09f1491fbf6a93c6c8",
                        "avatarUrl": "/avatars/886859aaffc7ab855f8e022c36ebb495.svg",
                        "isPro": false,
                        "fullname": "Yonggyu Kim",
                        "user": "yonggyuarthur",
                        "type": "user"
                    },
                    "name": "Yonggyu Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:05:41.152Z",
                    "hidden": false
                },
                {
                    "_id": "666e9b96bc840e67481f20f5",
                    "user": {
                        "_id": "6786d01913da730e75fb88e3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WALoMzoiaIGPJf6lXhT_Q.png",
                        "isPro": false,
                        "fullname": "Hyunjoon Lee",
                        "user": "hyunjoonlee12",
                        "type": "user"
                    },
                    "name": "Hyunjoon Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:05:48.108Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-05-30T16:18:05.000Z",
            "submittedOnDailyAt": "2025-04-03T06:19:33.352Z",
            "title": "Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
                "isPro": false,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "The generation of high-quality human images through text-to-image (T2I)\nmethods is a significant yet challenging task. Distinct from general image\ngeneration, human image synthesis must satisfy stringent criteria related to\nhuman pose, anatomy, and alignment with textual prompts, making it particularly\ndifficult to achieve realistic results. Recent advancements in T2I generation\nbased on diffusion models have shown promise, yet challenges remain in meeting\nhuman-specific preferences. In this paper, we introduce a novel approach\ntailored specifically for human image generation utilizing Direct Preference\nOptimization (DPO). Specifically, we introduce an efficient method for\nconstructing a specialized DPO dataset for training human image generation\nmodels without the need for costly human feedback. We also propose a modified\nloss function that enhances the DPO training process by minimizing artifacts\nand improving image fidelity. Our method demonstrates its versatility and\neffectiveness in generating human images, including personalized text-to-image\ngeneration. Through comprehensive evaluations, we show that our approach\nsignificantly advances the state of human image generation, achieving superior\nresults in terms of natural anatomies, poses, and text-image alignment.",
            "upvotes": 12,
            "discussionId": "666e9b9cbc840e67481f2329",
            "ai_keywords": [
                "diffusion models",
                "Direct Preference Optimization (DPO)",
                "DPO dataset",
                "specialized DPO dataset",
                "modified loss function",
                "artifacts",
                "image fidelity",
                "personalized text-to-image generation",
                "natural anatomies",
                "poses",
                "text-image alignment"
            ]
        },
        "publishedAt": "2024-05-30T12:18:05.000Z",
        "title": "Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback",
        "summary": "The generation of high-quality human images through text-to-image (T2I)\nmethods is a significant yet challenging task. Distinct from general image\ngeneration, human image synthesis must satisfy stringent criteria related to\nhuman pose, anatomy, and alignment with textual prompts, making it particularly\ndifficult to achieve realistic results. Recent advancements in T2I generation\nbased on diffusion models have shown promise, yet challenges remain in meeting\nhuman-specific preferences. In this paper, we introduce a novel approach\ntailored specifically for human image generation utilizing Direct Preference\nOptimization (DPO). Specifically, we introduce an efficient method for\nconstructing a specialized DPO dataset for training human image generation\nmodels without the need for costly human feedback. We also propose a modified\nloss function that enhances the DPO training process by minimizing artifacts\nand improving image fidelity. Our method demonstrates its versatility and\neffectiveness in generating human images, including personalized text-to-image\ngeneration. Through comprehensive evaluations, we show that our approach\nsignificantly advances the state of human image generation, achieving superior\nresults in terms of natural anatomies, poses, and text-image alignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20216.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 530
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.01308",
            "authors": [
                {
                    "_id": "67ede544ed9c94861b82b29f",
                    "user": {
                        "_id": "64060b49a577649430bf6974",
                        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
                        "isPro": false,
                        "fullname": "Jiawei Wang",
                        "user": "Jarvis1111",
                        "type": "user"
                    },
                    "name": "Jiawei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:27.815Z",
                    "hidden": false
                },
                {
                    "_id": "67ede544ed9c94861b82b2a0",
                    "user": {
                        "_id": "643e9efa2263cdc630f88f5c",
                        "avatarUrl": "/avatars/96cea51f17e7d41ffb6a4b438e05f5cb.svg",
                        "isPro": false,
                        "fullname": "Yushen Zuo",
                        "user": "YSZuo",
                        "type": "user"
                    },
                    "name": "Yushen Zuo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:04:22.576Z",
                    "hidden": false
                },
                {
                    "_id": "67ede544ed9c94861b82b2a1",
                    "user": {
                        "_id": "64756323d815855e4ef945a0",
                        "avatarUrl": "/avatars/29f5150805dafce2b3f9da441c8be988.svg",
                        "isPro": false,
                        "fullname": "Chai",
                        "user": "AllenChai",
                        "type": "user"
                    },
                    "name": "Yuanjun Chai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:23.962Z",
                    "hidden": false
                },
                {
                    "_id": "67ede544ed9c94861b82b2a2",
                    "user": {
                        "_id": "6267a1aff91d1c1633bf3fc0",
                        "avatarUrl": "/avatars/c3a6be3e4cea8c447ea2ba18059a566c.svg",
                        "isPro": false,
                        "fullname": "ZhenDong Liu",
                        "user": "Zaneec",
                        "type": "user"
                    },
                    "name": "Zhendong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:04:31.772Z",
                    "hidden": false
                },
                {
                    "_id": "67ede544ed9c94861b82b2a3",
                    "user": {
                        "_id": "6528ce81598467feb33d992d",
                        "avatarUrl": "/avatars/e98a7bf16e6fd5118e861d562f93bb9b.svg",
                        "isPro": false,
                        "fullname": "Yicheng Fu",
                        "user": "sofyc",
                        "type": "user"
                    },
                    "name": "Yichen Fu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-03T01:32:57.886Z",
                    "hidden": false
                },
                {
                    "_id": "67ede544ed9c94861b82b2a4",
                    "user": {
                        "_id": "67b6e2769fd5603fe15b84b9",
                        "avatarUrl": "/avatars/e6b42c49954a840518bc3e3aa2617912.svg",
                        "isPro": false,
                        "fullname": "fengyichun",
                        "user": "fengyichun",
                        "type": "user"
                    },
                    "name": "Yichun Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:04:42.397Z",
                    "hidden": false
                },
                {
                    "_id": "67ede544ed9c94861b82b2a5",
                    "name": "Kin-man Lam",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T02:35:19.000Z",
            "submittedOnDailyAt": "2025-04-03T00:10:56.307Z",
            "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
            "submittedOnDailyBy": {
                "_id": "64060b49a577649430bf6974",
                "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
                "isPro": false,
                "fullname": "Jiawei Wang",
                "user": "Jarvis1111",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
            "upvotes": 11,
            "discussionId": "67ede549ed9c94861b82b433",
            "githubRepo": "https://github.com/JarvisUSTC/DiffPure-RobustVLM",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "Large Language Models (LLMs)",
                "noise-augmented training",
                "Gaussian noise",
                "Robust-VLGuard",
                "multimodal safety dataset",
                "aligned / misaligned image-text pairs",
                "noise-augmented fine-tuning",
                "diffusion models",
                "DiffPure-VLM",
                "diffusion model",
                "distribution-shifting property",
                "adversarial perturbations"
            ]
        },
        "publishedAt": "2025-04-01T22:35:19.000Z",
        "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
        "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01308.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64060b49a577649430bf6974",
            "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
            "fullname": "Jiawei Wang",
            "name": "Jarvis1111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23573",
            "authors": [
                {
                    "_id": "67ecf726f87008271b1d07d5",
                    "user": {
                        "_id": "67d19d9a3e04b5840ab8684f",
                        "avatarUrl": "/avatars/58b2e28c7360b546141a3cd91a392eed.svg",
                        "isPro": false,
                        "fullname": "Maximilian Augustin",
                        "user": "M4ximal",
                        "type": "user"
                    },
                    "name": "Maximilian Augustin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:05:03.470Z",
                    "hidden": false
                },
                {
                    "_id": "67ecf726f87008271b1d07d6",
                    "user": {
                        "_id": "650971dbce83a0c12a851000",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
                        "isPro": false,
                        "fullname": "Yannic Neuhaus",
                        "user": "YanNeu",
                        "type": "user"
                    },
                    "name": "Yannic Neuhaus",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:51:14.837Z",
                    "hidden": false
                },
                {
                    "_id": "67ecf726f87008271b1d07d7",
                    "name": "Matthias Hein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T19:45:09.000Z",
            "submittedOnDailyAt": "2025-04-03T09:18:51.291Z",
            "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs",
            "submittedOnDailyBy": {
                "_id": "650971dbce83a0c12a851000",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
                "isPro": false,
                "fullname": "Yannic Neuhaus",
                "user": "YanNeu",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) are prone to object hallucinations, where they\nerroneously indicate the presenceof certain objects in an image. Existing\nbenchmarks quantify hallucinations using relatively small, labeled datasets.\nHowever, this approach is i) insufficient to assess hallucinations that arise\nin open-world settings, where VLMs are widely used, and ii) inadequate for\ndetecting systematic errors in VLMs. We propose DASH (Detection and Assessment\nof Systematic Hallucinations), an automatic, large-scale pipeline designed to\nidentify systematic hallucinations of VLMs on real-world images in an\nopen-world setting. A key component is DASH-OPT for image-based retrieval,\nwhere we optimize over the ''natural image manifold'' to generate images that\nmislead the VLM. The output of DASH consists of clusters of real and\nsemantically similar images for which the VLM hallucinates an object. We apply\nDASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in\ntotal, find more than 19k clusters with 950k images. We study the transfer of\nthe identified systematic hallucinations to other VLMs and show that\nfine-tuning PaliGemma with the model-specific images obtained with DASH\nmitigates object hallucinations. Code and data are available at\nhttps://YanNeu.github.io/DASH.",
            "upvotes": 9,
            "discussionId": "67ecf72bf87008271b1d0997",
            "projectPage": "https://yanneu.github.io/DASH",
            "githubRepo": "https://github.com/YanNeu/DASH",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "object hallucinations",
                "small, labeled datasets",
                "open-world settings",
                "systematic errors",
                "DASH (Detection and Assessment of Systematic Hallucinations)",
                "automatic, large-scale pipeline",
                "image-based retrieval",
                "DASH-OPT",
                "natural image manifold",
                "clusters",
                "semantically similar images",
                "PaliGemma",
                "LLaVA-NeXT models",
                "object classes",
                "model-specific images",
                "fine-tuning",
                "code and data"
            ]
        },
        "publishedAt": "2025-03-30T15:45:09.000Z",
        "title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs",
        "summary": "Vision-language models (VLMs) are prone to object hallucinations, where they\nerroneously indicate the presenceof certain objects in an image. Existing\nbenchmarks quantify hallucinations using relatively small, labeled datasets.\nHowever, this approach is i) insufficient to assess hallucinations that arise\nin open-world settings, where VLMs are widely used, and ii) inadequate for\ndetecting systematic errors in VLMs. We propose DASH (Detection and Assessment\nof Systematic Hallucinations), an automatic, large-scale pipeline designed to\nidentify systematic hallucinations of VLMs on real-world images in an\nopen-world setting. A key component is DASH-OPT for image-based retrieval,\nwhere we optimize over the ''natural image manifold'' to generate images that\nmislead the VLM. The output of DASH consists of clusters of real and\nsemantically similar images for which the VLM hallucinates an object. We apply\nDASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in\ntotal, find more than 19k clusters with 950k images. We study the transfer of\nthe identified systematic hallucinations to other VLMs and show that\nfine-tuning PaliGemma with the model-specific images obtained with DASH\nmitigates object hallucinations. Code and data are available at\nhttps://YanNeu.github.io/DASH.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23573.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650971dbce83a0c12a851000",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
            "fullname": "Yannic Neuhaus",
            "name": "YanNeu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23135",
            "authors": [
                {
                    "_id": "67eb3d2110032c28d1ea109f",
                    "user": {
                        "_id": "628ece6054698ce61d1e7be3",
                        "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
                        "isPro": false,
                        "fullname": "Ao Wang",
                        "user": "jameslahm",
                        "type": "user"
                    },
                    "name": "Ao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-02T16:11:10.373Z",
                    "hidden": false
                },
                {
                    "_id": "67eb3d2110032c28d1ea10a0",
                    "user": {
                        "_id": "670d30d232944974d65fdf47",
                        "avatarUrl": "/avatars/193566fcdf5768512d17e4c1801adf45.svg",
                        "isPro": false,
                        "fullname": "Hui Chen",
                        "user": "huichen-cs",
                        "type": "user"
                    },
                    "name": "Hui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:06:28.314Z",
                    "hidden": false
                },
                {
                    "_id": "67eb3d2110032c28d1ea10a1",
                    "name": "Zijia Lin",
                    "hidden": false
                },
                {
                    "_id": "67eb3d2110032c28d1ea10a2",
                    "name": "Jungong Han",
                    "hidden": false
                },
                {
                    "_id": "67eb3d2110032c28d1ea10a3",
                    "name": "Guiguang Ding",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
            ],
            "publishedAt": "2025-03-29T16:00:54.000Z",
            "submittedOnDailyAt": "2025-04-03T00:26:03.944Z",
            "title": "LSNet: See Large, Focus Small",
            "submittedOnDailyBy": {
                "_id": "628ece6054698ce61d1e7be3",
                "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
                "isPro": false,
                "fullname": "Ao Wang",
                "user": "jameslahm",
                "type": "user"
            },
            "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
            "upvotes": 4,
            "discussionId": "67eb3d2310032c28d1ea1108",
            "projectPage": "https://github.com/THU-MIG/lsnet",
            "githubRepo": "https://github.com/THU-MIG/lsnet",
            "ai_keywords": [
                "Convolutional Neural Networks",
                "Vision Transformers",
                "lightweight and efficient network designs",
                "self-attention mechanisms",
                "token mixing",
                "small-kernel aggregation",
                "dynamic heteroscale vision ability",
                "human vision system",
                "``See Large, Focus Small'' strategy",
                "LS (\\textbf{L}arge-\\textbf{S}mall) convolution",
                "large-kernel perception",
                "precise feature aggregation",
                "visual representations",
                "efficient processing of visual information",
                "LSNet",
                "superior performance",
                "efficiency"
            ]
        },
        "publishedAt": "2025-03-29T12:00:54.000Z",
        "title": "LSNet: See Large, Focus Small",
        "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23135.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "628ece6054698ce61d1e7be3",
            "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
            "fullname": "Ao Wang",
            "name": "jameslahm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 26
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00406",
            "authors": [
                {
                    "_id": "67ee3e63e7defc1b8655c8f6",
                    "user": {
                        "_id": "63b0e5a7f2eb87a4d695398a",
                        "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
                        "isPro": false,
                        "fullname": "Jiuzhou Han",
                        "user": "Jiuzhouh",
                        "type": "user"
                    },
                    "name": "Jiuzhou Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:06:39.670Z",
                    "hidden": false
                },
                {
                    "_id": "67ee3e63e7defc1b8655c8f7",
                    "user": {
                        "_id": "65bf02eb4d1795fa8ede0d3f",
                        "avatarUrl": "/avatars/91bdd8eabdf19d61b55493193c222a73.svg",
                        "isPro": false,
                        "fullname": "Wray Buntine",
                        "user": "wbuntine",
                        "type": "user"
                    },
                    "name": "Wray Buntine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:06:46.133Z",
                    "hidden": false
                },
                {
                    "_id": "67ee3e63e7defc1b8655c8f8",
                    "user": {
                        "_id": "65f2a8220effe20744129714",
                        "avatarUrl": "/avatars/524681fa6dda88d6ae20cb68c1bbe6d8.svg",
                        "isPro": false,
                        "fullname": "Ehsan Shareghi",
                        "user": "eehsan",
                        "type": "user"
                    },
                    "name": "Ehsan Shareghi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:06:53.451Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T04:05:03.000Z",
            "submittedOnDailyAt": "2025-04-03T06:26:28.255Z",
            "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
            "submittedOnDailyBy": {
                "_id": "63b0e5a7f2eb87a4d695398a",
                "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
                "isPro": false,
                "fullname": "Jiuzhou Han",
                "user": "Jiuzhouh",
                "type": "user"
            },
            "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
            "upvotes": 2,
            "discussionId": "67ee3e64e7defc1b8655c93b",
            "githubRepo": "https://github.com/Jiuzhouh/VerifiAgent",
            "ai_keywords": [
                "reasoning capabilities",
                "verification methods",
                "unified verification agent",
                "meta-verification",
                "completeness",
                "consistency",
                "tool-based adaptive verification",
                "verification tools",
                "mathematical reasoning",
                "logical reasoning",
                "commonsense reasoning",
                "adaptive approach",
                "verification scenarios",
                "baseline verification methods",
                "deductive verifier",
                "backward verifier",
                "reasoning accuracy",
                "feedback",
                "inference scaling",
                "generated samples",
                "process reward models"
            ]
        },
        "publishedAt": "2025-04-01T00:05:03.000Z",
        "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
        "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00406.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b0e5a7f2eb87a4d695398a",
            "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
            "fullname": "Jiuzhou Han",
            "name": "Jiuzhouh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.23798",
            "authors": [
                {
                    "_id": "67ec9ed83d267d266640c03d",
                    "user": {
                        "_id": "659d98feec9f8f337f145ec4",
                        "avatarUrl": "/avatars/7de3bea0f203326e78916882655b5d55.svg",
                        "isPro": false,
                        "fullname": "xuan luo",
                        "user": "xuan-luo",
                        "type": "user"
                    },
                    "name": "Xuan Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:44.959Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9ed83d267d266640c03e",
                    "user": {
                        "_id": "63d34004b734eaa4d4faeccf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
                        "isPro": false,
                        "fullname": "Weizhi Wang",
                        "user": "weizhiwang",
                        "type": "user"
                    },
                    "name": "Weizhi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:22:42.010Z",
                    "hidden": false
                },
                {
                    "_id": "67ec9ed83d267d266640c03f",
                    "name": "Xifeng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T07:20:58.000Z",
            "submittedOnDailyAt": "2025-04-03T21:28:17.167Z",
            "title": "Adaptive Layer-skipping in Pre-trained LLMs",
            "submittedOnDailyBy": {
                "_id": "63d34004b734eaa4d4faeccf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
                "isPro": false,
                "fullname": "Weizhi Wang",
                "user": "weizhiwang",
                "type": "user"
            },
            "summary": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.",
            "upvotes": 2,
            "discussionId": "67ec9ed93d267d266640c064",
            "ai_keywords": [
                "FlexiDepth",
                "Transformer layers",
                "layer-skipping methods",
                "token generation",
                "large language models (LLMs)",
                "plug-in router",
                "adapter",
                "benchmark performance",
                "token type",
                "repetitive tokens",
                "fixed phrases",
                "computation or high uncertainty",
                "adaptive allocation pattern"
            ]
        },
        "publishedAt": "2025-03-31T03:20:58.000Z",
        "title": "Adaptive Layer-skipping in Pre-trained LLMs",
        "summary": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23798.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
            "fullname": "Weizhi Wang",
            "name": "weizhiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22879",
            "authors": [
                {
                    "_id": "67eea40fe01fcf131baa149e",
                    "user": {
                        "_id": "65ca7d8d5cf913133d3b493a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ca7d8d5cf913133d3b493a/i9qx6xdYeGSuwsxgQCb7N.png",
                        "isPro": false,
                        "fullname": "Hung-Yueh Chiang",
                        "user": "hychiang",
                        "type": "user"
                    },
                    "name": "Hung-Yueh Chiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T19:20:45.914Z",
                    "hidden": false
                },
                {
                    "_id": "67eea40fe01fcf131baa149f",
                    "name": "Chi-Chih Chang",
                    "hidden": false
                },
                {
                    "_id": "67eea40fe01fcf131baa14a0",
                    "name": "Natalia Frumkin",
                    "hidden": false
                },
                {
                    "_id": "67eea40fe01fcf131baa14a1",
                    "name": "Kai-Chiang Wu",
                    "hidden": false
                },
                {
                    "_id": "67eea40fe01fcf131baa14a2",
                    "name": "Mohamed S. Abdelfattah",
                    "hidden": false
                },
                {
                    "_id": "67eea40fe01fcf131baa14a3",
                    "name": "Diana Marculescu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-28T21:10:39.000Z",
            "submittedOnDailyAt": "2025-04-03T19:11:56.281Z",
            "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models",
            "submittedOnDailyBy": {
                "_id": "65ca7d8d5cf913133d3b493a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ca7d8d5cf913133d3b493a/i9qx6xdYeGSuwsxgQCb7N.png",
                "isPro": false,
                "fullname": "Hung-Yueh Chiang",
                "user": "hychiang",
                "type": "user"
            },
            "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input x, combined\nwith a per-state-group quantization for input-dependent parameters B and C.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3times and 3times speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4times memory reduction with only a 1.6%\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.",
            "upvotes": 2,
            "discussionId": "67eea410e01fcf131baa14f6",
            "projectPage": "https://hychiang.info/projects/quamba2",
            "githubRepo": "https://github.com/enyac-group/Quamba",
            "ai_keywords": [
                "State Space Models (SSMs)",
                "Transformers",
                "memory usage",
                "high performance",
                "quantizing",
                "bit-width data formats",
                "quantization-induced errors",
                "low bit-width",
                "W4A8",
                "W4A16",
                "channel order preserving",
                "activation persistence",
                "offline approach",
                "linear recurrence",
                "sorting and clustering",
                "input-dependent parameters",
                "per-state-group quantization",
                "compute-invariance",
                "reordering weights"
            ]
        },
        "publishedAt": "2025-03-28T17:10:39.000Z",
        "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for\n  Selective State Space Models",
        "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input x, combined\nwith a per-state-group quantization for input-dependent parameters B and C.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3times and 3times speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4times memory reduction with only a 1.6%\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22879.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ca7d8d5cf913133d3b493a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ca7d8d5cf913133d3b493a/i9qx6xdYeGSuwsxgQCb7N.png",
            "fullname": "Hung-Yueh Chiang",
            "name": "hychiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01201",
            "authors": [
                {
                    "_id": "67ee95f48b0b5419e49c37dc",
                    "user": {
                        "_id": "66a02bb71bb371390c4ae519",
                        "avatarUrl": "/avatars/2c4791516aae0b20a9c9ce0542dc966e.svg",
                        "isPro": false,
                        "fullname": "Krithik Vishwanath",
                        "user": "KrithikV",
                        "type": "user"
                    },
                    "name": "Krithik Vishwanath",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-03T14:06:45.697Z",
                    "hidden": false
                },
                {
                    "_id": "67ee95f48b0b5419e49c37dd",
                    "name": "Anton Alyakin",
                    "hidden": false
                },
                {
                    "_id": "67ee95f48b0b5419e49c37de",
                    "name": "Daniel Alexander Alber",
                    "hidden": false
                },
                {
                    "_id": "67ee95f48b0b5419e49c37df",
                    "name": "Jin Vivian Lee",
                    "hidden": false
                },
                {
                    "_id": "67ee95f48b0b5419e49c37e0",
                    "name": "Douglas Kondziolka",
                    "hidden": false
                },
                {
                    "_id": "67ee95f48b0b5419e49c37e1",
                    "name": "Eric Karl Oermann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T21:34:01.000Z",
            "submittedOnDailyAt": "2025-04-03T12:38:35.211Z",
            "title": "Medical large language models are easily distracted",
            "submittedOnDailyBy": {
                "_id": "66a02bb71bb371390c4ae519",
                "avatarUrl": "/avatars/2c4791516aae0b20a9c9ce0542dc966e.svg",
                "isPro": false,
                "fullname": "Krithik Vishwanath",
                "user": "KrithikV",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have the potential to transform medicine, but\nreal-world clinical scenarios contain extraneous information that can hinder\nperformance. The rise of assistive technologies like ambient dictation, which\nautomatically generates draft notes from live patient encounters, has the\npotential to introduce additional noise making it crucial to assess the ability\nof LLM's to filter relevant data. To investigate this, we developed\nMedDistractQA, a benchmark using USMLE-style questions embedded with simulated\nreal-world distractions. Our findings show that distracting statements\n(polysemous words with clinical meanings used in a non-clinical context or\nreferences to unrelated health conditions) can reduce LLM accuracy by up to\n17.9%. Commonly proposed solutions to improve model performance such as\nretrieval-augmented generation (RAG) and medical fine-tuning did not change\nthis effect and in some cases introduced their own confounders and further\ndegraded performance. Our findings suggest that LLMs natively lack the logical\nmechanisms necessary to distinguish relevant from irrelevant clinical\ninformation, posing challenges for real-world applications. MedDistractQA and\nour results highlights the need for robust mitigation strategies to enhance LLM\nresilience to extraneous information.",
            "upvotes": 1,
            "discussionId": "67ee95f58b0b5419e49c382e",
            "githubRepo": "https://github.com/nyuolab/MedDistractQA",
            "ai_keywords": [
                "ambient dictation",
                "retrieval-augmented generation (RAG)",
                "medical fine-tuning",
                "USMLE-style questions",
                "polysemous words",
                "clinical meanings",
                "non-clinical context",
                "unrelated health conditions",
                "MedDistractQA"
            ]
        },
        "publishedAt": "2025-04-01T17:34:01.000Z",
        "title": "Medical large language models are easily distracted",
        "summary": "Large language models (LLMs) have the potential to transform medicine, but\nreal-world clinical scenarios contain extraneous information that can hinder\nperformance. The rise of assistive technologies like ambient dictation, which\nautomatically generates draft notes from live patient encounters, has the\npotential to introduce additional noise making it crucial to assess the ability\nof LLM's to filter relevant data. To investigate this, we developed\nMedDistractQA, a benchmark using USMLE-style questions embedded with simulated\nreal-world distractions. Our findings show that distracting statements\n(polysemous words with clinical meanings used in a non-clinical context or\nreferences to unrelated health conditions) can reduce LLM accuracy by up to\n17.9%. Commonly proposed solutions to improve model performance such as\nretrieval-augmented generation (RAG) and medical fine-tuning did not change\nthis effect and in some cases introduced their own confounders and further\ndegraded performance. Our findings suggest that LLMs natively lack the logical\nmechanisms necessary to distinguish relevant from irrelevant clinical\ninformation, posing challenges for real-world applications. MedDistractQA and\nour results highlights the need for robust mitigation strategies to enhance LLM\nresilience to extraneous information.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01201.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a02bb71bb371390c4ae519",
            "avatarUrl": "/avatars/2c4791516aae0b20a9c9ce0542dc966e.svg",
            "fullname": "Krithik Vishwanath",
            "name": "KrithikV",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.18950",
            "authors": [
                {
                    "_id": "67ee9fb0ffcd7f6fd1bfffbc",
                    "name": "Taeksoo Kim",
                    "hidden": false
                },
                {
                    "_id": "67ee9fb0ffcd7f6fd1bfffbd",
                    "name": "Hanbyul Joo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b1673635c2e9909c266407/m9WFlVzDqgi4IetoxGS_a.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/64b1673635c2e9909c266407/nZcc_-bg42iQivvRYm-Qe.png"
            ],
            "publishedAt": "2025-03-24T17:59:59.000Z",
            "submittedOnDailyAt": "2025-04-03T14:00:45.029Z",
            "title": "Target-Aware Video Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "64b1673635c2e9909c266407",
                "avatarUrl": "/avatars/2a0fdca5983dcaf1f2b09eac56cf586b.svg",
                "isPro": false,
                "fullname": "Taeksoo Kim",
                "user": "Taeksoo",
                "type": "user"
            },
            "summary": "We present a target-aware video diffusion model that generates videos from an\ninput image in which an actor interacts with a specified target while\nperforming a desired action. The target is defined by a segmentation mask and\nthe desired action is described via a text prompt. Unlike existing controllable\nimage-to-video diffusion models that often rely on dense structural or motion\ncues to guide the actor's movements toward the target, our target-aware model\nrequires only a simple mask to indicate the target, leveraging the\ngeneralization capabilities of pretrained models to produce plausible actions.\nThis makes our method particularly effective for human-object interaction (HOI)\nscenarios, where providing precise action guidance is challenging, and further\nenables the use of video diffusion models for high-level action planning in\napplications such as robotics. We build our target-aware model by extending a\nbaseline model to incorporate the target mask as an additional input. To\nenforce target awareness, we introduce a special token that encodes the\ntarget's spatial information within the text prompt. We then fine-tune the\nmodel with our curated dataset using a novel cross-attention loss that aligns\nthe cross-attention maps associated with this token with the input target mask.\nTo further improve performance, we selectively apply this loss to the most\nsemantically relevant transformer blocks and attention regions. Experimental\nresults show that our target-aware model outperforms existing solutions in\ngenerating videos where actors interact accurately with the specified targets.\nWe further demonstrate its efficacy in two downstream applications: video\ncontent creation and zero-shot 3D HOI motion synthesis.",
            "upvotes": 1,
            "discussionId": "67ee9fb4ffcd7f6fd1c000b8",
            "projectPage": "https://taeksuu.github.io/tavid/",
            "ai_keywords": [
                "target-aware video diffusion model",
                "segmentation mask",
                "text prompt",
                "human-object interaction (HOI)",
                "pretrained models",
                "special token",
                "cross-attention loss",
                "transformer blocks",
                "attention regions",
                "zero-shot 3D HOI motion synthesis"
            ]
        },
        "publishedAt": "2025-03-24T13:59:59.000Z",
        "title": "Target-Aware Video Diffusion Models",
        "summary": "We present a target-aware video diffusion model that generates videos from an\ninput image in which an actor interacts with a specified target while\nperforming a desired action. The target is defined by a segmentation mask and\nthe desired action is described via a text prompt. Unlike existing controllable\nimage-to-video diffusion models that often rely on dense structural or motion\ncues to guide the actor's movements toward the target, our target-aware model\nrequires only a simple mask to indicate the target, leveraging the\ngeneralization capabilities of pretrained models to produce plausible actions.\nThis makes our method particularly effective for human-object interaction (HOI)\nscenarios, where providing precise action guidance is challenging, and further\nenables the use of video diffusion models for high-level action planning in\napplications such as robotics. We build our target-aware model by extending a\nbaseline model to incorporate the target mask as an additional input. To\nenforce target awareness, we introduce a special token that encodes the\ntarget's spatial information within the text prompt. We then fine-tune the\nmodel with our curated dataset using a novel cross-attention loss that aligns\nthe cross-attention maps associated with this token with the input target mask.\nTo further improve performance, we selectively apply this loss to the most\nsemantically relevant transformer blocks and attention regions. Experimental\nresults show that our target-aware model outperforms existing solutions in\ngenerating videos where actors interact accurately with the specified targets.\nWe further demonstrate its efficacy in two downstream applications: video\ncontent creation and zero-shot 3D HOI motion synthesis.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b1673635c2e9909c266407/m9WFlVzDqgi4IetoxGS_a.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64b1673635c2e9909c266407/nZcc_-bg42iQivvRYm-Qe.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18950.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b1673635c2e9909c266407",
            "avatarUrl": "/avatars/2a0fdca5983dcaf1f2b09eac56cf586b.svg",
            "fullname": "Taeksoo Kim",
            "name": "Taeksoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.18817",
            "authors": [
                {
                    "_id": "67ede492bdd88c72dc99fbd7",
                    "user": {
                        "_id": "654b4c9cfabd2cc66874806c",
                        "avatarUrl": "/avatars/c48b423d3657ee27eed0d3d83738ec2e.svg",
                        "isPro": false,
                        "fullname": "jeonghyeon kim",
                        "user": "mawjdgus",
                        "type": "user"
                    },
                    "name": "Jeonghyeon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:31.201Z",
                    "hidden": false
                },
                {
                    "_id": "67ede492bdd88c72dc99fbd8",
                    "user": {
                        "_id": "64e6ff6b09375662071ae8c8",
                        "avatarUrl": "/avatars/4e1086476a29733eb4680eb8771d2f68.svg",
                        "isPro": false,
                        "fullname": "Sangheum Hwang",
                        "user": "beopst",
                        "type": "user"
                    },
                    "name": "Sangheum Hwang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-03T14:07:05.202Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-24T16:00:21.000Z",
            "submittedOnDailyAt": "2025-04-03T06:37:34.505Z",
            "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations",
            "submittedOnDailyBy": {
                "_id": "654b4c9cfabd2cc66874806c",
                "avatarUrl": "/avatars/c48b423d3657ee27eed0d3d83738ec2e.svg",
                "isPro": false,
                "fullname": "jeonghyeon kim",
                "user": "mawjdgus",
                "type": "user"
            },
            "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.",
            "upvotes": 1,
            "discussionId": "67ede493bdd88c72dc99fc2d",
            "githubRepo": "https://github.com/ma-kjh/CMA-OoDD",
            "ai_keywords": [
                "out-of-distribution detection (OoDD)",
                "single-modality models",
                "large-scale pretrained vision-language models",
                "CLIP",
                "zero-shot learning",
                "prompt learning",
                "multi-modal fine-tuning (MMFT)",
                "downstream datasets",
                "fine-tuning methods",
                "modality gap",
                "in-distribution (ID) embeddings",
                "cross-modal alignment",
                "regularization",
                "image and text embeddings",
                "hyperspherical representation space",
                "energy-based model",
                "NegLabel",
                "post-hoc OoDD approaches",
                "ImageNet-1k",
                "state-of-the-art OoDD performance",
                "ID accuracy"
            ]
        },
        "publishedAt": "2025-03-24T12:00:21.000Z",
        "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations",
        "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18817.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654b4c9cfabd2cc66874806c",
            "avatarUrl": "/avatars/c48b423d3657ee27eed0d3d83738ec2e.svg",
            "fullname": "jeonghyeon kim",
            "name": "mawjdgus",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2502.18924",
            "authors": [
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e0",
                    "user": {
                        "_id": "67dbdf261956dcedf0f0a7e1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/BRzXe_7jytEYadJ0byMyD.png",
                        "isPro": false,
                        "fullname": "ZiyueJiang",
                        "user": "ZiyueJiang",
                        "type": "user"
                    },
                    "name": "Ziyue Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T08:28:36.564Z",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e1",
                    "name": "Yi Ren",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e2",
                    "name": "Ruiqi Li",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e3",
                    "name": "Shengpeng Ji",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e4",
                    "name": "Boyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e5",
                    "name": "Zhenhui Ye",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e6",
                    "name": "Chen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e7",
                    "name": "Bai Jionghao",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e8",
                    "name": "Xiaoda Yang",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1e9",
                    "name": "Jialong Zuo",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1ea",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1eb",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1ec",
                    "name": "Xiang Yin",
                    "hidden": false
                },
                {
                    "_id": "67ee3cf7042e8ba3e96fc1ed",
                    "name": "Zhou Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-02-26T08:22:00.000Z",
            "submittedOnDailyAt": "2025-04-03T12:53:34.642Z",
            "title": "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "While recent zero-shot text-to-speech (TTS) models have significantly\nimproved speech quality and expressiveness, mainstream systems still suffer\nfrom issues related to speech-text alignment modeling: 1) models without\nexplicit speech-text alignment modeling exhibit less robustness, especially for\nhard sentences in practical applications; 2) predefined alignment-based models\nsuffer from naturalness constraints of forced alignments. This paper introduces\nMegaTTS 3, a TTS system featuring an innovative sparse alignment\nalgorithm that guides the latent diffusion transformer (DiT). Specifically, we\nprovide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of\nalignment without limiting the search space, thereby achieving high\nnaturalness. Moreover, we employ a multi-condition classifier-free guidance\nstrategy for accent intensity adjustment and adopt the piecewise rectified flow\ntechnique to accelerate the generation process. Experiments demonstrate that\nMegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports\nhighly flexible control over accent intensity. Notably, our system can generate\nhigh-quality one-minute speech with only 8 sampling steps. Audio samples are\navailable at https://sditdemo.github.io/sditdemo/.",
            "upvotes": 1,
            "discussionId": "67ee3cf8042e8ba3e96fc22e",
            "projectPage": "https://sditdemo.github.io/sditdemo/",
            "ai_keywords": [
                "sparse alignment algorithm",
                "latent diffusion transformer (DiT)",
                "sparse alignment boundaries",
                "multi-condition classifier-free guidance",
                "piecewise rectified flow"
            ]
        },
        "publishedAt": "2025-02-26T03:22:00.000Z",
        "title": "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis",
        "summary": "While recent zero-shot text-to-speech (TTS) models have significantly\nimproved speech quality and expressiveness, mainstream systems still suffer\nfrom issues related to speech-text alignment modeling: 1) models without\nexplicit speech-text alignment modeling exhibit less robustness, especially for\nhard sentences in practical applications; 2) predefined alignment-based models\nsuffer from naturalness constraints of forced alignments. This paper introduces\nMegaTTS 3, a TTS system featuring an innovative sparse alignment\nalgorithm that guides the latent diffusion transformer (DiT). Specifically, we\nprovide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of\nalignment without limiting the search space, thereby achieving high\nnaturalness. Moreover, we employ a multi-condition classifier-free guidance\nstrategy for accent intensity adjustment and adopt the piecewise rectified flow\ntechnique to accelerate the generation process. Experiments demonstrate that\nMegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports\nhighly flexible control over accent intensity. Notably, our system can generate\nhigh-quality one-minute speech with only 8 sampling steps. Audio samples are\navailable at https://sditdemo.github.io/sditdemo/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 808
        },
        "isAuthorParticipating": false
    }
]