[
    {
        "paper": {
            "id": "2504.01990",
            "authors": [
                {
                    "_id": "67ef8723d325fe100f36107e",
                    "user": {
                        "_id": "654a97282d2fcd6bf2851173",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
                        "isPro": false,
                        "fullname": "Bang Liu",
                        "user": "Bang-UdeM-Mila",
                        "type": "user"
                    },
                    "name": "Bang Liu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-04T12:50:44.274Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36107f",
                    "user": {
                        "_id": "65af5f8f3db2280ece7fac79",
                        "avatarUrl": "/avatars/66d88b2d744c8d00e11d39a55ab86c2e.svg",
                        "isPro": false,
                        "fullname": "Xin-Feng Li",
                        "user": "xinfeng1i",
                        "type": "user"
                    },
                    "name": "Xinfeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:26:45.785Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361080",
                    "user": {
                        "_id": "64b78a39954ae43365984448",
                        "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "Peiyan",
                        "type": "user"
                    },
                    "name": "Jiayi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:28:49.444Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361081",
                    "user": {
                        "_id": "64324bb3034ecbefddd99863",
                        "avatarUrl": "/avatars/3b8cdc2066251999a3a7e6d5565dceb5.svg",
                        "isPro": false,
                        "fullname": "Jinlin Wang",
                        "user": "JinlinW",
                        "type": "user"
                    },
                    "name": "Jinlin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:27:02.727Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361082",
                    "name": "Tanjin He",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361083",
                    "name": "Sirui Hong",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361084",
                    "name": "Hongzhang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361085",
                    "name": "Shaokun Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361086",
                    "user": {
                        "_id": "5fc0b2b61160c47d1d438568",
                        "avatarUrl": "/avatars/90beea6b452c662d579197dbf592423a.svg",
                        "isPro": false,
                        "fullname": "Kaitao Song",
                        "user": "KaitaoSong",
                        "type": "user"
                    },
                    "name": "Kaitao Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:27:47.151Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361087",
                    "user": {
                        "_id": "64c090a9f613170e7be93d2f",
                        "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
                        "isPro": false,
                        "fullname": "KunlunZhu",
                        "user": "KunlunZhu",
                        "type": "user"
                    },
                    "name": "Kunlun Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:28:03.582Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361088",
                    "name": "Yuheng Cheng",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361089",
                    "user": {
                        "_id": "62bb1e0f3ff437e49a3088e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
                        "isPro": true,
                        "fullname": "Suyuchen Wang",
                        "user": "sheryc",
                        "type": "user"
                    },
                    "name": "Suyuchen Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:28:21.351Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36108a",
                    "user": {
                        "_id": "655c092183186f133f959108",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WMVgGhjQbVJXK9eh4EuT9.jpeg",
                        "isPro": false,
                        "fullname": "Xiaoqiang Wang",
                        "user": "qindomitable",
                        "type": "user"
                    },
                    "name": "Xiaoqiang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:28:26.707Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36108b",
                    "name": "Yuyu Luo",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36108c",
                    "user": {
                        "_id": "648ee5fe9ae7cc4fcffa9aef",
                        "avatarUrl": "/avatars/9bcc5eb91452c1360b9a0a4f9def8af8.svg",
                        "isPro": false,
                        "fullname": "Haibo Jin",
                        "user": "Nick233",
                        "type": "user"
                    },
                    "name": "Haibo Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:28:40.177Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36108d",
                    "user": {
                        "_id": "64b78a39954ae43365984448",
                        "avatarUrl": "/avatars/6de9d4bf320a69eca6b758e718ee116c.svg",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "Peiyan",
                        "type": "user"
                    },
                    "name": "Peiyan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:29:02.679Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36108e",
                    "user": {
                        "_id": "66197a8afeb55cbe39e50ae8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pcSS1TsCoHxRcAEkcMNm0.png",
                        "isPro": false,
                        "fullname": "Ollie Liu",
                        "user": "oliu-io",
                        "type": "user"
                    },
                    "name": "Ollie Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:29:10.114Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36108f",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361090",
                    "user": {
                        "_id": "6719d581a6cad13741b8bc7f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
                        "isPro": false,
                        "fullname": "Huan Zhang",
                        "user": "huanzhang12",
                        "type": "user"
                    },
                    "name": "Huan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:29:24.614Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361091",
                    "user": {
                        "_id": "640dc84b474aa6f89554d518",
                        "avatarUrl": "/avatars/64f47f76d97c5e91b7ab8380bcada61c.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Yu",
                        "user": "MoshiQAQ",
                        "type": "user"
                    },
                    "name": "Zhaoyang Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:29:38.969Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361092",
                    "name": "Haochen Shi",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361093",
                    "name": "Boyan Li",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361094",
                    "name": "Dekun Wu",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361095",
                    "user": {
                        "_id": "6402e8fb06c715b93407442d",
                        "avatarUrl": "/avatars/12b67f0632be5a53b56d8a68586a7f98.svg",
                        "isPro": false,
                        "fullname": "Fengwei Teng",
                        "user": "leavendough",
                        "type": "user"
                    },
                    "name": "Fengwei Teng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:30:09.492Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361096",
                    "name": "Xiaojun Jia",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361097",
                    "name": "Jiawei Xu",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361098",
                    "name": "Jinyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f361099",
                    "name": "Yizhang Lin",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36109a",
                    "name": "Tianming Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36109b",
                    "name": "Tongliang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36109c",
                    "name": "Yu Su",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36109d",
                    "name": "Huan Sun",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36109e",
                    "user": {
                        "_id": "66a8fa5fd909c30167f1f5cd",
                        "avatarUrl": "/avatars/c9b26d5b2dd78bed9661df429012fd97.svg",
                        "isPro": false,
                        "fullname": "Glen Berseth",
                        "user": "gberseth",
                        "type": "user"
                    },
                    "name": "Glen Berseth",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:30:19.433Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f36109f",
                    "name": "Jianyun Nie",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a0",
                    "name": "Ian Foster",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a1",
                    "name": "Logan Ward",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a2",
                    "name": "Qingyun Wu",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a3",
                    "user": {
                        "_id": "5e7e595230dc073f817a2bb5",
                        "avatarUrl": "/avatars/d5ff36e45555d9e169cf56c845736444.svg",
                        "isPro": false,
                        "fullname": "Yu Gu",
                        "user": "entslscheia",
                        "type": "user"
                    },
                    "name": "Yu Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:50:40.603Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a4",
                    "user": {
                        "_id": "64403daae44f30a72323e4ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
                        "isPro": false,
                        "fullname": "mingchen zhuge",
                        "user": "tjpxiaoming",
                        "type": "user"
                    },
                    "name": "Mingchen Zhuge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:26:28.011Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a5",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a6",
                    "name": "Haohan Wang",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a7",
                    "name": "Jiaxuan You",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a8",
                    "name": "Chi Wang",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610a9",
                    "user": {
                        "_id": "670918b02806bda07e44780c",
                        "avatarUrl": "/avatars/c08ba5048d9e911ef488862e8869792f.svg",
                        "isPro": false,
                        "fullname": "Jian Pei",
                        "user": "StrawHat2333",
                        "type": "user"
                    },
                    "name": "Jian Pei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:36:21.496Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610aa",
                    "name": "Qiang Yang",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610ab",
                    "user": {
                        "_id": "6342829eb9454d65a2e7a4c4",
                        "avatarUrl": "/avatars/4438abdf189dbe26a52948800d79a7c5.svg",
                        "isPro": false,
                        "fullname": "Xiaoliang Qi",
                        "user": "phynics",
                        "type": "user"
                    },
                    "name": "Xiaoliang Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:30:59.175Z",
                    "hidden": false
                },
                {
                    "_id": "67ef8723d325fe100f3610ac",
                    "name": "Chenglin Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T18:00:29.000Z",
            "submittedOnDailyAt": "2025-04-04T05:46:58.338Z",
            "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
            "submittedOnDailyBy": {
                "_id": "64403daae44f30a72323e4ca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
                "isPro": false,
                "fullname": "mingchen zhuge",
                "user": "tjpxiaoming",
                "type": "user"
            },
            "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
            "upvotes": 98,
            "discussionId": "67ef8727d325fe100f3611aa",
            "githubRepo": "https://github.com/FoundationAgents/awesome-foundation-agents"
        },
        "publishedAt": "2025-03-31T14:00:29.000Z",
        "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
        "summary": "The advent of large language models (LLMs) has catalyzed a transformative\nshift in artificial intelligence, paving the way for advanced intelligent\nagents capable of sophisticated reasoning, robust perception, and versatile\naction across diverse domains. As these agents increasingly drive AI research\nand practical applications, their design, evaluation, and continuous\nimprovement present intricate, multifaceted challenges. This survey provides a\ncomprehensive overview, framing intelligent agents within a modular,\nbrain-inspired architecture that integrates principles from cognitive science,\nneuroscience, and computational research. We structure our exploration into\nfour interconnected parts. First, we delve into the modular foundation of\nintelligent agents, systematically mapping their cognitive, perceptual, and\noperational modules onto analogous human brain functionalities, and elucidating\ncore components such as memory, world modeling, reward processing, and\nemotion-like systems. Second, we discuss self-enhancement and adaptive\nevolution mechanisms, exploring how agents autonomously refine their\ncapabilities, adapt to dynamic environments, and achieve continual learning\nthrough automated optimization paradigms, including emerging AutoML and\nLLM-driven optimization strategies. Third, we examine collaborative and\nevolutionary multi-agent systems, investigating the collective intelligence\nemerging from agent interactions, cooperation, and societal structures,\nhighlighting parallels to human social dynamics. Finally, we address the\ncritical imperative of building safe, secure, and beneficial AI systems,\nemphasizing intrinsic and extrinsic security threats, ethical alignment,\nrobustness, and practical mitigation strategies necessary for trustworthy\nreal-world deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01990.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64403daae44f30a72323e4ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png",
            "fullname": "mingchen zhuge",
            "name": "tjpxiaoming",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02826",
            "authors": [
                {
                    "_id": "67ef4be0985aa66b67021ddc",
                    "user": {
                        "_id": "6530e62f536dbca918e71c3e",
                        "avatarUrl": "/avatars/efc93bc767e561c6c6d429f65c23382d.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Z",
                        "user": "PhoenixZ",
                        "type": "user"
                    },
                    "name": "Xiangyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:07.389Z",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021ddd",
                    "user": {
                        "_id": "6710be3e6d1b33cf24417e38",
                        "avatarUrl": "/avatars/f60bc9a67bb58f5997cbcc28cb93c079.svg",
                        "isPro": false,
                        "fullname": "zpy",
                        "user": "zpy777",
                        "type": "user"
                    },
                    "name": "Peiyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:11.816Z",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021dde",
                    "user": {
                        "_id": "662516d72419feed62fb3a0a",
                        "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
                        "isPro": false,
                        "fullname": "Dian",
                        "user": "KexianTang",
                        "type": "user"
                    },
                    "name": "Kexian Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:21:18.584Z",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021ddf",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021de0",
                    "name": "Zicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021de1",
                    "user": {
                        "_id": "65535b125413c1a54e6fb243",
                        "avatarUrl": "/avatars/03bcf1d58865f5406aff49a415e78bdc.svg",
                        "isPro": false,
                        "fullname": "Guangtao Zhai",
                        "user": "GTZhai",
                        "type": "user"
                    },
                    "name": "Guangtao Zhai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:22:23.057Z",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021de2",
                    "user": {
                        "_id": "667289f903c802764985d8c6",
                        "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
                        "isPro": false,
                        "fullname": "Junchi Yan",
                        "user": "Rethinker",
                        "type": "user"
                    },
                    "name": "Junchi Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:22:30.311Z",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021de3",
                    "name": "Hua Yang",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021de4",
                    "user": {
                        "_id": "648e77184cae4f6921dbb382",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e77184cae4f6921dbb382/zAAJRvOStC0wZplqVWrk_.jpeg",
                        "isPro": false,
                        "fullname": "Xue Yang",
                        "user": "yangxue",
                        "type": "user"
                    },
                    "name": "Xue Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:09.267Z",
                    "hidden": false
                },
                {
                    "_id": "67ef4be0985aa66b67021de5",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:22:37.146Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
            ],
            "publishedAt": "2025-04-03T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-04T01:35:35.280Z",
            "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
            "submittedOnDailyBy": {
                "_id": "63ee1379190ddd6214efd73a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                "isPro": false,
                "fullname": "HAODONG DUAN",
                "user": "KennyUTC",
                "type": "user"
            },
            "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
            "upvotes": 55,
            "discussionId": "67ef4be4985aa66b67021ef2",
            "githubRepo": "https://github.com/PhoenixZ810/RISEBench",
            "ai_keywords": [
                "Large Multi-modality Models (LMMs)",
                "General Visual Editing",
                "Temporal Reasoning",
                "Causal Reasoning",
                "Spatial Reasoning",
                "Logical Reasoning",
                "RISEBench",
                "Instruction Reasoning",
                "Appearance Consistency",
                "Visual Plausibility",
                "GPT-4o-Native",
                "multimodal systems"
            ]
        },
        "publishedAt": "2025-04-03T13:59:56.000Z",
        "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
        "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/PQFr3_6S3BeUSNb79jMSO.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/0rGBBJT_JcUPBk5cE_Te0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02826.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "fullname": "HAODONG DUAN",
            "name": "KennyUTC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02507",
            "authors": [
                {
                    "_id": "67ef5a3d4417508df8d99dad",
                    "user": {
                        "_id": "62cd4b03c5cc157be82f0b56",
                        "avatarUrl": "/avatars/351e963c1c763d507ae78cbcd62966a3.svg",
                        "isPro": false,
                        "fullname": "Abhay kumar",
                        "user": "akanyaani",
                        "type": "user"
                    },
                    "name": "Abhay Kumar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:18.512Z",
                    "hidden": false
                },
                {
                    "_id": "67ef5a3d4417508df8d99dae",
                    "user": {
                        "_id": "6071c4b270e11b30cfcfd7a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
                        "isPro": false,
                        "fullname": "Louis Owen",
                        "user": "louisowen6",
                        "type": "user"
                    },
                    "name": "Louis Owen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:21.051Z",
                    "hidden": false
                },
                {
                    "_id": "67ef5a3d4417508df8d99daf",
                    "user": {
                        "_id": "645a0d3dd6648853107c5fdc",
                        "avatarUrl": "/avatars/1e3b6a4f5ce81a707ba7cbdf81631091.svg",
                        "isPro": false,
                        "fullname": "Nilabhra Roy Chowdhury",
                        "user": "nilabhra",
                        "type": "user"
                    },
                    "name": "Nilabhra Roy Chowdhury",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:12.764Z",
                    "hidden": false
                },
                {
                    "_id": "67ef5a3d4417508df8d99db0",
                    "user": {
                        "_id": "65e4be59e8b017ee1310a1b6",
                        "avatarUrl": "/avatars/c3f7cdf5d0859cb80bfb2b970a675dfa.svg",
                        "isPro": false,
                        "fullname": "Fabian",
                        "user": "gueraf",
                        "type": "user"
                    },
                    "name": "Fabian GÃ¼ra",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:16.132Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T11:41:55.000Z",
            "submittedOnDailyAt": "2025-04-04T02:34:36.631Z",
            "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
            "submittedOnDailyBy": {
                "_id": "6071c4b270e11b30cfcfd7a3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
                "isPro": false,
                "fullname": "Louis Owen",
                "user": "louisowen6",
                "type": "user"
            },
            "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
            "upvotes": 46,
            "discussionId": "67ef5a3e4417508df8d99dfc",
            "githubRepo": "https://github.com/bluorion-com/ZClip/",
            "ai_keywords": [
                "large language models (LLMs)",
                "gradient instability",
                "loss spikes",
                "catastrophic divergence",
                "checkpoint restoration",
                "data batch skipping",
                "traditional gradient clipping techniques",
                "norm-based methods",
                "adaptive gradient clipping",
                "clipping threshold",
                "statistical properties of gradient norms",
                "z-score-based anomaly detection",
                "malignant loss spikes",
                "convergence"
            ]
        },
        "publishedAt": "2025-04-03T07:41:55.000Z",
        "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
        "summary": "Training large language models (LLMs) presents numerous challenges, including\ngradient instability and loss spikes. These phenomena can lead to catastrophic\ndivergence, requiring costly checkpoint restoration and data batch skipping.\nTraditional gradient clipping techniques, such as constant or norm-based\nmethods, fail to address these issues effectively due to their reliance on\nfixed thresholds or heuristics, leading to inefficient learning and requiring\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\ngradient clipping algorithm that dynamically adjusts the clipping threshold\nbased on statistical properties of gradient norms over time. Unlike prior\nreactive strategies, ZClip proactively adapts to training dynamics without\nmaking any prior assumptions on the scale and the temporal evolution of\ngradient norms. At its core, it leverages z-score-based anomaly detection to\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\nwhile not interfering with convergence otherwise. Our code is available at:\nhttps://github.com/bluorion-com/ZClip.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02507.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6071c4b270e11b30cfcfd7a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6071c4b270e11b30cfcfd7a3/-1ekCBzSTpqxkkul0bgmI.jpeg",
            "fullname": "Louis Owen",
            "name": "louisowen6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02782",
            "authors": [
                {
                    "_id": "67ef502ce803d818f00e1b94",
                    "name": "Zhiyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b95",
                    "user": {
                        "_id": "66978ee0b8656f6506b4acb2",
                        "avatarUrl": "/avatars/298acb8222e189fce4368985ee5374a1.svg",
                        "isPro": false,
                        "fullname": "Junyan Ye",
                        "user": "Yejy53",
                        "type": "user"
                    },
                    "name": "Junyan Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:17:10.032Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b96",
                    "user": {
                        "_id": "66d5b56c77a026c3d2086a79",
                        "avatarUrl": "/avatars/45da07fd82fd455955faa05b27a6393f.svg",
                        "isPro": false,
                        "fullname": "Weijia Li",
                        "user": "liweijia",
                        "type": "user"
                    },
                    "name": "Weijia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:16:44.819Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b97",
                    "user": {
                        "_id": "6487e158f675b4a7867f45fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
                        "isPro": false,
                        "fullname": "Zilong Huang",
                        "user": "SereinH",
                        "type": "user"
                    },
                    "name": "Zilong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:56.501Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b98",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:05.246Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b99",
                    "user": {
                        "_id": "67ef53656c7ba428e7c2e605",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/fFVJTpMRKF15Bf63yZEG_.png",
                        "isPro": false,
                        "fullname": "He",
                        "user": "shawnxyh",
                        "type": "user"
                    },
                    "name": "Xiangyang He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:02.573Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b9a",
                    "user": {
                        "_id": "6459a47e4fe72fae522b4fc9",
                        "avatarUrl": "/avatars/a4139f8e348081e45b28dd99d96908d3.svg",
                        "isPro": false,
                        "fullname": "Kaiqing.Lin",
                        "user": "lin6123",
                        "type": "user"
                    },
                    "name": "Kaiqing Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:16:28.452Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b9b",
                    "user": {
                        "_id": "670ddb69d6ac6394419d88c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XxnGNaX3FWug4aiZVjg93.png",
                        "isPro": false,
                        "fullname": "Jun He",
                        "user": "JunHe0915",
                        "type": "user"
                    },
                    "name": "Jun He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:59.877Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b9c",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:17:18.123Z",
                    "hidden": false
                },
                {
                    "_id": "67ef502ce803d818f00e1b9d",
                    "user": {
                        "_id": "66135a5e50350afe76beebce",
                        "avatarUrl": "/avatars/370a4b83949355feb050c2cb0425c264.svg",
                        "isPro": false,
                        "fullname": "yl2488",
                        "user": "yl2488",
                        "type": "user"
                    },
                    "name": "Li Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:40.281Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T17:23:16.000Z",
            "submittedOnDailyAt": "2025-04-04T01:51:34.697Z",
            "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
            "upvotes": 34,
            "discussionId": "67ef502fe803d818f00e1c70",
            "githubRepo": "https://github.com/PicoTrex/GPT-ImgEval",
            "ai_keywords": [
                "auto-regressive (AR)",
                "diffusion-based head",
                "VAR-like architectures",
                "multi-round image editing",
                "image forensic models"
            ]
        },
        "publishedAt": "2025-04-03T13:23:16.000Z",
        "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
        "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 37
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02587",
            "authors": [
                {
                    "_id": "67ef3f9804be7fba0c882738",
                    "user": {
                        "_id": "633fc70529b5a95f6e15a6b7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
                        "isPro": false,
                        "fullname": "Yan Ma",
                        "user": "ManTle",
                        "type": "user"
                    },
                    "name": "Yan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:17:53.820Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3f9804be7fba0c882739",
                    "user": {
                        "_id": "64b370fe6d953e7c75ede314",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b370fe6d953e7c75ede314/RdP2q3hGXWE4E2zfSv0KU.png",
                        "isPro": false,
                        "fullname": "Steffi Chern",
                        "user": "steffichern",
                        "type": "user"
                    },
                    "name": "Steffi Chern",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:14.660Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3f9804be7fba0c88273a",
                    "user": {
                        "_id": "642e4d4d6748dd4f8eeb7732",
                        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
                        "isPro": false,
                        "fullname": "Xuyang Shen",
                        "user": "Ryan1122",
                        "type": "user"
                    },
                    "name": "Xuyang Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:17:40.397Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3f9804be7fba0c88273b",
                    "user": {
                        "_id": "64c525e4d68946edad6c7067",
                        "avatarUrl": "/avatars/1b108661634af602717a4ab4b66a151f.svg",
                        "isPro": false,
                        "fullname": "Yiran Zhong",
                        "user": "IanZhong",
                        "type": "user"
                    },
                    "name": "Yiran Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:16.707Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3f9804be7fba0c88273c",
                    "user": {
                        "_id": "6144a0c4ff1146bbd84d9865",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
                        "isPro": false,
                        "fullname": "Pengfei Liu",
                        "user": "Pengfei",
                        "type": "user"
                    },
                    "name": "Pengfei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:17:34.472Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T13:53:28.000Z",
            "submittedOnDailyAt": "2025-04-04T00:42:23.044Z",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "submittedOnDailyBy": {
                "_id": "633fc70529b5a95f6e15a6b7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
                "isPro": false,
                "fullname": "Yan Ma",
                "user": "ManTle",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
            "upvotes": 24,
            "discussionId": "67ef3f9904be7fba0c882772",
            "ai_keywords": [
                "reinforcement learning",
                "reasoning capabilities",
                "large language models",
                "vision-language models",
                "reproducibility",
                "accessibility",
                "standardized evaluation protocols",
                "transparent framework",
                "four-step pipeline",
                "training dynamics",
                "reflective behaviors",
                "visual reasoning tasks",
                "response length",
                "reflection",
                "supervised fine-tuning"
            ]
        },
        "publishedAt": "2025-04-03T09:53:28.000Z",
        "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
        "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02587.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "633fc70529b5a95f6e15a6b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
            "fullname": "Yan Ma",
            "name": "ManTle",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00939",
            "authors": [
                {
                    "_id": "67efffab5e8b84102499ebfc",
                    "name": "Alexander Martin",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ebfd",
                    "name": "Reno Kriz",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ebfe",
                    "name": "William Gantt Walden",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ebff",
                    "name": "Kate Sanders",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ec00",
                    "name": "Hannah Recknor",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ec01",
                    "name": "Eugene Yang",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ec02",
                    "name": "Francis Ferraro",
                    "hidden": false
                },
                {
                    "_id": "67efffab5e8b84102499ec03",
                    "name": "Benjamin Van Durme",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T16:22:15.000Z",
            "submittedOnDailyAt": "2025-04-04T14:20:30.139Z",
            "title": "WikiVideo: Article Generation from Multiple Videos",
            "submittedOnDailyBy": {
                "_id": "644ffb154f731658826945c8",
                "avatarUrl": "/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg",
                "isPro": false,
                "fullname": "Alex Martin",
                "user": "alexmartin1722",
                "type": "user"
            },
            "summary": "We present the challenging task of automatically creating a high-level\nWikipedia-style article that aggregates information from multiple diverse\nvideos about real-world events, such as natural disasters or political\nelections. Videos are intuitive sources for retrieval-augmented generation\n(RAG), but most contemporary RAG workflows focus heavily on text and existing\nmethods for video-based summarization focus on low-level scene understanding\nrather than high-level event semantics. To close this gap, we introduce\nWikiVideo, a benchmark consisting of expert-written articles and densely\nannotated videos that provide evidence for articles' claims, facilitating the\nintegration of video into RAG pipelines and enabling the creation of in-depth\ncontent that is grounded in multimodal sources. We further propose\nCollaborative Article Generation (CAG), a novel interactive method for article\ncreation from multiple videos. CAG leverages an iterative interaction between\nan r1-style reasoning model and a VideoLLM to draw higher level inferences\nabout the target event than is possible with VideoLLMs alone, which fixate on\nlow-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in\nboth oracle retrieval and RAG settings and find that CAG consistently\noutperforms alternative methods, while suggesting intriguing avenues for future\nwork.",
            "upvotes": 22,
            "discussionId": "67efffad5e8b84102499ec6e",
            "projectPage": "https://nlp.jhu.edu/magmar/",
            "githubRepo": "https://github.com/alexmartin1722/wikivideo",
            "ai_keywords": [
                "retrieval-augmented generation (RAG)",
                "WikiVideo",
                "Collaborative Article Generation (CAG)",
                "r1-style reasoning model",
                "VideoLLM"
            ]
        },
        "publishedAt": "2025-04-01T12:22:15.000Z",
        "title": "WikiVideo: Article Generation from Multiple Videos",
        "summary": "We present the challenging task of automatically creating a high-level\nWikipedia-style article that aggregates information from multiple diverse\nvideos about real-world events, such as natural disasters or political\nelections. Videos are intuitive sources for retrieval-augmented generation\n(RAG), but most contemporary RAG workflows focus heavily on text and existing\nmethods for video-based summarization focus on low-level scene understanding\nrather than high-level event semantics. To close this gap, we introduce\nWikiVideo, a benchmark consisting of expert-written articles and densely\nannotated videos that provide evidence for articles' claims, facilitating the\nintegration of video into RAG pipelines and enabling the creation of in-depth\ncontent that is grounded in multimodal sources. We further propose\nCollaborative Article Generation (CAG), a novel interactive method for article\ncreation from multiple videos. CAG leverages an iterative interaction between\nan r1-style reasoning model and a VideoLLM to draw higher level inferences\nabout the target event than is possible with VideoLLMs alone, which fixate on\nlow-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in\nboth oracle retrieval and RAG settings and find that CAG consistently\noutperforms alternative methods, while suggesting intriguing avenues for future\nwork.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00939.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644ffb154f731658826945c8",
            "avatarUrl": "/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg",
            "fullname": "Alex Martin",
            "name": "alexmartin1722",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02398",
            "authors": [
                {
                    "_id": "67ef63b5e8b932ae7a8d3043",
                    "user": {
                        "_id": "66b9bc2dacdbc1d0b39c3b50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                        "isPro": false,
                        "fullname": "Gallil Maimon",
                        "user": "gallilmaimon",
                        "type": "user"
                    },
                    "name": "Gallil Maimon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:06.509Z",
                    "hidden": false
                },
                {
                    "_id": "67ef63b5e8b932ae7a8d3044",
                    "user": {
                        "_id": "6547411a9295970f878aa52e",
                        "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
                        "isPro": false,
                        "fullname": "Michael Hassid",
                        "user": "hassid",
                        "type": "user"
                    },
                    "name": "Michael Hassid",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:24:39.934Z",
                    "hidden": false
                },
                {
                    "_id": "67ef63b5e8b932ae7a8d3045",
                    "user": {
                        "_id": "64b7b7b38ba7d6c922d753d6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7b7b38ba7d6c922d753d6/rt0thjYa84VZHy1BEcW4p.jpeg",
                        "isPro": false,
                        "fullname": "Amit Roth",
                        "user": "MajoRoth",
                        "type": "user"
                    },
                    "name": "Amit Roth",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:24:48.483Z",
                    "hidden": false
                },
                {
                    "_id": "67ef63b5e8b932ae7a8d3046",
                    "user": {
                        "_id": "6481e135578646b5c2386728",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481e135578646b5c2386728/SPva4iNw0pORiCXD45cx9.jpeg",
                        "isPro": false,
                        "fullname": "Yossi Adi",
                        "user": "adiyoss",
                        "type": "user"
                    },
                    "name": "Yossi Adi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:24:54.851Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
            ],
            "publishedAt": "2025-04-03T08:46:56.000Z",
            "submittedOnDailyAt": "2025-04-04T03:52:15.607Z",
            "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
            "submittedOnDailyBy": {
                "_id": "66b9bc2dacdbc1d0b39c3b50",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                "isPro": false,
                "fullname": "Gallil Maimon",
                "user": "gallilmaimon",
                "type": "user"
            },
            "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
            "upvotes": 20,
            "discussionId": "67ef63b6e8b932ae7a8d306d",
            "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/sims/",
            "githubRepo": "https://github.com/slp-rl/slamkit",
            "ai_keywords": [
                "Speech Language Model (SLM)",
                "TextLMs",
                "speech-text interleaving",
                "scaling analysis",
                "compute",
                "knowledge transfer",
                "textless-SLMs",
                "scaling trends",
                "scaling-dynamics",
                "training tokens",
                "synthetic data",
                "model families",
                "speech semantic metrics"
            ]
        },
        "publishedAt": "2025-04-03T04:46:56.000Z",
        "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
        "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\nThey predict that SLMs require much more compute and data compared to text,\nleading some to question the feasibility of training high-quality SLMs.\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\nspeech-text interleaving to allow knowledge transfer. This raises the question\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\ntraining several dozen and analysing the scaling trends. We see that under this\nsetup SLMs scale more efficiently with compute. Additionally, our results\nindicate that the scaling-dynamics are significantly different than\ntextless-SLMs, suggesting one should allocate notably more of the compute\nbudget for increasing model size over training tokens. We also study the role\nof synthetic data and TextLM model families in unlocking this potential.\nResults suggest, that our scaled up model achieves comparable performance with\nleading models on speech semantic metrics while using less compute and data\nthan other approaches. We open source models, samples, and data -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/wqUq-bT-DvKoNybPX46uL.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02398.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "fullname": "Gallil Maimon",
            "name": "gallilmaimon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02495",
            "authors": [
                {
                    "_id": "67efb66889b2961e6d95eb5b",
                    "name": "Zijun Liu",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb5c",
                    "name": "Peiyi Wang",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb5d",
                    "name": "Runxin Xu",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb5e",
                    "name": "Shirong Ma",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb5f",
                    "name": "Chong Ruan",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb60",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb61",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "67efb66889b2961e6d95eb62",
                    "name": "Yu Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T11:19:49.000Z",
            "submittedOnDailyAt": "2025-04-04T09:08:22.628Z",
            "title": "Inference-Time Scaling for Generalist Reward Modeling",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that proper learning\nmethods could enable effective inference-time scalability. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the inference-time scalability of generalist RM, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in DeepSeek-GRM models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.",
            "upvotes": 19,
            "discussionId": "67efb66989b2961e6d95ebc5",
            "ai_keywords": [
                "pointwise generative reward modeling (GRM)",
                "Self-Principled Critique Tuning (SPCT)",
                "DeepSeek-GRM",
                "meta RM (reward modeling)",
                "online RL (reinforcement learning)"
            ]
        },
        "publishedAt": "2025-04-03T07:19:49.000Z",
        "title": "Inference-Time Scaling for Generalist Reward Modeling",
        "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that proper learning\nmethods could enable effective inference-time scalability. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the inference-time scalability of generalist RM, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in DeepSeek-GRM models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02495.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 37
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02436",
            "authors": [
                {
                    "_id": "67ef3dfae8b932ae7a832950",
                    "user": {
                        "_id": "617ba1820e4237bd1731b867",
                        "avatarUrl": "/avatars/f9de06363e64bddd7dc977e96e85df8a.svg",
                        "isPro": false,
                        "fullname": "zhengcong fei",
                        "user": "onion",
                        "type": "user"
                    },
                    "name": "Zhengcong Fei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:19:16.548Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832951",
                    "user": {
                        "_id": "65dc3a850af7e21ba40e939f",
                        "avatarUrl": "/avatars/e129c64617675edd05d4317d39604318.svg",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "Debang",
                        "type": "user"
                    },
                    "name": "Debang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:19:27.042Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832952",
                    "user": {
                        "_id": "65bef422fdb8d33cefeaccc3",
                        "avatarUrl": "/avatars/d40b0d7dda21fa1a68c291d11bc357ec.svg",
                        "isPro": false,
                        "fullname": "Qiu Di",
                        "user": "diqiu7",
                        "type": "user"
                    },
                    "name": "Di Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:19:41.458Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832953",
                    "name": "Jiahua Wang",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832954",
                    "name": "Yikun Dou",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832955",
                    "user": {
                        "_id": "62e0f1314db2175cd270ad08",
                        "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
                        "isPro": false,
                        "fullname": "Rui Wang",
                        "user": "ruiwang",
                        "type": "user"
                    },
                    "name": "Rui Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:20:11.206Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832956",
                    "user": {
                        "_id": "666a674967c686801acf25bb",
                        "avatarUrl": "/avatars/c1f3edd63fd378dfb555e6413a966932.svg",
                        "isPro": false,
                        "fullname": "jingtao xu",
                        "user": "raul678",
                        "type": "user"
                    },
                    "name": "Jingtao Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:20:20.880Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832957",
                    "user": {
                        "_id": "634672bfb7b4e71c7f45360f",
                        "avatarUrl": "/avatars/4b646fc3e271be90b9ec619d42ce3e99.svg",
                        "isPro": false,
                        "fullname": "Fan Mingyuan",
                        "user": "MichaelFan",
                        "type": "user"
                    },
                    "name": "Mingyuan Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:20:32.597Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832958",
                    "name": "Guibin Chen",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a832959",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "67ef3dfae8b932ae7a83295a",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T09:50:50.000Z",
            "submittedOnDailyAt": "2025-04-04T00:33:57.000Z",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
            "upvotes": 19,
            "discussionId": "67ef3dfee8b932ae7a832a97",
            "ai_keywords": [
                "elements-to-video (E2V)",
                "image-text joint embedding model",
                "prompt-reference-video triplets",
                "generative process",
                "multi-element representations",
                "strict consistency",
                "coherent composition",
                "natural outputs",
                "output stability",
                "A2 Bench (benchmark)",
                "high-quality videos",
                "precise element control",
                "open-source commercial grade model"
            ]
        },
        "publishedAt": "2025-04-03T05:50:50.000Z",
        "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
        "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02436.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6578
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.23377",
            "authors": [
                {
                    "_id": "67eea084cdb7049bd6f79dd7",
                    "user": {
                        "_id": "678bdcbe600666579235a1f3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png",
                        "isPro": false,
                        "fullname": "KAI LIU",
                        "user": "kkail8",
                        "type": "user"
                    },
                    "name": "Kai Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T19:20:50.100Z",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79dd8",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79dd9",
                    "name": "Lai Chen",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79dda",
                    "name": "Shengqiong Wu",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79ddb",
                    "name": "Yanhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79ddc",
                    "name": "Jiayi Ji",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79ddd",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79dde",
                    "name": "Rongxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79ddf",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79de0",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:46.362Z",
                    "hidden": false
                },
                {
                    "_id": "67eea084cdb7049bd6f79de1",
                    "name": "Tat-Seng Chua",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/O-XG_EhdP60fyZQs0qAF-.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/_BpDeIkGdoNNOpNAnn0_U.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/dD5iGFLmXO1gLUGpeD3fj.png"
            ],
            "publishedAt": "2025-03-30T09:40:42.000Z",
            "submittedOnDailyAt": "2025-04-04T14:07:10.064Z",
            "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical\n  Spatio-Temporal Prior Synchronization",
            "submittedOnDailyBy": {
                "_id": "647773a1168cb428e00e9a8f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                "isPro": false,
                "fullname": "Hao Fei",
                "user": "scofield7419",
                "type": "user"
            },
            "summary": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion\nTransformer designed for synchronized audio-video generation (JAVG). Built upon\nthe powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to\ngenerate high-quality audio and video content simultaneously from open-ended\nuser prompts. To ensure optimal synchronization, we introduce a fine-grained\nspatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal\nSynchronized Prior (HiST-Sypo) Estimator. This module extracts both global and\nfine-grained spatio-temporal priors, guiding the synchronization between the\nvisual and auditory components. Furthermore, we propose a new benchmark,\nJavisBench, consisting of 10,140 high-quality text-captioned sounding videos\nspanning diverse scenes and complex real-world scenarios. Further, we\nspecifically devise a robust metric for evaluating the synchronization between\ngenerated audio-video pairs in real-world complex content. Experimental results\ndemonstrate that JavisDiT significantly outperforms existing methods by\nensuring both high-quality generation and precise synchronization, setting a\nnew standard for JAVG tasks. Our code, model, and dataset will be made publicly\navailable at https://javisdit.github.io/.",
            "upvotes": 17,
            "discussionId": "67eea086cdb7049bd6f79e78",
            "projectPage": "https://javisdit.github.io/",
            "githubRepo": "https://github.com/JavisDiT/JavisDiT",
            "ai_keywords": [
                "Joint Audio-Video Diffusion Transformer",
                "Diffusion Transformer (DiT)",
                "synchronized audio-video generation (JAVG)",
                "Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator",
                "spatio-temporal priors",
                "JavisBench",
                "synchronization between generated audio-video pairs",
                "real-world complex content"
            ]
        },
        "publishedAt": "2025-03-30T05:40:42.000Z",
        "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical\n  Spatio-Temporal Prior Synchronization",
        "summary": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion\nTransformer designed for synchronized audio-video generation (JAVG). Built upon\nthe powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to\ngenerate high-quality audio and video content simultaneously from open-ended\nuser prompts. To ensure optimal synchronization, we introduce a fine-grained\nspatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal\nSynchronized Prior (HiST-Sypo) Estimator. This module extracts both global and\nfine-grained spatio-temporal priors, guiding the synchronization between the\nvisual and auditory components. Furthermore, we propose a new benchmark,\nJavisBench, consisting of 10,140 high-quality text-captioned sounding videos\nspanning diverse scenes and complex real-world scenarios. Further, we\nspecifically devise a robust metric for evaluating the synchronization between\ngenerated audio-video pairs in real-world complex content. Experimental results\ndemonstrate that JavisDiT significantly outperforms existing methods by\nensuring both high-quality generation and precise synchronization, setting a\nnew standard for JAVG tasks. Our code, model, and dataset will be made publicly\navailable at https://javisdit.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/O-XG_EhdP60fyZQs0qAF-.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/_BpDeIkGdoNNOpNAnn0_U.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/dD5iGFLmXO1gLUGpeD3fj.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23377.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "fullname": "Hao Fei",
            "name": "scofield7419",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.00502",
            "authors": [
                {
                    "_id": "67ef72898667ee5c99026d16",
                    "user": {
                        "_id": "67014d33126f9ab39fc52481",
                        "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
                        "isPro": false,
                        "fullname": "Qianhao Yuan",
                        "user": "yuanqianhao",
                        "type": "user"
                    },
                    "name": "Qianhao Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:23:28.149Z",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d17",
                    "name": "Qingyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d18",
                    "name": "Yanjiang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d19",
                    "user": {
                        "_id": "654c7fbe6b51714c2a6ff590",
                        "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
                        "isPro": false,
                        "fullname": "Jiawei Chen",
                        "user": "chenjiawei-icip",
                        "type": "user"
                    },
                    "name": "Jiawei Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:04.134Z",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d1a",
                    "user": {
                        "_id": "6216496a9b34d2fb49144599",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6216496a9b34d2fb49144599/41CKA_h1Ffj3RzVabSAkm.jpeg",
                        "isPro": false,
                        "fullname": "Yaojie Lu",
                        "user": "luyaojie",
                        "type": "user"
                    },
                    "name": "Yaojie Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:24:09.511Z",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d1b",
                    "user": {
                        "_id": "6711c702f858a456b4b9f3a4",
                        "avatarUrl": "/avatars/178e9567c3111ab22717c3c0dd003a6a.svg",
                        "isPro": false,
                        "fullname": "Hongyu  Lin",
                        "user": "sanmusunrise",
                        "type": "user"
                    },
                    "name": "Hongyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:24:15.188Z",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d1c",
                    "name": "Jia Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d1d",
                    "user": {
                        "_id": "65e99a77e71555ed193609cf",
                        "avatarUrl": "/avatars/38ceb127883944677665da967d17dd18.svg",
                        "isPro": false,
                        "fullname": "Xianpei Han",
                        "user": "xphan",
                        "type": "user"
                    },
                    "name": "Xianpei Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:24:23.046Z",
                    "hidden": false
                },
                {
                    "_id": "67ef72898667ee5c99026d1e",
                    "name": "Le Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T07:47:55.000Z",
            "submittedOnDailyAt": "2025-04-04T04:19:46.946Z",
            "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
            "submittedOnDailyBy": {
                "_id": "67014d33126f9ab39fc52481",
                "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
                "isPro": false,
                "fullname": "Qianhao Yuan",
                "user": "yuanqianhao",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
            "upvotes": 16,
            "discussionId": "67ef728a8667ee5c99026d69",
            "githubRepo": "https://github.com/icip-cas/ShortV",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "Layer Contribution (LC)",
                "visual tokens",
                "transformations",
                "layer-wise redundancy",
                "model output",
                "divergence",
                "ineffective layers",
                "training-free method",
                "visual token updates",
                "computational costs",
                "FLOPs",
                "LLaVA-NeXT-13B"
            ]
        },
        "publishedAt": "2025-04-01T03:47:55.000Z",
        "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
        "summary": "Multimodal Large Language Models (MLLMs) suffer from high computational costs\ndue to their massive size and the large number of visual tokens. In this paper,\nwe investigate layer-wise redundancy in MLLMs by introducing a novel metric,\nLayer Contribution (LC), which quantifies the impact of a layer's\ntransformations on visual and text tokens, respectively. The calculation of LC\ninvolves measuring the divergence in model output that results from removing\nthe layer's transformations on the specified tokens. Our pilot experiment\nreveals that many layers of MLLMs exhibit minimal contribution during the\nprocessing of visual tokens. Motivated by this observation, we propose ShortV,\na training-free method that leverages LC to identify ineffective layers, and\nfreezes visual token updates in these layers. Experiments show that ShortV can\nfreeze visual token in approximately 60\\% of the MLLM layers, thereby\ndramatically reducing computational costs related to updating visual tokens.\nFor example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while\nmaintaining superior performance. The code will be publicly available at\nhttps://github.com/icip-cas/ShortV",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67014d33126f9ab39fc52481",
            "avatarUrl": "/avatars/60d1f791e7f3201ce1aef72e9216ff78.svg",
            "fullname": "Qianhao Yuan",
            "name": "yuanqianhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02542",
            "authors": [
                {
                    "_id": "67ef3773ac0c701df7fd98aa",
                    "user": {
                        "_id": "6264a7dfc39850dc093eb68a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650763566575-noauth.png",
                        "isPro": false,
                        "fullname": "Fa-Ting Hong",
                        "user": "HarlanHong",
                        "type": "user"
                    },
                    "name": "Fa-Ting Hong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:38.641Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98ab",
                    "user": {
                        "_id": "6481523b3fb124fc9850afed",
                        "avatarUrl": "/avatars/ddde178c88713662800aafd2343647a4.svg",
                        "isPro": false,
                        "fullname": "Zunnan Xu",
                        "user": "xuzn",
                        "type": "user"
                    },
                    "name": "Zunnan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:23.733Z",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98ac",
                    "name": "Zixiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98ad",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98ae",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98af",
                    "name": "Qin Lin",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98b0",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "67ef3773ac0c701df7fd98b1",
                    "user": {
                        "_id": "66feab48651e00e22f33222e",
                        "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
                        "isPro": false,
                        "fullname": "Dan Xu",
                        "user": "danxuhk",
                        "type": "user"
                    },
                    "name": "Dan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:20.987Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
            ],
            "publishedAt": "2025-04-03T12:44:41.000Z",
            "submittedOnDailyAt": "2025-04-04T01:37:45.934Z",
            "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
            "submittedOnDailyBy": {
                "_id": "66feab48651e00e22f33222e",
                "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
                "isPro": false,
                "fullname": "Dan Xu",
                "user": "danxuhk",
                "type": "user"
            },
            "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
            "upvotes": 13,
            "discussionId": "67ef3775ac0c701df7fd994c",
            "projectPage": "https://harlanhong.github.io/publications/actalker/index.html",
            "githubRepo": "https://github.com/harlanhong/ACTalker",
            "ai_keywords": [
                "ACTalker",
                "video diffusion framework",
                "multi-signals control",
                "parallel mamba structure",
                "driving signals",
                "gate mechanism",
                "temporal coordination",
                "spatial coordination",
                "feature tokens",
                "mask-drop strategy",
                "facial videos",
                "multiple driving modalities"
            ]
        },
        "publishedAt": "2025-04-03T08:44:41.000Z",
        "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
        "summary": "Talking head synthesis is vital for virtual avatars and human-computer\ninteraction. However, most existing methods are typically limited to accepting\ncontrol from a single primary modality, restricting their practical utility. To\nthis end, we introduce ACTalker, an end-to-end video diffusion\nframework that supports both multi-signals control and single-signal control\nfor talking head video generation. For multiple control, we design a parallel\nmamba structure with multiple branches, each utilizing a separate driving\nsignal to control specific facial regions. A gate mechanism is applied across\nall branches, providing flexible control over video generation. To ensure\nnatural coordination of the controlled video both temporally and spatially, we\nemploy the mamba structure, which enables driving signals to manipulate feature\ntokens across both dimensions in each branch. Additionally, we introduce a\nmask-drop strategy that allows each driving signal to independently control its\ncorresponding facial region within the mamba structure, preventing control\nconflicts. Experimental results demonstrate that our method produces\nnatural-looking facial videos driven by diverse signals and that the mamba\nlayer seamlessly integrates multiple driving modalities without conflict.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/VOMaQBXsDjs1295R5NVOh.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/tksGfiG1zEDZ4QUg4IorF.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/aUp8tmNM2pdPYCa3A3Noh.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66feab48651e00e22f33222e/MpCNSPEP6OSjZR_0m70PP.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02542.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66feab48651e00e22f33222e",
            "avatarUrl": "/avatars/7344377e2c796c7ec85194bb2fc78521.svg",
            "fullname": "Dan Xu",
            "name": "danxuhk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02119",
            "authors": [
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a32",
                    "user": {
                        "_id": "670826649e319cca029ff240",
                        "avatarUrl": "/avatars/6d12b3abf75f714d75d1775d88885345.svg",
                        "isPro": false,
                        "fullname": "rtfvbhkuj",
                        "user": "wwdd7718",
                        "type": "user"
                    },
                    "name": "Wang Wei",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-04T02:20:24.253Z",
                    "hidden": false
                },
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a33",
                    "user": {
                        "_id": "66e4e50a52356419c4a1ad14",
                        "avatarUrl": "/avatars/4be3ce17671785cbe7126b9c1141478b.svg",
                        "isPro": false,
                        "fullname": "Tiankai Yang",
                        "user": "tiankaiy",
                        "type": "user"
                    },
                    "name": "Tiankai Yang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-04T02:25:15.747Z",
                    "hidden": false
                },
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a34",
                    "name": "Hongjie Chen",
                    "hidden": false
                },
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a35",
                    "user": {
                        "_id": "62a3ab83e4dd6252344d27cd",
                        "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
                        "isPro": false,
                        "fullname": "Ryan A. Rossi",
                        "user": "ryanrossi",
                        "type": "user"
                    },
                    "name": "Ryan A. Rossi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:23:05.421Z",
                    "hidden": false
                },
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a36",
                    "name": "Yue Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a37",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-04T02:34:51.212Z",
                    "hidden": false
                },
                {
                    "_id": "67ef41e7efcb0a2fbfbb6a38",
                    "name": "Hoda Eldardiry",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T20:33:27.000Z",
            "submittedOnDailyAt": "2025-04-04T00:50:35.167Z",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
            "upvotes": 9,
            "discussionId": "67ef41e8efcb0a2fbfbb6a93",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "model selection",
                "time series forecasting",
                "meta-learning approaches",
                "pre-constructed performance matrices",
                "reasoning capabilities",
                "experiments",
                "LLaMA",
                "GPT",
                "Gemini",
                "heuristic baselines",
                "computational overhead"
            ]
        },
        "publishedAt": "2025-04-02T16:33:27.000Z",
        "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
        "summary": "Model selection is a critical step in time series forecasting, traditionally\nrequiring extensive performance evaluations across various datasets.\nMeta-learning approaches aim to automate this process, but they typically\ndepend on pre-constructed performance matrices, which are costly to build. In\nthis work, we propose to leverage Large Language Models (LLMs) as a lightweight\nalternative for model selection. Our method eliminates the need for explicit\nperformance matrices by utilizing the inherent knowledge and reasoning\ncapabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini,\nwe demonstrate that our approach outperforms traditional meta-learning\ntechniques and heuristic baselines, while significantly reducing computational\noverhead. These findings underscore the potential of LLMs in efficient model\nselection for time series forecasting.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02119.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22444",
            "authors": [
                {
                    "_id": "67ef33a4456bcf30fa95b2f1",
                    "user": {
                        "_id": "655fb8a122ce47e5fa491c72",
                        "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
                        "isPro": false,
                        "fullname": "Pengsong Zhang",
                        "user": "universea",
                        "type": "user"
                    },
                    "name": "Pengsong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:42.152Z",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f2",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f3",
                    "name": "Huazhe Xu",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f4",
                    "name": "Renjun Xu",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f5",
                    "name": "Zhenting Wang",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f6",
                    "name": "Cong Wang",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f7",
                    "name": "Animesh Garg",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f8",
                    "name": "Zhibin Li",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2f9",
                    "name": "Arash Ajoudani",
                    "hidden": false
                },
                {
                    "_id": "67ef33a4456bcf30fa95b2fa",
                    "name": "Xinyu Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
            ],
            "publishedAt": "2025-03-28T14:00:27.000Z",
            "submittedOnDailyAt": "2025-04-04T06:29:27.852Z",
            "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
            "submittedOnDailyBy": {
                "_id": "655fb8a122ce47e5fa491c72",
                "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
                "isPro": false,
                "fullname": "Pengsong Zhang",
                "user": "universea",
                "type": "user"
            },
            "summary": "Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.",
            "upvotes": 9,
            "discussionId": "67ef33a5456bcf30fa95b35e",
            "githubRepo": "https://github.com/openags/Awesome-AI-Scientist-Papers"
        },
        "publishedAt": "2025-03-28T10:00:27.000Z",
        "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
        "summary": "Scientific discovery is poised for rapid advancement through advanced\nrobotics and artificial intelligence. Current scientific practices face\nsubstantial limitations as manual experimentation remains time-consuming and\nresource-intensive, while multidisciplinary research demands knowledge\nintegration beyond individual researchers' expertise boundaries. Here, we\nenvision an autonomous generalist scientist (AGS) concept combines agentic AI\nand embodied robotics to automate the entire research lifecycle. This system\ncould dynamically interact with both physical and virtual environments while\nfacilitating the integration of knowledge across diverse scientific\ndisciplines. By deploying these technologies throughout every research stage --\nspanning literature review, hypothesis generation, experimentation, and\nmanuscript writing -- and incorporating internal reflection alongside external\nfeedback, this system aims to significantly reduce the time and resources\nneeded for scientific discovery. Building on the evolution from virtual AI\nscientists to versatile generalist AI-based robot scientists, AGS promises\ngroundbreaking potential. As these autonomous systems become increasingly\nintegrated into the research process, we hypothesize that scientific discovery\nmight adhere to new scaling laws, potentially shaped by the number and\ncapabilities of these autonomous systems, offering novel perspectives on how\nknowledge is generated and evolves. The adaptability of embodied robots to\nextreme environments, paired with the flywheel effect of accumulating\nscientific knowledge, holds the promise of continually pushing beyond both\nphysical and intellectual frontiers.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655fb8a122ce47e5fa491c72/d-38XQKRw3vnyFhzZFTEd.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22444.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655fb8a122ce47e5fa491c72",
            "avatarUrl": "/avatars/d320fe777649b68fbd1372865a2f4def.svg",
            "fullname": "Pengsong Zhang",
            "name": "universea",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01871",
            "authors": [
                {
                    "_id": "67eea9e5117231f8bb04402b",
                    "user": {
                        "_id": "65d0c00b0954f06e472909f4",
                        "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
                        "isPro": false,
                        "fullname": "tom bush",
                        "user": "tuphs",
                        "type": "user"
                    },
                    "name": "Thomas Bush",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T19:20:34.085Z",
                    "hidden": false
                },
                {
                    "_id": "67eea9e5117231f8bb04402c",
                    "name": "Stephen Chung",
                    "hidden": false
                },
                {
                    "_id": "67eea9e5117231f8bb04402d",
                    "name": "Usman Anwar",
                    "hidden": false
                },
                {
                    "_id": "67eea9e5117231f8bb04402e",
                    "user": {
                        "_id": "645ecd18f0f92653b9f33d4e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645ecd18f0f92653b9f33d4e/nHDMWtM9ZHrji0c4Y4XW1.jpeg",
                        "isPro": false,
                        "fullname": "AdriÃ  Garriga-Alonso",
                        "user": "agaralon",
                        "type": "user"
                    },
                    "name": "AdriÃ  Garriga-Alonso",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-03T15:31:53.577Z",
                    "hidden": false
                },
                {
                    "_id": "67eea9e5117231f8bb04402f",
                    "name": "David Krueger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T16:24:23.000Z",
            "submittedOnDailyAt": "2025-04-04T07:00:29.802Z",
            "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65d0c00b0954f06e472909f4",
                "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
                "isPro": false,
                "fullname": "tom bush",
                "user": "tuphs",
                "type": "user"
            },
            "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
            "upvotes": 7,
            "discussionId": "67eea9e9117231f8bb044167",
            "ai_keywords": [
                "model-free reinforcement learning",
                "concept-based interpretability",
                "Sokoban",
                "DRC",
                "learned concept representations",
                "plan formation",
                "causal effect",
                "parallelized bidirectional search",
                "LLMs",
                "emergent planning",
                "reasoning capabilities"
            ]
        },
        "publishedAt": "2025-04-02T12:24:23.000Z",
        "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
        "summary": "We present the first mechanistic evidence that model-free reinforcement\nlearning agents can learn to plan. This is achieved by applying a methodology\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\ncommonly used benchmark for studying planning. Specifically, we demonstrate\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\nlearned concept representations to internally formulate plans that both predict\nthe long-term effects of actions on the environment and influence action\nselection. Our methodology involves: (1) probing for planning-relevant\nconcepts, (2) investigating plan formation within the agent's representations,\nand (3) verifying that discovered plans (in the agent's representations) have a\ncausal effect on the agent's behavior through interventions. We also show that\nthe emergence of these plans coincides with the emergence of a planning-like\nproperty: the ability to benefit from additional test-time compute. Finally, we\nperform a qualitative analysis of the planning algorithm learned by the agent\nand discover a strong resemblance to parallelized bidirectional search. Our\nfindings advance understanding of the internal mechanisms underlying planning\nbehavior in agents, which is important given the recent trend of emergent\nplanning and reasoning capabilities in LLMs through RL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01871.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d0c00b0954f06e472909f4",
            "avatarUrl": "/avatars/7dd76d922b781ed9895c7f4e62fefd9c.svg",
            "fullname": "tom bush",
            "name": "tuphs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.02154",
            "authors": [
                {
                    "_id": "67efe4bde065b7e7675be166",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "67efe4bde065b7e7675be167",
                    "name": "Susan Liang",
                    "hidden": false
                },
                {
                    "_id": "67efe4bde065b7e7675be168",
                    "name": "Yunlong Tang",
                    "hidden": false
                },
                {
                    "_id": "67efe4bde065b7e7675be169",
                    "name": "Li Ma",
                    "hidden": false
                },
                {
                    "_id": "67efe4bde065b7e7675be16a",
                    "name": "Yapeng Tian",
                    "hidden": false
                },
                {
                    "_id": "67efe4bde065b7e7675be16b",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T22:03:11.000Z",
            "submittedOnDailyAt": "2025-04-04T12:30:02.891Z",
            "title": "FreSca: Unveiling the Scaling Space in Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "65763434a4ee9a4fe7cfb156",
                "avatarUrl": "/avatars/2c4a23ff309f750dd9c0d67bd9fd7abc.svg",
                "isPro": false,
                "fullname": "Susan Liang",
                "user": "susanliang",
                "type": "user"
            },
            "summary": "Diffusion models offer impressive controllability for image tasks, primarily\nthrough noise predictions that encode task-specific information and\nclassifier-free guidance enabling adjustable scaling. This scaling mechanism\nimplicitly defines a ``scaling space'' whose potential for fine-grained\nsemantic manipulation remains underexplored. We investigate this space,\nstarting with inversion-based editing where the difference between\nconditional/unconditional noise predictions carries key semantic information.\nOur core contribution stems from a Fourier analysis of noise predictions,\nrevealing that its low- and high-frequency components evolve differently\nthroughout diffusion. Based on this insight, we introduce FreSca, a\nstraightforward method that applies guidance scaling independently to different\nfrequency bands in the Fourier domain. FreSca demonstrably enhances existing\nimage editing methods without retraining. Excitingly, its effectiveness extends\nto image understanding tasks such as depth estimation, yielding quantitative\ngains across multiple datasets.",
            "upvotes": 6,
            "discussionId": "67efe4c3e065b7e7675be2ca",
            "projectPage": "https://wikichao.github.io/FreSca/",
            "githubRepo": "https://github.com/WikiChao/FreSca",
            "ai_keywords": [
                "diffusion models",
                "noise predictions",
                "classifier-free guidance",
                "scaling space",
                "inversion-based editing",
                "Fourier analysis",
                "low-frequency components",
                "high-frequency components",
                "Fourier domain",
                "FreSca",
                "image editing methods",
                "depth estimation",
                "datasets"
            ]
        },
        "publishedAt": "2025-04-02T18:03:11.000Z",
        "title": "FreSca: Unveiling the Scaling Space in Diffusion Models",
        "summary": "Diffusion models offer impressive controllability for image tasks, primarily\nthrough noise predictions that encode task-specific information and\nclassifier-free guidance enabling adjustable scaling. This scaling mechanism\nimplicitly defines a ``scaling space'' whose potential for fine-grained\nsemantic manipulation remains underexplored. We investigate this space,\nstarting with inversion-based editing where the difference between\nconditional/unconditional noise predictions carries key semantic information.\nOur core contribution stems from a Fourier analysis of noise predictions,\nrevealing that its low- and high-frequency components evolve differently\nthroughout diffusion. Based on this insight, we introduce FreSca, a\nstraightforward method that applies guidance scaling independently to different\nfrequency bands in the Fourier domain. FreSca demonstrably enhances existing\nimage editing methods without retraining. Excitingly, its effectiveness extends\nto image understanding tasks such as depth estimation, yielding quantitative\ngains across multiple datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02154.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65763434a4ee9a4fe7cfb156",
            "avatarUrl": "/avatars/2c4a23ff309f750dd9c0d67bd9fd7abc.svg",
            "fullname": "Susan Liang",
            "name": "susanliang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02821",
            "authors": [
                {
                    "_id": "67ef9a8885ea9d1d7db3fdb5",
                    "name": "Mateusz Pach",
                    "hidden": false
                },
                {
                    "_id": "67ef9a8885ea9d1d7db3fdb6",
                    "name": "Shyamgopal Karthik",
                    "hidden": false
                },
                {
                    "_id": "67ef9a8885ea9d1d7db3fdb7",
                    "name": "Quentin Bouniot",
                    "hidden": false
                },
                {
                    "_id": "67ef9a8885ea9d1d7db3fdb8",
                    "name": "Serge Belongie",
                    "hidden": false
                },
                {
                    "_id": "67ef9a8885ea9d1d7db3fdb9",
                    "name": "Zeynep Akata",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T17:58:35.000Z",
            "submittedOnDailyAt": "2025-04-04T07:08:47.568Z",
            "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "6254599b6e36fe62e141c8f9",
                "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
                "isPro": false,
                "fullname": "Shyamgopal Karthik",
                "user": "shyamgopal",
                "type": "user"
            },
            "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
            "upvotes": 5,
            "discussionId": "67ef9a8985ea9d1d7db3fe20",
            "githubRepo": "https://github.com/ExplainableML/sae-for-vlm",
            "ai_keywords": [
                "Sparse Autoencoders (SAEs)",
                "Large Language Models (LLMs)",
                "Vision-Language Models (VLMs)",
                "CLIP",
                "monosemanticity",
                "vision representations",
                "hierarchical representations",
                "iNaturalist taxonomy",
                "multimodal LLMs",
                "LLaVA",
                "unsupervised approach"
            ]
        },
        "publishedAt": "2025-04-03T13:58:35.000Z",
        "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
        "summary": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6254599b6e36fe62e141c8f9",
            "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
            "fullname": "Shyamgopal Karthik",
            "name": "shyamgopal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.00891",
            "authors": [
                {
                    "_id": "67ef62342a18e60aeee0ea02",
                    "name": "Jian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea03",
                    "user": {
                        "_id": "667187ba9ab144eb3ac43a1b",
                        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
                        "isPro": false,
                        "fullname": "Runze Liu",
                        "user": "RyanLiu112",
                        "type": "user"
                    },
                    "name": "Runze Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:08:09.923Z",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea04",
                    "user": {
                        "_id": "60bc94cd85a3ab33829b6211",
                        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "iseesaw",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:25:22.709Z",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea05",
                    "name": "Zhimu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea06",
                    "user": {
                        "_id": "67ab05fe4c6ca2d5db4c0c52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
                        "isPro": false,
                        "fullname": "Junqi Gao",
                        "user": "ChetKao",
                        "type": "user"
                    },
                    "name": "Junqi Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:25:38.624Z",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea07",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea08",
                    "user": {
                        "_id": "6562db314e8918182da42706",
                        "avatarUrl": "/avatars/b113bbbb496bf4dac254f0e840f08e10.svg",
                        "isPro": false,
                        "fullname": "Jiafei Lyu",
                        "user": "dmux",
                        "type": "user"
                    },
                    "name": "Jiafei Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:25:45.394Z",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea09",
                    "user": {
                        "_id": "65b34c5785b6c2144807db37",
                        "avatarUrl": "/avatars/4c1cb03cda250d4ec760ebf7815a3bce.svg",
                        "isPro": false,
                        "fullname": "Qianzhouyi",
                        "user": "Saputello",
                        "type": "user"
                    },
                    "name": "Zhouyi Qian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:26:00.763Z",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea0a",
                    "user": {
                        "_id": "645d9c3058f9ee315148116d",
                        "avatarUrl": "/avatars/165e18f27b5a50738bf1d22857118478.svg",
                        "isPro": false,
                        "fullname": "Biqing Qi",
                        "user": "jackqi7",
                        "type": "user"
                    },
                    "name": "Biqing Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:26:06.517Z",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea0b",
                    "name": "Xiu Li",
                    "hidden": false
                },
                {
                    "_id": "67ef62342a18e60aeee0ea0c",
                    "user": {
                        "_id": "669f614b59adf5b56e05bce3",
                        "avatarUrl": "/avatars/ffd4189efbceb0e63a03db273065a44b.svg",
                        "isPro": false,
                        "fullname": "BowenZhou",
                        "user": "bowenZhou",
                        "type": "user"
                    },
                    "name": "Bowen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:26:14.312Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T15:21:05.000Z",
            "submittedOnDailyAt": "2025-04-04T03:13:15.991Z",
            "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
            "submittedOnDailyBy": {
                "_id": "667187ba9ab144eb3ac43a1b",
                "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
                "isPro": false,
                "fullname": "Runze Liu",
                "user": "RyanLiu112",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
            "upvotes": 5,
            "discussionId": "67ef62352a18e60aeee0ea4b",
            "projectPage": "https://ryanliu112.github.io/GenPRM",
            "githubRepo": "https://github.com/RyanLiu112/GenPRM",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Process Reward Models (PRMs)",
                "Chain-of-Thought (CoT) reasoning",
                "Relative Progress Estimation (RPE)",
                "ProcessBench",
                "MATH dataset",
                "GPT-4",
                "Qwen2.5-Math-PRM-72B",
                "critic model",
                "policy model refinement"
            ]
        },
        "publishedAt": "2025-04-01T11:21:05.000Z",
        "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
        "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "fullname": "Runze Liu",
            "name": "RyanLiu112",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23162",
            "authors": [
                {
                    "_id": "67efd3e5afbb03a0596ff310",
                    "name": "Zhenyu Tang",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff311",
                    "name": "Chaoran Feng",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff312",
                    "name": "Xinhua Cheng",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff313",
                    "name": "Wangbo Yu",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff314",
                    "name": "Junwu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff315",
                    "name": "Yuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff316",
                    "name": "Xiaoxiao Long",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff317",
                    "name": "Wenping Wang",
                    "hidden": false
                },
                {
                    "_id": "67efd3e5afbb03a0596ff318",
                    "name": "Li Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-29T17:36:53.000Z",
            "submittedOnDailyAt": "2025-04-04T11:14:07.122Z",
            "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations",
            "submittedOnDailyBy": {
                "_id": "65250cda87ad4a39f84d482d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65250cda87ad4a39f84d482d/ryzc7hx6x09DbL4eiEBip.jpeg",
                "isPro": false,
                "fullname": "Feng Chaoran",
                "user": "Falcary",
                "type": "user"
            },
            "summary": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering\nspeed, but with millions of 3D Gaussians and significant storage and\ntransmission costs. Recent 3DGS compression methods mainly concentrate on\ncompressing Scaffold-GS, achieving impressive performance but with an\nadditional voxel structure and a complex encoding and quantization strategy. In\nthis paper, we aim to develop a simple yet effective method called NeuralGS\nthat explores in another way to compress the original 3DGS into a compact\nrepresentation without the voxel structure and complex quantization strategies.\nOur observation is that neural fields like NeRF can represent complex 3D scenes\nwith Multi-Layer Perceptron (MLP) neural networks using only a few megabytes.\nThus, NeuralGS effectively adopts the neural field representation to encode the\nattributes of 3D Gaussians with MLPs, only requiring a small storage size even\nfor a large-scale scene. To achieve this, we adopt a clustering strategy and\nfit the Gaussians with different tiny MLPs for each cluster, based on\nimportance scores of Gaussians as fitting weights. We experiment on multiple\ndatasets, achieving a 45-times average model size reduction without harming the\nvisual quality. The compression performance of our method on original 3DGS is\ncomparable to the dedicated Scaffold-GS-based compression methods, which\ndemonstrate the huge potential of directly compressing original 3DGS with\nneural fields.",
            "upvotes": 5,
            "discussionId": "67efd3e7afbb03a0596ff38e",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "3DGS",
                "Scaffold-GS",
                "voxel structure",
                "encoding",
                "quantization strategy",
                "NeuralGS",
                "neural fields",
                "NeRF",
                "Multi-Layer Perceptron",
                "MLP",
                "clustering strategy",
                "fitting weights",
                "model size reduction",
                "visual quality"
            ]
        },
        "publishedAt": "2025-03-29T13:36:53.000Z",
        "title": "NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations",
        "summary": "3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering\nspeed, but with millions of 3D Gaussians and significant storage and\ntransmission costs. Recent 3DGS compression methods mainly concentrate on\ncompressing Scaffold-GS, achieving impressive performance but with an\nadditional voxel structure and a complex encoding and quantization strategy. In\nthis paper, we aim to develop a simple yet effective method called NeuralGS\nthat explores in another way to compress the original 3DGS into a compact\nrepresentation without the voxel structure and complex quantization strategies.\nOur observation is that neural fields like NeRF can represent complex 3D scenes\nwith Multi-Layer Perceptron (MLP) neural networks using only a few megabytes.\nThus, NeuralGS effectively adopts the neural field representation to encode the\nattributes of 3D Gaussians with MLPs, only requiring a small storage size even\nfor a large-scale scene. To achieve this, we adopt a clustering strategy and\nfit the Gaussians with different tiny MLPs for each cluster, based on\nimportance scores of Gaussians as fitting weights. We experiment on multiple\ndatasets, achieving a 45-times average model size reduction without harming the\nvisual quality. The compression performance of our method on original 3DGS is\ncomparable to the dedicated Scaffold-GS-based compression methods, which\ndemonstrate the huge potential of directly compressing original 3DGS with\nneural fields.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23162.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65250cda87ad4a39f84d482d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65250cda87ad4a39f84d482d/ryzc7hx6x09DbL4eiEBip.jpeg",
            "fullname": "Feng Chaoran",
            "name": "Falcary",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02012",
            "authors": [
                {
                    "_id": "67ef5af0724d484dd41afe5c",
                    "user": {
                        "_id": "66189b980da4c017c401fb5d",
                        "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
                        "isPro": false,
                        "fullname": "soro bedio",
                        "user": "bedio",
                        "type": "user"
                    },
                    "name": "Soro Bedionita",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-04T07:51:20.621Z",
                    "hidden": false
                },
                {
                    "_id": "67ef5af0724d484dd41afe5d",
                    "name": "Bruno Andreis",
                    "hidden": false
                },
                {
                    "_id": "67ef5af0724d484dd41afe5e",
                    "name": "Song Chong",
                    "hidden": false
                },
                {
                    "_id": "67ef5af0724d484dd41afe5f",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T05:50:19.000Z",
            "submittedOnDailyAt": "2025-04-04T02:38:11.321Z",
            "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
            "submittedOnDailyBy": {
                "_id": "66189b980da4c017c401fb5d",
                "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
                "isPro": false,
                "fullname": "soro bedio",
                "user": "bedio",
                "type": "user"
            },
            "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
            "upvotes": 4,
            "discussionId": "67ef5af1724d484dd41afef3",
            "ai_keywords": [
                "diffusion models",
                "IGPG (Instruction Guided Parameter Generation)",
                "VQ-VAE",
                "autoregressive framework",
                "token level",
                "parameter synthesis",
                "inter-layer coherence",
                "vision datasets",
                "pretrained models",
                "pretrained weight retrieval",
                "model selection",
                "task-specific fine-tuning"
            ]
        },
        "publishedAt": "2025-04-02T01:50:19.000Z",
        "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
        "summary": "Learning to generate neural network parameters conditioned on task\ndescriptions and architecture specifications is pivotal for advancing model\nadaptability and transfer learning. Existing methods especially those based on\ndiffusion models suffer from limited scalability to large architectures,\nrigidity in handling varying network depths, and disjointed parameter\ngeneration that undermines inter-layer coherence. In this work, we propose IGPG\n(Instruction Guided Parameter Generation), an autoregressive framework that\nunifies parameter synthesis across diverse tasks and architectures. IGPG\nleverages a VQ-VAE and an autoregressive model to generate neural network\nparameters, conditioned on task instructions, dataset, and architecture\ndetails. By autoregressively generating neural network weights' tokens, IGPG\nensures inter-layer coherence and enables efficient adaptation across models\nand datasets. Operating at the token level, IGPG effectively captures complex\nparameter distributions aggregated from a broad spectrum of pretrained models.\nExtensive experiments on multiple vision datasets demonstrate that IGPG\nconsolidates diverse pretrained models into a single, flexible generative\nframework. The synthesized parameters achieve competitive or superior\nperformance relative to state-of-the-art methods, especially in terms of\nscalability and efficiency when applied to large architectures. These results\nunderscore ICPG potential as a powerful tool for pretrained weight retrieval,\nmodel selection, and rapid task-specific fine-tuning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02012.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66189b980da4c017c401fb5d",
            "avatarUrl": "/avatars/d5184741e4a333435022bcb9a4a9b9d8.svg",
            "fullname": "soro bedio",
            "name": "bedio",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.23542",
            "authors": [
                {
                    "_id": "67ef7e3e8d4b8473f157ea59",
                    "user": {
                        "_id": "621ff334fa5492893dc03d82",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621ff334fa5492893dc03d82/EAIr-l3O4OeM10f1boLux.jpeg",
                        "isPro": false,
                        "fullname": "Xabier de Zuazo",
                        "user": "zuazo",
                        "type": "user"
                    },
                    "name": "Xabier de Zuazo",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-04T10:58:52.768Z",
                    "hidden": false
                },
                {
                    "_id": "67ef7e3e8d4b8473f157ea5a",
                    "name": "Eva Navas",
                    "hidden": false
                },
                {
                    "_id": "67ef7e3e8d4b8473f157ea5b",
                    "name": "Ibon Saratxaga",
                    "hidden": false
                },
                {
                    "_id": "67ef7e3e8d4b8473f157ea5c",
                    "user": {
                        "_id": "65f47c863462e375c27a8439",
                        "avatarUrl": "/avatars/b92fd8a8d7741cf27935e98b083d147f.svg",
                        "isPro": false,
                        "fullname": "Inma Hernaez",
                        "user": "inmahernaez",
                        "type": "user"
                    },
                    "name": "Inma HernÃ¡ez Rioja",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-04T06:37:52.727Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-30T18:03:52.000Z",
            "submittedOnDailyAt": "2025-04-04T09:42:04.836Z",
            "title": "Whisper-LM: Improving ASR Models with Language Models for Low-Resource\n  Languages",
            "submittedOnDailyBy": {
                "_id": "621ff334fa5492893dc03d82",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621ff334fa5492893dc03d82/EAIr-l3O4OeM10f1boLux.jpeg",
                "isPro": false,
                "fullname": "Xabier de Zuazo",
                "user": "zuazo",
                "type": "user"
            },
            "summary": "Automatic speech recognition systems have undoubtedly advanced with the\nintegration of multilingual and multitask models such as Whisper, which have\nshown a promising ability to understand and process speech across a wide range\nof languages. Despite their robustness, these models often fall short in\nhandling the linguistic distinctions of minority languages. This study\naddresses this gap by integrating traditional and novel language models with\nfine-tuned Whisper models to raise their performance in less commonly studied\nlanguages. Through rigorous fine-tuning and evaluation across multiple\ndatasets, we demonstrate substantial improvements in word error rate,\nparticularly in low-resource scenarios. Our approach not only does take\nadvantage of the extensive data Whisper was pre-trained on, but also\ncomplements its linguistic adaptability by incorporating language models. We\nobtained improvements up to 51\\% for in-distribution datasets and up to 34\\%\nfor out-of-distribution sentences using statistical language models, while\nlarge language models provided moderate but consistently robust improvement\nacross diverse linguistic contexts. The findings reveal that, while the\nintegration reliably benefits all model sizes, the extent of improvement\nvaries, highlighting the importance of optimized language model parameters.\nFinally, we emphasize the importance of selecting appropriate evaluation\nparameters when reporting the results using transformer-based ASR models. In\nsummary, this research clears the way for more inclusive ASR technologies that\nperform better across languages by enriching their linguistic knowledge. For\nfurther implementation details of this study, the technical documentation and\nsource code are available at http://www.github.com/hitz-zentroa/whisper-lm.",
            "upvotes": 3,
            "discussionId": "67ef7e408d4b8473f157ead2",
            "githubRepo": "https://github.com/hitz-zentroa/whisper-lm",
            "ai_keywords": [
                "Whisper",
                "multilingual models",
                "multitask models",
                "fine-tuned Whisper models",
                "statistical language models",
                "large language models",
                "transformer-based ASR models"
            ]
        },
        "publishedAt": "2025-03-30T14:03:52.000Z",
        "title": "Whisper-LM: Improving ASR Models with Language Models for Low-Resource\n  Languages",
        "summary": "Automatic speech recognition systems have undoubtedly advanced with the\nintegration of multilingual and multitask models such as Whisper, which have\nshown a promising ability to understand and process speech across a wide range\nof languages. Despite their robustness, these models often fall short in\nhandling the linguistic distinctions of minority languages. This study\naddresses this gap by integrating traditional and novel language models with\nfine-tuned Whisper models to raise their performance in less commonly studied\nlanguages. Through rigorous fine-tuning and evaluation across multiple\ndatasets, we demonstrate substantial improvements in word error rate,\nparticularly in low-resource scenarios. Our approach not only does take\nadvantage of the extensive data Whisper was pre-trained on, but also\ncomplements its linguistic adaptability by incorporating language models. We\nobtained improvements up to 51\\% for in-distribution datasets and up to 34\\%\nfor out-of-distribution sentences using statistical language models, while\nlarge language models provided moderate but consistently robust improvement\nacross diverse linguistic contexts. The findings reveal that, while the\nintegration reliably benefits all model sizes, the extent of improvement\nvaries, highlighting the importance of optimized language model parameters.\nFinally, we emphasize the importance of selecting appropriate evaluation\nparameters when reporting the results using transformer-based ASR models. In\nsummary, this research clears the way for more inclusive ASR technologies that\nperform better across languages by enriching their linguistic knowledge. For\nfurther implementation details of this study, the technical documentation and\nsource code are available at http://www.github.com/hitz-zentroa/whisper-lm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23542.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "621ff334fa5492893dc03d82",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621ff334fa5492893dc03d82/EAIr-l3O4OeM10f1boLux.jpeg",
            "fullname": "Xabier de Zuazo",
            "name": "zuazo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.01955",
            "authors": [
                {
                    "_id": "67f00313579927adf0ee1d8b",
                    "name": "Oliver Hahn",
                    "hidden": false
                },
                {
                    "_id": "67f00313579927adf0ee1d8c",
                    "name": "Christoph Reich",
                    "hidden": false
                },
                {
                    "_id": "67f00313579927adf0ee1d8d",
                    "name": "Nikita Araslanov",
                    "hidden": false
                },
                {
                    "_id": "67f00313579927adf0ee1d8e",
                    "name": "Daniel Cremers",
                    "hidden": false
                },
                {
                    "_id": "67f00313579927adf0ee1d8f",
                    "name": "Christian Rupprecht",
                    "hidden": false
                },
                {
                    "_id": "67f00313579927adf0ee1d90",
                    "name": "Stefan Roth",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T17:58:46.000Z",
            "submittedOnDailyAt": "2025-04-04T17:04:55.213Z",
            "title": "Scene-Centric Unsupervised Panoptic Segmentation",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Unsupervised panoptic segmentation aims to partition an image into\nsemantically meaningful regions and distinct object instances without training\non manually annotated data. In contrast to prior work on unsupervised panoptic\nscene understanding, we eliminate the need for object-centric training data,\nenabling the unsupervised understanding of complex scenes. To that end, we\npresent the first unsupervised panoptic method that directly trains on\nscene-centric imagery. In particular, we propose an approach to obtain\nhigh-resolution panoptic pseudo labels on complex scene-centric data, combining\nvisual representations, depth, and motion cues. Utilizing both pseudo-label\ntraining and a panoptic self-training strategy yields a novel approach that\naccurately predicts panoptic segmentation of complex scenes without requiring\nany human annotations. Our approach significantly improves panoptic quality,\ne.g., surpassing the recent state of the art in unsupervised panoptic\nsegmentation on Cityscapes by 9.4% points in PQ.",
            "upvotes": 1,
            "discussionId": "67f00319579927adf0ee1f18",
            "ai_keywords": [
                "panoptic segmentation",
                "object-centric training data",
                "scene-centric imagery",
                "panoptic pseudo labels",
                "visual representations",
                "depth cues",
                "motion cues",
                "pseudo-label training",
                "panoptic self-training strategy",
                "semantically meaningful regions",
                "distinct object instances"
            ]
        },
        "publishedAt": "2025-04-02T13:58:46.000Z",
        "title": "Scene-Centric Unsupervised Panoptic Segmentation",
        "summary": "Unsupervised panoptic segmentation aims to partition an image into\nsemantically meaningful regions and distinct object instances without training\non manually annotated data. In contrast to prior work on unsupervised panoptic\nscene understanding, we eliminate the need for object-centric training data,\nenabling the unsupervised understanding of complex scenes. To that end, we\npresent the first unsupervised panoptic method that directly trains on\nscene-centric imagery. In particular, we propose an approach to obtain\nhigh-resolution panoptic pseudo labels on complex scene-centric data, combining\nvisual representations, depth, and motion cues. Utilizing both pseudo-label\ntraining and a panoptic self-training strategy yields a novel approach that\naccurately predicts panoptic segmentation of complex scenes without requiring\nany human annotations. Our approach significantly improves panoptic quality,\ne.g., surpassing the recent state of the art in unsupervised panoptic\nsegmentation on Cityscapes by 9.4% points in PQ.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01955.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 809
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.01943",
            "authors": [
                {
                    "_id": "67edf81f5e87fcaa485b0df5",
                    "user": {
                        "_id": "67ec45ba92673a326c067b52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PcD60kFRczOblxgc1bWcl.png",
                        "isPro": false,
                        "fullname": "Wasi Uddin Ahmad",
                        "user": "wasiuddina",
                        "type": "user"
                    },
                    "name": "Wasi Uddin Ahmad",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-04T07:09:48.768Z",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0df6",
                    "name": "Sean Narenthiran",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0df7",
                    "user": {
                        "_id": "61eb5b2f55d896e8b6c1c376",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652937892550-61eb5b2f55d896e8b6c1c376.jpeg",
                        "isPro": false,
                        "fullname": "Somshubra Majumdar",
                        "user": "smajumdar",
                        "type": "user"
                    },
                    "name": "Somshubra Majumdar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-03T07:52:08.410Z",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0df8",
                    "name": "Aleksander Ficek",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0df9",
                    "name": "Siddhartha Jain",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0dfa",
                    "name": "Jocelyn Huang",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0dfb",
                    "name": "Vahid Noroozi",
                    "hidden": false
                },
                {
                    "_id": "67edf81f5e87fcaa485b0dfc",
                    "name": "Boris Ginsburg",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T17:50:31.000Z",
            "submittedOnDailyAt": "2025-04-04T21:26:04.316Z",
            "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
            "submittedOnDailyBy": {
                "_id": "6254f8e5d21e4cc386b881ad",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649899774659-6254f8e5d21e4cc386b881ad.jpeg",
                "isPro": false,
                "fullname": "Somshubra Majumdar",
                "user": "smajumdar94",
                "type": "user"
            },
            "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.",
            "upvotes": 0,
            "discussionId": "67edf8205e87fcaa485b0e2b",
            "ai_keywords": [
                "reasoning-based large language models",
                "distilling reasoning capabilities",
                "student models",
                "coding tasks",
                "supervised fine-tuning (SFT)",
                "LiveCodeBench",
                "CodeContests",
                "reinforcement learning",
                "code execution filtering",
                "instruction diversity",
                "solution diversity",
                "benchmark accuracy",
                "token efficiency",
                "reasoning patterns"
            ]
        },
        "publishedAt": "2025-04-02T13:50:31.000Z",
        "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
        "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01943.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6254f8e5d21e4cc386b881ad",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649899774659-6254f8e5d21e4cc386b881ad.jpeg",
            "fullname": "Somshubra Majumdar",
            "name": "smajumdar94",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": false
    }
]