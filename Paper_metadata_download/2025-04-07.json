[
    {
        "paper": {
            "id": "2504.02605",
            "authors": [
                {
                    "_id": "67ef4d92c1e251f239495a13",
                    "name": "Daoguang Zan",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a14",
                    "name": "Zhirong Huang",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a15",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a16",
                    "name": "Hanwu Chen",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a17",
                    "name": "Linhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a18",
                    "name": "Shulin Xin",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a19",
                    "name": "Lu Chen",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a1a",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a1b",
                    "name": "Xiaojian Zhong",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a1c",
                    "name": "Aoyan Li",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a1d",
                    "name": "Siyao Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a1e",
                    "name": "Yongsheng Xiao",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a1f",
                    "name": "Liangqiang Chen",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a20",
                    "name": "Yuyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a21",
                    "name": "Jing Su",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a22",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a23",
                    "name": "Rui Long",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a24",
                    "name": "Kai Shen",
                    "hidden": false
                },
                {
                    "_id": "67ef4d92c1e251f239495a25",
                    "name": "Liang Xiang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/PS_Q49kWYAB6DdJy5YY9k.png",
                "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/GhnnOocFnA-YN2oyeNjPB.png"
            ],
            "publishedAt": "2025-04-03T14:06:17.000Z",
            "submittedOnDailyAt": "2025-04-07T02:30:50.286Z",
            "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
            "submittedOnDailyBy": {
                "_id": "61527edf8b55dbdae72874fa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
                "isPro": false,
                "fullname": "Daoguang Zan",
                "user": "Daoguang",
                "type": "user"
            },
            "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
            "upvotes": 30,
            "discussionId": "67ef4d93c1e251f239495a9b",
            "projectPage": "https://multi-swe-bench.github.io",
            "githubRepo": "https://github.com/multi-swe-bench/multi-swe-bench",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Multi-SWE-bench",
                "Agentless",
                "SWE-agent",
                "OpenHands",
                "Multi-SWE-RL",
                "reinforcement learning (RL)",
                "AGI"
            ]
        },
        "publishedAt": "2025-04-03T10:06:17.000Z",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/PS_Q49kWYAB6DdJy5YY9k.png",
            "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/GhnnOocFnA-YN2oyeNjPB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02605.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61527edf8b55dbdae72874fa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
            "fullname": "Daoguang Zan",
            "name": "Daoguang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02807",
            "authors": [
                {
                    "_id": "67efe6e839d83c66f96ef051",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef052",
                    "name": "Zengzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef053",
                    "name": "Nikhil Ranjan",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef054",
                    "name": "Zhoujun Cheng",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef055",
                    "name": "Liping Tang",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef056",
                    "name": "Guowei He",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef057",
                    "name": "Zhengzhong Liu",
                    "hidden": false
                },
                {
                    "_id": "67efe6e839d83c66f96ef058",
                    "name": "Eric P. Xing",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/628f6e5ab90dde28ef57d293/WNxrlIEMuRO6z1IF5op1-.png",
                "https://cdn-uploads.huggingface.co/production/uploads/628f6e5ab90dde28ef57d293/VRjSgFv994luajXWJ42V7.png"
            ],
            "publishedAt": "2025-04-03T17:52:07.000Z",
            "submittedOnDailyAt": "2025-04-07T09:34:49.638Z",
            "title": "MegaMath: Pushing the Limits of Open Math Corpora",
            "submittedOnDailyBy": {
                "_id": "628f6e5ab90dde28ef57d293",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f6e5ab90dde28ef57d293/AxNzR2nvrND6Rf3RPkYMk.jpeg",
                "isPro": false,
                "fullname": "Fan Zhou",
                "user": "koalazf99",
                "type": "user"
            },
            "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.",
            "upvotes": 20,
            "discussionId": "67efe6e939d83c66f96ef08d",
            "projectPage": "https://huggingface.co/datasets/LLM360/MegaMath",
            "githubRepo": "https://github.com/LLM360/MegaMath",
            "ai_keywords": [
                "mathematical documents",
                "Common Crawl",
                "math-oriented HTML optimizations",
                "fasttext-based filtering",
                "deduplication",
                "high quality math-related code",
                "Stack-V2",
                "QA-style text",
                "interleaved text-code blocks",
                "math pre-training datasets"
            ]
        },
        "publishedAt": "2025-04-03T13:52:07.000Z",
        "title": "MegaMath: Pushing the Limits of Open Math Corpora",
        "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/628f6e5ab90dde28ef57d293/WNxrlIEMuRO6z1IF5op1-.png",
            "https://cdn-uploads.huggingface.co/production/uploads/628f6e5ab90dde28ef57d293/VRjSgFv994luajXWJ42V7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02807.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f6e5ab90dde28ef57d293",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f6e5ab90dde28ef57d293/AxNzR2nvrND6Rf3RPkYMk.jpeg",
            "fullname": "Fan Zhou",
            "name": "koalazf99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03553",
            "authors": [
                {
                    "_id": "67f345c983edbd64f15deeb3",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeb4",
                    "name": "Zhisong Qiu",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeb5",
                    "name": "Baochang Ren",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeb6",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeb7",
                    "name": "Xiangyuan Ru",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeb8",
                    "name": "Ningyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeb9",
                    "name": "Xiang Chen",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deeba",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deebb",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deebc",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "67f345c983edbd64f15deebd",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T16:03:38.000Z",
            "submittedOnDailyAt": "2025-04-07T02:45:21.106Z",
            "title": "Agentic Knowledgeable Self-awareness",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
            "upvotes": 19,
            "discussionId": "67f345cd83edbd64f15def73",
            "githubRepo": "https://github.com/zjunlp/KnowSelf",
            "ai_keywords": [
                "agentic planning",
                "flood irrigation methodology",
                "gold trajectories",
                "external feedback",
                "domain knowledge",
                "self-awareness",
                "decision-making",
                "agentic knowledgeable self-awareness",
                "KnowSelf",
                "data-centric approach",
                "situation judgement criterion",
                "special tokens",
                "two-stage training process",
                "trajectory-based training"
            ]
        },
        "publishedAt": "2025-04-04T12:03:38.000Z",
        "title": "Agentic Knowledgeable Self-awareness",
        "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03553.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03561",
            "authors": [
                {
                    "_id": "67f351f068751b2bb84cc751",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc752",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc753",
                    "name": "Yuan Liang",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc754",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc755",
                    "name": "Jialong Wu",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc756",
                    "name": "Zekun Xi",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc757",
                    "name": "Ningyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc758",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc759",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc75a",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "67f351f068751b2bb84cc75b",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T16:10:57.000Z",
            "submittedOnDailyAt": "2025-04-07T02:48:19.567Z",
            "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
            "upvotes": 15,
            "discussionId": "67f351f168751b2bb84cc789",
            "ai_keywords": [
                "LLM-based agents",
                "multi-step action invocation",
                "Monte Carlo Tree Search (MCTS)"
            ]
        },
        "publishedAt": "2025-04-04T12:10:57.000Z",
        "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
        "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03561.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03641",
            "authors": [
                {
                    "_id": "67f34f5dfb6d8a613926ac2b",
                    "name": "Wulin Xie",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac2c",
                    "name": "Yi-Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac2d",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac2e",
                    "name": "Yang Shi",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac2f",
                    "name": "Bingyan Nie",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac30",
                    "name": "Hongkai Chen",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac31",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac32",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "67f34f5dfb6d8a613926ac33",
                    "name": "Tieniu Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T17:59:55.000Z",
            "submittedOnDailyAt": "2025-04-07T02:38:07.467Z",
            "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
            "submittedOnDailyBy": {
                "_id": "623d8ca4c29adf5ef6175615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                "isPro": false,
                "fullname": "Yi-Fan Zhang",
                "user": "yifanzhang114",
                "type": "user"
            },
            "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/.",
            "upvotes": 10,
            "discussionId": "67f34f64fb6d8a613926ada9",
            "projectPage": "https://mme-unify.github.io/",
            "githubRepo": "https://github.com/MME-Benchmarks/MME-Unify"
        },
        "publishedAt": "2025-04-04T13:59:55.000Z",
        "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
        "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03641.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03601",
            "authors": [
                {
                    "_id": "67f36505e11bd4b05579afbf",
                    "name": "Akshara Prabhakar",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc0",
                    "name": "Zuxin Liu",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc1",
                    "name": "Weiran Yao",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc2",
                    "name": "Jianguo Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc3",
                    "name": "Ming Zhu",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc4",
                    "name": "Shiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc5",
                    "name": "Zhiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc6",
                    "name": "Tulika Awalgaonkar",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc7",
                    "name": "Haolin Chen",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc8",
                    "name": "Thai Hoang",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afc9",
                    "name": "Juan Carlos Niebles",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afca",
                    "name": "Shelby Heinecke",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afcb",
                    "name": "Huan Wang",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afcc",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "67f36505e11bd4b05579afcd",
                    "name": "Caiming Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T17:13:57.000Z",
            "submittedOnDailyAt": "2025-04-07T04:10:25.529Z",
            "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on tau-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
            "upvotes": 10,
            "discussionId": "67f36507e11bd4b05579b020",
            "ai_keywords": [
                "agentic pipeline",
                "task blueprints",
                "ground-truth actions",
                "LLM reviewers",
                "iterative feedback loops",
                "simulated human-agent interplay",
                "xLAM-2-fc-r series",
                "$\\tau$-bench",
                "BFCL benchmarks",
                "multi-turn settings",
                "verified blueprint-to-details approach"
            ]
        },
        "publishedAt": "2025-04-04T13:13:57.000Z",
        "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
        "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on tau-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03601.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6602
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02949",
            "authors": [
                {
                    "_id": "67f350a5e11bd4b05575a831",
                    "name": "Xianwei Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a832",
                    "name": "Yuxin Xie",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a833",
                    "name": "Yufan Deng",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a834",
                    "name": "Dongchao Yang",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a835",
                    "name": "Liming Liang",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a836",
                    "name": "Jinghan Ru",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a837",
                    "name": "Yuguo Yin",
                    "hidden": false
                },
                {
                    "_id": "67f350a5e11bd4b05575a838",
                    "name": "Yuexian Zou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T18:06:28.000Z",
            "submittedOnDailyAt": "2025-04-07T02:42:39.671Z",
            "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
            "upvotes": 10,
            "discussionId": "67f350a9e11bd4b05575a921",
            "githubRepo": "https://github.com/VARGPT-family/VARGPT-v1.1",
            "ai_keywords": [
                "unified visual autoregressive model",
                "next-token prediction",
                "next-scale generation",
                "iterative visual instruction tuning",
                "reinforcement learning",
                "Direct Preference Optimization (DPO)",
                "visual-generative instruction pairs",
                "Qwen2",
                "image generation resolution",
                "emergent image editing capabilities",
                "multimodal understanding",
                "text-to-image instruction-following tasks",
                "comprehension and generation metrics",
                "large language models (LLMs)"
            ]
        },
        "publishedAt": "2025-04-03T14:06:28.000Z",
        "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
        "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02949.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 44
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2503.24067",
            "authors": [
                {
                    "_id": "67ecb89d0210bff02fd3591e",
                    "user": {
                        "_id": "66b4e3d850b87d84498bbc89",
                        "avatarUrl": "/avatars/526c0cabf3a2c019a13bb82fcc7b43e9.svg",
                        "isPro": false,
                        "fullname": "YixingLi",
                        "user": "Yixinglee",
                        "type": "user"
                    },
                    "name": "Yixing Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-02T04:10:06.783Z",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd3591f",
                    "name": "Ruobing Xie",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35920",
                    "user": {
                        "_id": "62c4057732fa66fedacca0db",
                        "avatarUrl": "/avatars/813e8c9fcd14dde1fdd415408f61bec2.svg",
                        "isPro": false,
                        "fullname": "AndyYang",
                        "user": "andyyang",
                        "type": "user"
                    },
                    "name": "Zhen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-02T08:21:39.891Z",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35921",
                    "name": "Xingwu Sun",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35922",
                    "name": "Shuaipeng Li",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35923",
                    "name": "Weidong Han",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35924",
                    "name": "Zhanhui Kang",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35925",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35926",
                    "name": "Chengzhong Xu",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35927",
                    "name": "Di Wang",
                    "hidden": false
                },
                {
                    "_id": "67ecb89d0210bff02fd35928",
                    "name": "Jie Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T13:26:24.000Z",
            "submittedOnDailyAt": "2025-04-07T07:35:17.947Z",
            "title": "TransMamba: Flexibly Switching between Transformer and Mamba",
            "submittedOnDailyBy": {
                "_id": "62c4057732fa66fedacca0db",
                "avatarUrl": "/avatars/813e8c9fcd14dde1fdd415408f61bec2.svg",
                "isPro": false,
                "fullname": "AndyYang",
                "user": "andyyang",
                "type": "user"
            },
            "summary": "Transformers are the cornerstone of modern large language models, but their\nquadratic computational complexity limits efficiency in long-sequence\nprocessing. Recent advancements in Mamba, a state space model (SSM) with linear\ncomplexity, offer promising efficiency gains but suffer from unstable\ncontextual learning and multitask generalization. This paper proposes\nTransMamba, a novel framework that unifies Transformer and Mamba through shared\nparameter matrices (e.g., QKV and CBx), and thus could dynamically switch\nbetween attention and SSM mechanisms at different token lengths and layers. We\ndesign the Memory converter to bridge Transformer and Mamba by converting\nattention outputs into SSM-compatible states, ensuring seamless information\nflow at TransPoints where the transformation happens. The TransPoint scheduling\nis also thoroughly explored for further improvements. We conducted extensive\nexperiments demonstrating that TransMamba achieves superior training efficiency\nand performance compared to baselines, and validated the deeper consistency\nbetween Transformer and Mamba paradigms, offering a scalable solution for\nnext-generation sequence modeling.",
            "upvotes": 9,
            "discussionId": "67ecb89e0210bff02fd3596b",
            "ai_keywords": [
                "Transformers",
                "state space model (SSM)",
                "attention",
                "QKV",
                "CBx",
                "TransMamba",
                "parameter matrices",
                "Memory converter",
                "TransPoints",
                "TransPoint scheduling",
                "next-generation sequence modeling"
            ]
        },
        "publishedAt": "2025-03-31T09:26:24.000Z",
        "title": "TransMamba: Flexibly Switching between Transformer and Mamba",
        "summary": "Transformers are the cornerstone of modern large language models, but their\nquadratic computational complexity limits efficiency in long-sequence\nprocessing. Recent advancements in Mamba, a state space model (SSM) with linear\ncomplexity, offer promising efficiency gains but suffer from unstable\ncontextual learning and multitask generalization. This paper proposes\nTransMamba, a novel framework that unifies Transformer and Mamba through shared\nparameter matrices (e.g., QKV and CBx), and thus could dynamically switch\nbetween attention and SSM mechanisms at different token lengths and layers. We\ndesign the Memory converter to bridge Transformer and Mamba by converting\nattention outputs into SSM-compatible states, ensuring seamless information\nflow at TransPoints where the transformation happens. The TransPoint scheduling\nis also thoroughly explored for further improvements. We conducted extensive\nexperiments demonstrating that TransMamba achieves superior training efficiency\nand performance compared to baselines, and validated the deeper consistency\nbetween Transformer and Mamba paradigms, offering a scalable solution for\nnext-generation sequence modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24067.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c4057732fa66fedacca0db",
            "avatarUrl": "/avatars/813e8c9fcd14dde1fdd415408f61bec2.svg",
            "fullname": "AndyYang",
            "name": "andyyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.22738",
            "authors": [
                {
                    "_id": "67f43e4d79e20b02392d9e33",
                    "name": "Zhaorun Chen",
                    "hidden": false
                },
                {
                    "_id": "67f43e4d79e20b02392d9e34",
                    "name": "Mintong Kang",
                    "hidden": false
                },
                {
                    "_id": "67f43e4d79e20b02392d9e35",
                    "name": "Bo Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-26T17:58:40.000Z",
            "submittedOnDailyAt": "2025-04-07T19:37:04.573Z",
            "title": "ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning",
            "submittedOnDailyBy": {
                "_id": "64d448d3d095077728f7c740",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d448d3d095077728f7c740/4CFkiVoXXDsu7uQPTU0n3.jpeg",
                "isPro": false,
                "fullname": "Zhaorun Chen",
                "user": "Zhaorun",
                "type": "user"
            },
            "summary": "Autonomous agents powered by foundation models have seen widespread adoption\nacross various real-world applications. However, they remain highly vulnerable\nto malicious instructions and attacks, which can result in severe consequences\nsuch as privacy breaches and financial losses. More critically, existing\nguardrails for LLMs are not applicable due to the complex and dynamic nature of\nagents. To tackle these challenges, we propose ShieldAgent, the first guardrail\nagent designed to enforce explicit safety policy compliance for the action\ntrajectory of other protected agents through logical reasoning. Specifically,\nShieldAgent first constructs a safety policy model by extracting verifiable\nrules from policy documents and structuring them into a set of action-based\nprobabilistic rule circuits. Given the action trajectory of the protected\nagent, ShieldAgent retrieves relevant rule circuits and generates a shielding\nplan, leveraging its comprehensive tool library and executable code for formal\nverification. In addition, given the lack of guardrail benchmarks for agents,\nwe introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent\ninstructions and action trajectories, collected via SOTA attacks across 6 web\nenvironments and 7 risk categories. Experiments show that ShieldAgent achieves\nSOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior\nmethods by 11.3% on average with a high recall of 90.1%. Additionally,\nShieldAgent reduces API queries by 64.7% and inference time by 58.2%,\ndemonstrating its high precision and efficiency in safeguarding agents.",
            "upvotes": 9,
            "discussionId": "67f43e4f79e20b02392d9ea2"
        },
        "publishedAt": "2025-03-26T13:58:40.000Z",
        "title": "ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning",
        "summary": "Autonomous agents powered by foundation models have seen widespread adoption\nacross various real-world applications. However, they remain highly vulnerable\nto malicious instructions and attacks, which can result in severe consequences\nsuch as privacy breaches and financial losses. More critically, existing\nguardrails for LLMs are not applicable due to the complex and dynamic nature of\nagents. To tackle these challenges, we propose ShieldAgent, the first guardrail\nagent designed to enforce explicit safety policy compliance for the action\ntrajectory of other protected agents through logical reasoning. Specifically,\nShieldAgent first constructs a safety policy model by extracting verifiable\nrules from policy documents and structuring them into a set of action-based\nprobabilistic rule circuits. Given the action trajectory of the protected\nagent, ShieldAgent retrieves relevant rule circuits and generates a shielding\nplan, leveraging its comprehensive tool library and executable code for formal\nverification. In addition, given the lack of guardrail benchmarks for agents,\nwe introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent\ninstructions and action trajectories, collected via SOTA attacks across 6 web\nenvironments and 7 risk categories. Experiments show that ShieldAgent achieves\nSOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior\nmethods by 11.3% on average with a high recall of 90.1%. Additionally,\nShieldAgent reduces API queries by 64.7% and inference time by 58.2%,\ndemonstrating its high precision and efficiency in safeguarding agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22738.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d448d3d095077728f7c740",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d448d3d095077728f7c740/4CFkiVoXXDsu7uQPTU0n3.jpeg",
            "fullname": "Zhaorun Chen",
            "name": "Zhaorun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03011",
            "authors": [
                {
                    "_id": "67f35d8bdb1a843e1ceff38f",
                    "name": "Junying Wang",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff390",
                    "name": "Jingyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff391",
                    "name": "Xin Sun",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff392",
                    "name": "Krishna Kumar Singh",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff393",
                    "name": "Zhixin Shu",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff394",
                    "name": "He Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff395",
                    "name": "Jimei Yang",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff396",
                    "name": "Nanxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff397",
                    "name": "Tuanfeng Y. Wang",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff398",
                    "name": "Simon S. Chen",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff399",
                    "name": "Ulrich Neumann",
                    "hidden": false
                },
                {
                    "_id": "67f35d8bdb1a843e1ceff39a",
                    "name": "Jae Shin Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T20:10:50.000Z",
            "submittedOnDailyAt": "2025-04-07T03:39:50.539Z",
            "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.",
            "upvotes": 7,
            "discussionId": "67f35d91db1a843e1ceff47c",
            "ai_keywords": [
                "diffusion models",
                "image prior",
                "coarse-to-fine framework",
                "temporal lighting model",
                "lighting cycle consistency",
                "spatio-temporal feature blending",
                "guided refinement"
            ]
        },
        "publishedAt": "2025-04-03T16:10:50.000Z",
        "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
        "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03011.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6602
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03536",
            "authors": [
                {
                    "_id": "67f364118188d683931bec4a",
                    "name": "Boyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec4b",
                    "name": "Runqi Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec4c",
                    "name": "Xiaofeng Wang",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec4d",
                    "user": {
                        "_id": "656e9b562cd7a3e348011d26",
                        "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
                        "isPro": false,
                        "fullname": "Zheng Zhu",
                        "user": "ZhengZhu",
                        "type": "user"
                    },
                    "name": "Zheng Zhu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-07T05:35:15.515Z",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec4e",
                    "name": "Guosheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec4f",
                    "name": "Chaojun Ni",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec50",
                    "name": "Guan Huang",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec51",
                    "name": "Lihong Liu",
                    "hidden": false
                },
                {
                    "_id": "67f364118188d683931bec52",
                    "name": "Xingang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T15:35:14.000Z",
            "submittedOnDailyAt": "2025-04-07T04:06:19.815Z",
            "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce HumanDreamer-X, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, HumanFixer is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models.",
            "upvotes": 6,
            "discussionId": "67f364138188d683931beca2",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "HumanDreamer-X",
                "HumanFixer",
                "attention modulation",
                "PSNR"
            ]
        },
        "publishedAt": "2025-04-04T11:35:14.000Z",
        "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
        "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce HumanDreamer-X, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, HumanFixer is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03536.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6602
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.03600",
            "authors": [
                {
                    "_id": "67f32c04be7fbc47d7ad7138",
                    "name": "Jun Ma",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad7139",
                    "name": "Zongxin Yang",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad713a",
                    "name": "Sumin Kim",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad713b",
                    "name": "Bihui Chen",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad713c",
                    "name": "Mohammed Baharoon",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad713d",
                    "name": "Adibvafa Fallahpour",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad713e",
                    "name": "Reza Asakereh",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad713f",
                    "name": "Hongwei Lyu",
                    "hidden": false
                },
                {
                    "_id": "67f32c04be7fbc47d7ad7140",
                    "name": "Bo Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T17:13:37.000Z",
            "submittedOnDailyAt": "2025-04-07T09:57:38.534Z",
            "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.",
            "upvotes": 5,
            "discussionId": "67f32c06be7fbc47d7ad71bb",
            "projectPage": "https://medsam2.github.io/",
            "githubRepo": "https://github.com/bowang-lab/MedSAM2",
            "ai_keywords": [
                "promptable segmentation foundation model",
                "fine-tuning",
                "Segment Anything Model 2",
                "3D image and video segmentation",
                "3D image-mask pairs",
                "organs",
                "lesions",
                "imaging modalities",
                "human-in-the-loop pipeline",
                "CT lesions",
                "liver MRI lesions",
                "echocardiogram video frames",
                "user-friendly interfaces",
                "local and cloud deployment"
            ]
        },
        "publishedAt": "2025-04-04T13:13:37.000Z",
        "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
        "summary": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 813
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02402",
            "authors": [
                {
                    "_id": "67f0a09a2c873f5ba90cd14a",
                    "user": {
                        "_id": "67ee782979018bf61e2522a4",
                        "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
                        "isPro": false,
                        "fullname": "HaoYin",
                        "user": "yyzqy",
                        "type": "user"
                    },
                    "name": "Hao Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-06T08:11:07.967Z",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd14b",
                    "name": "Shi Guo",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd14c",
                    "name": "Xu Jia",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd14d",
                    "name": "Xudong XU",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd14e",
                    "name": "Lu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd14f",
                    "name": "Si Liu",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd150",
                    "name": "Dong Wang",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd151",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "67f0a09a2c873f5ba90cd152",
                    "name": "Tianfan Xue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T08:51:17.000Z",
            "submittedOnDailyAt": "2025-04-07T05:44:25.380Z",
            "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
            "submittedOnDailyBy": {
                "_id": "67ee782979018bf61e2522a4",
                "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
                "isPro": false,
                "fullname": "HaoYin",
                "user": "yyzqy",
                "type": "user"
            },
            "summary": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
            "upvotes": 5,
            "discussionId": "67f0a09e2c873f5ba90cd26c",
            "projectPage": "https://yyzq1.github.io/EvMic/",
            "githubRepo": "https://github.com/yyzq1/EvMic",
            "ai_keywords": [
                "event camera",
                "high-frequency signals",
                "event stream",
                "spatial-temporal information",
                "novel simulation pipeline",
                "Mamba",
                "spatial aggregation block",
                "laser matrix"
            ]
        },
        "publishedAt": "2025-04-03T04:51:17.000Z",
        "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
        "summary": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02402.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ee782979018bf61e2522a4",
            "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
            "fullname": "HaoYin",
            "name": "yyzqy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2503.24310",
            "authors": [
                {
                    "_id": "67ecb513c8ae971f9ad15bd1",
                    "user": {
                        "_id": "6478fc1512ae749b62ebbbd5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
                        "isPro": false,
                        "fullname": "Alok Abhishek",
                        "user": "alokabhishek",
                        "type": "user"
                    },
                    "name": "Alok Abhishek",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-02T03:59:10.671Z",
                    "hidden": false
                },
                {
                    "_id": "67ecb513c8ae971f9ad15bd2",
                    "name": "Lisa Erickson",
                    "hidden": false
                },
                {
                    "_id": "67ecb513c8ae971f9ad15bd3",
                    "user": {
                        "_id": "657372396da136b50f5489a0",
                        "avatarUrl": "/avatars/57a693058e9a05a2c32f02bab1d8e819.svg",
                        "isPro": false,
                        "fullname": "Tushar Bandopadhyay",
                        "user": "tbandopa",
                        "type": "user"
                    },
                    "name": "Tushar Bandopadhyay",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-02T08:13:38.947Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-03-31T16:56:52.000Z",
            "submittedOnDailyAt": "2025-04-07T03:47:34.752Z",
            "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "6478fc1512ae749b62ebbbd5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
                "isPro": false,
                "fullname": "Alok Abhishek",
                "user": "alokabhishek",
                "type": "user"
            },
            "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
            "upvotes": 4,
            "discussionId": "67ecb513c8ae971f9ad15c02",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Bias benchmark",
                "Bias",
                "Ethics",
                "Fairness",
                "Factuality",
                "Demographic biases",
                "Cognitive biases",
                "Social biases",
                "Ethical reasoning",
                "Group fairness",
                "Factuality related misinformation risk",
                "Equitable behavior",
                "Responsible AI evaluation",
                "Critical decision making systems",
                "Scalable methodology",
                "Statistically rigorous methodology",
                "Diagnose factors driving biases",
                "Mitigation strategies",
                "Socially responsible AI models",
                "Ethically aligned AI models"
            ]
        },
        "publishedAt": "2025-03-31T12:56:52.000Z",
        "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
        "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24310.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6478fc1512ae749b62ebbbd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
            "fullname": "Alok Abhishek",
            "name": "alokabhishek",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.03597",
            "authors": [
                {
                    "_id": "67f3cea16faa90ba79c618d7",
                    "name": "Jad Abou-Chakra",
                    "hidden": false
                },
                {
                    "_id": "67f3cea16faa90ba79c618d8",
                    "name": "Lingfeng Sun",
                    "hidden": false
                },
                {
                    "_id": "67f3cea16faa90ba79c618d9",
                    "name": "Krishan Rana",
                    "hidden": false
                },
                {
                    "_id": "67f3cea16faa90ba79c618da",
                    "name": "Brandon May",
                    "hidden": false
                },
                {
                    "_id": "67f3cea16faa90ba79c618db",
                    "name": "Karl Schmeckpeper",
                    "hidden": false
                },
                {
                    "_id": "67f3cea16faa90ba79c618dc",
                    "name": "Maria Vittoria Minniti",
                    "hidden": false
                },
                {
                    "_id": "67f3cea16faa90ba79c618dd",
                    "name": "Laura Herlant",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T17:05:56.000Z",
            "submittedOnDailyAt": "2025-04-07T13:15:39.897Z",
            "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation",
            "submittedOnDailyBy": {
                "_id": "666c76ad3ae28e1bcfe02c16",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666c76ad3ae28e1bcfe02c16/GG9ImF8QF8BC__7ofbaxO.jpeg",
                "isPro": false,
                "fullname": "Brandon B. May",
                "user": "bmay",
                "type": "user"
            },
            "summary": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com.",
            "upvotes": 3,
            "discussionId": "67f3cea66faa90ba79c6198d"
        },
        "publishedAt": "2025-04-04T13:05:56.000Z",
        "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation",
        "summary": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03597.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666c76ad3ae28e1bcfe02c16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666c76ad3ae28e1bcfe02c16/GG9ImF8QF8BC__7ofbaxO.jpeg",
            "fullname": "Brandon B. May",
            "name": "bmay",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.02534",
            "authors": [
                {
                    "_id": "67f385626d3bac325a2fc231",
                    "name": "Mykola Lavreniuk",
                    "hidden": false
                },
                {
                    "_id": "67f385626d3bac325a2fc232",
                    "name": "Nataliia Kussul",
                    "hidden": false
                },
                {
                    "_id": "67f385626d3bac325a2fc233",
                    "name": "Andrii Shelestov",
                    "hidden": false
                },
                {
                    "_id": "67f385626d3bac325a2fc234",
                    "name": "Bohdan Yailymov",
                    "hidden": false
                },
                {
                    "_id": "67f385626d3bac325a2fc235",
                    "name": "Yevhenii Salii",
                    "hidden": false
                },
                {
                    "_id": "67f385626d3bac325a2fc236",
                    "name": "Volodymyr Kuzin",
                    "hidden": false
                },
                {
                    "_id": "67f385626d3bac325a2fc237",
                    "name": "Zoltan Szantoi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-03T12:37:04.000Z",
            "submittedOnDailyAt": "2025-04-07T14:03:29.728Z",
            "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything.",
            "upvotes": 3,
            "discussionId": "67f385646d3bac325a2fc26c",
            "githubRepo": "https://github.com/Lavreniuk/Delineate-Anything"
        },
        "publishedAt": "2025-04-03T08:37:04.000Z",
        "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on\n  Satellite Imagery",
        "summary": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02534.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 813
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.01328",
            "authors": [
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f1",
                    "name": "Min Shi",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f2",
                    "name": "Shihao Wang",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f3",
                    "name": "Chieh-Yun Chen",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f4",
                    "name": "Jitesh Jain",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f5",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f6",
                    "name": "Junjun Xiong",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f7",
                    "name": "Guilin Liu",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f8",
                    "user": {
                        "_id": "66c8037c737ba92ae3fe0322",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg",
                        "isPro": false,
                        "fullname": "Zhiding Yu",
                        "user": "Zhiding",
                        "type": "user"
                    },
                    "name": "Zhiding Yu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-07T12:58:56.336Z",
                    "hidden": false
                },
                {
                    "_id": "67f3cc0bbb7b1ce2a0a428f9",
                    "name": "Humphrey Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T03:24:58.000Z",
            "submittedOnDailyAt": "2025-04-07T11:29:53.172Z",
            "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "65d66b494bbd0d92b641cdbb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
                "isPro": false,
                "fullname": "Andres Marafioti",
                "user": "andito",
                "type": "user"
            },
            "summary": "Balancing temporal resolution and spatial detail under limited compute budget\nremains a key challenge for video-based multi-modal large language models\n(MLLMs). Existing methods typically compress video representations using\npredefined rules before feeding them into the LLM, resulting in irreversible\ninformation loss and often ignoring input instructions. To address this, we\npropose a novel slow-fast architecture that naturally circumvents this\ntrade-off, enabling the use of more input frames while preserving spatial\ndetails. Inspired by how humans first skim a video before focusing on relevant\nparts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual\ntokens -- a compact set of compressed video features -- are fed into the LLM\nalongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens\n-- uncompressed video features -- are cross-attended by text embeddings through\nspecially designed hybrid decoder layers, enabling instruction-aware extraction\nof relevant visual details with linear complexity. We conduct systematic\nexploration to optimize both the overall architecture and key components.\nExperiments show that our model significantly outperforms self-attention-only\nbaselines, extending the input capacity from 16 to 128 frames with just a 3%\nincrease in computation, and achieving a 16% average performance improvement\nacross five video understanding benchmarks. Our 7B model achieves\nstate-of-the-art performance among models of similar size. Furthermore, our\nslow-fast architecture is a plug-and-play design that can be integrated into\nother video MLLMs to improve efficiency and scalability.",
            "upvotes": 3,
            "discussionId": "67f3cc10bb7b1ce2a0a429d3"
        },
        "publishedAt": "2025-04-01T23:24:58.000Z",
        "title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
        "summary": "Balancing temporal resolution and spatial detail under limited compute budget\nremains a key challenge for video-based multi-modal large language models\n(MLLMs). Existing methods typically compress video representations using\npredefined rules before feeding them into the LLM, resulting in irreversible\ninformation loss and often ignoring input instructions. To address this, we\npropose a novel slow-fast architecture that naturally circumvents this\ntrade-off, enabling the use of more input frames while preserving spatial\ndetails. Inspired by how humans first skim a video before focusing on relevant\nparts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual\ntokens -- a compact set of compressed video features -- are fed into the LLM\nalongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens\n-- uncompressed video features -- are cross-attended by text embeddings through\nspecially designed hybrid decoder layers, enabling instruction-aware extraction\nof relevant visual details with linear complexity. We conduct systematic\nexploration to optimize both the overall architecture and key components.\nExperiments show that our model significantly outperforms self-attention-only\nbaselines, extending the input capacity from 16 to 128 frames with just a 3%\nincrease in computation, and achieving a 16% average performance improvement\nacross five video understanding benchmarks. Our 7B model achieves\nstate-of-the-art performance among models of similar size. Furthermore, our\nslow-fast architecture is a plug-and-play design that can be integrated into\nother video MLLMs to improve efficiency and scalability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01328.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "fullname": "Andres Marafioti",
            "name": "andito",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 131
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.00396",
            "authors": [
                {
                    "_id": "67f3c4f40db583bb63e03152",
                    "name": "Xiaole Xian",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03153",
                    "name": "Zhichao Liao",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03154",
                    "name": "Qingyu Li",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03155",
                    "name": "Wenyu Qin",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03156",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03157",
                    "name": "Weicheng Xie",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03158",
                    "name": "Long Zeng",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e03159",
                    "name": "Linlin Shen",
                    "hidden": false
                },
                {
                    "_id": "67f3c4f40db583bb63e0315a",
                    "name": "Pingfa Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-01T03:37:30.000Z",
            "submittedOnDailyAt": "2025-04-07T10:59:24.471Z",
            "title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic\n  Pollution-Free Fine-tuning",
            "submittedOnDailyBy": {
                "_id": "66eb9d5d7f17088991ecbae2",
                "avatarUrl": "/avatars/10c77724829d4a1c65cafb4814614090.svg",
                "isPro": false,
                "fullname": "Zhichao Liao",
                "user": "ChaosLiao",
                "type": "user"
            },
            "summary": "Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait\ndataset is the mainstream method for text-driven customization of portrait\nattributes. Due to Semantic Pollution during fine-tuning, existing methods\nstruggle to maintain the original model's behavior and achieve incremental\nlearning while customizing target attributes. To address this issue, we propose\nSPF-Portrait, a pioneering work to purely understand customized semantics while\neliminating semantic pollution in text-driven portrait customization. In our\nSPF-Portrait, we propose a dual-path pipeline that introduces the original\nmodel as a reference for the conventional fine-tuning path. Through contrastive\nlearning, we ensure adaptation to target attributes and purposefully align\nother unrelated attributes with the original portrait. We introduce a novel\nSemantic-Aware Fine Control Map, which represents the precise response regions\nof the target semantics, to spatially guide the alignment process between the\ncontrastive paths. This alignment process not only effectively preserves the\nperformance of the original model but also avoids over-alignment. Furthermore,\nwe propose a novel response enhancement mechanism to reinforce the performance\nof target attributes, while mitigating representation discrepancy inherent in\ndirect cross-modal supervision. Extensive experiments demonstrate that\nSPF-Portrait achieves state-of-the-art performance. Project webpage:\nhttps://spf-portrait.github.io/SPF-Portrait/",
            "upvotes": 1,
            "discussionId": "67f3c4fa0db583bb63e0326e",
            "projectPage": "https://spf-portrait.github.io/SPF-Portrait/",
            "githubRepo": "https://github.com/SPF-Portrait/SPF-Portrait/"
        },
        "publishedAt": "2025-03-31T23:37:30.000Z",
        "title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic\n  Pollution-Free Fine-tuning",
        "summary": "Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait\ndataset is the mainstream method for text-driven customization of portrait\nattributes. Due to Semantic Pollution during fine-tuning, existing methods\nstruggle to maintain the original model's behavior and achieve incremental\nlearning while customizing target attributes. To address this issue, we propose\nSPF-Portrait, a pioneering work to purely understand customized semantics while\neliminating semantic pollution in text-driven portrait customization. In our\nSPF-Portrait, we propose a dual-path pipeline that introduces the original\nmodel as a reference for the conventional fine-tuning path. Through contrastive\nlearning, we ensure adaptation to target attributes and purposefully align\nother unrelated attributes with the original portrait. We introduce a novel\nSemantic-Aware Fine Control Map, which represents the precise response regions\nof the target semantics, to spatially guide the alignment process between the\ncontrastive paths. This alignment process not only effectively preserves the\nperformance of the original model but also avoids over-alignment. Furthermore,\nwe propose a novel response enhancement mechanism to reinforce the performance\nof target attributes, while mitigating representation discrepancy inherent in\ndirect cross-modal supervision. Extensive experiments demonstrate that\nSPF-Portrait achieves state-of-the-art performance. Project webpage:\nhttps://spf-portrait.github.io/SPF-Portrait/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00396.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66eb9d5d7f17088991ecbae2",
            "avatarUrl": "/avatars/10c77724829d4a1c65cafb4814614090.svg",
            "fullname": "Zhichao Liao",
            "name": "ChaosLiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]