[
    {
        "paper": {
            "id": "2504.05741",
            "authors": [
                {
                    "_id": "67f726dc0b5aa5777fd3a431",
                    "user": {
                        "_id": "66615c855fd9d736e670e0a9",
                        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                        "isPro": false,
                        "fullname": "wangshuai",
                        "user": "wangsssssss",
                        "type": "user"
                    },
                    "name": "Shuai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:44:49.192Z",
                    "hidden": false
                },
                {
                    "_id": "67f726dc0b5aa5777fd3a432",
                    "name": "Zhi Tian",
                    "hidden": false
                },
                {
                    "_id": "67f726dc0b5aa5777fd3a433",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "67f726dc0b5aa5777fd3a434",
                    "user": {
                        "_id": "62c77f4352d8ae531f5511f9",
                        "avatarUrl": "/avatars/50198ccb02ccd286975a4613fbabee28.svg",
                        "isPro": false,
                        "fullname": "Limin Wang",
                        "user": "lmwang",
                        "type": "user"
                    },
                    "name": "Limin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T07:58:42.903Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-08T07:17:45.000Z",
            "submittedOnDailyAt": "2025-04-10T00:40:02.945Z",
            "title": "DDT: Decoupled Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "66615c855fd9d736e670e0a9",
                "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                "isPro": false,
                "fullname": "wangshuai",
                "user": "wangsssssss",
                "type": "user"
            },
            "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\color{ddtD}ecoupled \\color{ddtD}iffusion\n\\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly 4times faster training convergence compared to previous\ndiffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
            "upvotes": 49,
            "discussionId": "67f726dd0b5aa5777fd3a463",
            "githubRepo": "https://github.com/MCG-NJU/DDT"
        },
        "publishedAt": "2025-04-08T03:17:45.000Z",
        "title": "DDT: Decoupled Diffusion Transformer",
        "summary": "Diffusion transformers have demonstrated remarkable generation quality,\nalbeit requiring longer training iterations and numerous inference steps. In\neach denoising step, diffusion transformers encode the noisy inputs to extract\nthe lower-frequency semantic component and then decode the higher frequency\nwith identical modules. This scheme creates an inherent optimization dilemma:\nencoding low-frequency semantics necessitates reducing high-frequency\ncomponents, creating tension between semantic encoding and high-frequency\ndecoding. To resolve this challenge, we propose a new\n\\color{ddtD}ecoupled \\color{ddtD}iffusion\n\\color{ddtT}ransformer~(\\color{ddtDDT}), with a decoupled\ndesign of a dedicated condition encoder for semantic extraction alongside a\nspecialized velocity decoder. Our experiments reveal that a more substantial\nencoder yields performance improvements as model size increases. For ImageNet\n256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of\n{1.31 FID}~(nearly 4times faster training convergence compared to previous\ndiffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a\nnew state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our\ndecoupled architecture enhances inference speed by enabling the sharing\nself-condition between adjacent denoising steps. To minimize performance\ndegradation, we propose a novel statistical dynamic programming approach to\nidentify optimal sharing strategies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05741.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "fullname": "wangshuai",
            "name": "wangsssssss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07096",
            "authors": [
                {
                    "_id": "67f72bb1f9d51b79dca06d0a",
                    "user": {
                        "_id": "635f46d1928a42bc95cfcf7c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
                        "isPro": false,
                        "fullname": "Jiacheng Liu",
                        "user": "liujch1998",
                        "type": "user"
                    },
                    "name": "Jiacheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:39:44.913Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d0b",
                    "user": {
                        "_id": "6675a65557208377a15f745b",
                        "avatarUrl": "/avatars/361dc6d0919f4d4545ff4fdd005332b5.svg",
                        "isPro": false,
                        "fullname": "Taylor Blanton",
                        "user": "taylorb",
                        "type": "user"
                    },
                    "name": "Taylor Blanton",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:02:05.681Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d0c",
                    "user": {
                        "_id": "623ca115a795593324c4353f",
                        "avatarUrl": "/avatars/bf11fe728df2786d52ed4d2de12b48d3.svg",
                        "isPro": false,
                        "fullname": "Yanai Elazar",
                        "user": "yanaiela",
                        "type": "user"
                    },
                    "name": "Yanai Elazar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:02:12.016Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d0d",
                    "user": {
                        "_id": "63a76d0de27a6dbd485fe863",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
                        "isPro": false,
                        "fullname": "Sewon Min",
                        "user": "sewon",
                        "type": "user"
                    },
                    "name": "Sewon Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:02:17.909Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d0e",
                    "user": {
                        "_id": "6697093a37d2483826562c24",
                        "avatarUrl": "/avatars/0e526b4be6db07e2485f7ef862080339.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Yensung",
                        "type": "user"
                    },
                    "name": "YenSung Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:02:26.950Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d0f",
                    "name": "Arnavi Chheda-Kothary",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d10",
                    "name": "Huy Tran",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d11",
                    "name": "Byron Bischoff",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d12",
                    "name": "Eric Marsh",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d13",
                    "name": "Michael Schmitz",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d14",
                    "name": "Cassidy Trier",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d15",
                    "user": {
                        "_id": "65b1520bf7638a13a641a620",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b1520bf7638a13a641a620/KTauZL0kXlmYnbkI2lFBG.png",
                        "isPro": false,
                        "fullname": "Aaron Sarnat",
                        "user": "aaronsarnat",
                        "type": "user"
                    },
                    "name": "Aaron Sarnat",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:03:34.212Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d16",
                    "name": "Jenna James",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d17",
                    "name": "Jon Borchardt",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d18",
                    "user": {
                        "_id": "65316953791d5a2611426c20",
                        "avatarUrl": "/avatars/e632a9a30a57f62d59f9fe42eba8fd7d.svg",
                        "isPro": false,
                        "fullname": "bailey kuehl",
                        "user": "baileyk",
                        "type": "user"
                    },
                    "name": "Bailey Kuehl",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:03:47.506Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d19",
                    "name": "Evie Cheng",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d1a",
                    "user": {
                        "_id": "66213c05e288b64070184cac",
                        "avatarUrl": "/avatars/ded6a173e60722200b372b8b046fc359.svg",
                        "isPro": false,
                        "fullname": "Karen Farley",
                        "user": "AI2Karen",
                        "type": "user"
                    },
                    "name": "Karen Farley",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:03:58.149Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d1b",
                    "name": "Sruthi Sreeram",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d1c",
                    "user": {
                        "_id": "65de20ad4e73a7dea7fb4f08",
                        "avatarUrl": "/avatars/f3b0ad6cc9417e8ea3f0607fa62824d1.svg",
                        "isPro": false,
                        "fullname": "Taira Anderson",
                        "user": "tairaa",
                        "type": "user"
                    },
                    "name": "Taira Anderson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:04:09.193Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d1d",
                    "name": "David Albright",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d1e",
                    "user": {
                        "_id": "6024546dc1f3c79f98e4b384",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1612993792778-6024546dc1f3c79f98e4b384.jpeg",
                        "isPro": false,
                        "fullname": "Carissa Schoenick",
                        "user": "CarissaS",
                        "type": "user"
                    },
                    "name": "Carissa Schoenick",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:04:25.492Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d1f",
                    "user": {
                        "_id": "5f04d8c45d08220171a0ad32",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f04d8c45d08220171a0ad32/uXEta6nqBabrUlAOXnS5g.jpeg",
                        "isPro": false,
                        "fullname": "Luca Soldaini",
                        "user": "soldni",
                        "type": "user"
                    },
                    "name": "Luca Soldaini",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:04:31.958Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d20",
                    "user": {
                        "_id": "60369745413a78f892e7339c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636671879171-60369745413a78f892e7339c.png",
                        "isPro": false,
                        "fullname": "Dirk Groeneveld",
                        "user": "dirkgr",
                        "type": "user"
                    },
                    "name": "Dirk Groeneveld",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:04:40.029Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d21",
                    "name": "Rock Yuren Pang",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d22",
                    "user": {
                        "_id": "641b4263abfce26bcf7b27de",
                        "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
                        "isPro": false,
                        "fullname": "Pang Wei Koh",
                        "user": "pangwei",
                        "type": "user"
                    },
                    "name": "Pang Wei Koh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:04:53.298Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d23",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d24",
                    "user": {
                        "_id": "65b301f04c9e50e74a893954",
                        "avatarUrl": "/avatars/f52366959f9e7613576603c0272ff2c5.svg",
                        "isPro": false,
                        "fullname": "Sophie Lebrecht",
                        "user": "Lebrechts",
                        "type": "user"
                    },
                    "name": "Sophie Lebrecht",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:05:06.279Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d25",
                    "user": {
                        "_id": "64d42729f63b01b7f676b176",
                        "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                        "isPro": false,
                        "fullname": "Yejin Choi",
                        "user": "yejinchoinka",
                        "type": "user"
                    },
                    "name": "Yejin Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:05:13.983Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d26",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d27",
                    "user": {
                        "_id": "6660d4c1818c5c5ca0f31266",
                        "avatarUrl": "/avatars/1d2972894cb3b9df1900fdb162d9c364.svg",
                        "isPro": false,
                        "fullname": "alifarhadi ",
                        "user": "alifarhadi051",
                        "type": "user"
                    },
                    "name": "Ali Farhadi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:05:24.948Z",
                    "hidden": false
                },
                {
                    "_id": "67f72bb1f9d51b79dca06d28",
                    "user": {
                        "_id": "6283f38567d336d3e5d5280e",
                        "avatarUrl": "/avatars/d0a54aaec74a90b050e671c191b87a80.svg",
                        "isPro": false,
                        "fullname": "Jesse Dodge",
                        "user": "JesseDodge",
                        "type": "user"
                    },
                    "name": "Jesse Dodge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:05:31.942Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T17:59:35.000Z",
            "submittedOnDailyAt": "2025-04-10T00:54:36.448Z",
            "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
            "submittedOnDailyBy": {
                "_id": "635f46d1928a42bc95cfcf7c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
                "isPro": false,
                "fullname": "Jiacheng Liu",
                "user": "liujch1998",
                "type": "user"
            },
            "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.",
            "upvotes": 42,
            "discussionId": "67f72bb3f9d51b79dca06d8c"
        },
        "publishedAt": "2025-04-09T13:59:35.000Z",
        "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training\n  Tokens",
        "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07096.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635f46d1928a42bc95cfcf7c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
            "fullname": "Jiacheng Liu",
            "name": "liujch1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07046",
            "authors": [
                {
                    "_id": "67f74727353d129fc7c4be7a",
                    "name": "Jifang Wang",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be7b",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be7c",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be7d",
                    "user": {
                        "_id": "639c379cdb7c5f35004066cb",
                        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                        "isPro": false,
                        "fullname": "Zhenran Xu",
                        "user": "imryanxu",
                        "type": "user"
                    },
                    "name": "Zhenran Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T09:58:11.729Z",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be7e",
                    "name": "Yiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be7f",
                    "name": "Yaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be80",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be81",
                    "name": "Kaifu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be82",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "67f74727353d129fc7c4be83",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/xXlL1RROzluluflNDOIRv.png"
            ],
            "publishedAt": "2025-04-09T17:04:14.000Z",
            "submittedOnDailyAt": "2025-04-10T07:56:53.441Z",
            "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
            "submittedOnDailyBy": {
                "_id": "639c379cdb7c5f35004066cb",
                "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                "isPro": false,
                "fullname": "Zhenran Xu",
                "user": "imryanxu",
                "type": "user"
            },
            "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.",
            "upvotes": 23,
            "discussionId": "67f7472b353d129fc7c4bf4b",
            "projectPage": "https://x.com/wangly0229/status/1910317936042295737?t=PR7EH5eB_NgTFgSUKZbSvA&s=19",
            "githubRepo": "https://github.com/HITsz-TMG/Agentic-CIGEval"
        },
        "publishedAt": "2025-04-09T13:04:14.000Z",
        "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
        "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/639c379cdb7c5f35004066cb/xXlL1RROzluluflNDOIRv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "fullname": "Zhenran Xu",
            "name": "imryanxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.06514",
            "authors": [
                {
                    "_id": "67f72e933eacf8888816f3b0",
                    "user": {
                        "_id": "64a8121e35fab7cd04c30ed0",
                        "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
                        "isPro": false,
                        "fullname": "Chenrui Fan",
                        "user": "Fcr09",
                        "type": "user"
                    },
                    "name": "Chenrui Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:05:59.999Z",
                    "hidden": false
                },
                {
                    "_id": "67f72e933eacf8888816f3b1",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67f72e933eacf8888816f3b2",
                    "user": {
                        "_id": "65a52766215aabac489e3468",
                        "avatarUrl": "/avatars/fe05e22cd7e12e961296426434e17c76.svg",
                        "isPro": false,
                        "fullname": "Lichao Sun",
                        "user": "sunlichao137",
                        "type": "user"
                    },
                    "name": "Lichao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:06:12.092Z",
                    "hidden": false
                },
                {
                    "_id": "67f72e933eacf8888816f3b3",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:39:37.906Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
            ],
            "publishedAt": "2025-04-09T01:25:27.000Z",
            "submittedOnDailyAt": "2025-04-10T01:07:13.718Z",
            "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
            "upvotes": 22,
            "discussionId": "67f72e943eacf8888816f3fa",
            "githubRepo": "https://github.com/tianyi-lab/MiP-Overthinking"
        },
        "publishedAt": "2025-04-08T21:25:27.000Z",
        "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
        "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/FrHRMBKuB2v57LZWVJPxi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06514.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07083",
            "authors": [
                {
                    "_id": "67f72c452eec6ce5c8b9e8e6",
                    "user": {
                        "_id": "64de20c5808492ba6e65d124",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
                        "isPro": false,
                        "fullname": "Zhang Mengchen",
                        "user": "Dubhe-zmc",
                        "type": "user"
                    },
                    "name": "Mengchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:39:42.813Z",
                    "hidden": false
                },
                {
                    "_id": "67f72c452eec6ce5c8b9e8e7",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "67f72c452eec6ce5c8b9e8e8",
                    "user": {
                        "_id": "65367c40061949598892dbdc",
                        "avatarUrl": "/avatars/4baf27263841471cbd5f629a8b99424d.svg",
                        "isPro": false,
                        "fullname": "Jing Tan",
                        "user": "jingtan",
                        "type": "user"
                    },
                    "name": "Jing Tan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:01:54.122Z",
                    "hidden": false
                },
                {
                    "_id": "67f72c452eec6ce5c8b9e8e9",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:01:11.166Z",
                    "hidden": false
                },
                {
                    "_id": "67f72c452eec6ce5c8b9e8ea",
                    "user": {
                        "_id": "6694e583ac96ca2c17131505",
                        "avatarUrl": "/avatars/6e7a31f257e36cf301da6f879dc0a122.svg",
                        "isPro": false,
                        "fullname": "Gordon Wetzstein",
                        "user": "wetzste1",
                        "type": "user"
                    },
                    "name": "Gordon Wetzstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:01:03.935Z",
                    "hidden": false
                },
                {
                    "_id": "67f72c452eec6ce5c8b9e8eb",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:00:57.092Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
            ],
            "publishedAt": "2025-04-09T17:56:01.000Z",
            "submittedOnDailyAt": "2025-04-10T01:13:43.884Z",
            "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
            "submittedOnDailyBy": {
                "_id": "64de20c5808492ba6e65d124",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
                "isPro": false,
                "fullname": "Zhang Mengchen",
                "user": "Dubhe-zmc",
                "type": "user"
            },
            "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.",
            "upvotes": 18,
            "discussionId": "67f72c472eec6ce5c8b9e97b",
            "projectPage": "https://kszpxxzmc.github.io/GenDoP/",
            "githubRepo": "https://github.com/3DTopia/GenDoP"
        },
        "publishedAt": "2025-04-09T13:56:01.000Z",
        "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of\n  Photography",
        "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64de20c5808492ba6e65d124/b1N08r8EbruYc8Yapg4J9.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07083.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64de20c5808492ba6e65d124",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de20c5808492ba6e65d124/58IX_TI5vJw73qS1knw56.jpeg",
            "fullname": "Zhang Mengchen",
            "name": "Dubhe-zmc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.04842",
            "authors": [
                {
                    "_id": "67f72ca8353d129fc7bdd504",
                    "name": "Mengchao Wang",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd505",
                    "user": {
                        "_id": "653b195c5f1703225b2fd571",
                        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
                        "isPro": false,
                        "fullname": "wangqiang",
                        "user": "wangqiang9",
                        "type": "user"
                    },
                    "name": "Qiang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:39:40.647Z",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd506",
                    "user": {
                        "_id": "63048ea19aef62c4013c77aa",
                        "avatarUrl": "/avatars/b2b2243ccc63cfb5a3289bc2eb1d6293.svg",
                        "isPro": false,
                        "fullname": "fanjiang",
                        "user": "fanjiang",
                        "type": "user"
                    },
                    "name": "Fan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:50:07.747Z",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd507",
                    "name": "Yaqi Fan",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd508",
                    "name": "Yunpeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd509",
                    "name": "Yonggang Qi",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd50a",
                    "name": "Kun Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f72ca8353d129fc7bdd50b",
                    "name": "Mu Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T08:56:01.000Z",
            "submittedOnDailyAt": "2025-04-10T00:58:44.876Z",
            "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
            "submittedOnDailyBy": {
                "_id": "653b195c5f1703225b2fd571",
                "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
                "isPro": false,
                "fullname": "wangqiang",
                "user": "wangqiang9",
                "type": "user"
            },
            "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
            "upvotes": 17,
            "discussionId": "67f72cac353d129fc7bdd60f",
            "projectPage": "https://fantasy-amap.github.io/fantasy-talking/",
            "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-talking"
        },
        "publishedAt": "2025-04-07T04:56:01.000Z",
        "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
        "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04842.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "fullname": "wangqiang",
            "name": "wangqiang9",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07081",
            "authors": [
                {
                    "_id": "67f7823da630bcdabbd8b3eb",
                    "name": "Gabriel Grand",
                    "hidden": false
                },
                {
                    "_id": "67f7823da630bcdabbd8b3ec",
                    "name": "Joshua B. Tenenbaum",
                    "hidden": false
                },
                {
                    "_id": "67f7823da630bcdabbd8b3ed",
                    "name": "Vikash K. Mansinghka",
                    "hidden": false
                },
                {
                    "_id": "67f7823da630bcdabbd8b3ee",
                    "user": {
                        "_id": "673cc08370644bb836283fec",
                        "avatarUrl": "/avatars/b8c7f1a10ddf76dcd06398c59f553b61.svg",
                        "isPro": false,
                        "fullname": "Alexander Lew",
                        "user": "alexanderlew",
                        "type": "user"
                    },
                    "name": "Alexander K. Lew",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-10T08:33:02.433Z",
                    "hidden": false
                },
                {
                    "_id": "67f7823da630bcdabbd8b3ef",
                    "name": "Jacob Andreas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T17:54:22.000Z",
            "submittedOnDailyAt": "2025-04-10T07:03:34.220Z",
            "title": "Self-Steering Language Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
            "upvotes": 11,
            "discussionId": "67f7823ea630bcdabbd8b42e"
        },
        "publishedAt": "2025-04-09T13:54:22.000Z",
        "title": "Self-Steering Language Models",
        "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07081.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6627
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.07086",
            "authors": [
                {
                    "_id": "67f75609b2d783993db63aba",
                    "user": {
                        "_id": "64ff3944f0d65cca9b867ed2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ff3944f0d65cca9b867ed2/jWnHkF4AUzh51MkC0UT6b.png",
                        "isPro": false,
                        "fullname": "Andreas Hochlehnert",
                        "user": "libeanim",
                        "type": "user"
                    },
                    "name": "Andreas Hochlehnert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:07:58.732Z",
                    "hidden": false
                },
                {
                    "_id": "67f75609b2d783993db63abb",
                    "user": {
                        "_id": "6556760b35f26c82c09a010f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6556760b35f26c82c09a010f/hNbwvXRBsKo6pqrHbXNzz.jpeg",
                        "isPro": false,
                        "fullname": "Hardik Bhatnagar",
                        "user": "hrdkbhatnagar",
                        "type": "user"
                    },
                    "name": "Hardik Bhatnagar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:08:04.922Z",
                    "hidden": false
                },
                {
                    "_id": "67f75609b2d783993db63abc",
                    "user": {
                        "_id": "6304da46ce6b12280b1bd575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6304da46ce6b12280b1bd575/V96ocKW4HOoysAGxuAH1X.jpeg",
                        "isPro": false,
                        "fullname": "Vishaal Udandarao",
                        "user": "vishaal27",
                        "type": "user"
                    },
                    "name": "Vishaal Udandarao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:08:10.717Z",
                    "hidden": false
                },
                {
                    "_id": "67f75609b2d783993db63abd",
                    "user": {
                        "_id": "62f3efefd6ba2ee26651f44a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660153837083-noauth.png",
                        "isPro": false,
                        "fullname": "Samuel Albanie",
                        "user": "albanie",
                        "type": "user"
                    },
                    "name": "Samuel Albanie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:08:16.809Z",
                    "hidden": false
                },
                {
                    "_id": "67f75609b2d783993db63abe",
                    "user": {
                        "_id": "6464a0d41683d3c81f51924a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
                        "isPro": false,
                        "fullname": "Ameya Prabhu",
                        "user": "AmeyaPrabhu",
                        "type": "user"
                    },
                    "name": "Ameya Prabhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:39:24.192Z",
                    "hidden": false
                },
                {
                    "_id": "67f75609b2d783993db63abf",
                    "name": "Matthias Bethge",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T17:58:17.000Z",
            "submittedOnDailyAt": "2025-04-10T03:54:51.677Z",
            "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
            "submittedOnDailyBy": {
                "_id": "6464a0d41683d3c81f51924a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
                "isPro": false,
                "fullname": "Ameya Prabhu",
                "user": "AmeyaPrabhu",
                "type": "user"
            },
            "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.",
            "upvotes": 10,
            "discussionId": "67f7560cb2d783993db63b6b",
            "projectPage": "https://bethgelab.github.io/sober-reasoning/",
            "githubRepo": "https://github.com/bethgelab/sober-reasoning"
        },
        "publishedAt": "2025-04-09T13:58:17.000Z",
        "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\n  to Reproducibility",
        "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07086.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "fullname": "Ameya Prabhu",
            "name": "AmeyaPrabhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07089",
            "authors": [
                {
                    "_id": "67f7676d0ab78ef7b16a820f",
                    "user": {
                        "_id": "6614fb3d5aed02b298a4b469",
                        "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
                        "isPro": false,
                        "fullname": "yiting lu",
                        "user": "yeeeeeyy",
                        "type": "user"
                    },
                    "name": "Yiting Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T07:58:03.992Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8210",
                    "user": {
                        "_id": "64a3d1ddb3239f3e3892b24b",
                        "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
                        "isPro": false,
                        "fullname": "Jiakang Yuan",
                        "user": "JiakangYuan",
                        "type": "user"
                    },
                    "name": "Jiakang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:08:31.119Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8211",
                    "user": {
                        "_id": "6285a9133ab6642179158944",
                        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
                        "isPro": false,
                        "fullname": "Zhen Li",
                        "user": "Paper99",
                        "type": "user"
                    },
                    "name": "Zhen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T13:23:24.319Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8212",
                    "name": "Shitian Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8213",
                    "user": {
                        "_id": "66bb136002fd8eb58bc84ffb",
                        "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
                        "isPro": false,
                        "fullname": "qinqi",
                        "user": "Dakerqi",
                        "type": "user"
                    },
                    "name": "Qi Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T08:06:06.570Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8214",
                    "name": "Xinyue Li",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8215",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8216",
                    "user": {
                        "_id": "64a7c43ae940d769194055df",
                        "avatarUrl": "/avatars/441ccadd62e039fb8cb112f138ed917d.svg",
                        "isPro": false,
                        "fullname": "Licheng Wen",
                        "user": "Wayne-lc",
                        "type": "user"
                    },
                    "name": "Licheng Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:09:07.684Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8217",
                    "user": {
                        "_id": "646f1bef075e11ca78da3bb7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
                        "isPro": false,
                        "fullname": "Dongyang Liu (Chris Liu)",
                        "user": "Cxxs",
                        "type": "user"
                    },
                    "name": "Dongyang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:09:21.398Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8218",
                    "name": "Yuewen Cao",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8219",
                    "user": {
                        "_id": "65b88b92e0bde92c176a888a",
                        "avatarUrl": "/avatars/fc1cb54328ca93860e97fc73a3c1eb2f.svg",
                        "isPro": false,
                        "fullname": "Xiangchao Yan",
                        "user": "yxc97",
                        "type": "user"
                    },
                    "name": "Xiangchao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:09:35.388Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a821a",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a821b",
                    "user": {
                        "_id": "643df87f7cd64d872cb9fabd",
                        "avatarUrl": "/avatars/c53bfabcee08de448dde973915e8b31d.svg",
                        "isPro": false,
                        "fullname": "Botian Shi",
                        "user": "friskit",
                        "type": "user"
                    },
                    "name": "Botian Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:09:41.754Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a821c",
                    "name": "Tao Chen",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a821d",
                    "user": {
                        "_id": "66d963e52e82d53d3b81031b",
                        "avatarUrl": "/avatars/302dbffc033ff47813a2435a2cec02f1.svg",
                        "isPro": false,
                        "fullname": "Zhibo Chen",
                        "user": "winhelp",
                        "type": "user"
                    },
                    "name": "Zhibo Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:10:04.682Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a821e",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a821f",
                    "user": {
                        "_id": "643dfd235aafbdca3a5792c0",
                        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
                        "isPro": false,
                        "fullname": "Bo Zhang",
                        "user": "BoZhang",
                        "type": "user"
                    },
                    "name": "Bo Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T09:57:56.032Z",
                    "hidden": false
                },
                {
                    "_id": "67f7676d0ab78ef7b16a8220",
                    "name": "Peng Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T17:58:58.000Z",
            "submittedOnDailyAt": "2025-04-10T05:22:13.319Z",
            "title": "OmniCaptioner: One Captioner to Rule Them All",
            "submittedOnDailyBy": {
                "_id": "6614fb3d5aed02b298a4b469",
                "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
                "isPro": false,
                "fullname": "yiting lu",
                "user": "yeeeeeyy",
                "type": "user"
            },
            "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
            "upvotes": 9,
            "discussionId": "67f767700ab78ef7b16a82d6"
        },
        "publishedAt": "2025-04-09T13:58:58.000Z",
        "title": "OmniCaptioner: One Captioner to Rule Them All",
        "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07089.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6614fb3d5aed02b298a4b469",
            "avatarUrl": "/avatars/d0ddb4f989ad1a3f24128cc843347bde.svg",
            "fullname": "yiting lu",
            "name": "yeeeeeyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.05541",
            "authors": [
                {
                    "_id": "67f7cebba5c42b93f487a2b3",
                    "user": {
                        "_id": "6344c87f0f69ad8aa61dfcf6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/m4QiQ_4c6GfOIZZFICy7T.jpeg",
                        "isPro": false,
                        "fullname": "Yolo Y. Tang",
                        "user": "yunlong10",
                        "type": "user"
                    },
                    "name": "Yunlong Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T15:45:42.596Z",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2b4",
                    "name": "Jing Bi",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2b5",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2b6",
                    "name": "Susan Liang",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2b7",
                    "name": "Daiki Shimada",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2b8",
                    "name": "Hang Hua",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2b9",
                    "name": "Yunzhong Xiao",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2ba",
                    "name": "Yizhi Song",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2bb",
                    "name": "Pinxin Liu",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2bc",
                    "name": "Mingqian Feng",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2bd",
                    "name": "Junjia Guo",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2be",
                    "name": "Zhuo Liu",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2bf",
                    "name": "Luchuan Song",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2c0",
                    "name": "Ali Vosoughi",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2c1",
                    "name": "Jinxi He",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2c2",
                    "name": "Liu He",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2c3",
                    "name": "Zeliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2c4",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "67f7cebba5c42b93f487a2c5",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T22:35:36.000Z",
            "submittedOnDailyAt": "2025-04-10T12:29:31.985Z",
            "title": "Caption Anything in Video: Fine-grained Object-centric Captioning via\n  Spatiotemporal Multimodal Prompting",
            "submittedOnDailyBy": {
                "_id": "6344c87f0f69ad8aa61dfcf6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/m4QiQ_4c6GfOIZZFICy7T.jpeg",
                "isPro": false,
                "fullname": "Yolo Y. Tang",
                "user": "yunlong10",
                "type": "user"
            },
            "summary": "We present CAT-V (Caption AnyThing in Video), a training-free framework for\nfine-grained object-centric video captioning that enables detailed descriptions\nof user-selected objects through time. CAT-V integrates three key components: a\nSegmenter based on SAMURAI for precise object segmentation across frames, a\nTemporal Analyzer powered by TRACE-Uni for accurate event boundary detection\nand temporal analysis, and a Captioner using InternVL-2.5 for generating\ndetailed object-centric descriptions. Through spatiotemporal visual prompts and\nchain-of-thought reasoning, our framework generates detailed, temporally-aware\ndescriptions of objects' attributes, actions, statuses, interactions, and\nenvironmental contexts without requiring additional training data. CAT-V\nsupports flexible user interactions through various visual prompts (points,\nbounding boxes, and irregular regions) and maintains temporal sensitivity by\ntracking object states and interactions across different time segments. Our\napproach addresses limitations of existing video captioning methods, which\neither produce overly abstract descriptions or lack object-level precision,\nenabling fine-grained, object-specific descriptions while maintaining temporal\ncoherence and spatial accuracy. The GitHub repository for this project is\navailable at https://github.com/yunlong10/CAT-V",
            "upvotes": 9,
            "discussionId": "67f7cebea5c42b93f487a352",
            "githubRepo": "https://github.com/yunlong10/CAT-V"
        },
        "publishedAt": "2025-04-07T18:35:36.000Z",
        "title": "Caption Anything in Video: Fine-grained Object-centric Captioning via\n  Spatiotemporal Multimodal Prompting",
        "summary": "We present CAT-V (Caption AnyThing in Video), a training-free framework for\nfine-grained object-centric video captioning that enables detailed descriptions\nof user-selected objects through time. CAT-V integrates three key components: a\nSegmenter based on SAMURAI for precise object segmentation across frames, a\nTemporal Analyzer powered by TRACE-Uni for accurate event boundary detection\nand temporal analysis, and a Captioner using InternVL-2.5 for generating\ndetailed object-centric descriptions. Through spatiotemporal visual prompts and\nchain-of-thought reasoning, our framework generates detailed, temporally-aware\ndescriptions of objects' attributes, actions, statuses, interactions, and\nenvironmental contexts without requiring additional training data. CAT-V\nsupports flexible user interactions through various visual prompts (points,\nbounding boxes, and irregular regions) and maintains temporal sensitivity by\ntracking object states and interactions across different time segments. Our\napproach addresses limitations of existing video captioning methods, which\neither produce overly abstract descriptions or lack object-level precision,\nenabling fine-grained, object-specific descriptions while maintaining temporal\ncoherence and spatial accuracy. The GitHub repository for this project is\navailable at https://github.com/yunlong10/CAT-V",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05541.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6344c87f0f69ad8aa61dfcf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/m4QiQ_4c6GfOIZZFICy7T.jpeg",
            "fullname": "Yolo Y. Tang",
            "name": "yunlong10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.04010",
            "authors": [
                {
                    "_id": "67f766cb1879ad2f13bee3d1",
                    "user": {
                        "_id": "63c9f93cdfac8071d01ed56f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674180895772-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Maksim Siniukov",
                        "user": "havent-invented",
                        "type": "user"
                    },
                    "name": "Maksim Siniukov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:10:42.914Z",
                    "hidden": false
                },
                {
                    "_id": "67f766cb1879ad2f13bee3d2",
                    "user": {
                        "_id": "64a5d8219f3b568c202b3137",
                        "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
                        "isPro": false,
                        "fullname": "Di Chang",
                        "user": "Boese0601",
                        "type": "user"
                    },
                    "name": "Di Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:10:49.652Z",
                    "hidden": false
                },
                {
                    "_id": "67f766cb1879ad2f13bee3d3",
                    "user": {
                        "_id": "632b6c08ca316c73cd3e4d8c",
                        "avatarUrl": "/avatars/a02aa14823dd729df0267a9b55779edd.svg",
                        "isPro": false,
                        "fullname": "Minh Tran",
                        "user": "minhtran",
                        "type": "user"
                    },
                    "name": "Minh Tran",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:10:55.537Z",
                    "hidden": false
                },
                {
                    "_id": "67f766cb1879ad2f13bee3d4",
                    "user": {
                        "_id": "6605cfc7b85b7b4ea506a33d",
                        "avatarUrl": "/avatars/92abda656abf2298c9d28b9b2e3643a3.svg",
                        "isPro": false,
                        "fullname": "Hongkun Gong",
                        "user": "hongkung",
                        "type": "user"
                    },
                    "name": "Hongkun Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:11:01.857Z",
                    "hidden": false
                },
                {
                    "_id": "67f766cb1879ad2f13bee3d5",
                    "user": {
                        "_id": "6541185dbd60d2bd193f7999",
                        "avatarUrl": "/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg",
                        "isPro": false,
                        "fullname": "Ashutosh Chaubey",
                        "user": "chaubeyG",
                        "type": "user"
                    },
                    "name": "Ashutosh Chaubey",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:11:08.391Z",
                    "hidden": false
                },
                {
                    "_id": "67f766cb1879ad2f13bee3d6",
                    "user": {
                        "_id": "65fcb99d383d3f256c3a92d2",
                        "avatarUrl": "/avatars/b85d32f4d7a19816b8d499e05b173ad1.svg",
                        "isPro": false,
                        "fullname": "Mohammad Soleymani",
                        "user": "msoleymani",
                        "type": "user"
                    },
                    "name": "Mohammad Soleymani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:11:15.030Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-05T01:19:46.000Z",
            "submittedOnDailyAt": "2025-04-10T05:06:42.197Z",
            "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
            "submittedOnDailyBy": {
                "_id": "64a5d8219f3b568c202b3137",
                "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
                "isPro": false,
                "fullname": "Di Chang",
                "user": "Boese0601",
                "type": "user"
            },
            "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.",
            "upvotes": 7,
            "discussionId": "67f766ce1879ad2f13bee47a",
            "projectPage": "https://cv.maxi.su/DiTaiListener/"
        },
        "publishedAt": "2025-04-04T21:19:46.000Z",
        "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with\n  Diffusion",
        "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04010.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a5d8219f3b568c202b3137",
            "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
            "fullname": "Di Chang",
            "name": "Boese0601",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.06958",
            "authors": [
                {
                    "_id": "67f783fc2eec6ce5c8d18be3",
                    "user": {
                        "_id": "672f8a28c53c174f39b08ac1",
                        "avatarUrl": "/avatars/9d865f757667de14381d7c4d7ba7e4c4.svg",
                        "isPro": false,
                        "fullname": "XINHAO LI",
                        "user": "xinhaoli",
                        "type": "user"
                    },
                    "name": "Xinhao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:52:46.553Z",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18be4",
                    "user": {
                        "_id": "65499e5f2a292b3e2e5715a3",
                        "avatarUrl": "/avatars/087b3e36dfb66e044265b856bab31657.svg",
                        "isPro": false,
                        "fullname": "ziang yan",
                        "user": "Aurorana",
                        "type": "user"
                    },
                    "name": "Ziang Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:52:52.960Z",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18be5",
                    "user": {
                        "_id": "63217b7231205fe84a9626ca",
                        "avatarUrl": "/avatars/ed1e96c713c0b884adc87b8c12faa32c.svg",
                        "isPro": false,
                        "fullname": "Desen Meng",
                        "user": "desenmeng",
                        "type": "user"
                    },
                    "name": "Desen Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:53:00.046Z",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18be6",
                    "user": {
                        "_id": "666946fdec88f15e04db6022",
                        "avatarUrl": "/avatars/84511d4cd2a6bdc229dd2b1057d4b2ab.svg",
                        "isPro": false,
                        "fullname": "Lu Dong",
                        "user": "donglu",
                        "type": "user"
                    },
                    "name": "Lu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:53:11.431Z",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18be7",
                    "user": {
                        "_id": "660a7e1c3fbd33a1d0b0e233",
                        "avatarUrl": "/avatars/ceff1231078115cae8f3f4f87d026963.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Zeng",
                        "user": "Lanxingxuan",
                        "type": "user"
                    },
                    "name": "Xiangyu Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:53:25.534Z",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18be8",
                    "user": {
                        "_id": "65b9d9961fe588f824fde191",
                        "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
                        "isPro": false,
                        "fullname": "Yinan He",
                        "user": "yinanhe",
                        "type": "user"
                    },
                    "name": "Yinan He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:53:32.159Z",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18be9",
                    "name": "Yali Wang",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18bea",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18beb",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "67f783fc2eec6ce5c8d18bec",
                    "user": {
                        "_id": "643d4996482011f5f2be271f",
                        "avatarUrl": "/avatars/134b8f5d44b85d55eaaa2bbe6c409917.svg",
                        "isPro": false,
                        "fullname": "limin wang",
                        "user": "flyacht",
                        "type": "user"
                    },
                    "name": "Limin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:53:52.538Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T15:09:27.000Z",
            "submittedOnDailyAt": "2025-04-10T07:11:25.167Z",
            "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.",
            "upvotes": 6,
            "discussionId": "67f783fd2eec6ce5c8d18c2e"
        },
        "publishedAt": "2025-04-09T11:09:27.000Z",
        "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning",
        "summary": "Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06958.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6627
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.07092",
            "authors": [
                {
                    "_id": "67f7826a8b50772851ccb603",
                    "user": {
                        "_id": "64198d7efdfc2970b350f48f",
                        "avatarUrl": "/avatars/c0a0f30e1cbc22f1eb6bbc4549a5709c.svg",
                        "isPro": false,
                        "fullname": "Alexander Rubinstein",
                        "user": "arubique",
                        "type": "user"
                    },
                    "name": "Alexander Rubinstein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:52:04.485Z",
                    "hidden": false
                },
                {
                    "_id": "67f7826a8b50772851ccb604",
                    "user": {
                        "_id": "6464a0d41683d3c81f51924a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
                        "isPro": false,
                        "fullname": "Ameya Prabhu",
                        "user": "AmeyaPrabhu",
                        "type": "user"
                    },
                    "name": "Ameya Prabhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:52:11.053Z",
                    "hidden": false
                },
                {
                    "_id": "67f7826a8b50772851ccb605",
                    "name": "Matthias Bethge",
                    "hidden": false
                },
                {
                    "_id": "67f7826a8b50772851ccb606",
                    "user": {
                        "_id": "638a50450f10aa3064f03f23",
                        "avatarUrl": "/avatars/0c068458e42950c851758a238225c3a6.svg",
                        "isPro": false,
                        "fullname": "Seong Joon Oh",
                        "user": "coallaoh",
                        "type": "user"
                    },
                    "name": "Seong Joon Oh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T08:52:30.797Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T17:59:05.000Z",
            "submittedOnDailyAt": "2025-04-10T07:04:07.299Z",
            "title": "Are We Done with Object-Centric Learning?",
            "submittedOnDailyBy": {
                "_id": "6464a0d41683d3c81f51924a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
                "isPro": false,
                "fullname": "Ameya Prabhu",
                "user": "AmeyaPrabhu",
                "type": "user"
            },
            "summary": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called Object-Centric\nClassification with Applied Masks (OCCAM), demonstrating that\nsegmentation-based encoding of individual objects significantly outperforms\nslot-based OCL methods. However, challenges in real-world applications remain.\nWe provide the toolbox for the OCL community to use scalable object-centric\nrepresentations, and focus on practical applications and fundamental questions,\nsuch as understanding object perception in human cognition. Our code is\navailable https://github.com/AlexanderRubinstein/OCCAM{here}.",
            "upvotes": 5,
            "discussionId": "67f7826b8b50772851ccb64c",
            "projectPage": "https://alexanderrubinstein.github.io/are-we-done-with-ocl/",
            "githubRepo": "https://github.com/AlexanderRubinstein/OCCAM"
        },
        "publishedAt": "2025-04-09T13:59:05.000Z",
        "title": "Are We Done with Object-Centric Learning?",
        "summary": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called Object-Centric\nClassification with Applied Masks (OCCAM), demonstrating that\nsegmentation-based encoding of individual objects significantly outperforms\nslot-based OCL methods. However, challenges in real-world applications remain.\nWe provide the toolbox for the OCL community to use scalable object-centric\nrepresentations, and focus on practical applications and fundamental questions,\nsuch as understanding object perception in human cognition. Our code is\navailable https://github.com/AlexanderRubinstein/OCCAM{here}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07092.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6464a0d41683d3c81f51924a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464a0d41683d3c81f51924a/s7yYVwfUB4WOhVFJS6A6T.jpeg",
            "fullname": "Ameya Prabhu",
            "name": "AmeyaPrabhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.06719",
            "authors": [
                {
                    "_id": "67f7932d7ce01a821788dadb",
                    "user": {
                        "_id": "67a37342e7acfe540dcd5f0e",
                        "avatarUrl": "/avatars/759a4769ab390072e62859f5c1db3c0a.svg",
                        "isPro": false,
                        "fullname": "Pedro Hermosilla",
                        "user": "phermosilla",
                        "type": "user"
                    },
                    "name": "Pedro Hermosilla",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T09:57:52.157Z",
                    "hidden": false
                },
                {
                    "_id": "67f7932d7ce01a821788dadc",
                    "name": "Christian Stippel",
                    "hidden": false
                },
                {
                    "_id": "67f7932d7ce01a821788dadd",
                    "user": {
                        "_id": "642150c201c62c1e41fc2390",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642150c201c62c1e41fc2390/4udBai02RARpIujIeiCX-.png",
                        "isPro": false,
                        "fullname": "Leon Sick",
                        "user": "leonsick",
                        "type": "user"
                    },
                    "name": "Leon Sick",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T13:23:18.469Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T09:19:49.000Z",
            "submittedOnDailyAt": "2025-04-10T08:49:46.695Z",
            "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and\n  Self-Supervised Learning in 3D Scene Understanding",
            "submittedOnDailyBy": {
                "_id": "67a37342e7acfe540dcd5f0e",
                "avatarUrl": "/avatars/759a4769ab390072e62859f5c1db3c0a.svg",
                "isPro": false,
                "fullname": "Pedro Hermosilla",
                "user": "phermosilla",
                "type": "user"
            },
            "summary": "Self-supervised learning has transformed 2D computer vision by enabling\nmodels trained on large, unannotated datasets to provide versatile\noff-the-shelf features that perform similarly to models trained with labels.\nHowever, in 3D scene understanding, self-supervised methods are typically only\nused as a weight initialization step for task-specific fine-tuning, limiting\ntheir utility for general-purpose feature extraction. This paper addresses this\nshortcoming by proposing a robust evaluation protocol specifically designed to\nassess the quality of self-supervised features for 3D scene understanding. Our\nprotocol uses multi-resolution feature sampling of hierarchical models to\ncreate rich point-level representations that capture the semantic capabilities\nof the model and, hence, are suitable for evaluation with linear probing and\nnearest-neighbor methods. Furthermore, we introduce the first self-supervised\nmodel that performs similarly to supervised models when only off-the-shelf\nfeatures are used in a linear probing setup. In particular, our model is\ntrained natively in 3D with a novel self-supervised approach based on a Masked\nScene Modeling objective, which reconstructs deep features of masked patches in\na bottom-up manner and is specifically tailored to hierarchical 3D models. Our\nexperiments not only demonstrate that our method achieves competitive\nperformance to supervised models, but also surpasses existing self-supervised\napproaches by a large margin. The model and training code can be found at our\nGithub repository (https://github.com/phermosilla/msm).",
            "upvotes": 5,
            "discussionId": "67f7932f7ce01a821788db81",
            "projectPage": "https://phermosilla.github.io/msm/",
            "githubRepo": "https://github.com/phermosilla/msm"
        },
        "publishedAt": "2025-04-09T05:19:49.000Z",
        "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and\n  Self-Supervised Learning in 3D Scene Understanding",
        "summary": "Self-supervised learning has transformed 2D computer vision by enabling\nmodels trained on large, unannotated datasets to provide versatile\noff-the-shelf features that perform similarly to models trained with labels.\nHowever, in 3D scene understanding, self-supervised methods are typically only\nused as a weight initialization step for task-specific fine-tuning, limiting\ntheir utility for general-purpose feature extraction. This paper addresses this\nshortcoming by proposing a robust evaluation protocol specifically designed to\nassess the quality of self-supervised features for 3D scene understanding. Our\nprotocol uses multi-resolution feature sampling of hierarchical models to\ncreate rich point-level representations that capture the semantic capabilities\nof the model and, hence, are suitable for evaluation with linear probing and\nnearest-neighbor methods. Furthermore, we introduce the first self-supervised\nmodel that performs similarly to supervised models when only off-the-shelf\nfeatures are used in a linear probing setup. In particular, our model is\ntrained natively in 3D with a novel self-supervised approach based on a Masked\nScene Modeling objective, which reconstructs deep features of masked patches in\na bottom-up manner and is specifically tailored to hierarchical 3D models. Our\nexperiments not only demonstrate that our method achieves competitive\nperformance to supervised models, but also surpasses existing self-supervised\napproaches by a large margin. The model and training code can be found at our\nGithub repository (https://github.com/phermosilla/msm).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06719.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a37342e7acfe540dcd5f0e",
            "avatarUrl": "/avatars/759a4769ab390072e62859f5c1db3c0a.svg",
            "fullname": "Pedro Hermosilla",
            "name": "phermosilla",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.05523",
            "authors": [
                {
                    "_id": "67f7bc4c03cfd3db7e6b75e1",
                    "user": {
                        "_id": "6706d8a8d218bb754322d731",
                        "avatarUrl": "/avatars/807083029d5bae3cbafc05e03035b0a2.svg",
                        "isPro": false,
                        "fullname": "Efi",
                        "user": "efi-99",
                        "type": "user"
                    },
                    "name": "Elisabeth Fittschen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T15:45:39.921Z",
                    "hidden": false
                },
                {
                    "_id": "67f7bc4c03cfd3db7e6b75e2",
                    "name": "Sabrina Li",
                    "hidden": false
                },
                {
                    "_id": "67f7bc4c03cfd3db7e6b75e3",
                    "user": {
                        "_id": "63418125a7582111c3f57bca",
                        "avatarUrl": "/avatars/8128615f2e5a8679cd2dcfd6b30cdc48.svg",
                        "isPro": false,
                        "fullname": "Tom Lippincott",
                        "user": "tom-lippincott",
                        "type": "user"
                    },
                    "name": "Tom Lippincott",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:27:35.414Z",
                    "hidden": false
                },
                {
                    "_id": "67f7bc4c03cfd3db7e6b75e4",
                    "user": {
                        "_id": "61bf40824b4300d0fb0acf59",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644224872623-61bf40824b4300d0fb0acf59.jpeg",
                        "isPro": false,
                        "fullname": "Leshem Choshen",
                        "user": "borgr",
                        "type": "user"
                    },
                    "name": "Leshem Choshen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:27:27.281Z",
                    "hidden": false
                },
                {
                    "_id": "67f7bc4c03cfd3db7e6b75e5",
                    "user": {
                        "_id": "64b8556209e7f9e822603f1c",
                        "avatarUrl": "/avatars/62925b4d9707fcff5b56f2903cced28b.svg",
                        "isPro": false,
                        "fullname": "Craig Messner",
                        "user": "cmessner",
                        "type": "user"
                    },
                    "name": "Craig Messner",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:27:19.225Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T21:51:32.000Z",
            "submittedOnDailyAt": "2025-04-10T11:13:02.756Z",
            "title": "Pretraining Language Models for Diachronic Linguistic Change Discovery",
            "submittedOnDailyBy": {
                "_id": "61bf40824b4300d0fb0acf59",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644224872623-61bf40824b4300d0fb0acf59.jpeg",
                "isPro": false,
                "fullname": "Leshem Choshen",
                "user": "borgr",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation.",
            "upvotes": 4,
            "discussionId": "67f7bc4d03cfd3db7e6b7608",
            "projectPage": "https://huggingface.co/Hplm",
            "githubRepo": "https://github.com/comp-int-hum/historical-perspectival-lm"
        },
        "publishedAt": "2025-04-07T17:51:32.000Z",
        "title": "Pretraining Language Models for Diachronic Linguistic Change Discovery",
        "summary": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05523.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bf40824b4300d0fb0acf59",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644224872623-61bf40824b4300d0fb0acf59.jpeg",
            "fullname": "Leshem Choshen",
            "name": "borgr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.06947",
            "authors": [
                {
                    "_id": "67f78485cfcd3569910c99ab",
                    "name": "Natalia Loukachevitch",
                    "hidden": false
                },
                {
                    "_id": "67f78485cfcd3569910c99ac",
                    "name": "Natalia Tkachenko",
                    "hidden": false
                },
                {
                    "_id": "67f78485cfcd3569910c99ad",
                    "name": "Anna Lapanitsyna",
                    "hidden": false
                },
                {
                    "_id": "67f78485cfcd3569910c99ae",
                    "user": {
                        "_id": "652cedbdf120598322ae358a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652cedbdf120598322ae358a/RrxrP0gtQus4SfNwfyAg_.jpeg",
                        "isPro": false,
                        "fullname": "Mikhail",
                        "user": "RefalMachine",
                        "type": "user"
                    },
                    "name": "Mikhail Tikhomirov",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-10T09:18:43.707Z",
                    "hidden": false
                },
                {
                    "_id": "67f78485cfcd3569910c99af",
                    "user": {
                        "_id": "64e62d11d27a8292c3637f86",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
                        "isPro": false,
                        "fullname": "Nicolay Rusnachenko",
                        "user": "nicolay-r",
                        "type": "user"
                    },
                    "name": "Nicolay Rusnachenko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T09:57:54.353Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/pVL1YkBNlHeaQUud0VmSt.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/1ph_RyOzjsRcMs_04naOR.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/ewSfbBvFUKclWyZTGCOiE.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/gbTZ0dZq7O6zWKcstfwWy.png"
            ],
            "publishedAt": "2025-04-09T14:54:00.000Z",
            "submittedOnDailyAt": "2025-04-10T07:29:49.621Z",
            "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts",
            "submittedOnDailyBy": {
                "_id": "64e62d11d27a8292c3637f86",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
                "isPro": false,
                "fullname": "Nicolay Rusnachenko",
                "user": "nicolay-r",
                "type": "user"
            },
            "summary": "In this paper, we introduce the Dialogue Evaluation shared task on extraction\nof structured opinions from Russian news texts. The task of the contest is to\nextract opinion tuples for a given sentence; the tuples are composed of a\nsentiment holder, its target, an expression and sentiment from the holder to\nthe target. In total, the task received more than 100 submissions. The\nparticipants experimented mainly with large language models in zero-shot,\nfew-shot and fine-tuning formats. The best result on the test set was obtained\nwith fine-tuning of a large language model. We also compared 30 prompts and 11\nopen source language models with 3-32 billion parameters in the 1-shot and\n10-shot settings and found the best models and prompts.",
            "upvotes": 3,
            "discussionId": "67f78486cfcd3569910c9a13",
            "projectPage": "https://codalab.lisn.upsaclay.fr/competitions/20244",
            "githubRepo": "https://github.com/dialogue-evaluation/RuOpinionNE-2024"
        },
        "publishedAt": "2025-04-09T10:54:00.000Z",
        "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts",
        "summary": "In this paper, we introduce the Dialogue Evaluation shared task on extraction\nof structured opinions from Russian news texts. The task of the contest is to\nextract opinion tuples for a given sentence; the tuples are composed of a\nsentiment holder, its target, an expression and sentiment from the holder to\nthe target. In total, the task received more than 100 submissions. The\nparticipants experimented mainly with large language models in zero-shot,\nfew-shot and fine-tuning formats. The best result on the test set was obtained\nwith fine-tuning of a large language model. We also compared 30 prompts and 11\nopen source language models with 3-32 billion parameters in the 1-shot and\n10-shot settings and found the best models and prompts.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/pVL1YkBNlHeaQUud0VmSt.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/1ph_RyOzjsRcMs_04naOR.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/ewSfbBvFUKclWyZTGCOiE.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64e62d11d27a8292c3637f86/gbTZ0dZq7O6zWKcstfwWy.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06947.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e62d11d27a8292c3637f86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e62d11d27a8292c3637f86/aptDeBHpCJxcREj6KPLN1.jpeg",
            "fullname": "Nicolay Rusnachenko",
            "name": "nicolay-r",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 123
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.03886",
            "authors": [
                {
                    "_id": "67f78d2850c25afaf8a1210f",
                    "name": "Jianhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "67f78d2850c25afaf8a12110",
                    "name": "Zihan Zhu",
                    "hidden": false
                },
                {
                    "_id": "67f78d2850c25afaf8a12111",
                    "user": {
                        "_id": "67d42d13d70098db698e8453",
                        "avatarUrl": "/avatars/0956711c9028edf871e41d10a3464bc9.svg",
                        "isPro": false,
                        "fullname": "Valentin Bieri",
                        "user": "bieriv",
                        "type": "user"
                    },
                    "name": "Valentin Bieri",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:28:57.349Z",
                    "hidden": false
                },
                {
                    "_id": "67f78d2850c25afaf8a12112",
                    "user": {
                        "_id": "67b5fa179782a5e2fd2cb26a",
                        "avatarUrl": "/avatars/62c38f29ec641e001eeddf840bea21a0.svg",
                        "isPro": false,
                        "fullname": "Marc Pollefeys",
                        "user": "mapo1",
                        "type": "user"
                    },
                    "name": "Marc Pollefeys",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:28:45.819Z",
                    "hidden": false
                },
                {
                    "_id": "67f78d2850c25afaf8a12113",
                    "user": {
                        "_id": "620cb0722d8bc91e1cf2e6e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644998719617-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Songyou Peng",
                        "user": "syp",
                        "type": "user"
                    },
                    "name": "Songyou Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:28:37.970Z",
                    "hidden": false
                },
                {
                    "_id": "67f78d2850c25afaf8a12114",
                    "user": {
                        "_id": "6745f90cf4d75fd11a2407ac",
                        "avatarUrl": "/avatars/882f56565b4ebfabf1c13e199d74a4de.svg",
                        "isPro": false,
                        "fullname": "Iro Armeni",
                        "user": "ir0",
                        "type": "user"
                    },
                    "name": "Iro Armeni",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-10T13:28:29.907Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-04T19:19:40.000Z",
            "submittedOnDailyAt": "2025-04-10T07:53:22.145Z",
            "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods.",
            "upvotes": 3,
            "discussionId": "67f78d2e50c25afaf8a122c9"
        },
        "publishedAt": "2025-04-04T15:19:40.000Z",
        "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
        "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03886.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6627
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.05410",
            "authors": [
                {
                    "_id": "67f80f73cce5a0f258129554",
                    "user": {
                        "_id": "613a6efea488863c1920731e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664283416127-613a6efea488863c1920731e.jpeg",
                        "isPro": false,
                        "fullname": "Ben Lipkin",
                        "user": "benlipkin",
                        "type": "user"
                    },
                    "name": "Benjamin Lipkin",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-10T18:46:57.881Z",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f258129555",
                    "name": "Benjamin LeBrun",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f258129556",
                    "name": "Jacob Hoover Vigly",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f258129557",
                    "user": {
                        "_id": "655c76dcf37a7d0b09348813",
                        "avatarUrl": "/avatars/6fed15a38652dcc62f9022c5ff905cf2.svg",
                        "isPro": true,
                        "fullname": "João Loula",
                        "user": "jloula",
                        "type": "user"
                    },
                    "name": "João Loula",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-10T18:35:33.249Z",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f258129558",
                    "name": "David R. MacIver",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f258129559",
                    "name": "Li Du",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f25812955a",
                    "name": "Jason Eisner",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f25812955b",
                    "name": "Ryan Cotterell",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f25812955c",
                    "name": "Vikash Mansinghka",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f25812955d",
                    "user": {
                        "_id": "632b7bed6110e37dba3f5793",
                        "avatarUrl": "/avatars/564d7b53f14f9dfc54aefc5dafb7f8a8.svg",
                        "isPro": false,
                        "fullname": "Tim O'Donnell",
                        "user": "todonnell",
                        "type": "user"
                    },
                    "name": "Timothy J. O'Donnell",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-10T18:36:14.652Z",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f25812955e",
                    "user": {
                        "_id": "673cc08370644bb836283fec",
                        "avatarUrl": "/avatars/b8c7f1a10ddf76dcd06398c59f553b61.svg",
                        "isPro": false,
                        "fullname": "Alexander Lew",
                        "user": "alexanderlew",
                        "type": "user"
                    },
                    "name": "Alexander K. Lew",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-10T18:35:33.249Z",
                    "hidden": false
                },
                {
                    "_id": "67f80f73cce5a0f25812955f",
                    "name": "Tim Vieira",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T18:30:18.000Z",
            "submittedOnDailyAt": "2025-04-10T17:25:56.962Z",
            "title": "Fast Controlled Generation from Language Models with Adaptive Weighted\n  Rejection Sampling",
            "submittedOnDailyBy": {
                "_id": "613a6efea488863c1920731e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664283416127-613a6efea488863c1920731e.jpeg",
                "isPro": false,
                "fullname": "Ben Lipkin",
                "user": "benlipkin",
                "type": "user"
            },
            "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed 100,000 tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models.",
            "upvotes": 0,
            "discussionId": "67f80f75cce5a0f2581295a8"
        },
        "publishedAt": "2025-04-07T14:30:18.000Z",
        "title": "Fast Controlled Generation from Language Models with Adaptive Weighted\n  Rejection Sampling",
        "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed 100,000 tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05410.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "613a6efea488863c1920731e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664283416127-613a6efea488863c1920731e.jpeg",
            "fullname": "Ben Lipkin",
            "name": "benlipkin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.05287",
            "authors": [
                {
                    "_id": "67f6b394b67801f1ab494709",
                    "user": {
                        "_id": "67f6b0fd2142abc30f1a193e",
                        "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
                        "isPro": false,
                        "fullname": "Hui Zhang",
                        "user": "ethHuiZhang",
                        "type": "user"
                    },
                    "name": "Hui Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-10T06:45:15.453Z",
                    "hidden": false
                },
                {
                    "_id": "67f6b394b67801f1ab49470a",
                    "name": "Zijian Wu",
                    "hidden": false
                },
                {
                    "_id": "67f6b394b67801f1ab49470b",
                    "name": "Linyi Huang",
                    "hidden": false
                },
                {
                    "_id": "67f6b394b67801f1ab49470c",
                    "name": "Sammy Christen",
                    "hidden": false
                },
                {
                    "_id": "67f6b394b67801f1ab49470d",
                    "name": "Jie Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T17:38:19.000Z",
            "submittedOnDailyAt": "2025-04-10T06:35:15.115Z",
            "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
            "submittedOnDailyBy": {
                "_id": "67f6b0fd2142abc30f1a193e",
                "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
                "isPro": false,
                "fullname": "Hui Zhang",
                "user": "ethHuiZhang",
                "type": "user"
            },
            "summary": "Robust grasping of various objects from single-view perception is fundamental\nfor dexterous robots. Previous works often rely on fully observable objects,\nexpert demonstrations, or static grasping poses, which restrict their\ngeneralization ability and adaptability to external disturbances. In this\npaper, we present a reinforcement-learning-based framework that enables\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\nsingle-view perception, while performing adaptive motions to external\ndisturbances. We utilize a hand-centric object representation for shape feature\nextraction that emphasizes interaction-relevant local shapes, enhancing\nrobustness to shape variance and uncertainty. To enable effective hand\nadaptation to disturbances with limited observations, we propose a mixed\ncurriculum learning strategy, which first utilizes imitation learning to\ndistill a policy trained with privileged real-time visual-tactile feedback, and\ngradually transfers to reinforcement learning to learn adaptive motions under\ndisturbances caused by observation noises and dynamic randomization. Our\nexperiments demonstrate strong generalization in grasping unseen objects with\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\nmethod to various disturbances, including unobserved object movement and\nexternal forces, through both quantitative and qualitative evaluations. Project\nPage: https://zdchan.github.io/Robust_DexGrasp/",
            "upvotes": 0,
            "discussionId": "67f6b399b67801f1ab49487f",
            "projectPage": "https://zdchan.github.io/Robust_DexGrasp/"
        },
        "publishedAt": "2025-04-07T13:38:19.000Z",
        "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception",
        "summary": "Robust grasping of various objects from single-view perception is fundamental\nfor dexterous robots. Previous works often rely on fully observable objects,\nexpert demonstrations, or static grasping poses, which restrict their\ngeneralization ability and adaptability to external disturbances. In this\npaper, we present a reinforcement-learning-based framework that enables\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\nsingle-view perception, while performing adaptive motions to external\ndisturbances. We utilize a hand-centric object representation for shape feature\nextraction that emphasizes interaction-relevant local shapes, enhancing\nrobustness to shape variance and uncertainty. To enable effective hand\nadaptation to disturbances with limited observations, we propose a mixed\ncurriculum learning strategy, which first utilizes imitation learning to\ndistill a policy trained with privileged real-time visual-tactile feedback, and\ngradually transfers to reinforcement learning to learn adaptive motions under\ndisturbances caused by observation noises and dynamic randomization. Our\nexperiments demonstrate strong generalization in grasping unseen objects with\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\nmethod to various disturbances, including unobserved object movement and\nexternal forces, through both quantitative and qualitative evaluations. Project\nPage: https://zdchan.github.io/Robust_DexGrasp/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05287.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67f6b0fd2142abc30f1a193e",
            "avatarUrl": "/avatars/abc81a1bef1055da378c780d435dcc0a.svg",
            "fullname": "Hui Zhang",
            "name": "ethHuiZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]