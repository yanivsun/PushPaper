[
    {
        "paper": {
            "id": "2504.07491",
            "authors": [
                {
                    "_id": "67f8a3db7de2391a06a3b2e0",
                    "name": "Kimi Team",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e1",
                    "name": "Angang Du",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e2",
                    "name": "Bohong Yin",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e3",
                    "user": {
                        "_id": "67503f270dfe827c4068a408",
                        "avatarUrl": "/avatars/4591c8229c7815bfd6dc4b98aea85ca8.svg",
                        "isPro": false,
                        "fullname": "Bowei Xing",
                        "user": "xingbowei",
                        "type": "user"
                    },
                    "name": "Bowei Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:27:42.989Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e4",
                    "name": "Bowen Qu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e5",
                    "name": "Bowen Wang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e6",
                    "name": "Cheng Chen",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e7",
                    "user": {
                        "_id": "644ce4e416703fd670260e2e",
                        "avatarUrl": "/avatars/db43b13c6913af31cc97f5be7bf30091.svg",
                        "isPro": false,
                        "fullname": "Chenlin Zhang",
                        "user": "tzzcl",
                        "type": "user"
                    },
                    "name": "Chenlin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:30:16.472Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e8",
                    "user": {
                        "_id": "64c21fb42426d683e56b42bf",
                        "avatarUrl": "/avatars/60359fe204e32af831d701d2975c4599.svg",
                        "isPro": false,
                        "fullname": "Du",
                        "user": "DuChenZhuang",
                        "type": "user"
                    },
                    "name": "Chenzhuang Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:30:30.673Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2e9",
                    "name": "Chu Wei",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2ea",
                    "user": {
                        "_id": "5eefd87c5e979253a010eee5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1603575136094-5eefd87c5e979253a010eee5.jpeg",
                        "isPro": false,
                        "fullname": "Congcong Wang",
                        "user": "congcongwang",
                        "type": "user"
                    },
                    "name": "Congcong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:40:18.553Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2eb",
                    "name": "Dehao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2ec",
                    "name": "Dikang Du",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2ed",
                    "user": {
                        "_id": "67652998288b8433a92f3c43",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/yJCzGJ8gl6JyXRc7A9IeI.png",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "dongliangwang",
                        "type": "user"
                    },
                    "name": "Dongliang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:40:52.991Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2ee",
                    "user": {
                        "_id": "6331606f18711776b4655e67",
                        "avatarUrl": "/avatars/1479c2ca743b9f92d845b0ed23fcd07b.svg",
                        "isPro": false,
                        "fullname": "Enming Yuan",
                        "user": "EnmingYuan",
                        "type": "user"
                    },
                    "name": "Enming Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:41:01.062Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2ef",
                    "user": {
                        "_id": "67aed930cc96f87ce3c3132f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/JDrhmbCRcuCtKir7i9z9n.png",
                        "isPro": false,
                        "fullname": "Lu",
                        "user": "Enzhe",
                        "type": "user"
                    },
                    "name": "Enzhe Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:41:12.500Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f0",
                    "name": "Fang Li",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f1",
                    "user": {
                        "_id": "6343d01a08c017b2c042305d",
                        "avatarUrl": "/avatars/790c4104d80da9887d481f9efb494d81.svg",
                        "isPro": false,
                        "fullname": "Flood Sung",
                        "user": "floodsung",
                        "type": "user"
                    },
                    "name": "Flood Sung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:24:56.624Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f2",
                    "name": "Guangda Wei",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f3",
                    "user": {
                        "_id": "63b4c71758f367a212c4f9ef",
                        "avatarUrl": "/avatars/d61736e0ae8b333a7c24eb411378698c.svg",
                        "isPro": false,
                        "fullname": "Lai",
                        "user": "Guokun",
                        "type": "user"
                    },
                    "name": "Guokun Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:41:40.586Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f4",
                    "name": "Han Zhu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f5",
                    "user": {
                        "_id": "67bdb4ff599d450529afecf4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/clUC8MtK-qlVAfJQ7v99H.png",
                        "isPro": false,
                        "fullname": "Hao Ding",
                        "user": "HaoDing",
                        "type": "user"
                    },
                    "name": "Hao Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:41:51.300Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f6",
                    "name": "Hao Hu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f7",
                    "user": {
                        "_id": "64ec364e7e2ec711a7601cde",
                        "avatarUrl": "/avatars/6ba47d496586de65df183f056d35982b.svg",
                        "isPro": false,
                        "fullname": "Hao Yang",
                        "user": "hayayanghao",
                        "type": "user"
                    },
                    "name": "Hao Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:24:58.807Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f8",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2f9",
                    "user": {
                        "_id": "63047ed2412a1b9d381b09c9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
                        "isPro": true,
                        "fullname": "Haoning Wu, Teo",
                        "user": "teowu",
                        "type": "user"
                    },
                    "name": "Haoning Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:01.006Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2fa",
                    "user": {
                        "_id": "642bcd9be8dfcc1fe4f4f853",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg",
                        "isPro": false,
                        "fullname": "Haotian Yao",
                        "user": "skylark-95",
                        "type": "user"
                    },
                    "name": "Haotian Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T12:01:56.033Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2fb",
                    "user": {
                        "_id": "64c206a3fd4d5966b453ed85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c206a3fd4d5966b453ed85/NemBrHcAJFm2ws_VQG8ia.jpeg",
                        "isPro": false,
                        "fullname": "Haoyu Lu",
                        "user": "Nealeon",
                        "type": "user"
                    },
                    "name": "Haoyu Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:42:25.550Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2fc",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2fd",
                    "user": {
                        "_id": "62728f4f6253fe2068da1021",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                        "isPro": false,
                        "fullname": "Hongcheng Gao",
                        "user": "HongchengGao",
                        "type": "user"
                    },
                    "name": "Hongcheng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:42:35.087Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2fe",
                    "user": {
                        "_id": "61860e1258cb1f8c362f9441",
                        "avatarUrl": "/avatars/8dbc8209ad0d918453c1ffacc8f61e7f.svg",
                        "isPro": false,
                        "fullname": "Huabin Zheng",
                        "user": "zhenghuabin",
                        "type": "user"
                    },
                    "name": "Huabin Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:42:42.807Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b2ff",
                    "user": {
                        "_id": "64f83e01d493d8b0d2ab4cd3",
                        "avatarUrl": "/avatars/788d42871df1be2c9b79b2916de3e4d0.svg",
                        "isPro": false,
                        "fullname": "Jiaming Li",
                        "user": "blabluble",
                        "type": "user"
                    },
                    "name": "Jiaming Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:42:50.314Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b300",
                    "user": {
                        "_id": "6404982cad54665351d7c1e0",
                        "avatarUrl": "/avatars/8fb6d01802cbd4a1cbb7f6a0d83faa3a.svg",
                        "isPro": false,
                        "fullname": "jianlin su",
                        "user": "bojone",
                        "type": "user"
                    },
                    "name": "Jianlin Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:42:57.607Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b301",
                    "user": {
                        "_id": "63be6bf6da08ed0544f1eb7a",
                        "avatarUrl": "/avatars/19b5be6d3296da402d8822e51d6376e2.svg",
                        "isPro": false,
                        "fullname": "jianzhouWang",
                        "user": "jianzhouWang",
                        "type": "user"
                    },
                    "name": "Jianzhou Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:43:04.777Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b302",
                    "user": {
                        "_id": "66eeeb2ae65d94c88e9af620",
                        "avatarUrl": "/avatars/a25657d634878e9d53ada19feb38149a.svg",
                        "isPro": false,
                        "fullname": "Jiaqi Deng",
                        "user": "MillanK",
                        "type": "user"
                    },
                    "name": "Jiaqi Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T12:01:58.189Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b303",
                    "name": "Jiezhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b304",
                    "name": "Jin Xie",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b305",
                    "name": "Jinhong Wang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b306",
                    "name": "Jingyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b307",
                    "name": "Junjie Yan",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b308",
                    "name": "Kun Ouyang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b309",
                    "name": "Liang Chen",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b30a",
                    "name": "Lin Sui",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b30b",
                    "name": "Longhui Yu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b30c",
                    "user": {
                        "_id": "6309d1e6a58e1be42eb6eb5e",
                        "avatarUrl": "/avatars/2a7a437e801389a9f79b49c164f85817.svg",
                        "isPro": false,
                        "fullname": "dong",
                        "user": "mengnan",
                        "type": "user"
                    },
                    "name": "Mengfan Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:43:53.204Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b30d",
                    "name": "Mengnan Dong",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b30e",
                    "name": "Nuo Xu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b30f",
                    "name": "Pengyu Cheng",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b310",
                    "name": "Qizheng Gu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b311",
                    "name": "Runjie Zhou",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b312",
                    "name": "Shaowei Liu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b313",
                    "name": "Sihan Cao",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b314",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b315",
                    "name": "Tianhui Song",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b316",
                    "name": "Tongtong Bai",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b317",
                    "name": "Wei Song",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b318",
                    "name": "Weiran He",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b319",
                    "user": {
                        "_id": "63c1052e894342c896483a84",
                        "avatarUrl": "/avatars/ef99a3c4487b2e3d4c4a266e77b42d15.svg",
                        "isPro": false,
                        "fullname": "Weixiao Huang",
                        "user": "ztxcydzz",
                        "type": "user"
                    },
                    "name": "Weixiao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:55:57.280Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b31a",
                    "name": "Weixin Xu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b31b",
                    "user": {
                        "_id": "66276d360601587f0befb9fd",
                        "avatarUrl": "/avatars/467c847cad08783ee8a47af90c65615d.svg",
                        "isPro": false,
                        "fullname": "Xiaokun Yuan",
                        "user": "kx233333",
                        "type": "user"
                    },
                    "name": "Xiaokun Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:21:16.827Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b31c",
                    "user": {
                        "_id": "67593edcd3ac91d6238a4901",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kN7Mf53FamPIi29hLxiII.png",
                        "isPro": false,
                        "fullname": "Xingcheng Yao",
                        "user": "sxyao",
                        "type": "user"
                    },
                    "name": "Xingcheng Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:23:35.004Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b31d",
                    "name": "Xingzhe Wu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b31e",
                    "name": "Xinxing Zu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b31f",
                    "name": "Xinyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b320",
                    "user": {
                        "_id": "63eb133a91a1b8ec4fbc4c2f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63eb133a91a1b8ec4fbc4c2f/dmaD56RAqkovB4izizv5m.png",
                        "isPro": false,
                        "fullname": "Xinyuan Wang",
                        "user": "buaa42wxy",
                        "type": "user"
                    },
                    "name": "Xinyuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:56:14.263Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b321",
                    "name": "Y. Charles",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b322",
                    "name": "Yan Zhong",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b323",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b324",
                    "name": "Yangyang Hu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b325",
                    "name": "Yanru Chen",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b326",
                    "name": "Yejie Wang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b327",
                    "name": "Yibo Liu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b328",
                    "user": {
                        "_id": "64a139c098fad0c8a5a627a4",
                        "avatarUrl": "/avatars/6eb508abd827d4a4f5abb6b24155b22d.svg",
                        "isPro": false,
                        "fullname": "Yibo Miao",
                        "user": "instro",
                        "type": "user"
                    },
                    "name": "Yibo Miao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:23:15.494Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b329",
                    "name": "Yidao Qin",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b32a",
                    "name": "Yimin Chen",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b32b",
                    "name": "Yiping Bao",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b32c",
                    "name": "Yiqin Wang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b32d",
                    "name": "Yongsheng Kang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b32e",
                    "user": {
                        "_id": "6489761dcaea79f577897f98",
                        "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
                        "isPro": false,
                        "fullname": "Yuanxin Liu",
                        "user": "lyx97",
                        "type": "user"
                    },
                    "name": "Yuanxin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:56:23.814Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b32f",
                    "user": {
                        "_id": "6340f31fb78ed99eab04ce33",
                        "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg",
                        "isPro": false,
                        "fullname": "Du",
                        "user": "Yulun",
                        "type": "user"
                    },
                    "name": "Yulun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:23:06.421Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b330",
                    "name": "Yuxin Wu",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b331",
                    "user": {
                        "_id": "67127a470a82509269d738ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/M9qLmI3P6dT2FIwEPFJq0.png",
                        "isPro": false,
                        "fullname": "yuzhi wang",
                        "user": "vin-tage",
                        "type": "user"
                    },
                    "name": "Yuzhi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:22:52.379Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b332",
                    "name": "Yuzi Yan",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b333",
                    "user": {
                        "_id": "64409d69518271b0d1c033a6",
                        "avatarUrl": "/avatars/c79eb36c4ad96286afda834e260a1c09.svg",
                        "isPro": false,
                        "fullname": "zhouzaida",
                        "user": "zhouzaida",
                        "type": "user"
                    },
                    "name": "Zaida Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:22:41.896Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b334",
                    "name": "Zhaowei Li",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b335",
                    "user": {
                        "_id": "662c6e8352e194d5d44d873c",
                        "avatarUrl": "/avatars/385a5cc7299faf2f61ccbabedd827f29.svg",
                        "isPro": false,
                        "fullname": "Zhejun Jiang",
                        "user": "Skewed",
                        "type": "user"
                    },
                    "name": "Zhejun Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:22:27.049Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b336",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b337",
                    "user": {
                        "_id": "64bf74154d2052b1aa5ca6d9",
                        "avatarUrl": "/avatars/7aa6f2952cdbc20cfa758fdd905f06a6.svg",
                        "isPro": false,
                        "fullname": "ZHILIN YANG",
                        "user": "bruceyannnn",
                        "type": "user"
                    },
                    "name": "Zhilin Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:22:17.469Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b338",
                    "name": "Zhiqi Huang",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b339",
                    "user": {
                        "_id": "66561c5a8ec33cfd8c724cf1",
                        "avatarUrl": "/avatars/88ce5bc8ce2d7b1ca97a33d7863bf184.svg",
                        "isPro": false,
                        "fullname": "Zihao Huang",
                        "user": "EdwardHzh",
                        "type": "user"
                    },
                    "name": "Zihao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T07:43:37.864Z",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b33a",
                    "name": "Zijia Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f8a3db7de2391a06a3b33b",
                    "name": "Ziwei Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/p-kLtTC-gIyuAzN76GIt9.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/hPa3VKuztFbrcKKj8LQDy.jpeg"
            ],
            "publishedAt": "2025-04-10T06:48:26.000Z",
            "submittedOnDailyAt": "2025-04-11T03:40:59.047Z",
            "title": "Kimi-VL Technical Report",
            "submittedOnDailyBy": {
                "_id": "63047ed2412a1b9d381b09c9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
                "isPro": true,
                "fullname": "Haoning Wu, Teo",
                "user": "teowu",
                "type": "user"
            },
            "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
            "upvotes": 70,
            "discussionId": "67f8a3de7de2391a06a3b420",
            "githubRepo": "https://github.com/MoonshotAI/Kimi-VL"
        },
        "publishedAt": "2025-04-10T02:48:26.000Z",
        "title": "Kimi-VL Technical Report",
        "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/p-kLtTC-gIyuAzN76GIt9.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63047ed2412a1b9d381b09c9/hPa3VKuztFbrcKKj8LQDy.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07491.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63047ed2412a1b9d381b09c9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg",
            "fullname": "Haoning Wu, Teo",
            "name": "teowu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 46
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07956",
            "authors": [
                {
                    "_id": "67f886f1a0a44c8f05b7a124",
                    "user": {
                        "_id": "66553907965ea394ee85f04c",
                        "avatarUrl": "/avatars/e0adfe83e9ec51b118e359ddc5c37a1b.svg",
                        "isPro": false,
                        "fullname": "qyk",
                        "user": "yukunqi",
                        "type": "user"
                    },
                    "name": "Yukun Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:24:01.756Z",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a125",
                    "name": "Yiming Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a126",
                    "name": "Yu Zeng",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a127",
                    "user": {
                        "_id": "672046bd4b2e5a664aca3084",
                        "avatarUrl": "/avatars/6635c617216eca67b38b64deec634fee.svg",
                        "isPro": false,
                        "fullname": "xikun bao",
                        "user": "ChthollyTree",
                        "type": "user"
                    },
                    "name": "Xikun Bao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:24:37.813Z",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a128",
                    "user": {
                        "_id": "67dc162ec8c00778e8689f42",
                        "avatarUrl": "/avatars/7abcd41d4d466ab751f22048050d7f53.svg",
                        "isPro": false,
                        "fullname": "Wenxuan Huang",
                        "user": "Osilly",
                        "type": "user"
                    },
                    "name": "Wenxuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:24:49.236Z",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a129",
                    "user": {
                        "_id": "64b02ec0e5000ae8a572ced5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                        "isPro": false,
                        "fullname": "Lin Chen",
                        "user": "Lin-Chen",
                        "type": "user"
                    },
                    "name": "Lin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:22.829Z",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a12a",
                    "user": {
                        "_id": "64892d31cbda0d1cdb956897",
                        "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
                        "isPro": false,
                        "fullname": "Zehui Chen",
                        "user": "lovesnowbest",
                        "type": "user"
                    },
                    "name": "Zehui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:24:56.718Z",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a12b",
                    "name": "Jie Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a12c",
                    "user": {
                        "_id": "660cc64f1b14f691990c0ea0",
                        "avatarUrl": "/avatars/f172d3120b22d745a41cc3f2eb499ce6.svg",
                        "isPro": false,
                        "fullname": "Zhongang Qi",
                        "user": "phoenixqza",
                        "type": "user"
                    },
                    "name": "Zhongang Qi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:25:12.007Z",
                    "hidden": false
                },
                {
                    "_id": "67f886f1a0a44c8f05b7a12d",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:59:03.000Z",
            "submittedOnDailyAt": "2025-04-11T01:36:38.929Z",
            "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
            "submittedOnDailyBy": {
                "_id": "64b02ec0e5000ae8a572ced5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                "isPro": false,
                "fullname": "Lin Chen",
                "user": "Lin-Chen",
                "type": "user"
            },
            "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
            "upvotes": 34,
            "discussionId": "67f886f7a0a44c8f05b7a282",
            "projectPage": "https://vlm-reasoning.github.io/VCR-Bench/",
            "githubRepo": "https://github.com/zhishuifeiqian/VCR-Bench"
        },
        "publishedAt": "2025-04-10T13:59:03.000Z",
        "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
        "summary": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07956.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "fullname": "Lin Chen",
            "name": "Lin-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 87
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07960",
            "authors": [
                {
                    "_id": "67f86e6884277ab58c40ce8a",
                    "user": {
                        "_id": "6740a5730bb4a675446a80ad",
                        "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
                        "isPro": true,
                        "fullname": "Zhong-Yu Li",
                        "user": "lzyhha",
                        "type": "user"
                    },
                    "name": "Zhong-Yu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:42.946Z",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce8b",
                    "user": {
                        "_id": "64a54586c0f13de8e7093314",
                        "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg",
                        "isPro": false,
                        "fullname": "Ruoyi Du",
                        "user": "RuoyiDu",
                        "type": "user"
                    },
                    "name": "Ruoyi Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:27:33.519Z",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce8c",
                    "user": {
                        "_id": "644616965691ca69b0e02e79",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/F2uQXO9SkQwHU6benSwQB.jpeg",
                        "isPro": false,
                        "fullname": "Juncheng Yan",
                        "user": "JonsonYan",
                        "type": "user"
                    },
                    "name": "Juncheng Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:27:41.530Z",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce8d",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce8e",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce8f",
                    "user": {
                        "_id": "67b299cc6f6dc4376d9e6c76",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UniMpmfOUlyiSOrf47wuT.png",
                        "isPro": false,
                        "fullname": "Peng Gao",
                        "user": "cosumosu25",
                        "type": "user"
                    },
                    "name": "Peng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:28:33.570Z",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce90",
                    "name": "Zhanyu Ma",
                    "hidden": false
                },
                {
                    "_id": "67f86e6884277ab58c40ce91",
                    "user": {
                        "_id": "64e496ae0195913c7fa91c66",
                        "avatarUrl": "/avatars/23f274f0a3b4ef6ded35205df9bfb564.svg",
                        "isPro": false,
                        "fullname": "chengmingming",
                        "user": "mingming8688",
                        "type": "user"
                    },
                    "name": "Ming-Ming Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:28:11.683Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6740a5730bb4a675446a80ad/-Krt-Txw86EBaMdaZXyKY.mp4"
            ],
            "publishedAt": "2025-04-10T17:59:42.000Z",
            "submittedOnDailyAt": "2025-04-11T01:51:23.709Z",
            "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
            "submittedOnDailyBy": {
                "_id": "6740a5730bb4a675446a80ad",
                "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
                "isPro": true,
                "fullname": "Zhong-Yu Li",
                "user": "lzyhha",
                "type": "user"
            },
            "summary": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures.",
            "upvotes": 32,
            "discussionId": "67f86e6a84277ab58c40cf08",
            "projectPage": "https://visualcloze.github.io/",
            "githubRepo": "https://github.com/lzyhha/VisualCloze"
        },
        "publishedAt": "2025-04-10T13:59:42.000Z",
        "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
        "summary": "Recent progress in diffusion models significantly advances various image\ngeneration tasks. However, the current mainstream approach remains focused on\nbuilding task-specific models, which have limited efficiency when supporting a\nwide range of different needs. While universal models attempt to address this\nlimitation, they face critical challenges, including generalizable task\ninstruction, appropriate task distributions, and unified architectural design.\nTo tackle these challenges, we propose VisualCloze, a universal image\ngeneration framework, which supports a wide range of in-domain tasks,\ngeneralization to unseen ones, unseen unification of multiple tasks, and\nreverse generation. Unlike existing methods that rely on language-based task\ninstruction, leading to task ambiguity and weak generalization, we integrate\nvisual in-context learning, allowing models to identify tasks from visual\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\nhampers the learning of transferable knowledge across tasks. To this end, we\nintroduce Graph200K, a graph-structured dataset that establishes various\ninterrelated tasks, enhancing task density and transferable knowledge.\nFurthermore, we uncover that our unified image generation formulation shared a\nconsistent objective with image infilling, enabling us to leverage the strong\ngenerative priors of pre-trained infilling models without modifying the\narchitectures.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6740a5730bb4a675446a80ad/-Krt-Txw86EBaMdaZXyKY.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07960.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6740a5730bb4a675446a80ad",
            "avatarUrl": "/avatars/27c08e33df88e4f73c136da65f2b5adb.svg",
            "fullname": "Zhong-Yu Li",
            "name": "lzyhha",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07128",
            "authors": [
                {
                    "_id": "67f888115ebbf40d96641662",
                    "user": {
                        "_id": "665f7b803fa4adb5dbd8dfbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665f7b803fa4adb5dbd8dfbc/7td4UWJFU1Y_b-hKZX4Ge.jpeg",
                        "isPro": false,
                        "fullname": "Sara Vera Marjanovic",
                        "user": "spaidartaigar",
                        "type": "user"
                    },
                    "name": "Sara Vera Marjanović",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:29:51.789Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641663",
                    "user": {
                        "_id": "631a523c04f8ed65eff16fb4",
                        "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
                        "isPro": false,
                        "fullname": "Arkil Patel",
                        "user": "arkilpatel",
                        "type": "user"
                    },
                    "name": "Arkil Patel",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-11T03:11:10.866Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641664",
                    "user": {
                        "_id": "61981af5d420757268e195ac",
                        "avatarUrl": "/avatars/8b59aaf33447224f83d497425fd7ea8f.svg",
                        "isPro": false,
                        "fullname": "Vaibhav Adlakha",
                        "user": "vaibhavad",
                        "type": "user"
                    },
                    "name": "Vaibhav Adlakha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:29:57.657Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641665",
                    "user": {
                        "_id": "6197f5213619d373ad154f73",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637348614474-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Milad Aghajohari",
                        "user": "miladink",
                        "type": "user"
                    },
                    "name": "Milad Aghajohari",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:30:03.453Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641666",
                    "user": {
                        "_id": "627d5ead401f42c57b6ce54c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627d5ead401f42c57b6ce54c/GajmN5G_MRUFRZs6ens0t.jpeg",
                        "isPro": false,
                        "fullname": "Parishad BehnamGhader",
                        "user": "parishadbehnam",
                        "type": "user"
                    },
                    "name": "Parishad BehnamGhader",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:30:09.878Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641667",
                    "name": "Mehar Bhatia",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641668",
                    "user": {
                        "_id": "6512e852a76fd5945b19e9a1",
                        "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
                        "isPro": false,
                        "fullname": "Aditi Khandelwal",
                        "user": "aditi184",
                        "type": "user"
                    },
                    "name": "Aditi Khandelwal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:16.938Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641669",
                    "name": "Austin Kraft",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d9664166a",
                    "user": {
                        "_id": "6270c58780d5f35f8dbe42be",
                        "avatarUrl": "/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg",
                        "isPro": false,
                        "fullname": "Benno Krojer",
                        "user": "BennoKrojer",
                        "type": "user"
                    },
                    "name": "Benno Krojer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:15.125Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d9664166b",
                    "user": {
                        "_id": "5fa9ff3ea13e063b8b2b60cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                        "isPro": false,
                        "fullname": "Xing Han Lu",
                        "user": "xhluca",
                        "type": "user"
                    },
                    "name": "Xing Han Lù",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:20.675Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d9664166c",
                    "user": {
                        "_id": "64527548fc4b47877aba7de0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64527548fc4b47877aba7de0/ht-mRRxNQT49A7NxArOGG.png",
                        "isPro": false,
                        "fullname": "Nicholas Meade",
                        "user": "ncmeade",
                        "type": "user"
                    },
                    "name": "Nicholas Meade",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:30:23.213Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d9664166d",
                    "user": {
                        "_id": "63340b24f68a3fb7efa62b3a",
                        "avatarUrl": "/avatars/44c960437c037553d90b1ca24c952977.svg",
                        "isPro": false,
                        "fullname": "Dongchan Shin",
                        "user": "ShinDC",
                        "type": "user"
                    },
                    "name": "Dongchan Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:18.900Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d9664166e",
                    "user": {
                        "_id": "63458f12d54fb141dedac508",
                        "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
                        "isPro": false,
                        "fullname": "Amirhossein Kazemnejad",
                        "user": "kazemnejad",
                        "type": "user"
                    },
                    "name": "Amirhossein Kazemnejad",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:30:29.842Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d9664166f",
                    "name": "Gaurav Kamath",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641670",
                    "user": {
                        "_id": "608865d2511e863acdb20bae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/608865d2511e863acdb20bae/WSJQHZ4PC9hjLMCWv9mSb.png",
                        "isPro": false,
                        "fullname": "Marius Mosbach",
                        "user": "mmosbach",
                        "type": "user"
                    },
                    "name": "Marius Mosbach",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:31:03.274Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641671",
                    "user": {
                        "_id": "60a66731e1db8bc33b8d4112",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
                        "isPro": false,
                        "fullname": "Karolina Stanczak",
                        "user": "Karolina",
                        "type": "user"
                    },
                    "name": "Karolina Stańczak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:30:57.269Z",
                    "hidden": false
                },
                {
                    "_id": "67f888115ebbf40d96641672",
                    "user": {
                        "_id": "624734dc4c731bb6bfab8af7",
                        "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
                        "isPro": false,
                        "fullname": "Siva Reddy",
                        "user": "sivareddyg",
                        "type": "user"
                    },
                    "name": "Siva Reddy",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-11T03:26:50.276Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T00:36:08.000Z",
            "submittedOnDailyAt": "2025-04-11T02:00:19.953Z",
            "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "6512e852a76fd5945b19e9a1",
                "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
                "isPro": false,
                "fullname": "Aditi Khandelwal",
                "user": "aditi184",
                "type": "user"
            },
            "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs.",
            "upvotes": 31,
            "discussionId": "67f888125ebbf40d966416bd",
            "projectPage": "https://mcgill-nlp.github.io/thoughtology/",
            "githubRepo": "https://github.com/mcgill-NLP/thoughtology"
        },
        "publishedAt": "2025-04-01T20:36:08.000Z",
        "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
        "summary": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs\napproach complex problems. Instead of directly producing an answer for a given\ninput, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly\n\"thinking\" about a problem before providing an answer. This reasoning process\nis publicly available to the user, creating endless opportunities for studying\nthe reasoning behaviour of the model and opening up the field of Thoughtology.\nStarting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,\nour analyses on DeepSeek-R1 investigate the impact and controllability of\nthought length, management of long or confusing contexts, cultural and safety\nconcerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such\nas human-like language processing and world modelling. Our findings paint a\nnuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,\nwhere extra inference time can impair model performance. Furthermore, we find a\ntendency for DeepSeek-R1 to persistently ruminate on previously explored\nproblem formulations, obstructing further exploration. We also note strong\nsafety vulnerabilities of DeepSeek-R1 compared to its non-reasoning\ncounterpart, which can also compromise safety-aligned LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07128.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6512e852a76fd5945b19e9a1",
            "avatarUrl": "/avatars/751526fbc0c939a972bea684937a34bf.svg",
            "fullname": "Aditi Khandelwal",
            "name": "aditi184",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07964",
            "authors": [
                {
                    "_id": "67f88c848178ca61d74e001d",
                    "user": {
                        "_id": "671002fd13203512e7b8f9e3",
                        "avatarUrl": "/avatars/313d8ea313ed300750cfdaaca44fdb6e.svg",
                        "isPro": false,
                        "fullname": "Zhongyang Li",
                        "user": "Lzy01241010",
                        "type": "user"
                    },
                    "name": "Zhongyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:08.340Z",
                    "hidden": false
                },
                {
                    "_id": "67f88c848178ca61d74e001e",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "67f88c848178ca61d74e001f",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:10.558Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-11T02:02:00.382Z",
            "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
            "upvotes": 28,
            "discussionId": "67f88c858178ca61d74e0061",
            "githubRepo": "https://github.com/tianyi-lab/C3PO"
        },
        "publishedAt": "2025-04-10T13:59:56.000Z",
        "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
        "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07964.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07957",
            "authors": [
                {
                    "_id": "67f8914516d43f88b3177ec1",
                    "user": {
                        "_id": "646cd947da8e99940b6e55cf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
                        "isPro": false,
                        "fullname": "Shengyuan Ding",
                        "user": "ChrisDing1105",
                        "type": "user"
                    },
                    "name": "Shengyuan Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:25:40.694Z",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec2",
                    "name": "Shenxi Wu",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec3",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec4",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:05.800Z",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec5",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:26:11.929Z",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec6",
                    "user": {
                        "_id": "67c0849ee08c178ef8d4e05c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
                        "isPro": false,
                        "fullname": "Xiaoyi Dong",
                        "user": "sweetFruit",
                        "type": "user"
                    },
                    "name": "Xiaoyi Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:26:18.423Z",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec7",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec8",
                    "user": {
                        "_id": "65000bef18830fabea469fdd",
                        "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
                        "isPro": false,
                        "fullname": "Cao Yuhang",
                        "user": "yhcao",
                        "type": "user"
                    },
                    "name": "Yuhang Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:26:51.425Z",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177ec9",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:26:59.045Z",
                    "hidden": false
                },
                {
                    "_id": "67f8914516d43f88b3177eca",
                    "user": {
                        "_id": "64638c4d51fa6e63060521b5",
                        "avatarUrl": "/avatars/c863ace5b1dc788a341bcf4ddbdfaec1.svg",
                        "isPro": false,
                        "fullname": "JIaqi",
                        "user": "Jiaqiwang",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:27:20.727Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:59:12.000Z",
            "submittedOnDailyAt": "2025-04-11T02:20:10.515Z",
            "title": "MM-IFEngine: Towards Multimodal Instruction Following",
            "submittedOnDailyBy": {
                "_id": "646cd947da8e99940b6e55cf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
                "isPro": false,
                "fullname": "Shengyuan Ding",
                "user": "ChrisDing1105",
                "type": "user"
            },
            "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval\n(+12.3%). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.",
            "upvotes": 26,
            "discussionId": "67f8914816d43f88b3177fa7",
            "projectPage": "https://syuan03.github.io/MM-IFEngine/",
            "githubRepo": "https://github.com/SYuan03/MM-IFEngine"
        },
        "publishedAt": "2025-04-10T13:59:12.000Z",
        "title": "MM-IFEngine: Towards Multimodal Instruction Following",
        "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval\n(+12.3%). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07957.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646cd947da8e99940b6e55cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg",
            "fullname": "Shengyuan Ding",
            "name": "ChrisDing1105",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07943",
            "authors": [
                {
                    "_id": "67f88643f5889583ce0499ae",
                    "user": {
                        "_id": "64b4eecf2fc8324fcb63b404",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
                        "isPro": false,
                        "fullname": "Yunhan Yang",
                        "user": "yhyang-myron",
                        "type": "user"
                    },
                    "name": "Yunhan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:28:45.498Z",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499af",
                    "user": {
                        "_id": "6346aaa3f06b237ba4e297b0",
                        "avatarUrl": "/avatars/5acb986e993eab1461200f3e9d99d022.svg",
                        "isPro": false,
                        "fullname": "Yuan-Chen Guo",
                        "user": "bennyguo",
                        "type": "user"
                    },
                    "name": "Yuan-Chen Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:28:51.310Z",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499b0",
                    "user": {
                        "_id": "638ee900ee7e45e0474a5712",
                        "avatarUrl": "/avatars/eadb5ae2fc92bd9af5516acbc8f1bdf0.svg",
                        "isPro": false,
                        "fullname": "Yukun Huang",
                        "user": "KevinHuang",
                        "type": "user"
                    },
                    "name": "Yukun Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:25.048Z",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499b1",
                    "user": {
                        "_id": "644dbf6453ad80c6593bf748",
                        "avatarUrl": "/avatars/0e170cf2aa8d7f0f3f83e36f06f023f8.svg",
                        "isPro": false,
                        "fullname": "Zixin Zou",
                        "user": "zouzx",
                        "type": "user"
                    },
                    "name": "Zi-Xin Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:29:00.746Z",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499b2",
                    "name": "Zhipeng Yu",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499b3",
                    "user": {
                        "_id": "64d71083a787c9bc7b9f1238",
                        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                        "isPro": false,
                        "fullname": "Yangguang Li",
                        "user": "Lp256",
                        "type": "user"
                    },
                    "name": "Yangguang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:29:23.548Z",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499b4",
                    "user": {
                        "_id": "638066faf022c8a5803f7eb8",
                        "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
                        "isPro": false,
                        "fullname": "Yanpei Cao",
                        "user": "pookiefoof",
                        "type": "user"
                    },
                    "name": "Yan-Pei Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:29:32.598Z",
                    "hidden": false
                },
                {
                    "_id": "67f88643f5889583ce0499b5",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:29:39.026Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:53:31.000Z",
            "submittedOnDailyAt": "2025-04-11T01:33:34.186Z",
            "title": "HoloPart: Generative 3D Part Amodal Segmentation",
            "submittedOnDailyBy": {
                "_id": "64b4eecf2fc8324fcb63b404",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
                "isPro": false,
                "fullname": "Yunhan Yang",
                "user": "yhyang-myron",
                "type": "user"
            },
            "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.",
            "upvotes": 22,
            "discussionId": "67f88646f5889583ce049a90",
            "projectPage": "https://vast-ai-research.github.io/HoloPart",
            "githubRepo": "https://github.com/VAST-AI-Research/HoloPart"
        },
        "publishedAt": "2025-04-10T13:53:31.000Z",
        "title": "HoloPart: Generative 3D Part Amodal Segmentation",
        "summary": "3D part amodal segmentation--decomposing a 3D shape into complete,\nsemantically meaningful parts, even when occluded--is a challenging but crucial\ntask for 3D content creation and understanding. Existing 3D part segmentation\nmethods only identify visible surface patches, limiting their utility. Inspired\nby 2D amodal segmentation, we introduce this novel task to the 3D domain and\npropose a practical, two-stage approach, addressing the key challenges of\ninferring occluded 3D geometry, maintaining global shape consistency, and\nhandling diverse shapes with limited training data. First, we leverage existing\n3D part segmentation to obtain initial, incomplete part segments. Second, we\nintroduce HoloPart, a novel diffusion-based model, to complete these segments\ninto full 3D parts. HoloPart utilizes a specialized architecture with local\nattention to capture fine-grained part geometry and global shape context\nattention to ensure overall shape consistency. We introduce new benchmarks\nbased on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart\nsignificantly outperforms state-of-the-art shape completion methods. By\nincorporating HoloPart with existing segmentation techniques, we achieve\npromising results on 3D part amodal segmentation, opening new avenues for\napplications in geometry editing, animation, and material assignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07943.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4eecf2fc8324fcb63b404",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
            "fullname": "Yunhan Yang",
            "name": "yhyang-myron",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07830",
            "authors": [
                {
                    "_id": "67f8886e334eb1a3942c4f3f",
                    "user": {
                        "_id": "64881deb8e004bb92b0f4845",
                        "avatarUrl": "/avatars/30a1e016d469bf7eb42c713351a9f65c.svg",
                        "isPro": false,
                        "fullname": "Genglin Liu",
                        "user": "genglinliu",
                        "type": "user"
                    },
                    "name": "Genglin Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:12.907Z",
                    "hidden": false
                },
                {
                    "_id": "67f8886e334eb1a3942c4f40",
                    "user": {
                        "_id": "625913bd5f80a3c1aad074b6",
                        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
                        "isPro": false,
                        "fullname": "Salman Rahman",
                        "user": "salmannyu",
                        "type": "user"
                    },
                    "name": "Salman Rahman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:32:16.232Z",
                    "hidden": false
                },
                {
                    "_id": "67f8886e334eb1a3942c4f41",
                    "user": {
                        "_id": "66bbf21e22c408695cd8b4f8",
                        "avatarUrl": "/avatars/04f0b017f36f83e560e47c5b7d4f9f8e.svg",
                        "isPro": false,
                        "fullname": "Elisa Kreiss",
                        "user": "elisakreiss",
                        "type": "user"
                    },
                    "name": "Elisa Kreiss",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:32:24.130Z",
                    "hidden": false
                },
                {
                    "_id": "67f8886e334eb1a3942c4f42",
                    "name": "Marzyeh Ghassemi",
                    "hidden": false
                },
                {
                    "_id": "67f8886e334eb1a3942c4f43",
                    "user": {
                        "_id": "62c49b35143622c92793638e",
                        "avatarUrl": "/avatars/96a9dd81d0e17b1f762410a8b1ab8724.svg",
                        "isPro": false,
                        "fullname": "Saadia Gabriel",
                        "user": "saadia",
                        "type": "user"
                    },
                    "name": "Saadia Gabriel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:31:48.956Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T15:06:54.000Z",
            "submittedOnDailyAt": "2025-04-11T01:44:23.981Z",
            "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
            "submittedOnDailyBy": {
                "_id": "625913bd5f80a3c1aad074b6",
                "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
                "isPro": false,
                "fullname": "Salman Rahman",
                "user": "salmannyu",
                "type": "user"
            },
            "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
            "upvotes": 14,
            "discussionId": "67f8886f334eb1a3942c4f5f",
            "githubRepo": "https://github.com/genglinliu/MOSAIC"
        },
        "publishedAt": "2025-04-10T11:06:54.000Z",
        "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
        "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07830.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "625913bd5f80a3c1aad074b6",
            "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
            "fullname": "Salman Rahman",
            "name": "salmannyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07951",
            "authors": [
                {
                    "_id": "67f8e4acc7b30121f9c054d1",
                    "name": "Mustafa Shukor",
                    "hidden": false
                },
                {
                    "_id": "67f8e4acc7b30121f9c054d2",
                    "name": "Enrico Fini",
                    "hidden": false
                },
                {
                    "_id": "67f8e4acc7b30121f9c054d3",
                    "name": "Victor Guilherme Turrisi da Costa",
                    "hidden": false
                },
                {
                    "_id": "67f8e4acc7b30121f9c054d4",
                    "name": "Matthieu Cord",
                    "hidden": false
                },
                {
                    "_id": "67f8e4acc7b30121f9c054d5",
                    "name": "Joshua Susskind",
                    "hidden": false
                },
                {
                    "_id": "67f8e4acc7b30121f9c054d6",
                    "name": "Alaaeldin El-Nouby",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:57:28.000Z",
            "submittedOnDailyAt": "2025-04-11T08:16:01.667Z",
            "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
            "upvotes": 12,
            "discussionId": "67f8e4adc7b30121f9c054fd"
        },
        "publishedAt": "2025-04-10T13:57:28.000Z",
        "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
        "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6633
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.07934",
            "authors": [
                {
                    "_id": "67f88bbaf1410163f7c3b68a",
                    "user": {
                        "_id": "655fed9fdef5905d38b84af3",
                        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                        "isPro": false,
                        "fullname": "Xiyao Wang",
                        "user": "russwang",
                        "type": "user"
                    },
                    "name": "Xiyao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:32:46.460Z",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b68b",
                    "user": {
                        "_id": "630713411801ecc7d2592a7c",
                        "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
                        "isPro": false,
                        "fullname": "Zhengyuan Yang",
                        "user": "zyang39",
                        "type": "user"
                    },
                    "name": "Zhengyuan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:32:53.708Z",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b68c",
                    "name": "Chao Feng",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b68d",
                    "name": "Hongjin Lu",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b68e",
                    "user": {
                        "_id": "63db16fff03c3d71ef397206",
                        "avatarUrl": "/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg",
                        "isPro": false,
                        "fullname": "Linjie Li",
                        "user": "linjieli222",
                        "type": "user"
                    },
                    "name": "Linjie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:33:27.401Z",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b68f",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b690",
                    "user": {
                        "_id": "6298fd95b58e71e2ac9f3ad8",
                        "avatarUrl": "/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg",
                        "isPro": false,
                        "fullname": "Kevin Lin",
                        "user": "kevinlin311tw",
                        "type": "user"
                    },
                    "name": "Kevin Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:33:58.928Z",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b691",
                    "user": {
                        "_id": "64cbc3e2a257a3212c00a115",
                        "avatarUrl": "/avatars/836e61be4aeda2080ddf2db9f2626cc6.svg",
                        "isPro": false,
                        "fullname": "Furong Huang Lab at UMD",
                        "user": "furongh-lab",
                        "type": "user"
                    },
                    "name": "Furong Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:34:07.020Z",
                    "hidden": false
                },
                {
                    "_id": "67f88bbaf1410163f7c3b692",
                    "user": {
                        "_id": "6413521d4e5305c14f22e110",
                        "avatarUrl": "/avatars/a6f8d0573e678f79bc3c0b7897b818ce.svg",
                        "isPro": false,
                        "fullname": "Lijuan Wang",
                        "user": "Lijuan",
                        "type": "user"
                    },
                    "name": "Lijuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:34:25.295Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:49:05.000Z",
            "submittedOnDailyAt": "2025-04-11T01:57:16.470Z",
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
            "submittedOnDailyBy": {
                "_id": "655fed9fdef5905d38b84af3",
                "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                "isPro": false,
                "fullname": "Xiyao Wang",
                "user": "russwang",
                "type": "user"
            },
            "summary": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL.",
            "upvotes": 10,
            "discussionId": "67f88bbbf1410163f7c3b6f4",
            "githubRepo": "https://github.com/si0wang/ThinkLite-VL"
        },
        "publishedAt": "2025-04-10T13:49:05.000Z",
        "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
        "summary": "In this paper, we present an effective method to enhance visual reasoning\nwith significantly fewer training samples, relying purely on self-improvement\nwith no knowledge distillation. Our key insight is that the difficulty of\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\nchallenging samples can substantially boost reasoning capabilities even when\nthe dataset is small. Despite being intuitive, the main challenge remains in\naccurately quantifying sample difficulty to enable effective data filtering. To\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\nto achieve that. Starting from our curated 70k open-source training samples, we\nintroduce an MCTS-based selection method that quantifies sample difficulty\nbased on the number of iterations required by the VLMs to solve each problem.\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\nand better identifies samples that are genuinely challenging. We filter and\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\nusing only 11k training samples with no knowledge distillation. This\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\ncomparable baselines that use classic selection methods such as accuracy-based\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\navailable at https://github.com/si0wang/ThinkLite-VL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07934.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655fed9fdef5905d38b84af3",
            "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
            "fullname": "Xiyao Wang",
            "name": "russwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.04974",
            "authors": [
                {
                    "_id": "67f8932aa1d82990423d99b5",
                    "user": {
                        "_id": "65031d01cccc7b28a388c719",
                        "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg",
                        "isPro": false,
                        "fullname": "Ming Li",
                        "user": "MingLiiii",
                        "type": "user"
                    },
                    "name": "Ming Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-11T03:57:35.193Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99b6",
                    "user": {
                        "_id": "64668f982da1abc242355cbb",
                        "avatarUrl": "/avatars/c9a248b8d70b1a8f7b78265c98690570.svg",
                        "isPro": false,
                        "fullname": "Ruiyi Zhang",
                        "user": "zhangry868",
                        "type": "user"
                    },
                    "name": "Ruiyi Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-11T03:57:35.193Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99b7",
                    "name": "Jian Chen",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99b8",
                    "user": {
                        "_id": "642467708d97ce93878e8124",
                        "avatarUrl": "/avatars/f0e464ddb4bd790f470fc0f10275fa26.svg",
                        "isPro": false,
                        "fullname": "Jiuxiang Gu",
                        "user": "JoshuaGu",
                        "type": "user"
                    },
                    "name": "Jiuxiang Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:36:21.275Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99b9",
                    "user": {
                        "_id": "635c2c2a7a165601151d3f85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666984965757-noauth.png",
                        "isPro": false,
                        "fullname": "Yufan Zhou",
                        "user": "YfZ",
                        "type": "user"
                    },
                    "name": "Yufan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:36:40.076Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99ba",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T07:25:03.265Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99bb",
                    "user": {
                        "_id": "633a7e91c8eb7b089034fb3c",
                        "avatarUrl": "/avatars/0e03d80aa469de131cc17bb0c5c016b9.svg",
                        "isPro": false,
                        "fullname": "Wanrong Zhu",
                        "user": "VegB",
                        "type": "user"
                    },
                    "name": "Wanrong Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:36:47.002Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99bc",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:37:06.061Z",
                    "hidden": false
                },
                {
                    "_id": "67f8932aa1d82990423d99bd",
                    "name": "Tong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T12:01:59.000Z",
            "submittedOnDailyAt": "2025-04-11T02:27:52.838Z",
            "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
            "upvotes": 4,
            "discussionId": "67f8932fa1d82990423d9b2a"
        },
        "publishedAt": "2025-04-07T08:01:59.000Z",
        "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
        "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04974.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07961",
            "authors": [
                {
                    "_id": "67f90d832af32ec4d45797cb",
                    "user": {
                        "_id": "679517e61703797636585760",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dmd8ws39tRZLwUH7wwk8i.png",
                        "isPro": false,
                        "fullname": "Zeren Jiang",
                        "user": "jzr99",
                        "type": "user"
                    },
                    "name": "Zeren Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-11T16:18:47.541Z",
                    "hidden": false
                },
                {
                    "_id": "67f90d832af32ec4d45797cc",
                    "name": "Chuanxia Zheng",
                    "hidden": false
                },
                {
                    "_id": "67f90d832af32ec4d45797cd",
                    "name": "Iro Laina",
                    "hidden": false
                },
                {
                    "_id": "67f90d832af32ec4d45797ce",
                    "name": "Diane Larlus",
                    "hidden": false
                },
                {
                    "_id": "67f90d832af32ec4d45797cf",
                    "name": "Andrea Vedaldi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:59:55.000Z",
            "submittedOnDailyAt": "2025-04-11T11:33:53.794Z",
            "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
            "submittedOnDailyBy": {
                "_id": "679517e61703797636585760",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dmd8ws39tRZLwUH7wwk8i.png",
                "isPro": false,
                "fullname": "Zeren Jiang",
                "user": "jzr99",
                "type": "user"
            },
            "summary": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes.",
            "upvotes": 1,
            "discussionId": "67f90d862af32ec4d4579863",
            "projectPage": "https://geo4d.github.io",
            "githubRepo": "https://github.com/jzr99/Geo4D"
        },
        "publishedAt": "2025-04-10T13:59:55.000Z",
        "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
        "summary": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07961.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "679517e61703797636585760",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dmd8ws39tRZLwUH7wwk8i.png",
            "fullname": "Zeren Jiang",
            "name": "jzr99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.06801",
            "authors": [
                {
                    "_id": "67f8c967e096f2970b79fc45",
                    "user": {
                        "_id": "631af10b492c0c57a22ea3e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
                        "isPro": false,
                        "fullname": "Rishubh Parihar",
                        "user": "RishubhPar",
                        "type": "user"
                    },
                    "name": "Rishubh Parihar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:37:25.657Z",
                    "hidden": false
                },
                {
                    "_id": "67f8c967e096f2970b79fc46",
                    "name": "Srinjay Sarkar",
                    "hidden": false
                },
                {
                    "_id": "67f8c967e096f2970b79fc47",
                    "name": "Sarthak Vora",
                    "hidden": false
                },
                {
                    "_id": "67f8c967e096f2970b79fc48",
                    "name": "Jogendra Kundu",
                    "hidden": false
                },
                {
                    "_id": "67f8c967e096f2970b79fc49",
                    "name": "R. Venkatesh Babu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T11:47:48.000Z",
            "submittedOnDailyAt": "2025-04-11T06:20:06.272Z",
            "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection",
            "submittedOnDailyBy": {
                "_id": "631af10b492c0c57a22ea3e2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
                "isPro": false,
                "fullname": "Rishubh Parihar",
                "user": "RishubhPar",
                "type": "user"
            },
            "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.",
            "upvotes": 1,
            "discussionId": "67f8c96ae096f2970b79fd04"
        },
        "publishedAt": "2025-04-09T07:47:48.000Z",
        "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection",
        "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06801.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631af10b492c0c57a22ea3e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
            "fullname": "Rishubh Parihar",
            "name": "RishubhPar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.06752",
            "authors": [
                {
                    "_id": "67f8c9f028f4f60ceb4103e4",
                    "user": {
                        "_id": "631af10b492c0c57a22ea3e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
                        "isPro": false,
                        "fullname": "Rishubh Parihar",
                        "user": "RishubhPar",
                        "type": "user"
                    },
                    "name": "Rishubh Parihar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:43:15.754Z",
                    "hidden": false
                },
                {
                    "_id": "67f8c9f028f4f60ceb4103e5",
                    "name": "Vaibhav Agrawal",
                    "hidden": false
                },
                {
                    "_id": "67f8c9f028f4f60ceb4103e6",
                    "user": {
                        "_id": "64f56843ad81562b476ec05d",
                        "avatarUrl": "/avatars/fa90a685259f89bd1b447ba93359187e.svg",
                        "isPro": false,
                        "fullname": "sachidanand vs",
                        "user": "sachi1",
                        "type": "user"
                    },
                    "name": "Sachidanand VS",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T08:43:46.850Z",
                    "hidden": false
                },
                {
                    "_id": "67f8c9f028f4f60ceb4103e7",
                    "name": "R. Venkatesh Babu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T10:15:15.000Z",
            "submittedOnDailyAt": "2025-04-11T06:22:20.904Z",
            "title": "Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "631af10b492c0c57a22ea3e2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
                "isPro": false,
                "fullname": "Rishubh Parihar",
                "user": "RishubhPar",
                "type": "user"
            },
            "summary": "Existing approaches for controlling text-to-image diffusion models, while\npowerful, do not allow for explicit 3D object-centric control, such as precise\ncontrol of object orientation. In this work, we address the problem of\nmulti-object orientation control in text-to-image diffusion models. This\nenables the generation of diverse multi-object scenes with precise orientation\ncontrol for each object. The key idea is to condition the diffusion model with\na set of orientation-aware compass tokens, one for each object, along\nwith text tokens. A light-weight encoder network predicts these compass tokens\ntaking object orientation as the input. The model is trained on a synthetic\ndataset of procedurally generated scenes, each containing one or two 3D assets\non a plain background. However, direct training this framework results in poor\norientation control as well as leads to entanglement among objects. To mitigate\nthis, we intervene in the generation process and constrain the cross-attention\nmaps of each compass token to its corresponding object regions. The trained\nmodel is able to achieve precise orientation control for a) complex objects not\nseen during training and b) multi-object scenes with more than two objects,\nindicating strong generalization capabilities. Further, when combined with\npersonalization methods, our method precisely controls the orientation of the\nnew object in diverse contexts. Our method achieves state-of-the-art\norientation control and text alignment, quantified with extensive evaluations\nand a user study.",
            "upvotes": 1,
            "discussionId": "67f8c9f428f4f60ceb4104e9"
        },
        "publishedAt": "2025-04-09T06:15:15.000Z",
        "title": "Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation",
        "summary": "Existing approaches for controlling text-to-image diffusion models, while\npowerful, do not allow for explicit 3D object-centric control, such as precise\ncontrol of object orientation. In this work, we address the problem of\nmulti-object orientation control in text-to-image diffusion models. This\nenables the generation of diverse multi-object scenes with precise orientation\ncontrol for each object. The key idea is to condition the diffusion model with\na set of orientation-aware compass tokens, one for each object, along\nwith text tokens. A light-weight encoder network predicts these compass tokens\ntaking object orientation as the input. The model is trained on a synthetic\ndataset of procedurally generated scenes, each containing one or two 3D assets\non a plain background. However, direct training this framework results in poor\norientation control as well as leads to entanglement among objects. To mitigate\nthis, we intervene in the generation process and constrain the cross-attention\nmaps of each compass token to its corresponding object regions. The trained\nmodel is able to achieve precise orientation control for a) complex objects not\nseen during training and b) multi-object scenes with more than two objects,\nindicating strong generalization capabilities. Further, when combined with\npersonalization methods, our method precisely controls the orientation of the\nnew object in diverse contexts. Our method achieves state-of-the-art\norientation control and text alignment, quantified with extensive evaluations\nand a user study.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06752.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631af10b492c0c57a22ea3e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662710000523-noauth.png",
            "fullname": "Rishubh Parihar",
            "name": "RishubhPar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.05579",
            "authors": [
                {
                    "_id": "67f8fccffe86522817d0fb03",
                    "user": {
                        "_id": "6267cdd3f91d1c1633c08bbf",
                        "avatarUrl": "/avatars/51d7961098f52f39ee406009a12982b8.svg",
                        "isPro": false,
                        "fullname": "Artem Zholus",
                        "user": "artemZholus",
                        "type": "user"
                    },
                    "name": "Artem Zholus",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T12:05:00.728Z",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb04",
                    "name": "Carl Doersch",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb05",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb06",
                    "user": {
                        "_id": "65451016321f0393f453cd7b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/XBmF81Uyj0N7lPclU1pUG.jpeg",
                        "isPro": false,
                        "fullname": "Skanda Koppula",
                        "user": "skoppula",
                        "type": "user"
                    },
                    "name": "Skanda Koppula",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T12:07:20.239Z",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb07",
                    "name": "Viorica Patraucean",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb08",
                    "name": "Xu Owen He",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb09",
                    "name": "Ignacio Rocco",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb0a",
                    "user": {
                        "_id": "6733e8c6d7e101ba376800a7",
                        "avatarUrl": "/avatars/5ed957b5cb8e0d6be0f270ed53683622.svg",
                        "isPro": false,
                        "fullname": "Mehdi Sajjadi",
                        "user": "msajjadi",
                        "type": "user"
                    },
                    "name": "Mehdi S. M. Sajjadi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T12:06:47.857Z",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb0b",
                    "user": {
                        "_id": "66fabb66c2bf89d75e8cdd4d",
                        "avatarUrl": "/avatars/c40f55a77e2fb34ba38a79f04df82893.svg",
                        "isPro": false,
                        "fullname": "Sarath Chandar",
                        "user": "apsarath",
                        "type": "user"
                    },
                    "name": "Sarath Chandar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T12:06:39.497Z",
                    "hidden": false
                },
                {
                    "_id": "67f8fccffe86522817d0fb0c",
                    "user": {
                        "_id": "6491a50cfa736a2ad5fa91ee",
                        "avatarUrl": "/avatars/41f7c0bf41e45309d5a86899fa72454f.svg",
                        "isPro": false,
                        "fullname": "Ross Goroshin",
                        "user": "rgoroshin",
                        "type": "user"
                    },
                    "name": "Ross Goroshin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-11T12:06:30.044Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-08T00:28:42.000Z",
            "submittedOnDailyAt": "2025-04-11T09:58:30.676Z",
            "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Tracking Any Point (TAP) in a video is a challenging computer vision problem\nwith many demonstrated applications in robotics, video editing, and 3D\nreconstruction. Existing methods for TAP rely heavily on complex\ntracking-specific inductive biases and heuristics, limiting their generality\nand potential for scaling. To address these challenges, we present TAPNext, a\nnew approach that casts TAP as sequential masked token decoding. Our model is\ncausal, tracks in a purely online fashion, and removes tracking-specific\ninductive biases. This enables TAPNext to run with minimal latency, and removes\nthe temporal windowing required by many existing state of art trackers. Despite\nits simplicity, TAPNext achieves a new state-of-the-art tracking performance\namong both online and offline trackers. Finally, we present evidence that many\nwidely used tracking heuristics emerge naturally in TAPNext through end-to-end\ntraining.",
            "upvotes": 1,
            "discussionId": "67f8fcd3fe86522817d0fc3f",
            "projectPage": "https://tap-next.github.io/",
            "githubRepo": "https://github.com/google-deepmind/tapnet"
        },
        "publishedAt": "2025-04-07T20:28:42.000Z",
        "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
        "summary": "Tracking Any Point (TAP) in a video is a challenging computer vision problem\nwith many demonstrated applications in robotics, video editing, and 3D\nreconstruction. Existing methods for TAP rely heavily on complex\ntracking-specific inductive biases and heuristics, limiting their generality\nand potential for scaling. To address these challenges, we present TAPNext, a\nnew approach that casts TAP as sequential masked token decoding. Our model is\ncausal, tracks in a purely online fashion, and removes tracking-specific\ninductive biases. This enables TAPNext to run with minimal latency, and removes\nthe temporal windowing required by many existing state of art trackers. Despite\nits simplicity, TAPNext achieves a new state-of-the-art tracking performance\namong both online and offline trackers. Finally, we present evidence that many\nwidely used tracking heuristics emerge naturally in TAPNext through end-to-end\ntraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05579.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 816
        },
        "isAuthorParticipating": false
    }
]