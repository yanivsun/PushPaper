[
    {
        "paper": {
            "id": "2504.08685",
            "authors": [
                {
                    "_id": "67fc6ffc59b22e7c34d64c2e",
                    "name": "Team Seawead",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c2f",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c30",
                    "user": {
                        "_id": "64415957bd0c9726529802f6",
                        "avatarUrl": "/avatars/1132d1ee68fb58ec635d57c8175caacd.svg",
                        "isPro": false,
                        "fullname": "Zhijie Lin",
                        "user": "Ikuinen",
                        "type": "user"
                    },
                    "name": "Zhijie Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-14T13:17:03.433Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c31",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c32",
                    "name": "Shanchuan Lin",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c33",
                    "user": {
                        "_id": "645d5a44680734460f9e7742",
                        "avatarUrl": "/avatars/66687e2c413acce55436d967071f8786.svg",
                        "isPro": false,
                        "fullname": "Zhibei Ma",
                        "user": "Brightmzb",
                        "type": "user"
                    },
                    "name": "Zhibei Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-14T13:19:16.342Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c34",
                    "name": "Haoyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c35",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c36",
                    "name": "Lu Qi",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c37",
                    "name": "Sen Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c38",
                    "name": "Feng Cheng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c39",
                    "name": "Feilong Zuo Xuejiao Zeng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3a",
                    "user": {
                        "_id": "63c98de85fdc575773c52098",
                        "avatarUrl": "/avatars/b66675e781695daed7c00a7c802c91f5.svg",
                        "isPro": false,
                        "fullname": "Ziyan Yang",
                        "user": "ziyany",
                        "type": "user"
                    },
                    "name": "Ziyan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-14T13:20:01.373Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3b",
                    "name": "Fangyuan Kong",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3c",
                    "name": "Zhiwu Qing",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3d",
                    "name": "Fei Xiao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3e",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c3f",
                    "name": "Tuyen Hoang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c40",
                    "name": "Siyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c41",
                    "name": "Peihao Zhu",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c42",
                    "name": "Qi Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c43",
                    "name": "Jiangqiao Yan",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c44",
                    "name": "Liangke Gui",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c45",
                    "name": "Sheng Bi",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c46",
                    "name": "Jiashi Li",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c47",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c48",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c49",
                    "name": "Huixia Li",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4a",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4b",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4c",
                    "user": {
                        "_id": "636a4e4fa55bbbdf8c877667",
                        "avatarUrl": "/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg",
                        "isPro": false,
                        "fullname": "Ling Feng",
                        "user": "lingff",
                        "type": "user"
                    },
                    "name": "Feng Ling",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:31.964Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4d",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4e",
                    "name": "Houmin Wei",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c4f",
                    "name": "Huafeng Kuang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c50",
                    "name": "Jerry Duncan",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c51",
                    "name": "Junda Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c52",
                    "name": "Junru Zheng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c53",
                    "name": "Li Sun",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c54",
                    "name": "Manlin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c55",
                    "name": "Renfei Sun",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c56",
                    "name": "Xiaobin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c57",
                    "name": "Xiaojie Li",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c58",
                    "name": "Xin Xia",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c59",
                    "name": "Xuyan Chi",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5a",
                    "name": "Yanghua Peng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5b",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5c",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5d",
                    "name": "Zhongkai Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5e",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c5f",
                    "name": "Zuquan Song",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c60",
                    "user": {
                        "_id": "6421183b69a2c2933882d652",
                        "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
                        "isPro": false,
                        "fullname": "Zhenheng Yang",
                        "user": "zhenheny",
                        "type": "user"
                    },
                    "name": "Zhenheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:34.053Z",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c61",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c62",
                    "name": "Jianchao Yang",
                    "hidden": false
                },
                {
                    "_id": "67fc6ffc59b22e7c34d64c63",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
            ],
            "publishedAt": "2025-04-11T16:46:20.000Z",
            "submittedOnDailyAt": "2025-04-14T00:55:27.428Z",
            "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
            "submittedOnDailyBy": {
                "_id": "64a5cba3bea0116f8f7187a7",
                "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
                "isPro": false,
                "fullname": "Lu Jiang",
                "user": "roadjiang",
                "type": "user"
            },
            "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
            "upvotes": 83,
            "discussionId": "67fc700159b22e7c34d64d78",
            "projectPage": "https://seaweed.video/"
        },
        "publishedAt": "2025-04-11T12:46:20.000Z",
        "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
        "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png",
        "numComments": 9,
        "submittedBy": {
            "_id": "64a5cba3bea0116f8f7187a7",
            "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
            "fullname": "Lu Jiang",
            "name": "roadjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08736",
            "authors": [
                {
                    "_id": "67fc8e37864dfcbd93d3b802",
                    "user": {
                        "_id": "668125557b50b433cda2a211",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
                        "isPro": false,
                        "fullname": "Tianwei Xiong",
                        "user": "YuuTennYi",
                        "type": "user"
                    },
                    "name": "Tianwei Xiong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:29.330Z",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b803",
                    "name": "Jun Hao Liew",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b804",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b805",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "67fc8e37864dfcbd93d3b806",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:26.140Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T17:59:58.000Z",
            "submittedOnDailyAt": "2025-04-14T02:57:04.488Z",
            "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
            "submittedOnDailyBy": {
                "_id": "668125557b50b433cda2a211",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
                "isPro": false,
                "fullname": "Tianwei Xiong",
                "user": "YuuTennYi",
                "type": "user"
            },
            "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
            "upvotes": 32,
            "discussionId": "67fc8e38864dfcbd93d3b836",
            "projectPage": "https://silentview.github.io/GigaTok/",
            "githubRepo": "https://github.com/SilentView/GigaTok"
        },
        "publishedAt": "2025-04-11T13:59:58.000Z",
        "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
        "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668125557b50b433cda2a211",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
            "fullname": "Tianwei Xiong",
            "name": "YuuTennYi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08388",
            "authors": [
                {
                    "_id": "67fc7367df5f5d1e87c14c6a",
                    "name": "Junliang Guo",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6b",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6c",
                    "name": "Tianyu He",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6d",
                    "name": "Haoyu Wu",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6e",
                    "name": "Yushu Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c6f",
                    "name": "Tim Pearce",
                    "hidden": false
                },
                {
                    "_id": "67fc7367df5f5d1e87c14c70",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T09:41:04.000Z",
            "submittedOnDailyAt": "2025-04-14T02:07:06.500Z",
            "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
            "upvotes": 25,
            "discussionId": "67fc7367df5f5d1e87c14ca6",
            "githubRepo": "https://github.com/microsoft/MineWorld"
        },
        "publishedAt": "2025-04-11T05:41:04.000Z",
        "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
        "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 46
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08600",
            "authors": [
                {
                    "_id": "67fc9e72b2383c63dc413dcb",
                    "name": "Peixian Ma",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dcc",
                    "user": {
                        "_id": "6575a625b951d40e7a4d8685",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
                        "isPro": false,
                        "fullname": "zhuangxialie",
                        "user": "ZhuangXialie",
                        "type": "user"
                    },
                    "name": "Xialie Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:23.810Z",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dcd",
                    "name": "Chengjin Xu",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dce",
                    "name": "Xuhui Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dcf",
                    "name": "Ran Chen",
                    "hidden": false
                },
                {
                    "_id": "67fc9e72b2383c63dc413dd0",
                    "name": "Jian Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T15:01:30.000Z",
            "submittedOnDailyAt": "2025-04-14T04:07:17.501Z",
            "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6575a625b951d40e7a4d8685",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
                "isPro": false,
                "fullname": "zhuangxialie",
                "user": "ZhuangXialie",
                "type": "user"
            },
            "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
            "upvotes": 15,
            "discussionId": "67fc9e73b2383c63dc413e19"
        },
        "publishedAt": "2025-04-11T11:01:30.000Z",
        "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
        "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6575a625b951d40e7a4d8685",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
            "fullname": "zhuangxialie",
            "name": "ZhuangXialie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08591",
            "authors": [
                {
                    "_id": "67fd2584b12728f2678f7dfb",
                    "user": {
                        "_id": "635b6bdfd63bcb528acefa78",
                        "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
                        "isPro": false,
                        "fullname": "Yongsheng Yu",
                        "user": "yeates",
                        "type": "user"
                    },
                    "name": "Yongsheng Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T18:39:46.656Z",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7dfc",
                    "name": "Haitian Zheng",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7dfd",
                    "name": "Zhifei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7dfe",
                    "name": "Jianming Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7dff",
                    "name": "Yuqian Zhou",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7e00",
                    "name": "Connelly Barnes",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7e01",
                    "name": "Yuchen Liu",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7e02",
                    "name": "Wei Xiong",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7e03",
                    "name": "Zhe Lin",
                    "hidden": false
                },
                {
                    "_id": "67fd2584b12728f2678f7e04",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/635b6bdfd63bcb528acefa78/667lI2aZC6G9xT5XrY0Kw.jpeg"
            ],
            "publishedAt": "2025-04-11T14:49:52.000Z",
            "submittedOnDailyAt": "2025-04-14T13:43:16.603Z",
            "title": "ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image\n  Restoration",
            "submittedOnDailyBy": {
                "_id": "635b6bdfd63bcb528acefa78",
                "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
                "isPro": false,
                "fullname": "Yongsheng Yu",
                "user": "yeates",
                "type": "user"
            },
            "summary": "Recent progress in generative models has significantly improved image\nrestoration capabilities, particularly through powerful diffusion models that\noffer remarkable recovery of semantic details and local fidelity. However,\ndeploying these models at ultra-high resolutions faces a critical trade-off\nbetween quality and efficiency due to the computational demands of long-range\nattention mechanisms. To address this, we introduce ZipIR, a novel framework\nthat enhances efficiency, scalability, and long-range modeling for high-res\nimage restoration. ZipIR employs a highly compressed latent representation that\ncompresses image 32x, effectively reducing the number of spatial tokens, and\nenabling the use of high-capacity models like the Diffusion Transformer (DiT).\nToward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that\nstructures the latent space into sub-bands to ease diffusion training. Trained\non full images up to 2K resolution, ZipIR surpasses existing diffusion-based\nmethods, offering unmatched speed and quality in restoring high-resolution\nimages from severely degraded inputs.",
            "upvotes": 13,
            "discussionId": "67fd2590b12728f2678f80d2"
        },
        "publishedAt": "2025-04-11T10:49:52.000Z",
        "title": "ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image\n  Restoration",
        "summary": "Recent progress in generative models has significantly improved image\nrestoration capabilities, particularly through powerful diffusion models that\noffer remarkable recovery of semantic details and local fidelity. However,\ndeploying these models at ultra-high resolutions faces a critical trade-off\nbetween quality and efficiency due to the computational demands of long-range\nattention mechanisms. To address this, we introduce ZipIR, a novel framework\nthat enhances efficiency, scalability, and long-range modeling for high-res\nimage restoration. ZipIR employs a highly compressed latent representation that\ncompresses image 32x, effectively reducing the number of spatial tokens, and\nenabling the use of high-capacity models like the Diffusion Transformer (DiT).\nToward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that\nstructures the latent space into sub-bands to ease diffusion training. Trained\non full images up to 2K resolution, ZipIR surpasses existing diffusion-based\nmethods, offering unmatched speed and quality in restoring high-resolution\nimages from severely degraded inputs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/635b6bdfd63bcb528acefa78/667lI2aZC6G9xT5XrY0Kw.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08591.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635b6bdfd63bcb528acefa78",
            "avatarUrl": "/avatars/54c62b41c147d0450bdfc72121860bb4.svg",
            "fullname": "Yongsheng Yu",
            "name": "yeates",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07615",
            "authors": [
                {
                    "_id": "67fc52b190075b82590d75db",
                    "name": "Haozhan Shen",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75dc",
                    "name": "Peng Liu",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75dd",
                    "name": "Jingcheng Li",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75de",
                    "name": "Chunxin Fang",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75df",
                    "name": "Yibo Ma",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e0",
                    "name": "Jiajia Liao",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e1",
                    "name": "Qiaoli Shen",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e2",
                    "name": "Zilun Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e3",
                    "name": "Kangjia Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e4",
                    "name": "Qianqian Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e5",
                    "name": "Ruochen Xu",
                    "hidden": false
                },
                {
                    "_id": "67fc52b190075b82590d75e6",
                    "name": "Tiancheng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T10:05:15.000Z",
            "submittedOnDailyAt": "2025-04-14T13:37:56.110Z",
            "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
            "submittedOnDailyBy": {
                "_id": "5f0de36419cb630495b8153c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658676776546-5f0de36419cb630495b8153c.jpeg",
                "isPro": false,
                "fullname": "Tony Zhao",
                "user": "tianchez",
                "type": "user"
            },
            "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1",
            "upvotes": 11,
            "discussionId": "67fc52b390075b82590d7634",
            "githubRepo": "https://github.com/om-ai-lab/VLM-R1"
        },
        "publishedAt": "2025-04-10T06:05:15.000Z",
        "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
        "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07615.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f0de36419cb630495b8153c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658676776546-5f0de36419cb630495b8153c.jpeg",
            "fullname": "Tony Zhao",
            "name": "tianchez",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.07963",
            "authors": [
                {
                    "_id": "67f86da6ac109135e18e150f",
                    "name": "Shoufa Chen",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1510",
                    "name": "Chongjian Ge",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1511",
                    "name": "Shilong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1512",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "67f86da6ac109135e18e1513",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
            ],
            "publishedAt": "2025-04-10T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-14T04:05:17.975Z",
            "title": "PixelFlow: Pixel-Space Generative Models with Flow",
            "submittedOnDailyBy": {
                "_id": "6412a33900634c4fe9873652",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
                "isPro": false,
                "fullname": "Shoufa Chen",
                "user": "ShoufaChen",
                "type": "user"
            },
            "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
            "upvotes": 10,
            "discussionId": "67f86da7ac109135e18e154b",
            "githubRepo": "https://github.com/ShoufaChen/PixelFlow"
        },
        "publishedAt": "2025-04-10T13:59:56.000Z",
        "title": "PixelFlow: Pixel-Space Generative Models with Flow",
        "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "fullname": "Shoufa Chen",
            "name": "ShoufaChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.07405",
            "authors": [
                {
                    "_id": "67fa383909d06d0501a5e34e",
                    "user": {
                        "_id": "660d844462d63ad0009a9859",
                        "avatarUrl": "/avatars/6822fc1a82c64dbfd40b88080a5fb1ae.svg",
                        "isPro": false,
                        "fullname": "Linyan Huang",
                        "user": "DevLinyan",
                        "type": "user"
                    },
                    "name": "Linyan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-13T19:24:47.680Z",
                    "hidden": false
                },
                {
                    "_id": "67fa383909d06d0501a5e34f",
                    "name": "Haonan Lin",
                    "hidden": false
                },
                {
                    "_id": "67fa383909d06d0501a5e350",
                    "name": "Yanning Zhou",
                    "hidden": false
                },
                {
                    "_id": "67fa383909d06d0501a5e351",
                    "name": "Kaiwen Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T02:58:22.000Z",
            "submittedOnDailyAt": "2025-04-14T00:54:04.513Z",
            "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).",
            "upvotes": 7,
            "discussionId": "67fa383c09d06d0501a5e3ef",
            "projectPage": "https://flexip-tech.github.io/flexip"
        },
        "publishedAt": "2025-04-09T22:58:22.000Z",
        "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
        "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 46
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.05262",
            "authors": [
                {
                    "_id": "67fcc9a980568c7ef6180dcb",
                    "user": {
                        "_id": "62cd5917299c0c2e0e435847",
                        "avatarUrl": "/avatars/b956b4feab86f6866c43cc87a44e25fc.svg",
                        "isPro": false,
                        "fullname": "Yang Yan",
                        "user": "kurileo",
                        "type": "user"
                    },
                    "name": "Yang Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T12:52:13.346Z",
                    "hidden": false
                },
                {
                    "_id": "67fcc9a980568c7ef6180dcc",
                    "name": "Yu Lu",
                    "hidden": false
                },
                {
                    "_id": "67fcc9a980568c7ef6180dcd",
                    "name": "Renjun Xu",
                    "hidden": false
                },
                {
                    "_id": "67fcc9a980568c7ef6180dce",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T16:57:10.000Z",
            "submittedOnDailyAt": "2025-04-14T07:11:56.706Z",
            "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "62ce6dd785cfd21c04c7e6f5",
                "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
                "isPro": false,
                "fullname": "ZhenzhongLan",
                "user": "DannyLan",
                "type": "user"
            },
            "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to leq7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
            "upvotes": 7,
            "discussionId": "67fcc9aa80568c7ef6180e24"
        },
        "publishedAt": "2025-04-07T12:57:10.000Z",
        "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
        "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to leq7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "62ce6dd785cfd21c04c7e6f5",
            "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
            "fullname": "ZhenzhongLan",
            "name": "DannyLan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07866",
            "authors": [
                {
                    "_id": "67f90407614ff5204ab48c23",
                    "name": "Yichun Yin",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c24",
                    "name": "Wenyong Huang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c25",
                    "name": "Kaikai Song",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c26",
                    "name": "Yehui Tang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c27",
                    "name": "Xueyu Wu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c28",
                    "name": "Wei Guo",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c29",
                    "name": "Peng Guo",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c2a",
                    "name": "Yaoyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c2b",
                    "name": "Xiaojun Meng",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c2c",
                    "name": "Yasheng Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c2d",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c2e",
                    "name": "Can Chen",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c2f",
                    "name": "Dandan Tu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c30",
                    "name": "Yin Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c31",
                    "name": "Fisher Yu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c32",
                    "name": "Ruiming Tang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c33",
                    "name": "Yunhe Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c34",
                    "name": "Baojun Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c35",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c36",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c37",
                    "name": "Boxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c38",
                    "name": "Changzheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c39",
                    "name": "Duyu Tang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c3a",
                    "name": "Fei Mi",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c3b",
                    "name": "Hui Jin",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c3c",
                    "name": "Jiansheng Wei",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c3d",
                    "name": "Jiarui Qin",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c3e",
                    "name": "Jinpeng Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c3f",
                    "name": "Jun Zhao",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c40",
                    "name": "Liqun Deng",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c41",
                    "name": "Lin Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c42",
                    "name": "Minghui Xu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c43",
                    "name": "Naifu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c44",
                    "name": "Nianzu Zheng",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c45",
                    "name": "Qiang Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c46",
                    "name": "Rongju Ruan",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c47",
                    "name": "Shengjun Cheng",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c48",
                    "name": "Tianyu Guo",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c49",
                    "name": "Wei He",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c4a",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c4b",
                    "name": "Weiwen Liu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c4c",
                    "name": "Wulong Liu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c4d",
                    "name": "Xinyi Dai",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c4e",
                    "name": "Yonghan Dong",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c4f",
                    "name": "Yu Pan",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c50",
                    "name": "Yue Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c51",
                    "name": "Yufei Wang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c52",
                    "name": "Yujun Li",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c53",
                    "name": "Yunsheng Ni",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c54",
                    "name": "Zhe Liu",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c55",
                    "name": "Zhenhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "67f90407614ff5204ab48c56",
                    "name": "Zhicheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T15:41:51.000Z",
            "submittedOnDailyAt": "2025-04-14T09:06:50.156Z",
            "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
                "isPro": false,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.",
            "upvotes": 6,
            "discussionId": "67f90409614ff5204ab48cea"
        },
        "publishedAt": "2025-04-10T11:41:51.000Z",
        "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs",
        "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07866.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 570
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08727",
            "authors": [
                {
                    "_id": "67fd083b3bf3cc207a879c3d",
                    "user": {
                        "_id": "66d3896c6839bd191d0c5ad3",
                        "avatarUrl": "/avatars/34acf042c946b5e7144ce3c002c9939f.svg",
                        "isPro": false,
                        "fullname": "Boyang Deng",
                        "user": "bydeng",
                        "type": "user"
                    },
                    "name": "Boyang Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T13:17:32.224Z",
                    "hidden": false
                },
                {
                    "_id": "67fd083b3bf3cc207a879c3e",
                    "user": {
                        "_id": "620cb0722d8bc91e1cf2e6e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644998719617-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Songyou Peng",
                        "user": "syp",
                        "type": "user"
                    },
                    "name": "Songyou Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T18:40:03.787Z",
                    "hidden": false
                },
                {
                    "_id": "67fd083b3bf3cc207a879c3f",
                    "name": "Kyle Genova",
                    "hidden": false
                },
                {
                    "_id": "67fd083b3bf3cc207a879c40",
                    "name": "Gordon Wetzstein",
                    "hidden": false
                },
                {
                    "_id": "67fd083b3bf3cc207a879c41",
                    "name": "Noah Snavely",
                    "hidden": false
                },
                {
                    "_id": "67fd083b3bf3cc207a879c42",
                    "name": "Leonidas Guibas",
                    "hidden": false
                },
                {
                    "_id": "67fd083b3bf3cc207a879c43",
                    "name": "Thomas Funkhouser",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T17:55:45.000Z",
            "submittedOnDailyAt": "2025-04-14T11:38:13.588Z",
            "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
            "upvotes": 5,
            "discussionId": "67fd08403bf3cc207a879d63"
        },
        "publishedAt": "2025-04-11T13:55:45.000Z",
        "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
        "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08727.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6648
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08716",
            "authors": [
                {
                    "_id": "67fca0ca05cd5b5035123b7e",
                    "user": {
                        "_id": "605350699008a64d7b43b78d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1616431735085-605350699008a64d7b43b78d.jpeg",
                        "isPro": false,
                        "fullname": "Wissam Antoun",
                        "user": "wissamantoun",
                        "type": "user"
                    },
                    "name": "Wissam Antoun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T18:40:06.158Z",
                    "hidden": false
                },
                {
                    "_id": "67fca0ca05cd5b5035123b7f",
                    "name": "Benot Sagot",
                    "hidden": false
                },
                {
                    "_id": "67fca0ca05cd5b5035123b80",
                    "name": "Djam Seddah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T17:29:35.000Z",
            "submittedOnDailyAt": "2025-04-14T04:20:01.034Z",
            "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
            "upvotes": 5,
            "discussionId": "67fca0ca05cd5b5035123ba6"
        },
        "publishedAt": "2025-04-11T13:29:35.000Z",
        "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
        "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2497
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08641",
            "authors": [
                {
                    "_id": "67fd26108a74846fc0ff3065",
                    "name": "Jialu Li",
                    "hidden": false
                },
                {
                    "_id": "67fd26108a74846fc0ff3066",
                    "name": "Shoubin Yu",
                    "hidden": false
                },
                {
                    "_id": "67fd26108a74846fc0ff3067",
                    "name": "Han Lin",
                    "hidden": false
                },
                {
                    "_id": "67fd26108a74846fc0ff3068",
                    "name": "Jaemin Cho",
                    "hidden": false
                },
                {
                    "_id": "67fd26108a74846fc0ff3069",
                    "name": "Jaehong Yoon",
                    "hidden": false
                },
                {
                    "_id": "67fd26108a74846fc0ff306a",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T15:41:43.000Z",
            "submittedOnDailyAt": "2025-04-14T13:43:45.350Z",
            "title": "Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization",
            "submittedOnDailyBy": {
                "_id": "65d69eb0bcf1fed72325a052",
                "avatarUrl": "/avatars/5ef735c3137555bb52e512ae820e437c.svg",
                "isPro": true,
                "fullname": "Jialu Li",
                "user": "jialuliluka",
                "type": "user"
            },
            "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.",
            "upvotes": 4,
            "discussionId": "67fd26128a74846fc0ff30e7",
            "projectPage": "https://video-msg.github.io/",
            "githubRepo": "https://github.com/jialuli-luka/Video-MSG"
        },
        "publishedAt": "2025-04-11T11:41:43.000Z",
        "title": "Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization",
        "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08641.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d69eb0bcf1fed72325a052",
            "avatarUrl": "/avatars/5ef735c3137555bb52e512ae820e437c.svg",
            "fullname": "Jialu Li",
            "name": "jialuliluka",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08366",
            "authors": [
                {
                    "_id": "67fc6d8d74c3c0f0d6f24c3b",
                    "name": "Sauradip Nag",
                    "hidden": false
                },
                {
                    "_id": "67fc6d8d74c3c0f0d6f24c3c",
                    "name": "Daniel Cohen-Or",
                    "hidden": false
                },
                {
                    "_id": "67fc6d8d74c3c0f0d6f24c3d",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fc6d8d74c3c0f0d6f24c3e",
                    "name": "Ali Mahdavi-Amiri",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T09:01:09.000Z",
            "submittedOnDailyAt": "2025-04-14T00:36:54.457Z",
            "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
            "submittedOnDailyBy": {
                "_id": "6399ab3296ce14c5dcf4ccbf",
                "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
                "isPro": false,
                "fullname": "Sauradip Nag",
                "user": "sauradip",
                "type": "user"
            },
            "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/",
            "upvotes": 4,
            "discussionId": "67fc6d9374c3c0f0d6f24e12",
            "projectPage": "https://in-2-4d.github.io/",
            "githubRepo": "https://github.com/sauradip/In-2-4D"
        },
        "publishedAt": "2025-04-11T05:01:09.000Z",
        "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
        "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6399ab3296ce14c5dcf4ccbf",
            "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
            "fullname": "Sauradip Nag",
            "name": "sauradip",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.01883",
            "authors": [
                {
                    "_id": "67fcb50ea69150c25fb4b645",
                    "name": "Aashiq Muhamed",
                    "hidden": false
                },
                {
                    "_id": "67fcb50ea69150c25fb4b646",
                    "name": "Mona Diab",
                    "hidden": false
                },
                {
                    "_id": "67fcb50ea69150c25fb4b647",
                    "name": "Virginia Smith",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T16:40:43.000Z",
            "submittedOnDailyAt": "2025-04-14T05:41:34.796Z",
            "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
            "submittedOnDailyBy": {
                "_id": "64755a83e0b188d3cb2579d8",
                "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
                "isPro": false,
                "fullname": "Aashiq Muhamed",
                "user": "aashiqmuhamed",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
            "upvotes": 4,
            "discussionId": "67fcb510a69150c25fb4b6b1"
        },
        "publishedAt": "2025-04-02T12:40:43.000Z",
        "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
        "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64755a83e0b188d3cb2579d8",
            "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
            "fullname": "Aashiq Muhamed",
            "name": "aashiqmuhamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.08635",
            "authors": [
                {
                    "_id": "67fd028b27381665ef977a14",
                    "user": {
                        "_id": "664de98d04ee28f9f944af1c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-HAg0eaZv4PXRfEcBWqhL.png",
                        "isPro": false,
                        "fullname": "Gabriele Lozupone",
                        "user": "gabrielelozupone98",
                        "type": "user"
                    },
                    "name": "Gabriele Lozupone",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-14T12:51:10.674Z",
                    "hidden": false
                },
                {
                    "_id": "67fd028b27381665ef977a15",
                    "name": "Alessandro Bria",
                    "hidden": false
                },
                {
                    "_id": "67fd028b27381665ef977a16",
                    "name": "Francesco Fontanella",
                    "hidden": false
                },
                {
                    "_id": "67fd028b27381665ef977a17",
                    "name": "Frederick J. A. Meijer",
                    "hidden": false
                },
                {
                    "_id": "67fd028b27381665ef977a18",
                    "name": "Claudio De Stefano",
                    "hidden": false
                },
                {
                    "_id": "67fd028b27381665ef977a19",
                    "name": "Henkjan Huisman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T15:37:46.000Z",
            "submittedOnDailyAt": "2025-04-14T11:16:25.493Z",
            "title": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging",
            "submittedOnDailyBy": {
                "_id": "664de98d04ee28f9f944af1c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-HAg0eaZv4PXRfEcBWqhL.png",
                "isPro": false,
                "fullname": "Gabriele Lozupone",
                "user": "gabrielelozupone98",
                "type": "user"
            },
            "summary": "This study presents Latent Diffusion Autoencoder (LDAE), a novel\nencoder-decoder diffusion-based framework for efficient and meaningful\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\nusing brain MR from the ADNI database as a case study. Unlike conventional\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\nprocess in a compressed latent representation, improving computational\nefficiency and making 3D medical imaging representation learning tractable. To\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\neffectively captures meaningful semantic representations on 3D brain MR\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\ngeneration and reconstruction while being computationally efficient.\nExperimental results support both hypotheses: (i) linear-probe evaluations\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\nrepresentations enable attribute manipulation, yielding anatomically plausible\nmodifications; (iii) semantic interpolation experiments show strong\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\ngap. Even for longer gaps (24 months), the model maintains robust performance\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\nsignificantly increases inference throughput (20x faster) while also enhancing\nreconstruction quality. These findings position LDAE as a promising framework\nfor scalable medical imaging applications, with the potential to serve as a\nfoundation model for medical image analysis. Code available at\nhttps://github.com/GabrieleLozupone/LDAE",
            "upvotes": 2,
            "discussionId": "67fd028c27381665ef977a8f",
            "githubRepo": "https://github.com/GabrieleLozupone/LDAE"
        },
        "publishedAt": "2025-04-11T11:37:46.000Z",
        "title": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging",
        "summary": "This study presents Latent Diffusion Autoencoder (LDAE), a novel\nencoder-decoder diffusion-based framework for efficient and meaningful\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\nusing brain MR from the ADNI database as a case study. Unlike conventional\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\nprocess in a compressed latent representation, improving computational\nefficiency and making 3D medical imaging representation learning tractable. To\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\neffectively captures meaningful semantic representations on 3D brain MR\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\ngeneration and reconstruction while being computationally efficient.\nExperimental results support both hypotheses: (i) linear-probe evaluations\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\nrepresentations enable attribute manipulation, yielding anatomically plausible\nmodifications; (iii) semantic interpolation experiments show strong\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\ngap. Even for longer gaps (24 months), the model maintains robust performance\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\nsignificantly increases inference throughput (20x faster) while also enhancing\nreconstruction quality. These findings position LDAE as a promising framework\nfor scalable medical imaging applications, with the potential to serve as a\nfoundation model for medical image analysis. Code available at\nhttps://github.com/GabrieleLozupone/LDAE",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08635.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664de98d04ee28f9f944af1c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-HAg0eaZv4PXRfEcBWqhL.png",
            "fullname": "Gabriele Lozupone",
            "name": "gabrielelozupone98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08192",
            "authors": [
                {
                    "_id": "67fcb3584a92187863e732d5",
                    "name": "Aashiq Muhamed",
                    "hidden": false
                },
                {
                    "_id": "67fcb3584a92187863e732d6",
                    "name": "Jacopo Bonato",
                    "hidden": false
                },
                {
                    "_id": "67fcb3584a92187863e732d7",
                    "name": "Mona Diab",
                    "hidden": false
                },
                {
                    "_id": "67fcb3584a92187863e732d8",
                    "name": "Virginia Smith",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T01:24:03.000Z",
            "submittedOnDailyAt": "2025-04-14T05:34:15.388Z",
            "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
            "submittedOnDailyBy": {
                "_id": "64755a83e0b188d3cb2579d8",
                "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
                "isPro": false,
                "fullname": "Aashiq Muhamed",
                "user": "aashiqmuhamed",
                "type": "user"
            },
            "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
            "upvotes": 2,
            "discussionId": "67fcb3594a92187863e732fa"
        },
        "publishedAt": "2025-04-10T21:24:03.000Z",
        "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
        "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64755a83e0b188d3cb2579d8",
            "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
            "fullname": "Aashiq Muhamed",
            "name": "aashiqmuhamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.06908",
            "authors": [
                {
                    "_id": "67fd59c56a3c533dc1b21c91",
                    "name": "Emmanuelle Bourigault",
                    "hidden": false
                },
                {
                    "_id": "67fd59c56a3c533dc1b21c92",
                    "name": "Amir Jamaludin",
                    "hidden": false
                },
                {
                    "_id": "67fd59c56a3c533dc1b21c93",
                    "name": "Abdullah Hamdi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T14:10:51.000Z",
            "submittedOnDailyAt": "2025-04-14T17:24:24.534Z",
            "title": "UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "In medical imaging, the primary challenge is collecting large-scale labeled\ndata due to privacy concerns, logistics, and high labeling costs. In this work,\nwe present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset\nof body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D\nimages) and more than 1.37 billion 2D segmentation masks of 72 organs, all\nbased on the UK Biobank MRI dataset. We utilize automatic labeling, introduce\nan automated label cleaning pipeline with organ-specific filters, and manually\nannotate a subset of 300 MRIs with 11 abdominal classes to validate the quality\n(referred to as UKBOB-manual). This approach allows for scaling up the dataset\ncollection while maintaining confidence in the labels. We further confirm the\nvalidity of the labels by demonstrating zero-shot generalization of trained\nmodels on the filtered UKBOB to other small labeled datasets from similar\ndomains (e.g., abdominal MRI). To further mitigate the effect of noisy labels,\nwe propose a novel method called Entropy Test-time Adaptation (ETTA) to refine\nthe segmentation output. We use UKBOB to train a foundation model, Swin-BOB,\nfor 3D medical image segmentation based on the Swin-UNetr architecture,\nachieving state-of-the-art results in several benchmarks in 3D medical imaging,\nincluding the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the\nBTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained\nmodels and the code are available at https://emmanuelleb985.github.io/ukbob ,\nand the filtered labels will be made available with the UK Biobank.",
            "upvotes": 2,
            "discussionId": "67fd59cb6a3c533dc1b21df7"
        },
        "publishedAt": "2025-04-09T10:10:51.000Z",
        "title": "UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image\n  Segmentation",
        "summary": "In medical imaging, the primary challenge is collecting large-scale labeled\ndata due to privacy concerns, logistics, and high labeling costs. In this work,\nwe present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset\nof body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D\nimages) and more than 1.37 billion 2D segmentation masks of 72 organs, all\nbased on the UK Biobank MRI dataset. We utilize automatic labeling, introduce\nan automated label cleaning pipeline with organ-specific filters, and manually\nannotate a subset of 300 MRIs with 11 abdominal classes to validate the quality\n(referred to as UKBOB-manual). This approach allows for scaling up the dataset\ncollection while maintaining confidence in the labels. We further confirm the\nvalidity of the labels by demonstrating zero-shot generalization of trained\nmodels on the filtered UKBOB to other small labeled datasets from similar\ndomains (e.g., abdominal MRI). To further mitigate the effect of noisy labels,\nwe propose a novel method called Entropy Test-time Adaptation (ETTA) to refine\nthe segmentation output. We use UKBOB to train a foundation model, Swin-BOB,\nfor 3D medical image segmentation based on the Swin-UNetr architecture,\nachieving state-of-the-art results in several benchmarks in 3D medical imaging,\nincluding the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the\nBTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained\nmodels and the code are available at https://emmanuelleb985.github.io/ukbob ,\nand the filtered labels will be made available with the UK Biobank.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06908.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 818
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.01786",
            "authors": [
                {
                    "_id": "67fd668da1510fb6779d80af",
                    "user": {
                        "_id": "649c8613667e8e1426b80c49",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WcSUg5ytefyeoKIAdabM2.png",
                        "isPro": false,
                        "fullname": "Yunqi Gu(Richard)",
                        "user": "richard-guyunqi",
                        "type": "user"
                    },
                    "name": "Yunqi Gu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-14T19:56:38.400Z",
                    "hidden": false
                },
                {
                    "_id": "67fd668da1510fb6779d80b0",
                    "user": {
                        "_id": "639ce51c66106be14370f510",
                        "avatarUrl": "/avatars/74f68f5951fff9c02c7607ff197812aa.svg",
                        "isPro": false,
                        "fullname": "Ian Huang",
                        "user": "ianhuang",
                        "type": "user"
                    },
                    "name": "Ian Huang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-14T19:48:33.297Z",
                    "hidden": false
                },
                {
                    "_id": "67fd668da1510fb6779d80b1",
                    "user": {
                        "_id": "65b96839f8f4f76f9100381b",
                        "avatarUrl": "/avatars/8eb3017043b86468ab324c6795382055.svg",
                        "isPro": false,
                        "fullname": "Jihyeon Je",
                        "user": "jihyeonj",
                        "type": "user"
                    },
                    "name": "Jihyeon Je",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-14T20:04:18.253Z",
                    "hidden": false
                },
                {
                    "_id": "67fd668da1510fb6779d80b2",
                    "name": "Guandao Yang",
                    "hidden": false
                },
                {
                    "_id": "67fd668da1510fb6779d80b3",
                    "name": "Leonidas Guibas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-02T14:51:45.000Z",
            "submittedOnDailyAt": "2025-04-14T18:28:11.697Z",
            "title": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing",
            "submittedOnDailyBy": {
                "_id": "649c8613667e8e1426b80c49",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WcSUg5ytefyeoKIAdabM2.png",
                "isPro": false,
                "fullname": "Yunqi Gu(Richard)",
                "user": "richard-guyunqi",
                "type": "user"
            },
            "summary": "3D graphics editing is crucial in applications like movie production and game\ndesign, yet it remains a time-consuming process that demands highly specialized\ndomain expertise. Automating this process is challenging because graphical\nediting requires performing a variety of tasks, each requiring distinct skill\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\nframework for automating the editing process, but their development and\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\nrequires human-level perception and presents real-world editing complexity. In\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\nobserve that even the state-of-the-art VLM system struggles with tasks\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\ninference scaling techniques impact VLM's performance on graphics editing\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\nof generation can itself be improved through inference scaling, complementing\nrecent insights on inference scaling of LLM generation in coding and math\ntasks. We further show that inference compute is not uniformly effective and\ncan be optimized by strategically distributing it between generation and\nverification.",
            "upvotes": 2,
            "discussionId": "67fd6691a1510fb6779d8166"
        },
        "publishedAt": "2025-04-02T10:51:45.000Z",
        "title": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing",
        "summary": "3D graphics editing is crucial in applications like movie production and game\ndesign, yet it remains a time-consuming process that demands highly specialized\ndomain expertise. Automating this process is challenging because graphical\nediting requires performing a variety of tasks, each requiring distinct skill\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\nframework for automating the editing process, but their development and\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\nrequires human-level perception and presents real-world editing complexity. In\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\nobserve that even the state-of-the-art VLM system struggles with tasks\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\ninference scaling techniques impact VLM's performance on graphics editing\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\nof generation can itself be improved through inference scaling, complementing\nrecent insights on inference scaling of LLM generation in coding and math\ntasks. We further show that inference compute is not uniformly effective and\ncan be optimized by strategically distributing it between generation and\nverification.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01786.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649c8613667e8e1426b80c49",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WcSUg5ytefyeoKIAdabM2.png",
            "fullname": "Yunqi Gu(Richard)",
            "name": "richard-guyunqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07891",
            "authors": [
                {
                    "_id": "67fdaecd1c5d730135243f5c",
                    "name": "Rui Pan",
                    "hidden": false
                },
                {
                    "_id": "67fdaecd1c5d730135243f5d",
                    "name": "Yinwei Dai",
                    "hidden": false
                },
                {
                    "_id": "67fdaecd1c5d730135243f5e",
                    "user": {
                        "_id": "6480ec2d856901b0edbbd47a",
                        "avatarUrl": "/avatars/dd8c22589c46fc19d7bf4a1329692d91.svg",
                        "isPro": false,
                        "fullname": "Zhihao Jia",
                        "user": "zhihaojia",
                        "type": "user"
                    },
                    "name": "Zhihao Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-15T00:56:48.883Z",
                    "hidden": false
                },
                {
                    "_id": "67fdaecd1c5d730135243f5f",
                    "name": "Gabriele Oliaro",
                    "hidden": false
                },
                {
                    "_id": "67fdaecd1c5d730135243f60",
                    "name": "Zhihao Jia",
                    "hidden": false
                },
                {
                    "_id": "67fdaecd1c5d730135243f61",
                    "name": "Ravi Netravali",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T16:05:19.000Z",
            "submittedOnDailyAt": "2025-04-14T23:27:23.780Z",
            "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "62221f7af14f39d27896de15",
                "avatarUrl": "/avatars/6318269df8c77e51f16445c29d65307c.svg",
                "isPro": false,
                "fullname": "Rui Pan",
                "user": "ruipeterpan",
                "type": "user"
            },
            "summary": "Recent advances in inference-time compute have significantly improved\nperformance on complex tasks by generating long chains of thought (CoTs) using\nLarge Reasoning Models (LRMs). However, this improved accuracy comes at the\ncost of high inference latency due to the length of generated reasoning\nsequences and the autoregressive nature of decoding. Our key insight in\ntackling these overheads is that LRM inference, and the reasoning that it\nembeds, is highly tolerant of approximations: complex tasks are typically\nbroken down into simpler steps, each of which brings utility based on the\nsemantic insight it provides for downstream steps rather than the exact tokens\nit generates. Accordingly, we introduce SpecReason, a system that automatically\naccelerates LRM inference by using a lightweight model to (speculatively) carry\nout simpler intermediate reasoning steps and reserving the costly base model\nonly to assess (and potentially correct) the speculated outputs. Importantly,\nSpecReason's focus on exploiting the semantic flexibility of thinking tokens in\npreserving final-answer accuracy is complementary to prior speculation\ntechniques, most notably speculative decoding, which demands token-level\nequivalence at each step. Across a variety of reasoning benchmarks, SpecReason\nachieves 1.5-2.5times speedup over vanilla LRM inference while improving\naccuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason,\ntheir combination yields an additional 19.4-44.2\\% latency reduction. We\nopen-source SpecReason at https://github.com/ruipeterpan/specreason.",
            "upvotes": 0,
            "discussionId": "67fdaed01c5d730135244062",
            "githubRepo": "https://github.com/ruipeterpan/specreason"
        },
        "publishedAt": "2025-04-10T12:05:19.000Z",
        "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative\n  Reasoning",
        "summary": "Recent advances in inference-time compute have significantly improved\nperformance on complex tasks by generating long chains of thought (CoTs) using\nLarge Reasoning Models (LRMs). However, this improved accuracy comes at the\ncost of high inference latency due to the length of generated reasoning\nsequences and the autoregressive nature of decoding. Our key insight in\ntackling these overheads is that LRM inference, and the reasoning that it\nembeds, is highly tolerant of approximations: complex tasks are typically\nbroken down into simpler steps, each of which brings utility based on the\nsemantic insight it provides for downstream steps rather than the exact tokens\nit generates. Accordingly, we introduce SpecReason, a system that automatically\naccelerates LRM inference by using a lightweight model to (speculatively) carry\nout simpler intermediate reasoning steps and reserving the costly base model\nonly to assess (and potentially correct) the speculated outputs. Importantly,\nSpecReason's focus on exploiting the semantic flexibility of thinking tokens in\npreserving final-answer accuracy is complementary to prior speculation\ntechniques, most notably speculative decoding, which demands token-level\nequivalence at each step. Across a variety of reasoning benchmarks, SpecReason\nachieves 1.5-2.5times speedup over vanilla LRM inference while improving\naccuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason,\ntheir combination yields an additional 19.4-44.2\\% latency reduction. We\nopen-source SpecReason at https://github.com/ruipeterpan/specreason.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62221f7af14f39d27896de15",
            "avatarUrl": "/avatars/6318269df8c77e51f16445c29d65307c.svg",
            "fullname": "Rui Pan",
            "name": "ruipeterpan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.05303",
            "authors": [
                {
                    "_id": "67fcbbe8daf0cf6803943949",
                    "user": {
                        "_id": "6492bf9681d93008eb33f167",
                        "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
                        "isPro": false,
                        "fullname": "Sai Kumar Dwivedi",
                        "user": "saidwivedi",
                        "type": "user"
                    },
                    "name": "Sai Kumar Dwivedi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:12.532Z",
                    "hidden": false
                },
                {
                    "_id": "67fcbbe8daf0cf680394394a",
                    "name": "Dimitrije Anti",
                    "hidden": false
                },
                {
                    "_id": "67fcbbe8daf0cf680394394b",
                    "name": "Shashank Tripathi",
                    "hidden": false
                },
                {
                    "_id": "67fcbbe8daf0cf680394394c",
                    "name": "Omid Taheri",
                    "hidden": false
                },
                {
                    "_id": "67fcbbe8daf0cf680394394d",
                    "name": "Cordelia Schmid",
                    "hidden": false
                },
                {
                    "_id": "67fcbbe8daf0cf680394394e",
                    "name": "Michael J. Black",
                    "hidden": false
                },
                {
                    "_id": "67fcbbe8daf0cf680394394f",
                    "name": "Dimitrios Tzionas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T17:59:33.000Z",
            "submittedOnDailyAt": "2025-04-14T06:14:23.936Z",
            "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
            "submittedOnDailyBy": {
                "_id": "6492bf9681d93008eb33f167",
                "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
                "isPro": false,
                "fullname": "Sai Kumar Dwivedi",
                "user": "saidwivedi",
                "type": "user"
            },
            "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
            "upvotes": 0,
            "discussionId": "67fcbbeadaf0cf68039439b9",
            "projectPage": "https://interactvlm.is.tue.mpg.de/",
            "githubRepo": "https://github.com/saidwivedi/InteractVLM"
        },
        "publishedAt": "2025-04-07T13:59:33.000Z",
        "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
        "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6492bf9681d93008eb33f167",
            "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
            "fullname": "Sai Kumar Dwivedi",
            "name": "saidwivedi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]