[
    {
        "paper": {
            "id": "2504.10479",
            "authors": [
                {
                    "_id": "67fdd08bda7816922cb67e54",
                    "name": "Jinguo Zhu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e55",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:09:23.250Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e56",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e57",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e58",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:09:57.293Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e59",
                    "user": {
                        "_id": "6541efc9109d78427198ea40",
                        "avatarUrl": "/avatars/c1252dd7da2e53b0b6757bf392139cdf.svg",
                        "isPro": false,
                        "fullname": "Lixin Gu",
                        "user": "gulixin0922",
                        "type": "user"
                    },
                    "name": "Lixin Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:04.312Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5a",
                    "user": {
                        "_id": "6495a4d6d1cec0e2c2c96cdd",
                        "avatarUrl": "/avatars/e8b42301514094c8af6fa1e5fcb53119.svg",
                        "isPro": false,
                        "fullname": "Duan Yuchen",
                        "user": "duanyuchen",
                        "type": "user"
                    },
                    "name": "Yuchen Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:30.238Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5b",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5c",
                    "user": {
                        "_id": "63e4562f9db5da2dc1f3b520",
                        "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
                        "isPro": false,
                        "fullname": "Weijie Su",
                        "user": "jackroos",
                        "type": "user"
                    },
                    "name": "Weijie Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:39.196Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5d",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5e",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e5f",
                    "user": {
                        "_id": "6579b818563044badca392fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6579b818563044badca392fc/XTKQ9Lhceibp9dnQADPQF.jpeg",
                        "isPro": false,
                        "fullname": "cuierfei",
                        "user": "cuierfei",
                        "type": "user"
                    },
                    "name": "Erfei Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:10:59.950Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e60",
                    "user": {
                        "_id": "6571382c7644d1128561cebe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bnJ-T3k_w1g1Mr7b7LglK.jpeg",
                        "isPro": false,
                        "fullname": "Cao Yue",
                        "user": "yuecao0119",
                        "type": "user"
                    },
                    "name": "Yue Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:11:07.485Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e61",
                    "name": "Yangzhou Liu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e62",
                    "name": "Weiye Xu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e63",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e64",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e65",
                    "user": {
                        "_id": "67c6fd7d85d2167189fce0e4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qpvzOJWcUJvD03l5yTBcN.jpeg",
                        "isPro": false,
                        "fullname": "Han Lv",
                        "user": "Hanrandom",
                        "type": "user"
                    },
                    "name": "Han Lv",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:11:58.666Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e66",
                    "name": "Dengnian Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e67",
                    "user": {
                        "_id": "6751a10224431db3bed1f701",
                        "avatarUrl": "/avatars/06dcc5ba6cec8aff217a8bbc7a7d3b73.svg",
                        "isPro": false,
                        "fullname": "Songze Li",
                        "user": "CatCatCat36",
                        "type": "user"
                    },
                    "name": "Songze Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:12:32.493Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e68",
                    "user": {
                        "_id": "65b9d9961fe588f824fde191",
                        "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
                        "isPro": false,
                        "fullname": "Yinan He",
                        "user": "yinanhe",
                        "type": "user"
                    },
                    "name": "Yinan He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:12:39.591Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e69",
                    "name": "Tan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6a",
                    "user": {
                        "_id": "647ea4aae4d52fe0e021bce4",
                        "avatarUrl": "/avatars/9e68004d04403acff004113c856451a6.svg",
                        "isPro": false,
                        "fullname": "Jiapeng Luo",
                        "user": "woolpeeker",
                        "type": "user"
                    },
                    "name": "Jiapeng Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:12:51.845Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6b",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6c",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:07.750Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6d",
                    "name": "Botian Shi",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6e",
                    "name": "Xingcheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e6f",
                    "user": {
                        "_id": "64b3fd42eec33e27dcc4c941",
                        "avatarUrl": "/avatars/5aa1a99468fa61d4b8b0e80b592c4e55.svg",
                        "isPro": false,
                        "fullname": "Wenqi Shao",
                        "user": "wqshao126",
                        "type": "user"
                    },
                    "name": "Wenqi Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:27.767Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e70",
                    "user": {
                        "_id": "66b593026cef22e6ba6adb8a",
                        "avatarUrl": "/avatars/8a94d55b85177e84d65dd0bd537e335f.svg",
                        "isPro": false,
                        "fullname": "JunjunHe",
                        "user": "JunjunHe",
                        "type": "user"
                    },
                    "name": "Junjun He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:19.980Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e71",
                    "user": {
                        "_id": "654395472cfe8660a3a492bb",
                        "avatarUrl": "/avatars/2010370112200e83ced979d3d4c87735.svg",
                        "isPro": false,
                        "fullname": "xiong",
                        "user": "xiongyingtong",
                        "type": "user"
                    },
                    "name": "Yingtong Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:12.302Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e72",
                    "user": {
                        "_id": "6684eb7045d8ceb446ffe9ff",
                        "avatarUrl": "/avatars/30586f289031696253842fcd4a8788b9.svg",
                        "isPro": false,
                        "fullname": "wenwenQu",
                        "user": "wenwenQu",
                        "type": "user"
                    },
                    "name": "Wenwen Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:14:47.178Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e73",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e74",
                    "name": "Penglong Jiao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e75",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e76",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:25.647Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e77",
                    "name": "Huipeng Deng",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e78",
                    "name": "Jiaye Ge",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e79",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7a",
                    "name": "Limin Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7b",
                    "name": "Min Dou",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7c",
                    "user": {
                        "_id": "65ead3ea908526a39082e641",
                        "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
                        "isPro": false,
                        "fullname": "Lewei Lu",
                        "user": "luotto",
                        "type": "user"
                    },
                    "name": "Lewei Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:14:05.716Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7d",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7e",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e7f",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:59.084Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e80",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e81",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "67fdd08bda7816922cb67e82",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:13:51.523Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
                "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
            ],
            "publishedAt": "2025-04-14T17:59:25.000Z",
            "submittedOnDailyAt": "2025-04-15T01:57:22.403Z",
            "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "619507e7b74b6c591f794340",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                "isPro": false,
                "fullname": "Weiyun Wang",
                "user": "Weiyun1025",
                "type": "user"
            },
            "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
            "upvotes": 170,
            "discussionId": "67fdd08cda7816922cb67ec2",
            "projectPage": "https://internvl.github.io/blog/2025-04-11-InternVL-3.0/",
            "githubRepo": "https://github.com/OpenGVLab/InternVL",
            "ai_keywords": [
                "multimodal pre-training paradigm",
                "text-only large language model (LLM)",
                "multimodal large language model (MLLM)",
                "diverse multimodal data",
                "pure-text corpora",
                "unified training paradigm",
                "variable visual position encoding (V2PE)",
                "extended multimodal contexts",
                "supervised fine-tuning (SFT)",
                "mixed preference optimization (MPO)",
                "test-time scaling strategies",
                "optimized training infrastructure",
                "MMMU benchmark",
                "open-source MLLMs",
                "ChatGPT-4o",
                "Claude 3.5 Sonnet",
                "Gemini 2.5 Pro"
            ]
        },
        "publishedAt": "2025-04-14T13:59:25.000Z",
        "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
        "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yA_DVt5PKVaFjiY-E93e5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/94PuKH6M3nhmkVS8RXtb4.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/qvaW8f5868-P75pyzr6XT.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/4GqLJBNhM2rUqS_4mawkk.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/yBSGw85-QcThBlXv2tLj0.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/CHvbRf2VJg6amEGS9OlEE.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VCBvp4nhvzfokH-viUz1g.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/HIC3iirmK6jaOax4GlDYt.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/TLlWnFPBguYJsvuGGqykz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/xJ-sFLuCkv1bqL_TXwxgH.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/Hsv_hehORgnRmL68xSqmd.png",
            "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/S1fnELtJWFU_FlEozV0Ik.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10479.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "619507e7b74b6c591f794340",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
            "fullname": "Weiyun Wang",
            "name": "Weiyun1025",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08791",
            "authors": [
                {
                    "_id": "67fdbde764a418633ee9fa1b",
                    "user": {
                        "_id": "647466b8b68461d5cf795e3c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
                        "isPro": false,
                        "fullname": "LIKirin",
                        "user": "LIKirin",
                        "type": "user"
                    },
                    "name": "Zonghang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:53.716Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1c",
                    "user": {
                        "_id": "669e0c108b279f0a2704a5ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e0c108b279f0a2704a5ba/uPyioyK0RWSG7CT52xBao.png",
                        "isPro": false,
                        "fullname": "TaoLi",
                        "user": "LiPhilip",
                        "type": "user"
                    },
                    "name": "Tao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:50.518Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1d",
                    "user": {
                        "_id": "6513c6c9c1fbded3b1977acc",
                        "avatarUrl": "/avatars/d169b3462f952c5639e1471ed8bf84c9.svg",
                        "isPro": false,
                        "fullname": "Wenjiao Feng",
                        "user": "NeuronNomad",
                        "type": "user"
                    },
                    "name": "Wenjiao Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:15:55.453Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1e",
                    "name": "Mohsen Guizani",
                    "hidden": false
                },
                {
                    "_id": "67fdbde764a418633ee9fa1f",
                    "name": "Hongfang Yu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
            ],
            "publishedAt": "2025-04-07T13:46:21.000Z",
            "submittedOnDailyAt": "2025-04-15T00:38:03.208Z",
            "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
            "submittedOnDailyBy": {
                "_id": "647466b8b68461d5cf795e3c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
                "isPro": false,
                "fullname": "LIKirin",
                "user": "LIKirin",
                "type": "user"
            },
            "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
            "upvotes": 94,
            "discussionId": "67fdbdeb64a418633ee9fb58",
            "projectPage": "https://github.com/Lizonghang/prima.cpp",
            "githubRepo": "https://github.com/Lizonghang/prima.cpp",
            "ai_keywords": [
                "distributed inference system",
                "mmap",
                "piped-ring parallelism",
                "prefetching",
                "heterogeneity",
                "token latency",
                "Halda algorithm",
                "NP-hard assignment problem",
                "llama.cpp",
                "exo",
                "dllama",
                "Llama 3",
                "DeepSeek R1",
                "Qwen 2.5",
                "QwQ"
            ]
        },
        "publishedAt": "2025-04-07T09:46:21.000Z",
        "title": "PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday\n  Home Clusters",
        "summary": "Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers\nfor running frontier large language models (LLMs) on home devices. While\nconsumer hardware is getting stronger and model quantization is improving,\nexisting end-side solutions still demand GPU clusters, large RAM/VRAM, and high\nbandwidth, far beyond what a common home cluster can handle. This paper\nintroduces prima.cpp, a distributed inference system that runs 70B-scale models\non everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and\ncross-platform support. It uses mmap to manage model weights and introduces\npiped-ring parallelism with prefetching to hide disk loading. By modeling\nheterogeneity in computation, communication, disk, memory (and its management\nbehavior), and OS, it optimally assigns model layers to each device's CPU and\nGPU, further reducing token latency. An elegant algorithm named Halda is\nproposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a\ncommon four-node home cluster. It outperforms llama.cpp, exo, and dllama on\n30B+ models while keeping memory pressure below 6%. This brings frontier\n30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home\nassistants, making advanced AI truly accessible to individuals. The code is\nopen source and available at https://github.com/Lizonghang/prima.cpp.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647466b8b68461d5cf795e3c/Z-XPxpdhg3sBxHCfogGzf.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08791.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "647466b8b68461d5cf795e3c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647466b8b68461d5cf795e3c/zaK6sdCbdPfYu14vg2Ty6.png",
            "fullname": "LIKirin",
            "name": "LIKirin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08003",
            "authors": [
                {
                    "_id": "67fdc0c50c63732d9e0b139a",
                    "name": "Ning Li",
                    "hidden": false
                },
                {
                    "_id": "67fdc0c50c63732d9e0b139b",
                    "user": {
                        "_id": "67fdc24ad2c9d1369d390f01",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/51JiReMgHiFHZiJ9OVdaf.png",
                        "isPro": false,
                        "fullname": "Jingran Zhang",
                        "user": "zhangjingran",
                        "type": "user"
                    },
                    "name": "Jingran Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:25:03.508Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc0c50c63732d9e0b139c",
                    "user": {
                        "_id": "65862671e878be571bf9fc52",
                        "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
                        "isPro": false,
                        "fullname": "cuijiaxing",
                        "user": "cuijiaxing",
                        "type": "user"
                    },
                    "name": "Justin Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:24:57.129Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
            ],
            "publishedAt": "2025-04-09T16:10:15.000Z",
            "submittedOnDailyAt": "2025-04-15T00:44:44.354Z",
            "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
            "submittedOnDailyBy": {
                "_id": "65862671e878be571bf9fc52",
                "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
                "isPro": false,
                "fullname": "cuijiaxing",
                "user": "cuijiaxing",
                "type": "user"
            },
            "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.",
            "upvotes": 38,
            "discussionId": "67fdc0c60c63732d9e0b13d2",
            "ai_keywords": [
                "multimodal GPT-4o",
                "semantic synthesis",
                "global instruction adherence",
                "fine-grained editing precision",
                "post-generation reasoning",
                "context-aware",
                "reasoning-grounded multimodal generation"
            ]
        },
        "publishedAt": "2025-04-09T12:10:15.000Z",
        "title": "Have we unified image generation and understanding yet? An empirical\n  study of GPT-4o's image generation ability",
        "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/B7iVn73glP5UR6kkdAApX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08003.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65862671e878be571bf9fc52",
            "avatarUrl": "/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg",
            "fullname": "cuijiaxing",
            "name": "cuijiaxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08837",
            "authors": [
                {
                    "_id": "67fdc483ba0d61664fb0a19d",
                    "user": {
                        "_id": "65bf52f0259bc6caeb74f8bf",
                        "avatarUrl": "/avatars/b38392e954466df784a5760ded5df804.svg",
                        "isPro": false,
                        "fullname": "Haozhe Wang",
                        "user": "JasperHaozhe",
                        "type": "user"
                    },
                    "name": "Haozhe Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:20.455Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a19e",
                    "name": "Chao Qu",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a19f",
                    "user": {
                        "_id": "6772524ed6f92f429bd343a3",
                        "avatarUrl": "/avatars/211e0c4641b2d048b0136d7cdeef2483.svg",
                        "isPro": false,
                        "fullname": "Zuming Huang",
                        "user": "zuminghuang",
                        "type": "user"
                    },
                    "name": "Zuming Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:17:55.505Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a1a0",
                    "name": "Wei Chu",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a1a1",
                    "name": "Fangzhen Lin",
                    "hidden": false
                },
                {
                    "_id": "67fdc483ba0d61664fb0a1a2",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-15T02:29:24.168Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:41:56.000Z",
            "submittedOnDailyAt": "2025-04-15T01:01:12.820Z",
            "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6313a86154e6e5d9f0f94e04",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                "isPro": false,
                "fullname": "Wenhu Chen",
                "user": "wenhu",
                "type": "user"
            },
            "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
            "upvotes": 36,
            "discussionId": "67fdc484ba0d61664fb0a1db",
            "projectPage": "https://tiger-ai-lab.github.io/VL-Rethinker/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/VL-Rethinker/",
            "ai_keywords": [
                "GRPO algorithm",
                "Selective Sample Replay (SSR)",
                "reinforcement learning",
                "vanishing advantages problem",
                "self-reflection",
                "self-verification",
                "Forced Rethinking",
                "VL-Rethinker",
                "MathVista",
                "MathVerse",
                "MathVision",
                "MMMU-Pro",
                "EMMA",
                "MEGA-Bench",
                "multi-disciplinary benchmarks",
                "state-of-the-art (SoTA)"
            ]
        },
        "publishedAt": "2025-04-10T13:41:56.000Z",
        "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
        "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse, and MathVision to achieve\n80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source\nSoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench,\nnarrowing the gap with GPT-o1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08837.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "fullname": "Wenhu Chen",
            "name": "wenhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 38
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09925",
            "authors": [
                {
                    "_id": "67fdb5f1913c97aa32f130bd",
                    "user": {
                        "_id": "6625ef13605f46d05c1d0031",
                        "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                        "isPro": false,
                        "fullname": "Zheng Liu",
                        "user": "starriver030515",
                        "type": "user"
                    },
                    "name": "Zheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:57.129Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130be",
                    "user": {
                        "_id": "672dbf1bec84b9c33412488f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/V6o6K28ydpkaS9XFA4Ac3.png",
                        "isPro": false,
                        "fullname": "Mengjie Liu",
                        "user": "Balalauuoo",
                        "type": "user"
                    },
                    "name": "Mengjie Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:16:15.647Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130bf",
                    "name": "Jingzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c0",
                    "user": {
                        "_id": "672da19bb2f2dc21e176b0de",
                        "avatarUrl": "/avatars/a942ad1c467b865cb9530927fe13f2b7.svg",
                        "isPro": false,
                        "fullname": "Jingwei Xu",
                        "user": "jingwei-xu-00",
                        "type": "user"
                    },
                    "name": "Jingwei Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:16:53.642Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c1",
                    "name": "Bin Cui",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c2",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:17:01.171Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb5f1913c97aa32f130c3",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T06:33:29.000Z",
            "submittedOnDailyAt": "2025-04-15T00:17:19.508Z",
            "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
            "submittedOnDailyBy": {
                "_id": "6625ef13605f46d05c1d0031",
                "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
                "isPro": false,
                "fullname": "Zheng Liu",
                "user": "starriver030515",
                "type": "user"
            },
            "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
            "upvotes": 34,
            "discussionId": "67fdb5f3913c97aa32f13141",
            "githubRepo": "https://github.com/starriver030515/FUSION",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "fully vision-language alignment",
                "late-stage modality interaction",
                "LLM decoding",
                "Text-Guided Unified Vision Encoding",
                "pixel-level integration",
                "Context-Aware Recursive Alignment Decoding",
                "fine-grained, question-level semantic integration",
                "Dual-Supervised Semantic Mapping Loss",
                "Synthesized Language-Driven Question-Answer (QA)",
                "data synthesis method",
                "high-quality QA pairs",
                "text-guided feature integration",
                "full-modality integration",
                "vision tokens",
                "Cambrian-1 8B",
                "Florence-VL 8B",
                "ablation studies",
                "LLaVA-NeXT",
                "dynamic resolution"
            ]
        },
        "publishedAt": "2025-04-14T02:33:29.000Z",
        "title": "FUSION: Fully Integration of Vision-Language Representations for Deep\n  Cross-Modal Understanding",
        "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09925.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6625ef13605f46d05c1d0031",
            "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg",
            "fullname": "Zheng Liu",
            "name": "starriver030515",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09643",
            "authors": [
                {
                    "_id": "67fe391f684ac7f04ce21df7",
                    "name": "Nikita Sorokin",
                    "hidden": false
                },
                {
                    "_id": "67fe391f684ac7f04ce21df8",
                    "name": "Ivan Sedykh",
                    "hidden": false
                },
                {
                    "_id": "67fe391f684ac7f04ce21df9",
                    "name": "Valentin Malykh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T16:34:17.000Z",
            "submittedOnDailyAt": "2025-04-15T09:27:11.869Z",
            "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
            "submittedOnDailyBy": {
                "_id": "636a9a07e3ad78bc68b1a5a2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg",
                "isPro": false,
                "fullname": "Dmitry Abulkhanov",
                "user": "mponty",
                "type": "user"
            },
            "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.",
            "upvotes": 29,
            "discussionId": "67fe394e684ac7f04ce229e8",
            "ai_keywords": [
                "Proximal Policy Optimization (PPO)",
                "reranker model",
                "iterative self-training",
                "reward/reranking model",
                "MultiPL-E dataset",
                "GPT-4"
            ]
        },
        "publishedAt": "2025-04-13T12:34:17.000Z",
        "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking",
        "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09643.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "636a9a07e3ad78bc68b1a5a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg",
            "fullname": "Dmitry Abulkhanov",
            "name": "mponty",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10068",
            "authors": [
                {
                    "_id": "67fdd1d7634e600357b5b7ff",
                    "user": {
                        "_id": "673c7319d11b1c2e246ead9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                        "isPro": false,
                        "fullname": "Yang Shi",
                        "user": "DogNeverSleep",
                        "type": "user"
                    },
                    "name": "Yang Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:11.086Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b800",
                    "user": {
                        "_id": "65377c30e48353201e6fdda0",
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:18:07.388Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b801",
                    "user": {
                        "_id": "66ac46766c3f950f4f10b9f9",
                        "avatarUrl": "/avatars/027b573bc6e5b18107e762645cec6069.svg",
                        "isPro": false,
                        "fullname": "Yushuo Guan",
                        "user": "UnnamedWatcher",
                        "type": "user"
                    },
                    "name": "Yushuo Guan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:18:14.282Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b802",
                    "user": {
                        "_id": "646f7b60799a974be3191889",
                        "avatarUrl": "/avatars/8fff4c87a2ea2d12958424074dd8e93d.svg",
                        "isPro": false,
                        "fullname": "oliver",
                        "user": "zhenhuawu",
                        "type": "user"
                    },
                    "name": "Zhenhua Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:18:32.198Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b803",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b804",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b805",
                    "name": "Weihong Lin",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b806",
                    "name": "Jingyun Hua",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b807",
                    "user": {
                        "_id": "656832dfbd65fd41ee7aa8cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg",
                        "isPro": false,
                        "fullname": "Zekun Wang",
                        "user": "kugwzk",
                        "type": "user"
                    },
                    "name": "Zekun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:22:34.448Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b808",
                    "name": "Xinlong Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b809",
                    "user": {
                        "_id": "6671214c92412fd4640714eb",
                        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                        "isPro": false,
                        "fullname": "bohan zeng",
                        "user": "zbhpku",
                        "type": "user"
                    },
                    "name": "Bohan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:08.962Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b80a",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b80b",
                    "user": {
                        "_id": "67c5945da1661d5fa6f29adb",
                        "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
                        "isPro": false,
                        "fullname": "Fuzheng Zhang",
                        "user": "Edrex",
                        "type": "user"
                    },
                    "name": "Fuzheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:21:57.922Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b80c",
                    "name": "Wenjing Yang",
                    "hidden": false
                },
                {
                    "_id": "67fdd1d7634e600357b5b80d",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:19:13.378Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T10:14:44.000Z",
            "submittedOnDailyAt": "2025-04-15T01:56:35.077Z",
            "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
            "submittedOnDailyBy": {
                "_id": "673c7319d11b1c2e246ead9c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
                "isPro": false,
                "fullname": "Yang Shi",
                "user": "DogNeverSleep",
                "type": "user"
            },
            "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose Mavors, a novel framework\nthat introduces Multi-granularity\nvideo representation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
            "upvotes": 26,
            "discussionId": "67fdd1db634e600357b5b8f4",
            "projectPage": "https://mavors-mllm.github.io/",
            "githubRepo": "https://github.com/DogNeverSleep/Mavors",
            "ai_keywords": [
                "Intra-chunk Vision Encoder (IVE)",
                "Inter-chunk Feature Aggregator (IFA)",
                "3D convolutions",
                "Vision Transformers",
                "transformer-based dependency modeling",
                "chunk-level rotary position encodings",
                "sub-image decomposition",
                "long-video modeling",
                "spatio-temporal reasoning"
            ]
        },
        "publishedAt": "2025-04-14T06:14:44.000Z",
        "title": "Mavors: Multi-granularity Video Representation for Multimodal Large\n  Language Model",
        "summary": "Long-context video understanding in multimodal large language models (MLLMs)\nfaces a critical challenge: balancing computational efficiency with the\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\nsparse sampling, dense sampling with low resolution, and token compression)\nsuffer from significant information loss in temporal dynamics, spatial details,\nor subtle interactions, particularly in videos with complex motion or varying\nresolutions. To address this, we propose Mavors, a novel framework\nthat introduces Multi-granularity\nvideo representation for holistic\nlong-video modeling. Specifically, Mavors directly encodes raw video content\ninto latent representations through two core components: 1) an Intra-chunk\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\n(IFA) that establishes temporal coherence across chunks using transformer-based\ndependency modeling with chunk-level rotary position encodings. Moreover, the\nframework unifies image and video understanding by treating images as\nsingle-frame videos via sub-image decomposition. Experiments across diverse\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\nand temporal continuity, significantly outperforming existing methods in tasks\nrequiring fine-grained spatio-temporal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10068.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "673c7319d11b1c2e246ead9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
            "fullname": "Yang Shi",
            "name": "DogNeverSleep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08942",
            "authors": [
                {
                    "_id": "67fdadafdc27362617bbe714",
                    "user": {
                        "_id": "5fa9ff3ea13e063b8b2b60cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                        "isPro": false,
                        "fullname": "Xing Han Lù",
                        "user": "xhluca",
                        "type": "user"
                    },
                    "name": "Xing Han Lù",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:55:07.746Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe715",
                    "user": {
                        "_id": "63458f12d54fb141dedac508",
                        "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
                        "isPro": false,
                        "fullname": "Amirhossein Kazemnejad",
                        "user": "kazemnejad",
                        "type": "user"
                    },
                    "name": "Amirhossein Kazemnejad",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:22:50.671Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe716",
                    "user": {
                        "_id": "64527548fc4b47877aba7de0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64527548fc4b47877aba7de0/ht-mRRxNQT49A7NxArOGG.png",
                        "isPro": false,
                        "fullname": "Nicholas Meade",
                        "user": "ncmeade",
                        "type": "user"
                    },
                    "name": "Nicholas Meade",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:22:56.742Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe717",
                    "user": {
                        "_id": "631a523c04f8ed65eff16fb4",
                        "avatarUrl": "/avatars/2b284403c88f140d7bef283f729f7a3e.svg",
                        "isPro": false,
                        "fullname": "Arkil Patel",
                        "user": "arkilpatel",
                        "type": "user"
                    },
                    "name": "Arkil Patel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:55:05.412Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe718",
                    "user": {
                        "_id": "619af75e7812aec847ee7729",
                        "avatarUrl": "/avatars/f50c05ee8b3105d20a8b291cc9f06ae4.svg",
                        "isPro": false,
                        "fullname": "Dong Chan Shin",
                        "user": "dongchans",
                        "type": "user"
                    },
                    "name": "Dongchan Shin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:23:03.255Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe719",
                    "user": {
                        "_id": "65f5133599c842dd93b7bacd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/nxxfWGP_K0hf7BG1oM7c0.png",
                        "isPro": false,
                        "fullname": "Alejandra Zambrano",
                        "user": "alzambranolu",
                        "type": "user"
                    },
                    "name": "Alejandra Zambrano",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:23:09.097Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe71a",
                    "user": {
                        "_id": "60a66731e1db8bc33b8d4112",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a66731e1db8bc33b8d4112/AY5Y0CnHh08u6lfEoQ6se.jpeg",
                        "isPro": false,
                        "fullname": "Karolina Stanczak",
                        "user": "Karolina",
                        "type": "user"
                    },
                    "name": "Karolina Stańczak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:23:14.768Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe71b",
                    "user": {
                        "_id": "631cf223fe95faea33561d5f",
                        "avatarUrl": "/avatars/ac0431955c6c5f4948461772a984a2ba.svg",
                        "isPro": false,
                        "fullname": "Peter Shaw",
                        "user": "PeterShaw",
                        "type": "user"
                    },
                    "name": "Peter Shaw",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:23:22.550Z",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe71c",
                    "name": "Christopher J. Pal",
                    "hidden": false
                },
                {
                    "_id": "67fdadafdc27362617bbe71d",
                    "user": {
                        "_id": "624734dc4c731bb6bfab8af7",
                        "avatarUrl": "/avatars/6b250b58710a3287b85e4733c1824558.svg",
                        "isPro": false,
                        "fullname": "Siva Reddy",
                        "user": "sivareddyg",
                        "type": "user"
                    },
                    "name": "Siva Reddy",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-15T00:51:59.987Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
            ],
            "publishedAt": "2025-04-11T19:49:22.000Z",
            "submittedOnDailyAt": "2025-04-15T00:59:48.327Z",
            "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
            "submittedOnDailyBy": {
                "_id": "5fa9ff3ea13e063b8b2b60cb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                "isPro": false,
                "fullname": "Xing Han Lù",
                "user": "xhluca",
                "type": "user"
            },
            "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
            "upvotes": 18,
            "discussionId": "67fdadb0dc27362617bbe749",
            "projectPage": "https://agent-reward-bench.github.io/",
            "githubRepo": "https://github.com/McGill-NLP/agent-reward-bench",
            "ai_keywords": [
                "web agents",
                "natural language interaction",
                "rule-based methods",
                "LLMs",
                "automatic evaluations",
                "benchmark",
                "trajectories",
                "success",
                "side effects",
                "repetitiveness",
                "LLM judges",
                "expert"
            ]
        },
        "publishedAt": "2025-04-11T15:49:22.000Z",
        "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
        "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5fa9ff3ea13e063b8b2b60cb/-XsJQfRUJ6uZhuI8h9cDm.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08942.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "fullname": "Xing Han Lù",
            "name": "xhluca",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10368",
            "authors": [
                {
                    "_id": "67fdd3a917e86592095a3ab7",
                    "user": {
                        "_id": "6617c98901ad3a0642a2a08f",
                        "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg",
                        "isPro": false,
                        "fullname": "Wenyuan Zhang",
                        "user": "WYRipple",
                        "type": "user"
                    },
                    "name": "Wenyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:05.425Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3a917e86592095a3ab8",
                    "user": {
                        "_id": "665e84f6152658fe8d478b1f",
                        "avatarUrl": "/avatars/9f08ce6aa78d7576c97e4feaddf77c1e.svg",
                        "isPro": false,
                        "fullname": "Shuaiyi Nie",
                        "user": "ShuaiyiNie",
                        "type": "user"
                    },
                    "name": "Shuaiyi Nie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:23:41.149Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3a917e86592095a3ab9",
                    "name": "Xinghua Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fdd3a917e86592095a3aba",
                    "user": {
                        "_id": "642c2dcec3694d2b74565c48",
                        "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg",
                        "isPro": false,
                        "fullname": "zhangzef",
                        "user": "Starrrrrry",
                        "type": "user"
                    },
                    "name": "Zefeng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:01.660Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3a917e86592095a3abb",
                    "name": "Tingwen Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T16:13:23.000Z",
            "submittedOnDailyAt": "2025-04-15T02:04:54.102Z",
            "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "656426e7ec7e2398990a2d34",
                "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
                "isPro": false,
                "fullname": "AIRobotZ",
                "user": "AIRobotZ",
                "type": "user"
            },
            "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
            "upvotes": 16,
            "discussionId": "67fdd3aa17e86592095a3b0b"
        },
        "publishedAt": "2025-04-14T12:13:23.000Z",
        "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
        "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10368.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656426e7ec7e2398990a2d34",
            "avatarUrl": "/avatars/e84aed6b55b0554ad9581ae3c138a16a.svg",
            "fullname": "AIRobotZ",
            "name": "AIRobotZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10127",
            "authors": [
                {
                    "_id": "67fdf68d04d0302ef5ec0239",
                    "user": {
                        "_id": "63b76e716fc56e43c3c22ca8",
                        "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
                        "isPro": false,
                        "fullname": "Junlei Zhang",
                        "user": "leoozy",
                        "type": "user"
                    },
                    "name": "Junlei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:52:43.667Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf68d04d0302ef5ec023a",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/ZT-oJrw5BsADC-gZT_i25.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:29:38.552Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf68d04d0302ef5ec023b",
                    "user": {
                        "_id": "637f22fd932a61b89aeeea37",
                        "avatarUrl": "/avatars/342957f8242d4edaf1d58e1274313afe.svg",
                        "isPro": false,
                        "fullname": "Chang Ma",
                        "user": "changma",
                        "type": "user"
                    },
                    "name": "Chang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:29:44.813Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf68d04d0302ef5ec023c",
                    "name": "Zijie Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdf68d04d0302ef5ec023d",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:30:05.846Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf68d04d0302ef5ec023e",
                    "name": "Zhenzhong Lan",
                    "hidden": false
                },
                {
                    "_id": "67fdf68d04d0302ef5ec023f",
                    "user": {
                        "_id": "615f34ec3f6d24d67c1b5c78",
                        "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
                        "isPro": false,
                        "fullname": "Junxian He",
                        "user": "jxhe",
                        "type": "user"
                    },
                    "name": "Junxian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:30:19.230Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T11:35:02.000Z",
            "submittedOnDailyAt": "2025-04-15T06:39:52.966Z",
            "title": "Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization",
            "submittedOnDailyBy": {
                "_id": "63b76e716fc56e43c3c22ca8",
                "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
                "isPro": false,
                "fullname": "Junlei Zhang",
                "user": "leoozy",
                "type": "user"
            },
            "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.",
            "upvotes": 14,
            "discussionId": "67fdf69304d0302ef5ec0377",
            "githubRepo": "https://github.com/hkust-nlp/GUIMid",
            "ai_keywords": [
                "Vision Language Models (VLMs)",
                "GUI perception",
                "multimodal reasoning",
                "textual reasoning",
                "AndroidWorld",
                "WebArena",
                "task generalization",
                "cross-modal generalization",
                "GUI perception data"
            ]
        },
        "publishedAt": "2025-04-14T07:35:02.000Z",
        "title": "Breaking the Data Barrier -- Building GUI Agents Through Task\n  Generalization",
        "summary": "Graphical User Interface (GUI) agents offer cross-platform solutions for\nautomating complex digital tasks, with significant potential to transform\nproductivity workflows. However, their performance is often constrained by the\nscarcity of high-quality trajectory data. To address this limitation, we\npropose training Vision Language Models (VLMs) on data-rich,\nreasoning-intensive tasks during a dedicated mid-training stage, and then\nexamine how incorporating these tasks facilitates generalization to GUI\nplanning scenarios. Specifically, we explore a range of tasks with readily\navailable instruction-tuning data, including GUI perception, multimodal\nreasoning, and textual reasoning. Through extensive experiments across 11\nmid-training tasks, we demonstrate that: (1) Task generalization proves highly\neffective, yielding substantial improvements across most settings. For\ninstance, multimodal mathematical reasoning enhances performance on\nAndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data\nsignificantly boosts GUI web agent performance, achieving a 5.6% improvement on\nWebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal\ngeneralization from text-based to visual domains; (2) Contrary to prior\nassumptions, GUI perception data - previously considered closely aligned with\nGUI agent tasks and widely utilized for training - has a comparatively limited\nimpact on final performance; (3) Building on these insights, we identify the\nmost effective mid-training tasks and curate optimized mixture datasets,\nresulting in absolute performance gains of 8.0% on WebArena and 12.2% on\nAndroidWorld. Our work provides valuable insights into cross-domain knowledge\ntransfer for GUI agents and offers a practical approach to addressing data\nscarcity challenges in this emerging field. The code, data and models will be\navailable at https://github.com/hkust-nlp/GUIMid.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10127.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63b76e716fc56e43c3c22ca8",
            "avatarUrl": "/avatars/99c760e6a60e65d69d57a8b92a40f623.svg",
            "fullname": "Junlei Zhang",
            "name": "leoozy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09710",
            "authors": [
                {
                    "_id": "67fdb42aa8deb632ed46d23d",
                    "user": {
                        "_id": "64dfcc62e8b6f3f3baa950e0",
                        "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
                        "isPro": false,
                        "fullname": "Zhenting Wang",
                        "user": "ztwang",
                        "type": "user"
                    },
                    "name": "Zhenting Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:59.070Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb42aa8deb632ed46d23e",
                    "user": {
                        "_id": "6670b3b78eac2e222ebf77d4",
                        "avatarUrl": "/avatars/0f1d231eac479ca78ddf106a72490faa.svg",
                        "isPro": false,
                        "fullname": "Guofeng Cui",
                        "user": "gfcui",
                        "type": "user"
                    },
                    "name": "Guofeng Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:24:11.131Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb42aa8deb632ed46d23f",
                    "user": {
                        "_id": "66274e02348a5304435dc9cc",
                        "avatarUrl": "/avatars/bda87559cd497c310597c2fc8430b31f.svg",
                        "isPro": false,
                        "fullname": "Kun Wan",
                        "user": "timecuriosity",
                        "type": "user"
                    },
                    "name": "Kun Wan",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-15T01:22:46.116Z",
                    "hidden": false
                },
                {
                    "_id": "67fdb42aa8deb632ed46d240",
                    "user": {
                        "_id": "66443629b23fe8d3f7f2d0c7",
                        "avatarUrl": "/avatars/98ff088036aa382f33a05c232604c565.svg",
                        "isPro": false,
                        "fullname": "Wentian Zhao",
                        "user": "zwt123home123",
                        "type": "user"
                    },
                    "name": "Wentian Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:24:30.442Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T20:10:27.000Z",
            "submittedOnDailyAt": "2025-04-15T00:08:28.286Z",
            "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
            "submittedOnDailyBy": {
                "_id": "64dfcc62e8b6f3f3baa950e0",
                "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
                "isPro": false,
                "fullname": "Zhenting Wang",
                "user": "ztwang",
                "type": "user"
            },
            "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.",
            "upvotes": 13,
            "discussionId": "67fdb457a8deb632ed46de2f",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "large language models (LLMs)",
                "reasoning capabilities",
                "distribution-level learnability",
                "policy advantages",
                "dynamic sampling probabilities",
                "Upper Confidence Bound (UCB)",
                "GRPO",
                "logic reasoning datasets",
                "distribution-aware curriculum strategies"
            ]
        },
        "publishedAt": "2025-04-13T16:10:27.000Z",
        "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM\n  Post-training",
        "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09710.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dfcc62e8b6f3f3baa950e0",
            "avatarUrl": "/avatars/21bbff67d46c08044efe2406575aa77e.svg",
            "fullname": "Zhenting Wang",
            "name": "ztwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10157",
            "authors": [
                {
                    "_id": "67fdc9adc47ae882c7a17c8a",
                    "user": {
                        "_id": "64b77e02a8c39dc07885179c",
                        "avatarUrl": "/avatars/172dd9eef0d4edfbd8f7cc5fb3feb206.svg",
                        "isPro": false,
                        "fullname": "xnzhang",
                        "user": "Lishi0905",
                        "type": "user"
                    },
                    "name": "Xinnong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:18.076Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c8b",
                    "name": "Jiayu Lin",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c8c",
                    "name": "Xinyi Mou",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c8d",
                    "name": "Shiyue Yang",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c8e",
                    "name": "Xiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c8f",
                    "user": {
                        "_id": "6522f5652d5eb02118d4d2e3",
                        "avatarUrl": "/avatars/f006828830590418d9c69b591fe61c69.svg",
                        "isPro": false,
                        "fullname": "Libo Sun",
                        "user": "libo-ca",
                        "type": "user"
                    },
                    "name": "Libo Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:26:53.104Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c90",
                    "name": "Hanjia Lyu",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c91",
                    "name": "Yihang Yang",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c92",
                    "name": "Weihong Qi",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c93",
                    "name": "Yue Chen",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c94",
                    "name": "Guanying Li",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c95",
                    "name": "Ling Yan",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c96",
                    "name": "Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c97",
                    "user": {
                        "_id": "6345de6cfe134dfd7a0ed1ec",
                        "avatarUrl": "/avatars/5e74fbdff4d9145c2d0b3c2c4c0145c7.svg",
                        "isPro": false,
                        "fullname": "Siming",
                        "user": "SimingChen",
                        "type": "user"
                    },
                    "name": "Siming Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:28:27.202Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c98",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c99",
                    "name": "Jingxuan Huang",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c9a",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c9b",
                    "user": {
                        "_id": "64b7727a88b86014d7eb9073",
                        "avatarUrl": "/avatars/bc4dda32363efcc16212b8eb97c2f813.svg",
                        "isPro": false,
                        "fullname": "Simon Tang",
                        "user": "tangshiping",
                        "type": "user"
                    },
                    "name": "Shiping Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:28:00.350Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c9c",
                    "name": "Libo Wu",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c9d",
                    "user": {
                        "_id": "67d066e70fdab2f434aa1488",
                        "avatarUrl": "/avatars/d17838856185856306ec5d4a121314f1.svg",
                        "isPro": false,
                        "fullname": "Baohua Zhou",
                        "user": "milesz7777",
                        "type": "user"
                    },
                    "name": "Baohua Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:27:40.001Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc9adc47ae882c7a17c9e",
                    "name": "Zhongyu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T12:12:52.000Z",
            "submittedOnDailyAt": "2025-04-15T01:29:17.826Z",
            "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
            "submittedOnDailyBy": {
                "_id": "64c939307dba66c3a7e4d215",
                "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
                "isPro": false,
                "fullname": "BruceLyu",
                "user": "brucelyu",
                "type": "user"
            },
            "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
            "upvotes": 11,
            "discussionId": "67fdc9aec47ae882c7a17cf1",
            "ai_keywords": [
                "large language models (LLMs)",
                "SocioVerse",
                "LLM-agent-driven",
                "world model",
                "social simulation",
                "alignment challenges",
                "population dynamics",
                "diversity",
                "credibility",
                "representativeness",
                "standardized procedures",
                "manual adjustments"
            ]
        },
        "publishedAt": "2025-04-14T08:12:52.000Z",
        "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
        "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10157.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c939307dba66c3a7e4d215",
            "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
            "fullname": "BruceLyu",
            "name": "brucelyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09763",
            "authors": [
                {
                    "_id": "67fdf0faea4d2ba44335ffa7",
                    "name": "Zaid Khan",
                    "hidden": false
                },
                {
                    "_id": "67fdf0faea4d2ba44335ffa8",
                    "user": {
                        "_id": "61781c4caf41befe8ff060e8",
                        "avatarUrl": "/avatars/8871d7b046fc28cbc8638228da8e9737.svg",
                        "isPro": false,
                        "fullname": "Elias Stengel-Eskin",
                        "user": "esteng",
                        "type": "user"
                    },
                    "name": "Elias Stengel-Eskin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:33:58.055Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf0faea4d2ba44335ffa9",
                    "user": {
                        "_id": "607aeae5d2cd8c150e6ae074",
                        "avatarUrl": "/avatars/a087743b98b6fe2181283a9610db4ec4.svg",
                        "isPro": false,
                        "fullname": "Archiki Prasad",
                        "user": "archiki",
                        "type": "user"
                    },
                    "name": "Archiki Prasad",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:34:04.468Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf0faea4d2ba44335ffaa",
                    "user": {
                        "_id": "5ffe32d8942cf3533d364449",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
                        "isPro": false,
                        "fullname": "Jaemin Cho",
                        "user": "j-min",
                        "type": "user"
                    },
                    "name": "Jaemin Cho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:34:12.982Z",
                    "hidden": false
                },
                {
                    "_id": "67fdf0faea4d2ba44335ffab",
                    "user": {
                        "_id": "665d9d3a057f7c508f98c625",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
                        "isPro": false,
                        "fullname": "Mohit Bansal",
                        "user": "mohitbansal",
                        "type": "user"
                    },
                    "name": "Mohit Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:34:19.233Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
            ],
            "publishedAt": "2025-04-14T00:06:48.000Z",
            "submittedOnDailyAt": "2025-04-15T04:15:51.576Z",
            "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
            "submittedOnDailyBy": {
                "_id": "6301c3e0a123c93a5fb295ff",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
                "isPro": false,
                "fullname": "Zaid Khan",
                "user": "codezakh",
                "type": "user"
            },
            "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
            "upvotes": 11,
            "discussionId": "67fdf0fbea4d2ba44335ffdd",
            "projectPage": "https://zaidkhan.me/EFAGen",
            "ai_keywords": [
                "EFA (Executable Functional Abstraction)",
                "program synthesis",
                "LLM (Large Language Model)",
                "step-by-step solution",
                "seed math problem",
                "executable unit tests",
                "verifiable rewards",
                "divers sources of competition-level math problems",
                "problem variations"
            ]
        },
        "publishedAt": "2025-04-13T20:06:48.000Z",
        "title": "Executable Functional Abstractions: Inferring Generative Programs for\n  Advanced Math Problems",
        "summary": "Scientists often infer abstract procedures from specific instances of\nproblems and use the abstractions to generate new, related instances. For\nexample, programs encoding the formal rules and properties of a system have\nbeen useful in fields ranging from RL (procedural environments) to physics\n(simulation engines). These programs can be seen as functions which execute to\ndifferent outputs based on their parameterizations (e.g., gridworld\nconfiguration or initial physical conditions). We introduce the term EFA\n(Executable Functional Abstraction) to denote such programs for math problems.\nEFA-like constructs have been shown to be useful for math reasoning as problem\ngenerators for stress-testing models. However, prior work has been limited to\nabstractions for grade-school math (whose simple rules are easy to encode in\nprograms), while generating EFAs for advanced math has thus far required human\nengineering. We explore the automatic construction of EFAs for advanced math\nproblems. We operationalize the task of automatically constructing EFAs as a\nprogram synthesis task, and develop EFAGen, which conditions an LLM on a seed\nmath problem and its step-by-step solution to generate candidate EFA programs\nthat are faithful to the generalized problem and solution class underlying the\nseed problem. Furthermore, we formalize properties any valid EFA must possess\nin terms of executable unit tests, and show how the tests can be used as\nverifiable rewards to train LLMs to become better writers of EFAs. We\ndemonstrate that EFAs constructed by EFAGen behave rationally by remaining\nfaithful to seed problems, produce learnable problem variations, and that\nEFAGen can infer EFAs across multiple diverse sources of competition-level math\nproblems. Finally, we show downstream uses of model-written EFAs e.g. finding\nproblem variations that are harder or easier for a learner to solve, as well as\ndata generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6301c3e0a123c93a5fb295ff/6qC9u4ryCuVji8lMCdWQN.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09763.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6301c3e0a123c93a5fb295ff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661060051926-noauth.jpeg",
            "fullname": "Zaid Khan",
            "name": "codezakh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10471",
            "authors": [
                {
                    "_id": "67fe0b5b45d0155d68b3c8c9",
                    "name": "Chenghao Xiao",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8ca",
                    "name": "Isaac Chung",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8cb",
                    "name": "Imene Kerboua",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8cc",
                    "name": "Jamie Stirling",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8cd",
                    "user": {
                        "_id": "63b6dbc8ccebeadccc888456",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
                        "isPro": false,
                        "fullname": "Xin Zhang",
                        "user": "izhx",
                        "type": "user"
                    },
                    "name": "Xin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:52:33.782Z",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8ce",
                    "name": "Márton Kardos",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8cf",
                    "name": "Roman Solomatin",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8d0",
                    "name": "Noura Al Moubayed",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8d1",
                    "name": "Kenneth Enevoldsen",
                    "hidden": false
                },
                {
                    "_id": "67fe0b5b45d0155d68b3c8d2",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:54:28.000Z",
            "submittedOnDailyAt": "2025-04-15T12:18:25.687Z",
            "title": "MIEB: Massive Image Embedding Benchmark",
            "submittedOnDailyBy": {
                "_id": "64cc0e80a257a3212c0c4b24",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
                "isPro": false,
                "fullname": "Isaac Chung",
                "user": "isaacchung",
                "type": "user"
            },
            "summary": "Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.",
            "upvotes": 10,
            "discussionId": "67fe0b5e45d0155d68b3c9a3",
            "githubRepo": "https://github.com/embeddings-benchmark/mteb",
            "ai_keywords": [
                "Massive Image Embedding Benchmark (MIEB)",
                "image embedding model",
                "image-text embedding models",
                "visual representation of texts",
                "interleaved encodings",
                "multimodal large language models"
            ]
        },
        "publishedAt": "2025-04-14T13:54:28.000Z",
        "title": "MIEB: Massive Image Embedding Benchmark",
        "summary": "Image representations are often evaluated through disjointed, task-specific\nprotocols, leading to a fragmented understanding of model capabilities. For\ninstance, it is unclear whether an image embedding model adept at clustering\nimages is equally good at retrieving relevant images given a piece of text. We\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\nperformance of image and image-text embedding models across the broadest\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\ngroup into 8 high-level categories. We benchmark 50 models across our\nbenchmark, finding that no single method dominates across all task categories.\nWe reveal hidden capabilities in advanced vision models such as their accurate\nvisual representation of texts, and their yet limited capabilities in\ninterleaved encodings and matching images and texts in the presence of\nconfounders. We also show that the performance of vision encoders on MIEB\ncorrelates highly with their performance when used in multimodal large language\nmodels. Our code, dataset, and leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10471.png",
        "numComments": 0,
        "submittedBy": {
            "_id": "64cc0e80a257a3212c0c4b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png",
            "fullname": "Isaac Chung",
            "name": "isaacchung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.09641",
            "authors": [
                {
                    "_id": "67fdceb2df4261c001394f59",
                    "user": {
                        "_id": "66448136bfe15e84d3987372",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
                        "isPro": false,
                        "fullname": "Zhang Xingjian",
                        "user": "Zhang199",
                        "type": "user"
                    },
                    "name": "Xingjian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:14.629Z",
                    "hidden": false
                },
                {
                    "_id": "67fdceb2df4261c001394f5a",
                    "user": {
                        "_id": "67f72edbc791c2e0f938203d",
                        "avatarUrl": "/avatars/c8e6d5a2f2122482e5fab7c6438440b5.svg",
                        "isPro": false,
                        "fullname": "si wei wen",
                        "user": "wenzz1",
                        "type": "user"
                    },
                    "name": "Siwei Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:29:04.694Z",
                    "hidden": false
                },
                {
                    "_id": "67fdceb2df4261c001394f5b",
                    "name": "Wenjun Wu",
                    "hidden": false
                },
                {
                    "_id": "67fdceb2df4261c001394f5c",
                    "name": "Lei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T16:32:49.000Z",
            "submittedOnDailyAt": "2025-04-15T01:46:43.707Z",
            "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
            "submittedOnDailyBy": {
                "_id": "66448136bfe15e84d3987372",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
                "isPro": false,
                "fullname": "Zhang Xingjian",
                "user": "Zhang199",
                "type": "user"
            },
            "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
            "upvotes": 10,
            "discussionId": "67fdceb5df4261c001394ff5",
            "ai_keywords": [
                "TinyLLaVA-Video",
                "TinyLLaVA-Video-R1",
                "reinforcement learning",
                "Video-QA",
                "video understanding",
                "\"aha moments\""
            ]
        },
        "publishedAt": "2025-04-13T12:32:49.000Z",
        "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning",
        "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09641.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66448136bfe15e84d3987372",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448136bfe15e84d3987372/SFEbWGgqhdZjxCS7qF3YL.jpeg",
            "fullname": "Zhang Xingjian",
            "name": "Zhang199",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08066",
            "authors": [
                {
                    "_id": "67fd9abb4f2e48847878c9e1",
                    "user": {
                        "_id": "63142272e29fb2e86d63db0a",
                        "avatarUrl": "/avatars/5726b5c40cea190a6cd9907fa596b66b.svg",
                        "isPro": false,
                        "fullname": "Yutaro",
                        "user": "yyamada",
                        "type": "user"
                    },
                    "name": "Yutaro Yamada",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-15T11:14:40.840Z",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e2",
                    "name": "Robert Tjarko Lange",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e3",
                    "user": {
                        "_id": "6311ec6ac7722fdac9a93f92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670192227454-6311ec6ac7722fdac9a93f92.jpeg",
                        "isPro": false,
                        "fullname": "Cong Lu",
                        "user": "conglu",
                        "type": "user"
                    },
                    "name": "Cong Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:55:14.712Z",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e4",
                    "name": "Shengran Hu",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e5",
                    "name": "Chris Lu",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e6",
                    "name": "Jakob Foerster",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e7",
                    "name": "Jeff Clune",
                    "hidden": false
                },
                {
                    "_id": "67fd9abb4f2e48847878c9e8",
                    "name": "David Ha",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T18:44:41.000Z",
            "submittedOnDailyAt": "2025-04-15T08:51:00.354Z",
            "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via\n  Agentic Tree Search",
            "submittedOnDailyBy": {
                "_id": "63142272e29fb2e86d63db0a",
                "avatarUrl": "/avatars/5726b5c40cea190a6cd9907fa596b66b.svg",
                "isPro": false,
                "fullname": "Yutaro",
                "user": "yyamada",
                "type": "user"
            },
            "summary": "AI is increasingly playing a pivotal role in transforming how scientific\ndiscoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic\nsystem capable of producing the first entirely AI generated\npeer-review-accepted workshop paper. This system iteratively formulates\nscientific hypotheses, designs and executes experiments, analyzes and\nvisualizes data, and autonomously authors scientific manuscripts. Compared to\nits predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2\neliminates the reliance on human-authored code templates, generalizes\neffectively across diverse machine learning domains, and leverages a novel\nprogressive agentic tree-search methodology managed by a dedicated experiment\nmanager agent. Additionally, we enhance the AI reviewer component by\nintegrating a Vision-Language Model (VLM) feedback loop for iterative\nrefinement of content and aesthetics of the figures. We evaluated The AI\nScientist-v2 by submitting three fully autonomous manuscripts to a\npeer-reviewed ICLR workshop. Notably, one manuscript achieved high enough\nscores to exceed the average human acceptance threshold, marking the first\ninstance of a fully AI-generated paper successfully navigating a peer review.\nThis accomplishment highlights the growing capability of AI in conducting all\naspects of scientific research. We anticipate that further advancements in\nautonomous scientific discovery technologies will profoundly impact human\nknowledge generation, enabling unprecedented scalability in research\nproductivity and significantly accelerating scientific breakthroughs, greatly\nbenefiting society at large. We have open-sourced the code at\nhttps://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of\nthis transformative technology. We also discuss the role of AI in science,\nincluding AI safety.",
            "upvotes": 9,
            "discussionId": "67fd9abd4f2e48847878ca5b",
            "ai_keywords": [
                "agentic system",
                "scientific hypotheses",
                "experiments",
                "data analysis",
                "data visualization",
                "autonomous authorship",
                "experiment manager agent",
                "Vision-Language Model (VLM)",
                "iterative refinement",
                "peer-reviewed ICLR workshop",
                "AI-generated paper",
                "scientific research",
                "autonomous scientific discovery",
                "research productivity",
                "scientific breakthroughs"
            ]
        },
        "publishedAt": "2025-04-10T14:44:41.000Z",
        "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via\n  Agentic Tree Search",
        "summary": "AI is increasingly playing a pivotal role in transforming how scientific\ndiscoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic\nsystem capable of producing the first entirely AI generated\npeer-review-accepted workshop paper. This system iteratively formulates\nscientific hypotheses, designs and executes experiments, analyzes and\nvisualizes data, and autonomously authors scientific manuscripts. Compared to\nits predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2\neliminates the reliance on human-authored code templates, generalizes\neffectively across diverse machine learning domains, and leverages a novel\nprogressive agentic tree-search methodology managed by a dedicated experiment\nmanager agent. Additionally, we enhance the AI reviewer component by\nintegrating a Vision-Language Model (VLM) feedback loop for iterative\nrefinement of content and aesthetics of the figures. We evaluated The AI\nScientist-v2 by submitting three fully autonomous manuscripts to a\npeer-reviewed ICLR workshop. Notably, one manuscript achieved high enough\nscores to exceed the average human acceptance threshold, marking the first\ninstance of a fully AI-generated paper successfully navigating a peer review.\nThis accomplishment highlights the growing capability of AI in conducting all\naspects of scientific research. We anticipate that further advancements in\nautonomous scientific discovery technologies will profoundly impact human\nknowledge generation, enabling unprecedented scalability in research\nproductivity and significantly accelerating scientific breakthroughs, greatly\nbenefiting society at large. We have open-sourced the code at\nhttps://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of\nthis transformative technology. We also discuss the role of AI in science,\nincluding AI safety.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08066.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63142272e29fb2e86d63db0a",
            "avatarUrl": "/avatars/5726b5c40cea190a6cd9907fa596b66b.svg",
            "fullname": "Yutaro",
            "name": "yyamada",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09130",
            "authors": [
                {
                    "_id": "67fe0d1f3a2e18d214499d3c",
                    "user": {
                        "_id": "627b73728b6ecd7ece822825",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
                        "isPro": false,
                        "fullname": "Yikun Wang",
                        "user": "LibraTree",
                        "type": "user"
                    },
                    "name": "Yikun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:52:28.698Z",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d3d",
                    "user": {
                        "_id": "64c3c631e77ea9f28111172a",
                        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
                        "isPro": false,
                        "fullname": "Siyin Wang",
                        "user": "sinwang",
                        "type": "user"
                    },
                    "name": "Siyin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:34:42.695Z",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d3e",
                    "name": "Qinyuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d3f",
                    "user": {
                        "_id": "629ef8544313a7c1dd671130",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
                        "isPro": false,
                        "fullname": "Zhaoye Fei",
                        "user": "ngc7293",
                        "type": "user"
                    },
                    "name": "Zhaoye Fei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:34:57.322Z",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d40",
                    "user": {
                        "_id": "641123b4230ce11b1be68fa1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641123b4230ce11b1be68fa1/kGURwBB-0f1TvgxwvcUWZ.png",
                        "isPro": false,
                        "fullname": "Liang Ding",
                        "user": "alphadl",
                        "type": "user"
                    },
                    "name": "Liang Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:35:04.032Z",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d41",
                    "user": {
                        "_id": "6491cd52b1e5d3444528edb1",
                        "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg",
                        "isPro": false,
                        "fullname": "Qipeng Guo",
                        "user": "QipengGuo",
                        "type": "user"
                    },
                    "name": "Qipeng Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:35:09.688Z",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d42",
                    "name": "Dacheng Tao",
                    "hidden": false
                },
                {
                    "_id": "67fe0d1f3a2e18d214499d43",
                    "user": {
                        "_id": "61457b8deff2c9fdb4de4988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
                        "isPro": false,
                        "fullname": "Xipeng Qiu",
                        "user": "xpqiu",
                        "type": "user"
                    },
                    "name": "Xipeng Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:35:23.363Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-12T08:37:30.000Z",
            "submittedOnDailyAt": "2025-04-15T06:45:12.936Z",
            "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
            "submittedOnDailyBy": {
                "_id": "627b73728b6ecd7ece822825",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
                "isPro": false,
                "fullname": "Yikun Wang",
                "user": "LibraTree",
                "type": "user"
            },
            "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning.",
            "upvotes": 7,
            "discussionId": "67fe0d203a2e18d214499d9f",
            "githubRepo": "https://github.com/ekonwang/VisuoThink",
            "ai_keywords": [
                "VisuoThink",
                "Visuospatial",
                "Linguistic domains",
                "Multimodal slow thinking",
                "Progressive visual-textual reasoning",
                "Test-time scaling",
                "Look-ahead tree search"
            ]
        },
        "publishedAt": "2025-04-12T04:37:30.000Z",
        "title": "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search",
        "summary": "Recent advancements in Large Vision-Language Models have showcased remarkable\ncapabilities. However, they often falter when confronted with complex reasoning\ntasks that humans typically address through visual aids and deliberate,\nstep-by-step thinking. While existing methods have explored text-based slow\nthinking or rudimentary visual assistance, they fall short of capturing the\nintricate, interleaved nature of human visual-verbal reasoning processes. To\novercome these limitations and inspired by the mechanisms of slow thinking in\nhuman cognition, we introduce VisuoThink, a novel framework that seamlessly\nintegrates visuospatial and linguistic domains. VisuoThink facilitates\nmultimodal slow thinking by enabling progressive visual-textual reasoning and\nincorporates test-time scaling through look-ahead tree search. Extensive\nexperiments demonstrate that VisuoThink significantly enhances reasoning\ncapabilities via inference-time scaling, even without fine-tuning, achieving\nstate-of-the-art performance in tasks involving geometry and spatial reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09130.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "fullname": "Yikun Wang",
            "name": "LibraTree",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10415",
            "authors": [
                {
                    "_id": "67fddc276a3c533dc1d3bcbe",
                    "user": {
                        "_id": "6520621836008ecc88699622",
                        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
                        "isPro": false,
                        "fullname": "Parshin Shojaee",
                        "user": "parshinsh",
                        "type": "user"
                    },
                    "name": "Parshin Shojaee",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-15T04:10:17.364Z",
                    "hidden": false
                },
                {
                    "_id": "67fddc276a3c533dc1d3bcbf",
                    "name": "Ngoc-Hieu Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67fddc276a3c533dc1d3bcc0",
                    "user": {
                        "_id": "67b4cdee376cfc783f9ec8cf",
                        "avatarUrl": "/avatars/d9a9f320cd01dc5addfca14feefefef4.svg",
                        "isPro": false,
                        "fullname": "Meidani",
                        "user": "mkmeidani",
                        "type": "user"
                    },
                    "name": "Kazem Meidani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:53:43.431Z",
                    "hidden": false
                },
                {
                    "_id": "67fddc276a3c533dc1d3bcc1",
                    "name": "Amir Barati Farimani",
                    "hidden": false
                },
                {
                    "_id": "67fddc276a3c533dc1d3bcc2",
                    "name": "Khoa D Doan",
                    "hidden": false
                },
                {
                    "_id": "67fddc276a3c533dc1d3bcc3",
                    "name": "Chandan K Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:00:13.000Z",
            "submittedOnDailyAt": "2025-04-15T02:43:55.941Z",
            "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6520621836008ecc88699622",
                "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
                "isPro": false,
                "fullname": "Parshin Shojaee",
                "user": "parshinsh",
                "type": "user"
            },
            "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
            "upvotes": 6,
            "discussionId": "67fddc296a3c533dc1d3bd43",
            "projectPage": "https://huggingface.co/datasets/nnheui/llm-srbench",
            "githubRepo": "https://github.com/deep-symbolic-mathematics/llm-srbench",
            "ai_keywords": [
                "LLM-SRBench",
                "LSR-Transform",
                "LSR-Synth",
                "symbolic accuracy"
            ]
        },
        "publishedAt": "2025-04-14T13:00:13.000Z",
        "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
        "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10415.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "fullname": "Parshin Shojaee",
            "name": "parshinsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10449",
            "authors": [
                {
                    "_id": "67fe15543aa5a5684ca7229e",
                    "user": {
                        "_id": "63dc68bea99b2c8a7c20f1d0",
                        "avatarUrl": "/avatars/cf8ddc91415ef1f895803f4390ff1f6f.svg",
                        "isPro": true,
                        "fullname": "Junxiong Wang",
                        "user": "JunxiongWang",
                        "type": "user"
                    },
                    "name": "Junxiong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:35:36.587Z",
                    "hidden": false
                },
                {
                    "_id": "67fe15543aa5a5684ca7229f",
                    "user": {
                        "_id": "62e221dfcb1f164f2cb8a66b",
                        "avatarUrl": "/avatars/06f05622e232304d3f0b8c291f3263be.svg",
                        "isPro": true,
                        "fullname": "Wen-Ding Li",
                        "user": "xu3kev",
                        "type": "user"
                    },
                    "name": "Wen-Ding Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:35:44.671Z",
                    "hidden": false
                },
                {
                    "_id": "67fe15543aa5a5684ca722a0",
                    "name": "Daniele Paliotta",
                    "hidden": false
                },
                {
                    "_id": "67fe15543aa5a5684ca722a1",
                    "name": "Daniel Ritter",
                    "hidden": false
                },
                {
                    "_id": "67fe15543aa5a5684ca722a2",
                    "user": {
                        "_id": "67745f42633d42196543820f",
                        "avatarUrl": "/avatars/6faced0d6486b04262a8d7bc3990262b.svg",
                        "isPro": false,
                        "fullname": "Alexander Rush",
                        "user": "voidptr74",
                        "type": "user"
                    },
                    "name": "Alexander M. Rush",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:36:08.329Z",
                    "hidden": false
                },
                {
                    "_id": "67fe15543aa5a5684ca722a3",
                    "user": {
                        "_id": "64b8a6b5cf14c2fabe98159b",
                        "avatarUrl": "/avatars/dbc009451865435bf290791beadc4723.svg",
                        "isPro": false,
                        "fullname": "Tri Dao",
                        "user": "tridao",
                        "type": "user"
                    },
                    "name": "Tri Dao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:36:25.942Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:38:25.000Z",
            "submittedOnDailyAt": "2025-04-15T06:44:40.178Z",
            "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.",
            "upvotes": 5,
            "discussionId": "67fe15553aa5a5684ca722d5",
            "projectPage": "https://github.com/jxiw/M1",
            "githubRepo": "https://github.com/jxiw/M1",
            "ai_keywords": [
                "Mamba architecture",
                "hybrid linear RNN reasoning model",
                "long chain-of-thought reasoning",
                "transformer-based models",
                "quadratic computational complexity",
                "linear memory requirements",
                "distillation process",
                "RL training",
                "AIME",
                "MATH benchmarks",
                "Deepseek R1 distilled reasoning models",
                "vLLM",
                "throughput speedup",
                "self-consistency voting"
            ]
        },
        "publishedAt": "2025-04-14T13:38:25.000Z",
        "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
        "summary": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10449.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6659
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.09689",
            "authors": [
                {
                    "_id": "67fdc937089aec0f3b154dd7",
                    "name": "Jiahao Qiu",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154dd8",
                    "user": {
                        "_id": "652abf5360e706730596e8f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zRFJ4FjZJZGkz2wH8PPmU.jpeg",
                        "isPro": false,
                        "fullname": "Yinghui He",
                        "user": "yinghuihe",
                        "type": "user"
                    },
                    "name": "Yinghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:30:49.206Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154dd9",
                    "user": {
                        "_id": "674500b57a76d46e9141af8b",
                        "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
                        "isPro": false,
                        "fullname": "Xinzhe Juan",
                        "user": "ChrisJuan",
                        "type": "user"
                    },
                    "name": "Xinzhe Juan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:30:56.417Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154dda",
                    "user": {
                        "_id": "67c0934fb47a12be9c9b0899",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c0934fb47a12be9c9b0899/V6uJJZ_5ldTSbbSeShN-H.jpeg",
                        "isPro": false,
                        "fullname": "WangYM999",
                        "user": "YimingWang",
                        "type": "user"
                    },
                    "name": "Yiming Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:31:02.273Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154ddb",
                    "name": "Yuhan Liu",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154ddc",
                    "user": {
                        "_id": "6657f2041d83f3ee61bf414d",
                        "avatarUrl": "/avatars/e678fbbba2e93536dfc702e8ae629a95.svg",
                        "isPro": false,
                        "fullname": "zixin",
                        "user": "yaozixin",
                        "type": "user"
                    },
                    "name": "Zixin Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:31:38.642Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154ddd",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154dde",
                    "name": "Xun Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154ddf",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67fdc937089aec0f3b154de0",
                    "user": {
                        "_id": "6599415e8c8ac79295e0b5e3",
                        "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
                        "isPro": false,
                        "fullname": "Mengdi Wang",
                        "user": "Edify-Kd2024",
                        "type": "user"
                    },
                    "name": "Mengdi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:32:09.308Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T18:47:22.000Z",
            "submittedOnDailyAt": "2025-04-15T01:30:41.548Z",
            "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
            "submittedOnDailyBy": {
                "_id": "674500b57a76d46e9141af8b",
                "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
                "isPro": false,
                "fullname": "Xinzhe Juan",
                "user": "ChrisJuan",
                "type": "user"
            },
            "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
            "upvotes": 4,
            "discussionId": "67fdc938089aec0f3b154e31",
            "githubRepo": "https://github.com/1akaman/EmoAgent",
            "ai_keywords": [
                "LLM-driven AI characters",
                "multi-agent AI framework",
                "EmoAgent",
                "mental health hazards",
                "human-AI interactions",
                "EmoEval",
                "virtual users",
                "clinically proven psychological and psychiatric assessment tools",
                "PHQ-9",
                "PDI",
                "PANSS",
                "mental health changes",
                "mental risks",
                "LLM",
                "EmoGuard",
                "intermediary",
                "users' mental status",
                "potential harm",
                "corrective feedback",
                "emotionally engaging dialogues",
                "psychological deterioration"
            ]
        },
        "publishedAt": "2025-04-13T14:47:22.000Z",
        "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety",
        "summary": "The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09689.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "674500b57a76d46e9141af8b",
            "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
            "fullname": "Xinzhe Juan",
            "name": "ChrisJuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09522",
            "authors": [
                {
                    "_id": "67fe132f1d1bc292f7cdbc0f",
                    "name": "Chen Sun",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc10",
                    "user": {
                        "_id": "6003c87f532970af8c4d3a4a",
                        "avatarUrl": "/avatars/a4c5fbe427791d02e3cee208f22f18c4.svg",
                        "isPro": false,
                        "fullname": "Renat Aksitov",
                        "user": "mendor",
                        "type": "user"
                    },
                    "name": "Renat Aksitov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:36:40.229Z",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc11",
                    "name": "Andrey Zhmoginov",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc12",
                    "name": "Nolan Andrew Miller",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc13",
                    "user": {
                        "_id": "6502fe0bb1792803da806d42",
                        "avatarUrl": "/avatars/60280ce59f1f0d67e4210b0453039282.svg",
                        "isPro": false,
                        "fullname": "Max Vladymyrov",
                        "user": "gozzo87",
                        "type": "user"
                    },
                    "name": "Max Vladymyrov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:37:00.587Z",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc14",
                    "name": "Ulrich Rueckert",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc15",
                    "name": "Been Kim",
                    "hidden": false
                },
                {
                    "_id": "67fe132f1d1bc292f7cdbc16",
                    "name": "Mark Sandler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T11:25:04.000Z",
            "submittedOnDailyAt": "2025-04-15T06:35:36.514Z",
            "title": "How new data permeates LLM knowledge and how to dilute it",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/",
            "upvotes": 4,
            "discussionId": "67fe13341d1bc292f7cdbd2f",
            "ai_keywords": [
                "gradient-based updates",
                "priming effect",
                "Outlandish",
                "token probability",
                "PALM-2",
                "Gemma",
                "Llama",
                "stepping-stone text augmentation strategy",
                "ignore-k update pruning method"
            ]
        },
        "publishedAt": "2025-04-13T07:25:04.000Z",
        "title": "How new data permeates LLM knowledge and how to dilute it",
        "summary": "Large language models learn and continually learn through the accumulation of\ngradient-based updates, but how individual pieces of new information affect\nexisting knowledge, leading to both beneficial generalization and problematic\nhallucination, remains poorly understood. We demonstrate that when learning new\ninformation, LLMs exhibit a \"priming\" effect: learning a new fact can cause the\nmodel to inappropriately apply that knowledge in unrelated contexts. To\nsystematically study this phenomenon, we introduce \"Outlandish,\" a carefully\ncurated dataset of 1320 diverse text samples designed to probe how new\nknowledge permeates through an LLM's existing knowledge base. Using this\ndataset, we show that the degree of priming after learning new information can\nbe predicted by measuring the token probability of key words before learning.\nThis relationship holds robustly across different model architectures (PALM-2,\nGemma, Llama), sizes, and training stages. Finally, we develop two novel\ntechniques to modulate how new knowledge affects existing model behavior: (1) a\n``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update\npruning method. These approaches reduce undesirable priming effects by 50-95\\%\nwhile preserving the model's ability to learn new information. Our findings\nprovide both empirical insights into how LLMs learn and practical tools for\nimproving the specificity of knowledge insertion in language models. Further\nmaterials: https://sunchipsster1.github.io/projects/outlandish/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09522.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6659
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.09858",
            "authors": [
                {
                    "_id": "67fe895897acbf357c1f9725",
                    "name": "Wenjie Ma",
                    "hidden": false
                },
                {
                    "_id": "67fe895897acbf357c1f9726",
                    "name": "Jingxuan He",
                    "hidden": false
                },
                {
                    "_id": "67fe895897acbf357c1f9727",
                    "name": "Charlie Snell",
                    "hidden": false
                },
                {
                    "_id": "67fe895897acbf357c1f9728",
                    "name": "Tyler Griggs",
                    "hidden": false
                },
                {
                    "_id": "67fe895897acbf357c1f9729",
                    "user": {
                        "_id": "63a76d0de27a6dbd485fe863",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
                        "isPro": false,
                        "fullname": "Sewon Min",
                        "user": "sewon",
                        "type": "user"
                    },
                    "name": "Sewon Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T16:46:49.689Z",
                    "hidden": false
                },
                {
                    "_id": "67fe895897acbf357c1f972a",
                    "name": "Matei Zaharia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T04:08:16.000Z",
            "submittedOnDailyAt": "2025-04-15T15:00:19.393Z",
            "title": "Reasoning Models Can Be Effective Without Thinking",
            "submittedOnDailyBy": {
                "_id": "63a76d0de27a6dbd485fe863",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
                "isPro": false,
                "fullname": "Sewon Min",
                "user": "sewon",
                "type": "user"
            },
            "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling.",
            "upvotes": 2,
            "discussionId": "67fe895997acbf357c1f97aa",
            "ai_keywords": [
                "DeepSeek-R1-Distill-Qwen",
                "NoThinking",
                "token",
                "thinking process",
                "mathematical problem solving",
                "formal theorem proving",
                "coding",
                "ACM 23",
                "pass@k",
                "parallel scaling",
                "task-specific verifiers",
                "best-of-N strategies",
                "confidence-based selection",
                "latency"
            ]
        },
        "publishedAt": "2025-04-14T00:08:16.000Z",
        "title": "Reasoning Models Can Be Effective Without Thinking",
        "summary": "Recent LLMs have significantly improved reasoning capabilities, primarily by\nincluding an explicit, lengthy Thinking process as part of generation. In this\npaper, we question whether this explicit thinking is necessary. Using the\nstate-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking\nprocess via simple prompting, denoted as NoThinking, can be surprisingly\neffective. When controlling for the number of tokens, NoThinking outperforms\nThinking across a diverse set of seven challenging reasoning\ndatasets--including mathematical problem solving, formal theorem proving, and\ncoding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with\n700 tokens. Notably, the performance of NoThinking becomes more competitive\nwith pass@k as k increases. Building on this observation, we demonstrate that a\nparallel scaling approach that uses NoThinking to generate N outputs\nindependently and aggregates them is highly effective. For aggregation, we use\ntask-specific verifiers when available, or we apply simple best-of-N strategies\nsuch as confidence-based selection. Our method outperforms a range of baselines\nwith similar latency using Thinking, and is comparable to Thinking with\nsignificantly longer latency (up to 9x). Together, our research encourages a\nreconsideration of the necessity of lengthy thinking processes, while also\nestablishing a competitive reference for achieving strong reasoning performance\nin low-budget settings or at low latency using parallel scaling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09858.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a76d0de27a6dbd485fe863",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
            "fullname": "Sewon Min",
            "name": "sewon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09518",
            "authors": [
                {
                    "_id": "67fe50f6b3a9473353b68941",
                    "name": "Ting Huang",
                    "hidden": false
                },
                {
                    "_id": "67fe50f6b3a9473353b68942",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T16:46:59.967Z",
                    "hidden": true
                },
                {
                    "_id": "67fe50f6b3a9473353b68943",
                    "name": "Yemin Wang",
                    "hidden": false
                },
                {
                    "_id": "67fe50f6b3a9473353b68944",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T11:10:47.000Z",
            "submittedOnDailyAt": "2025-04-15T10:59:24.713Z",
            "title": "3D CoCa: Contrastive Learners are 3D Captioners",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": false,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "3D captioning, which aims to describe the content of 3D scenes in natural\nlanguage, remains highly challenging due to the inherent sparsity of point\nclouds and weak cross-modal alignment in existing methods. To address these\nchallenges, we propose 3D CoCa, a novel unified framework that seamlessly\ncombines contrastive vision-language learning with 3D caption generation in a\nsingle architecture. Our approach leverages a frozen CLIP vision-language\nbackbone to provide rich semantic priors, a spatially-aware 3D scene encoder to\ncapture geometric context, and a multi-modal decoder to generate descriptive\ncaptions. Unlike prior two-stage methods that rely on explicit object\nproposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a\nshared feature space, eliminating the need for external detectors or\nhandcrafted proposals. This joint training paradigm yields stronger spatial\nreasoning and richer semantic grounding by aligning 3D and textual\nrepresentations. Extensive experiments on the ScanRefer and Nr3D benchmarks\ndemonstrate that 3D CoCa significantly outperforms current state-of-the-arts by\n10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at\nhttps://github.com/AIGeeksGroup/3DCoCa.",
            "upvotes": 2,
            "discussionId": "67fe50f7b3a9473353b689f6",
            "ai_keywords": [
                "contrastive vision-language learning",
                "3D caption generation",
                "CLIP vision-language backbone",
                "spatially-aware 3D scene encoder",
                "multi-modal decoder",
                "contrastive and captioning objectives",
                "shared feature space",
                "spatial reasoning",
                "semantic grounding",
                "ScanRefer",
                "Nr3D",
                "CIDEr at 0.5IoU"
            ]
        },
        "publishedAt": "2025-04-13T07:10:47.000Z",
        "title": "3D CoCa: Contrastive Learners are 3D Captioners",
        "summary": "3D captioning, which aims to describe the content of 3D scenes in natural\nlanguage, remains highly challenging due to the inherent sparsity of point\nclouds and weak cross-modal alignment in existing methods. To address these\nchallenges, we propose 3D CoCa, a novel unified framework that seamlessly\ncombines contrastive vision-language learning with 3D caption generation in a\nsingle architecture. Our approach leverages a frozen CLIP vision-language\nbackbone to provide rich semantic priors, a spatially-aware 3D scene encoder to\ncapture geometric context, and a multi-modal decoder to generate descriptive\ncaptions. Unlike prior two-stage methods that rely on explicit object\nproposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a\nshared feature space, eliminating the need for external detectors or\nhandcrafted proposals. This joint training paradigm yields stronger spatial\nreasoning and richer semantic grounding by aligning 3D and textual\nrepresentations. Extensive experiments on the ScanRefer and Nr3D benchmarks\ndemonstrate that 3D CoCa significantly outperforms current state-of-the-arts by\n10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at\nhttps://github.com/AIGeeksGroup/3DCoCa.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09518.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08120",
            "authors": [
                {
                    "_id": "67fe3a3999bb7d059fc7bd31",
                    "name": "Daniil Larionov",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd32",
                    "name": "Sotaro Takeshita",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd33",
                    "name": "Ran Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd34",
                    "name": "Yanran Chen",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd35",
                    "name": "Christoph Leiter",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd36",
                    "name": "Zhipin Wang",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd37",
                    "name": "Christian Greisinger",
                    "hidden": false
                },
                {
                    "_id": "67fe3a3999bb7d059fc7bd38",
                    "name": "Steffen Eger",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60524a44194785248da0894c/SrP0R9C7FNTmB6Rm5J8wt.png"
            ],
            "publishedAt": "2025-04-10T20:39:18.000Z",
            "submittedOnDailyAt": "2025-04-15T09:29:27.111Z",
            "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?",
            "submittedOnDailyBy": {
                "_id": "60524a44194785248da0894c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636989238554-60524a44194785248da0894c.jpeg",
                "isPro": false,
                "fullname": "Daniil Larionov",
                "user": "Rexhaif",
                "type": "user"
            },
            "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated\nimpressive performance in complex logical and mathematical tasks, yet their\neffectiveness in evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI\no3) with their non-reasoning counterparts across machine translation (MT) and\ntext summarization (TS) evaluation tasks. We evaluate eight models across three\narchitectural categories, including state-of-the-art reasoning models, their\ndistilled variants (ranging from 8B to 70B parameters), and equivalent\nconventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval\nbenchmarks reveal that the benefits of reasoning capabilities are highly model\nand task-dependent: while OpenAI o3-mini models show consistent performance\nimprovements with increased reasoning intensity, DeepSeek-R1 underperforms\ncompared to its non-reasoning variant, with exception to certain aspects of TS\nevaluation. Correlation analysis demonstrates that increased reasoning token\nusage positively correlates with evaluation quality in o3-mini models.\nFurthermore, our results show that distillation of reasoning capabilities\nmaintains reasonable performance in medium-sized models (32B) but degrades\nsubstantially in smaller variants (8B). This work provides the first\ncomprehensive assessment of reasoning LLMs for NLG evaluation and offers\ninsights into their practical use.",
            "upvotes": 2,
            "discussionId": "67fe3a3b99bb7d059fc7bd9b",
            "ai_keywords": [
                "reasoning-enabled large language models (LLMs)",
                "DeepSeek-R1",
                "OpenAI o3",
                "machine translation (MT)",
                "text summarization (TS)",
                "evaluation tasks",
                "state-of-the-art reasoning models",
                "distilled variants",
                "conventional, non-reasoning LLMs",
                "WMT23",
                "SummEval",
                "reasoning capabilities",
                "reasoning token usage",
                "correlation analysis",
                "distillation of reasoning capabilities"
            ]
        },
        "publishedAt": "2025-04-10T16:39:18.000Z",
        "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?",
        "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated\nimpressive performance in complex logical and mathematical tasks, yet their\neffectiveness in evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI\no3) with their non-reasoning counterparts across machine translation (MT) and\ntext summarization (TS) evaluation tasks. We evaluate eight models across three\narchitectural categories, including state-of-the-art reasoning models, their\ndistilled variants (ranging from 8B to 70B parameters), and equivalent\nconventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval\nbenchmarks reveal that the benefits of reasoning capabilities are highly model\nand task-dependent: while OpenAI o3-mini models show consistent performance\nimprovements with increased reasoning intensity, DeepSeek-R1 underperforms\ncompared to its non-reasoning variant, with exception to certain aspects of TS\nevaluation. Correlation analysis demonstrates that increased reasoning token\nusage positively correlates with evaluation quality in o3-mini models.\nFurthermore, our results show that distillation of reasoning capabilities\nmaintains reasonable performance in medium-sized models (32B) but degrades\nsubstantially in smaller variants (8B). This work provides the first\ncomprehensive assessment of reasoning LLMs for NLG evaluation and offers\ninsights into their practical use.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60524a44194785248da0894c/SrP0R9C7FNTmB6Rm5J8wt.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08120.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60524a44194785248da0894c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1636989238554-60524a44194785248da0894c.jpeg",
            "fullname": "Daniil Larionov",
            "name": "Rexhaif",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.05782",
            "authors": [
                {
                    "_id": "67fe419a7161674011524204",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524205",
                    "name": "Fanrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524206",
                    "name": "Xiaopeng Peng",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524207",
                    "name": "Zhaopan Xu",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524208",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524209",
                    "name": "Yansheng Qiu",
                    "hidden": false
                },
                {
                    "_id": "67fe419a716167401152420a",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "67fe419a716167401152420b",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "67fe419a716167401152420c",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67fe419a716167401152420d",
                    "name": "Yukang Feng",
                    "hidden": false
                },
                {
                    "_id": "67fe419a716167401152420e",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "67fe419a716167401152420f",
                    "name": "Haoquan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524210",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524211",
                    "name": "Xiaofeng Mao",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524212",
                    "name": "Wangbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524213",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524214",
                    "name": "Xiaojun Chang",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524215",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524216",
                    "name": "Yang You",
                    "hidden": false
                },
                {
                    "_id": "67fe419a7161674011524217",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-08T08:06:53.000Z",
            "submittedOnDailyAt": "2025-04-15T09:53:18.868Z",
            "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\n  Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Multimodal reasoning, which integrates language and visual cues into problem\nsolving and decision making, is a fundamental aspect of human intelligence and\na crucial step toward artificial general intelligence. However, the evaluation\nof multimodal reasoning capabilities in Multimodal Large Language Models\n(MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained\nby limited data size, narrow domain coverage, and unstructured knowledge\ndistribution. To close these gaps, we introduce MDK12-Bench, a\nmulti-disciplinary benchmark assessing the reasoning capabilities of MLLMs via\nreal-world K-12 examinations. Spanning six disciplines (math, physics,\nchemistry, biology, geography, and information science), our benchmark\ncomprises 140K reasoning instances across diverse difficulty levels from\nprimary school to 12th grade. It features 6,827 instance-level knowledge point\nannotations based on a well-organized knowledge structure, detailed answer\nexplanations, difficulty labels and cross-year partitions, providing a robust\nplatform for comprehensive evaluation. Additionally, we present a novel dynamic\nevaluation framework to mitigate data contamination issues by bootstrapping\nquestion forms, question types, and image styles during evaluation. Extensive\nexperiment on MDK12-Bench reveals the significant limitation of current MLLMs\nin multimodal reasoning. The findings on our benchmark provide insights into\nthe development of the next-generation models. Our data and codes are available\nat https://github.com/LanceZPF/MDK12.",
            "upvotes": 2,
            "discussionId": "67fe419c71616740115242c7",
            "ai_keywords": [
                "multimodal reasoning",
                "Multimodal Large Language Models (MLLMs)",
                "MDK12-Bench",
                "knowledge point annotations",
                "difficulty labels",
                "dynamic evaluation framework",
                "data contamination issues"
            ]
        },
        "publishedAt": "2025-04-08T04:06:53.000Z",
        "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\n  Multimodal Large Language Models",
        "summary": "Multimodal reasoning, which integrates language and visual cues into problem\nsolving and decision making, is a fundamental aspect of human intelligence and\na crucial step toward artificial general intelligence. However, the evaluation\nof multimodal reasoning capabilities in Multimodal Large Language Models\n(MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained\nby limited data size, narrow domain coverage, and unstructured knowledge\ndistribution. To close these gaps, we introduce MDK12-Bench, a\nmulti-disciplinary benchmark assessing the reasoning capabilities of MLLMs via\nreal-world K-12 examinations. Spanning six disciplines (math, physics,\nchemistry, biology, geography, and information science), our benchmark\ncomprises 140K reasoning instances across diverse difficulty levels from\nprimary school to 12th grade. It features 6,827 instance-level knowledge point\nannotations based on a well-organized knowledge structure, detailed answer\nexplanations, difficulty labels and cross-year partitions, providing a robust\nplatform for comprehensive evaluation. Additionally, we present a novel dynamic\nevaluation framework to mitigate data contamination issues by bootstrapping\nquestion forms, question types, and image styles during evaluation. Extensive\nexperiment on MDK12-Bench reveals the significant limitation of current MLLMs\nin multimodal reasoning. The findings on our benchmark provide insights into\nthe development of the next-generation models. Our data and codes are available\nat https://github.com/LanceZPF/MDK12.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05782.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10430",
            "authors": [
                {
                    "_id": "67fe093302ae092e4306af12",
                    "user": {
                        "_id": "64c32a75d15a8812b71afc48",
                        "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
                        "isPro": false,
                        "fullname": "Minqian Liu",
                        "user": "mqliu",
                        "type": "user"
                    },
                    "name": "Minqian Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:32:23.136Z",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af13",
                    "user": {
                        "_id": "64b6c686cf5117d7962d8f62",
                        "avatarUrl": "/avatars/96ed7a9602aa4c21b3a3d89608e76dc8.svg",
                        "isPro": false,
                        "fullname": "Zhiyang Xu",
                        "user": "Zhiyang03",
                        "type": "user"
                    },
                    "name": "Zhiyang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:32:36.033Z",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af14",
                    "name": "Xinyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af15",
                    "user": {
                        "_id": "679d30bf48f48796199c415e",
                        "avatarUrl": "/avatars/c28e0dd7c9f6f62bccdd8eb2c8772c14.svg",
                        "isPro": false,
                        "fullname": "Heajun An",
                        "user": "aneverfull",
                        "type": "user"
                    },
                    "name": "Heajun An",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:32:49.203Z",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af16",
                    "user": {
                        "_id": "626e75252c2c6d44b30b1523",
                        "avatarUrl": "/avatars/458102026030f20af5e8c3c34c9b598c.svg",
                        "isPro": false,
                        "fullname": "Sarvech Qadir",
                        "user": "sarvech123",
                        "type": "user"
                    },
                    "name": "Sarvech Qadir",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-15T08:32:54.912Z",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af17",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af18",
                    "name": "Pamela J. Wisniewski",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af19",
                    "name": "Jin-Hee Cho",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af1a",
                    "name": "Sang Won Lee",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af1b",
                    "name": "Ruoxi Jia",
                    "hidden": false
                },
                {
                    "_id": "67fe093302ae092e4306af1c",
                    "name": "Lifu Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:20:34.000Z",
            "submittedOnDailyAt": "2025-04-15T05:53:52.240Z",
            "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64c32a75d15a8812b71afc48",
                "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
                "isPro": false,
                "fullname": "Minqian Liu",
                "user": "mqliu",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
            "upvotes": 1,
            "discussionId": "67fe093402ae092e4306af42",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "PersuSafety",
                "persuasion scene creation",
                "persuasive conversation simulation",
                "persuasion safety assessment"
            ]
        },
        "publishedAt": "2025-04-14T13:20:34.000Z",
        "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
        "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10430.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c32a75d15a8812b71afc48",
            "avatarUrl": "/avatars/4186c592b00aea73fb8c5bb719935ce7.svg",
            "fullname": "Minqian Liu",
            "name": "mqliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09513",
            "authors": [
                {
                    "_id": "67fe515cfd3df6c5de11c9f2",
                    "name": "Puyu Han",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f3",
                    "name": "Jiaju Kang",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f4",
                    "name": "Yuhang Pan",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f5",
                    "name": "Erting Pan",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f6",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": false,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T16:46:58.052Z",
                    "hidden": true
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f7",
                    "name": "Qunchao Jin",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f8",
                    "name": "Juntao Jiang",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9f9",
                    "name": "Zhichen Liu",
                    "hidden": false
                },
                {
                    "_id": "67fe515cfd3df6c5de11c9fa",
                    "name": "Luqi Gong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T10:35:25.000Z",
            "submittedOnDailyAt": "2025-04-15T11:00:41.524Z",
            "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": false,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Large-scale pre-trained diffusion models have produced excellent results in\nthe field of conditional image generation. However, restoration of ancient\nmurals, as an important downstream task in this field, poses significant\nchallenges to diffusion model-based restoration methods due to its large\ndefective area and scarce training samples. Conditional restoration tasks are\nmore concerned with whether the restored part meets the aesthetic standards of\nmural restoration in terms of overall style and seam detail, and such metrics\nfor evaluating heuristic image complements are lacking in current research. We\ntherefore propose DiffuMural, a combined Multi-scale convergence and\nCollaborative Diffusion mechanism with ControlNet and cyclic consistency loss\nto optimise the matching between the generated images and the conditional\ncontrol. DiffuMural demonstrates outstanding capabilities in mural restoration,\nleveraging training data from 23 large-scale Dunhuang murals that exhibit\nconsistent visual aesthetics. The model excels in restoring intricate details,\nachieving a coherent overall appearance, and addressing the unique challenges\nposed by incomplete murals lacking factual grounding. Our evaluation framework\nincorporates four key metrics to quantitatively assess incomplete murals:\nfactual accuracy, textural detail, contextual semantics, and holistic visual\ncoherence. Furthermore, we integrate humanistic value assessments to ensure the\nrestored murals retain their cultural and artistic significance. Extensive\nexperiments validate that our method outperforms state-of-the-art (SOTA)\napproaches in both qualitative and quantitative metrics.",
            "upvotes": 0,
            "discussionId": "67fe5162fd3df6c5de11cbc8",
            "ai_keywords": [
                "diffusion models",
                "conditional image generation",
                "mural restoration",
                "Multi-scale convergence",
                "Collaborative Diffusion mechanism",
                "ControlNet",
                "cyclic consistency loss",
                "heuristic image complements",
                "factual accuracy",
                "textural detail",
                "contextual semantics",
                "holistic visual coherence"
            ]
        },
        "publishedAt": "2025-04-13T06:35:25.000Z",
        "title": "DiffuMural: Restoring Dunhuang Murals with Multi-scale Diffusion",
        "summary": "Large-scale pre-trained diffusion models have produced excellent results in\nthe field of conditional image generation. However, restoration of ancient\nmurals, as an important downstream task in this field, poses significant\nchallenges to diffusion model-based restoration methods due to its large\ndefective area and scarce training samples. Conditional restoration tasks are\nmore concerned with whether the restored part meets the aesthetic standards of\nmural restoration in terms of overall style and seam detail, and such metrics\nfor evaluating heuristic image complements are lacking in current research. We\ntherefore propose DiffuMural, a combined Multi-scale convergence and\nCollaborative Diffusion mechanism with ControlNet and cyclic consistency loss\nto optimise the matching between the generated images and the conditional\ncontrol. DiffuMural demonstrates outstanding capabilities in mural restoration,\nleveraging training data from 23 large-scale Dunhuang murals that exhibit\nconsistent visual aesthetics. The model excels in restoring intricate details,\nachieving a coherent overall appearance, and addressing the unique challenges\nposed by incomplete murals lacking factual grounding. Our evaluation framework\nincorporates four key metrics to quantitatively assess incomplete murals:\nfactual accuracy, textural detail, contextual semantics, and holistic visual\ncoherence. Furthermore, we integrate humanistic value assessments to ensure the\nrestored murals retain their cultural and artistic significance. Extensive\nexperiments validate that our method outperforms state-of-the-art (SOTA)\napproaches in both qualitative and quantitative metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09513.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.03767",
            "authors": [
                {
                    "_id": "67fee58201428cf3b8408447",
                    "name": "Brandon Radosevich",
                    "hidden": false
                },
                {
                    "_id": "67fee58201428cf3b8408448",
                    "name": "John Halloran",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/Jj2_8dTF4ijUxLpJ-DKvQ.png",
                "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/LvYD1V_idO6Ei14jiqRWX.png",
                "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/aS3SAflJDy4dNkdOHwYqz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/et-WWzPCoDuAsiTKifDGW.png",
                "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/elprWpxMgxsJ1FQP8uAbH.png",
                "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/xjqV8f0e2hk4EedV1t2IC.png"
            ],
            "publishedAt": "2025-04-02T21:46:02.000Z",
            "submittedOnDailyAt": "2025-04-15T22:06:21.714Z",
            "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits",
            "submittedOnDailyBy": {
                "_id": "658749a2d861072dc5de6f76",
                "avatarUrl": "/avatars/88c73e5044eb73c0102fc4eb343589ac.svg",
                "isPro": true,
                "fullname": "John Halloran",
                "user": "johnhalloran",
                "type": "user"
            },
            "summary": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner",
            "upvotes": 0,
            "discussionId": "67fee58401428cf3b84084bd",
            "githubRepo": "https://github.com/johnhalloran321/mcpSafetyScanner",
            "ai_keywords": [
                "Model Context Protocol (MCP)",
                "large language models (LLMs)",
                "agentic tools",
                "automated workflows",
                "adversarial samples",
                "vulnerabilities",
                "remediations",
                "security report",
                "MCPSafetyScanner",
                "agentic workflows"
            ]
        },
        "publishedAt": "2025-04-02T17:46:02.000Z",
        "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits",
        "summary": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/Jj2_8dTF4ijUxLpJ-DKvQ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/LvYD1V_idO6Ei14jiqRWX.png",
            "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/aS3SAflJDy4dNkdOHwYqz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/et-WWzPCoDuAsiTKifDGW.png",
            "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/elprWpxMgxsJ1FQP8uAbH.png",
            "https://cdn-uploads.huggingface.co/production/uploads/658749a2d861072dc5de6f76/xjqV8f0e2hk4EedV1t2IC.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03767.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "658749a2d861072dc5de6f76",
            "avatarUrl": "/avatars/88c73e5044eb73c0102fc4eb343589ac.svg",
            "fullname": "John Halloran",
            "name": "johnhalloran",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]