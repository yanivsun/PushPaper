[
    {
        "paper": {
            "id": "2504.10481",
            "authors": [
                {
                    "_id": "67fdc1b41d1bc292f7b9358e",
                    "user": {
                        "_id": "64e18e9ec20c27fcc8df384e",
                        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
                        "isPro": false,
                        "fullname": "Ding Chen",
                        "user": "Hush-cd",
                        "type": "user"
                    },
                    "name": "Ding Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b9358f",
                    "user": {
                        "_id": "6455ff584095c967f9a847bb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6455ff584095c967f9a847bb/A5wjtWsudC73fLVmgASBr.jpeg",
                        "isPro": false,
                        "fullname": "Qingchen Yu",
                        "user": "Duguce",
                        "type": "user"
                    },
                    "name": "Qingchen Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:23:45.275Z",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93590",
                    "name": "Pengyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93591",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93592",
                    "name": "Bo Tang",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93593",
                    "name": "Feiyu Xiong",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93594",
                    "name": "Xinchi Li",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93595",
                    "name": "Minchuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67fdc1b41d1bc292f7b93596",
                    "name": "Zhiyu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:59:36.000Z",
            "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
            "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
            "submittedOnDailyBy": {
                "_id": "64e18e9ec20c27fcc8df384e",
                "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
                "isPro": false,
                "fullname": "Ding Chen",
                "user": "Hush-cd",
                "type": "user"
            },
            "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
            "upvotes": 62,
            "discussionId": "67fdc1b51d1bc292f7b935e8",
            "githubRepo": "https://github.com/IAAR-Shanghai/xVerify",
            "ai_keywords": [
                "reasoning models",
                "o1 model",
                "slow thinking strategies",
                "complex reasoning",
                "intermediate steps",
                "self-reflection",
                "evaluation methods",
                "LLM output",
                "reference answer",
                "final answer",
                "xVerify",
                "equivalence judgment",
                "VAR dataset",
                "multi-round annotation process",
                "F1 scores",
                "xVerify-0.5B-I",
                "xVerify-3B-Ib",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-04-14T13:59:36.000Z",
        "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
        "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "fullname": "Ding Chen",
            "name": "Hush-cd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08672",
            "authors": [
                {
                    "_id": "67fcb7294a92187863e805ee",
                    "user": {
                        "_id": "64e6cf78ecce34cb442dc889",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                        "isPro": false,
                        "fullname": "Fangzhi Xu",
                        "user": "xufangzhi",
                        "type": "user"
                    },
                    "name": "Fangzhi Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-14T09:46:16.537Z",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805ef",
                    "name": "Hang Yan",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f0",
                    "name": "Chang Ma",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f1",
                    "name": "Haiteng Zhao",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f2",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:24:08.107Z",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f3",
                    "name": "Kanzhi Cheng",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f4",
                    "name": "Junxian He",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f5",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "67fcb7294a92187863e805f6",
                    "name": "Zhiyong Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T16:26:23.000Z",
            "submittedOnDailyAt": "2025-04-16T05:46:28.754Z",
            "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
            "submittedOnDailyBy": {
                "_id": "64e6cf78ecce34cb442dc889",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                "isPro": false,
                "fullname": "Fangzhi Xu",
                "user": "xufangzhi",
                "type": "user"
            },
            "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
            "upvotes": 41,
            "discussionId": "67fcb72a4a92187863e8061b",
            "projectPage": "https://github.com/xufangzhi/Genius",
            "githubRepo": "https://github.com/xufangzhi/Genius",
            "ai_keywords": [
                "self-training framework",
                "Genius",
                "stepwise foresight re-sampling strategy",
                "advantage-calibrated optimization (ACO) loss function"
            ]
        },
        "publishedAt": "2025-04-11T12:26:23.000Z",
        "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
        "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "fullname": "Fangzhi Xu",
            "name": "xufangzhi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10766",
            "authors": [
                {
                    "_id": "67ff114a3026f8abc4bf7e43",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67ff114a3026f8abc4bf7e44",
                    "name": "Yanhong Li",
                    "hidden": false
                },
                {
                    "_id": "67ff114a3026f8abc4bf7e45",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "67ff114a3026f8abc4bf7e46",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:46:19.086Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T23:53:47.000Z",
            "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
            "upvotes": 30,
            "discussionId": "67ff11503026f8abc4bf7fed",
            "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
            "ai_keywords": [
                "spectral analysis",
                "layer-wise gradients",
                "low/high-quality instruction",
                "reasoning data",
                "IFD",
                "InsTag",
                "Difficulty",
                "Reward",
                "singular value decomposition (SVD)",
                "nuclear norms",
                "effective ranks",
                "gradient structures",
                "training stability"
            ]
        },
        "publishedAt": "2025-04-14T19:53:47.000Z",
        "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
        "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10337",
            "authors": [
                {
                    "_id": "67fddae99a03686367721718",
                    "user": {
                        "_id": "6471a24381ded91f253ceb1c",
                        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
                        "isPro": false,
                        "fullname": "Wesley Shi",
                        "user": "WesleyShi",
                        "type": "user"
                    },
                    "name": "Wenlei Shi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
                    "hidden": false
                },
                {
                    "_id": "67fddae99a03686367721719",
                    "name": "Xing Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T15:46:33.000Z",
            "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
            "title": "Heimdall: test-time scaling on the generative verification",
            "submittedOnDailyBy": {
                "_id": "6471a24381ded91f253ceb1c",
                "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
                "isPro": false,
                "fullname": "Wesley Shi",
                "user": "WesleyShi",
                "type": "user"
            },
            "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
            "upvotes": 28,
            "discussionId": "67fddaea9a03686367721776",
            "ai_keywords": [
                "Chain-of-Thought reasoning",
                "LLMs (Large Language Models)",
                "Heimdall",
                "long CoT verification",
                "pure reinforcement learning",
                "synthetic math problems",
                "human evaluation",
                "generalization capabilities",
                "Pessimistic Verification",
                "DeepSeek-R1-Distill-Qwen-32B",
                "AIME2025",
                "Gemini 2.5 Pro",
                "solution accuracy",
                "automatic knowledge discovery system",
                "ternary system",
                "NuminaMath",
                "data synthesis",
                "data records",
                "flawed data"
            ]
        },
        "publishedAt": "2025-04-14T11:46:33.000Z",
        "title": "Heimdall: test-time scaling on the generative verification",
        "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "fullname": "Wesley Shi",
            "name": "WesleyShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11346",
            "authors": [
                {
                    "_id": "67ff18961dc5d56fdd6ca724",
                    "name": "Yu Gao",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca725",
                    "name": "Lixue Gong",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca726",
                    "name": "Qiushan Guo",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca727",
                    "name": "Xiaoxia Hou",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca728",
                    "name": "Zhichao Lai",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca729",
                    "name": "Fanshi Li",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca72a",
                    "name": "Liang Li",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca72b",
                    "name": "Xiaochen Lian",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca72c",
                    "name": "Chao Liao",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca72d",
                    "name": "Liyang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca72e",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca72f",
                    "name": "Yichun Shi",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca730",
                    "name": "Shiqi Sun",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca731",
                    "name": "Yu Tian",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca732",
                    "name": "Zhi Tian",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca733",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca734",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca735",
                    "name": "Xuanda Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca736",
                    "name": "Xun Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca737",
                    "name": "Ye Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca738",
                    "name": "Guofeng Wu",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca739",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca73a",
                    "name": "Xin Xia",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca73b",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca73c",
                    "name": "Zhonghua Zhai",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca73d",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca73e",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca73f",
                    "name": "Yuwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca740",
                    "name": "Shijia Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca741",
                    "name": "Jianchao Yang",
                    "hidden": false
                },
                {
                    "_id": "67ff18961dc5d56fdd6ca742",
                    "name": "Weilin Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T16:19:07.000Z",
            "submittedOnDailyAt": "2025-04-16T01:10:39.295Z",
            "title": "Seedream 3.0 Technical Report",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
            "upvotes": 25,
            "discussionId": "67ff189c1dc5d56fdd6ca8e0",
            "projectPage": "https://team.doubao.com/zh/tech/seedream3_0",
            "ai_keywords": [
                "mixed-resolution training",
                "cross-modality RoPE",
                "representation alignment loss",
                "resolution-aware timestep sampling",
                "SFT (Supervised Fine-Tuning)",
                "VLM (Vision Language Model)",
                "consistent noise expectation",
                "importance-aware timestep sampling"
            ]
        },
        "publishedAt": "2025-04-15T12:19:07.000Z",
        "title": "Seedream 3.0 Technical Report",
        "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 47
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10465",
            "authors": [
                {
                    "_id": "67ff26c3414c03ebc1d42529",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d4252a",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:42:53.598Z",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d4252b",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d4252c",
                    "name": "Yanwei Li",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d4252d",
                    "name": "Weixian Lei",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d4252e",
                    "name": "Xueqing Deng",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d4252f",
                    "name": "Shihao Chen",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d42530",
                    "name": "Shunping Ji",
                    "hidden": false
                },
                {
                    "_id": "67ff26c3414c03ebc1d42531",
                    "name": "Jiashi Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:52:22.000Z",
            "submittedOnDailyAt": "2025-04-16T02:11:29.898Z",
            "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
            "submittedOnDailyBy": {
                "_id": "63958b4414513eaf9029ebf1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                "isPro": false,
                "fullname": "Xiangtai Li",
                "user": "LXT",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
            "upvotes": 22,
            "discussionId": "67ff26c6414c03ebc1d425de",
            "ai_keywords": [
                "multimodal large language models (MLLMs)",
                "pixel-level understanding",
                "vision encoder (CLIP)",
                "segmentation experts",
                "single transformer as a unified vision-language model (SAIL)",
                "pixel-wise MLLM tasks",
                "learnable upsampling module",
                "visual prompt injection",
                "visual prompt embeddings",
                "vision expert distillation",
                "pixel understanding benchmark (PerBench)",
                "detailed object description",
                "visual prompt-based question answering",
                "visual-text referring segmentation",
                "referring segmentation benchmarks",
                "visual prompt benchmark"
            ]
        },
        "publishedAt": "2025-04-14T13:52:22.000Z",
        "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
        "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "fullname": "Xiangtai Li",
            "name": "LXT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11442",
            "authors": [
                {
                    "_id": "67ff1387e1bfbb6bdd79ab72",
                    "user": {
                        "_id": "628b671f8fb67b90658613f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653303027789-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Leon Guertler",
                        "user": "LeonGuertler",
                        "type": "user"
                    },
                    "name": "Leon Guertler",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:43:02.481Z",
                    "hidden": false
                },
                {
                    "_id": "67ff1387e1bfbb6bdd79ab73",
                    "user": {
                        "_id": "653879fbf5f5016df355d010",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653879fbf5f5016df355d010/hU3mrTOw3DQ8auUGoQceW.jpeg",
                        "isPro": false,
                        "fullname": "Bobby Cheng",
                        "user": "bobbycxy",
                        "type": "user"
                    },
                    "name": "Bobby Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:43:06.026Z",
                    "hidden": false
                },
                {
                    "_id": "67ff1387e1bfbb6bdd79ab74",
                    "user": {
                        "_id": "636681feaa6a4af6073ba73e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
                        "isPro": true,
                        "fullname": "Simon Yu",
                        "user": "simonycl",
                        "type": "user"
                    },
                    "name": "Simon Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:43:08.368Z",
                    "hidden": false
                },
                {
                    "_id": "67ff1387e1bfbb6bdd79ab75",
                    "user": {
                        "_id": "635e3a76106f984574c36409",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                        "isPro": false,
                        "fullname": "Bo Liu",
                        "user": "Benjamin-eecs",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:46:16.265Z",
                    "hidden": false
                },
                {
                    "_id": "67ff1387e1bfbb6bdd79ab76",
                    "name": "Leshem Choshen",
                    "hidden": false
                },
                {
                    "_id": "67ff1387e1bfbb6bdd79ab77",
                    "name": "Cheston Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T17:55:20.000Z",
            "submittedOnDailyAt": "2025-04-16T01:06:25.215Z",
            "title": "TextArena",
            "submittedOnDailyBy": {
                "_id": "635e3a76106f984574c36409",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
                "isPro": false,
                "fullname": "Bo Liu",
                "user": "Benjamin-eecs",
                "type": "user"
            },
            "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
            "upvotes": 21,
            "discussionId": "67ff1388e1bfbb6bdd79abbe",
            "projectPage": "https://textarena.ai/",
            "githubRepo": "https://github.com/LeonGuertler/TextArena",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "TrueSkill scores",
                "negotiation",
                "theory of mind",
                "deception",
                "dynamic social skills"
            ]
        },
        "publishedAt": "2025-04-15T13:55:20.000Z",
        "title": "TextArena",
        "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "fullname": "Bo Liu",
            "name": "Benjamin-eecs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10462",
            "authors": [
                {
                    "_id": "67ff2aa6a0346c2e622afdb2",
                    "name": "Weixian Lei",
                    "hidden": false
                },
                {
                    "_id": "67ff2aa6a0346c2e622afdb3",
                    "name": "Jiacong Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff2aa6a0346c2e622afdb4",
                    "name": "Haochen Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff2aa6a0346c2e622afdb5",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:42:51.715Z",
                    "hidden": false
                },
                {
                    "_id": "67ff2aa6a0346c2e622afdb6",
                    "name": "Jun Hao Liew",
                    "hidden": false
                },
                {
                    "_id": "67ff2aa6a0346c2e622afdb7",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "67ff2aa6a0346c2e622afdb8",
                    "name": "Zilong Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:50:20.000Z",
            "submittedOnDailyAt": "2025-04-16T02:27:49.065Z",
            "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
            "submittedOnDailyBy": {
                "_id": "63958b4414513eaf9029ebf1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                "isPro": false,
                "fullname": "Xiangtai Li",
                "user": "LXT",
                "type": "user"
            },
            "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
            "upvotes": 12,
            "discussionId": "67ff2aa7a0346c2e622afe08",
            "ai_keywords": [
                "single transformer",
                "unified multimodal large language model (MLLM)",
                "raw pixel encoding",
                "language decoding",
                "vision transformer (ViT)",
                "mix-attention mechanisms",
                "multimodal positional encodings",
                "scalability",
                "cross-modal information flow patterns",
                "visual representation capabilities",
                "semantic segmentation",
                "ViT-22B"
            ]
        },
        "publishedAt": "2025-04-14T13:50:20.000Z",
        "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
        "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "fullname": "Xiangtai Li",
            "name": "LXT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10903",
            "authors": [
                {
                    "_id": "67ff8a2b81740ab021196e98",
                    "user": {
                        "_id": "67a4a26d5e65aa63c6d30e68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
                        "isPro": false,
                        "fullname": "FENG SICHENG",
                        "user": "FSCCS",
                        "type": "user"
                    },
                    "name": "Sicheng Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:57:54.189Z",
                    "hidden": false
                },
                {
                    "_id": "67ff8a2b81740ab021196e99",
                    "user": {
                        "_id": "646a1939c37ca1e12308fe81",
                        "avatarUrl": "/avatars/752e9d86018e7d33ad8bcd741203fd86.svg",
                        "isPro": false,
                        "fullname": "Gongfan Fang",
                        "user": "Vinnnf",
                        "type": "user"
                    },
                    "name": "Gongfan Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T13:42:05.338Z",
                    "hidden": false
                },
                {
                    "_id": "67ff8a2b81740ab021196e9a",
                    "name": "Xinyin Ma",
                    "hidden": false
                },
                {
                    "_id": "67ff8a2b81740ab021196e9b",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T06:28:00.000Z",
            "submittedOnDailyAt": "2025-04-16T11:13:22.093Z",
            "title": "Efficient Reasoning Models: A Survey",
            "submittedOnDailyBy": {
                "_id": "67a4a26d5e65aa63c6d30e68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
                "isPro": false,
                "fullname": "FENG SICHENG",
                "user": "FSCCS",
                "type": "user"
            },
            "summary": "Reasoning models have demonstrated remarkable progress in solving complex and\nlogic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to\narriving at a final answer. Yet, the emergence of this \"slow-thinking\"\nparadigm, with numerous tokens generated in sequence, inevitably introduces\nsubstantial computational overhead. To this end, it highlights an urgent need\nfor effective acceleration. This survey aims to provide a comprehensive\noverview of recent advances in efficient reasoning. It categorizes existing\nworks into three key directions: (1) shorter - compressing lengthy CoTs into\nconcise yet effective reasoning chains; (2) smaller - developing compact\nlanguage models with strong reasoning capabilities through techniques such as\nknowledge distillation, other model compression techniques, and reinforcement\nlearning; and (3) faster - designing efficient decoding strategies to\naccelerate inference. A curated collection of papers discussed in this survey\nis available in our GitHub repository.",
            "upvotes": 10,
            "discussionId": "67ff8a2d81740ab021196efb",
            "githubRepo": "https://github.com/fscdc/Awesome-Efficient-Reasoning-Models",
            "ai_keywords": [
                "Chain-of-Thoughts",
                "computational overhead",
                "knowledge distillation",
                "model compression techniques",
                "reinforcement learning",
                "efficient decoding strategies",
                "inference"
            ]
        },
        "publishedAt": "2025-04-15T02:28:00.000Z",
        "title": "Efficient Reasoning Models: A Survey",
        "summary": "Reasoning models have demonstrated remarkable progress in solving complex and\nlogic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to\narriving at a final answer. Yet, the emergence of this \"slow-thinking\"\nparadigm, with numerous tokens generated in sequence, inevitably introduces\nsubstantial computational overhead. To this end, it highlights an urgent need\nfor effective acceleration. This survey aims to provide a comprehensive\noverview of recent advances in efficient reasoning. It categorizes existing\nworks into three key directions: (1) shorter - compressing lengthy CoTs into\nconcise yet effective reasoning chains; (2) smaller - developing compact\nlanguage models with strong reasoning capabilities through techniques such as\nknowledge distillation, other model compression techniques, and reinforcement\nlearning; and (3) faster - designing efficient decoding strategies to\naccelerate inference. A curated collection of papers discussed in this survey\nis available in our GitHub repository.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10903.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a4a26d5e65aa63c6d30e68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
            "fullname": "FENG SICHENG",
            "name": "FSCCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10559",
            "authors": [
                {
                    "_id": "67ff1df03b42083b37219456",
                    "name": "Keyu Duan",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b37219457",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b37219458",
                    "name": "Xin Mao",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b37219459",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b3721945a",
                    "name": "Changyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b3721945b",
                    "name": "Qiguang Chen",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b3721945c",
                    "name": "Michael Qizhe Shieh",
                    "hidden": false
                },
                {
                    "_id": "67ff1df03b42083b3721945d",
                    "user": {
                        "_id": "6214e4ee1e35c843d42d1f88",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
                        "isPro": true,
                        "fullname": "Longxu Dou",
                        "user": "dreamerdeo",
                        "type": "user"
                    },
                    "name": "Longxu Dou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:43:00.444Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T14:53:56.000Z",
            "submittedOnDailyAt": "2025-04-16T01:34:03.069Z",
            "title": "Efficient Process Reward Model Training via Active Learning",
            "submittedOnDailyBy": {
                "_id": "6214e4ee1e35c843d42d1f88",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
                "isPro": true,
                "fullname": "Longxu Dou",
                "user": "dreamerdeo",
                "type": "user"
            },
            "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
            "upvotes": 10,
            "discussionId": "67ff1df23b42083b372194a8",
            "githubRepo": "https://github.com/sail-sg/ActivePRM",
            "ai_keywords": [
                "active learning",
                "ActPRM",
                "uncertainty estimation",
                "labeling costs",
                "vanilla fine-tuning",
                "pool-based active learning",
                "annotation efficiency",
                "math reasoning trajectories",
                "state-of-the-art (SOTA)",
                "ProcessBench",
                "PRMBench"
            ]
        },
        "publishedAt": "2025-04-14T10:53:56.000Z",
        "title": "Efficient Process Reward Model Training via Active Learning",
        "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "fullname": "Longxu Dou",
            "name": "dreamerdeo",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11427",
            "authors": [
                {
                    "_id": "67ff1cc4372d6790b1b7da90",
                    "name": "Yanrui Bin",
                    "hidden": false
                },
                {
                    "_id": "67ff1cc4372d6790b1b7da91",
                    "user": {
                        "_id": "657a7458afbb0117ba15c59f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                        "isPro": false,
                        "fullname": "Wenbo Hu",
                        "user": "wbhu-tc",
                        "type": "user"
                    },
                    "name": "Wenbo Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:57:59.932Z",
                    "hidden": false
                },
                {
                    "_id": "67ff1cc4372d6790b1b7da92",
                    "name": "Haoyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff1cc4372d6790b1b7da93",
                    "name": "Xinya Chen",
                    "hidden": false
                },
                {
                    "_id": "67ff1cc4372d6790b1b7da94",
                    "name": "Bing Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
            ],
            "publishedAt": "2025-04-15T17:39:07.000Z",
            "submittedOnDailyAt": "2025-04-16T01:28:48.790Z",
            "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
            "submittedOnDailyBy": {
                "_id": "657a7458afbb0117ba15c59f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
                "isPro": false,
                "fullname": "Wenbo Hu",
                "user": "wbhu-tc",
                "type": "user"
            },
            "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
            "upvotes": 9,
            "discussionId": "67ff1cc5372d6790b1b7daee",
            "projectPage": "https://normalcrafter.github.io/",
            "githubRepo": "https://github.com/Binyr/NormalCrafter",
            "ai_keywords": [
                "video diffusion models",
                "Semantic Feature Regularization (SFR)",
                "latent space",
                "pixel space",
                "temporal coherence",
                "spatial accuracy",
                "long temporal context",
                "temporally consistent",
                "intricate details"
            ]
        },
        "publishedAt": "2025-04-15T13:39:07.000Z",
        "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
        "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "fullname": "Wenbo Hu",
            "name": "wbhu-tc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10188",
            "authors": [
                {
                    "_id": "67fe602166c0e8f3c2df22a9",
                    "user": {
                        "_id": "649d59cec6b4fdd84ebe0d47",
                        "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
                        "isPro": false,
                        "fullname": "Deyuan Liu",
                        "user": "SempraETY",
                        "type": "user"
                    },
                    "name": "Deyuan Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-15T16:46:56.270Z",
                    "hidden": false
                },
                {
                    "_id": "67fe602166c0e8f3c2df22aa",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "67fe602166c0e8f3c2df22ab",
                    "name": "Xufeng Li",
                    "hidden": false
                },
                {
                    "_id": "67fe602166c0e8f3c2df22ac",
                    "name": "Tao Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T12:43:17.000Z",
            "submittedOnDailyAt": "2025-04-16T03:03:14.152Z",
            "title": "Efficient Generative Model Training via Embedded Representation Warmup",
            "submittedOnDailyBy": {
                "_id": "649d59cec6b4fdd84ebe0d47",
                "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
                "isPro": false,
                "fullname": "Deyuan Liu",
                "user": "SempraETY",
                "type": "user"
            },
            "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
            "upvotes": 8,
            "discussionId": "67fe602266c0e8f3c2df2334",
            "projectPage": "https://lins-lab.github.io/ERW/",
            "githubRepo": "https://github.com/LINs-lab/ERW",
            "ai_keywords": [
                "diffusion models",
                "high-dimensional data",
                "self-supervised methods",
                "high-quality representations",
                "semantic representations",
                "structural pattern learning",
                "Embedded Representation Warmup (ERW)",
                "warmup",
                "early layers",
                "representation processing region",
                "pretrained representations",
                "convergence",
                "training convergence",
                "representation quality",
                "REPA"
            ]
        },
        "publishedAt": "2025-04-14T08:43:17.000Z",
        "title": "Efficient Generative Model Training via Embedded Representation Warmup",
        "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "649d59cec6b4fdd84ebe0d47",
            "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
            "fullname": "Deyuan Liu",
            "name": "SempraETY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11393",
            "authors": [
                {
                    "_id": "67fffbd0fd5600e97f8deeff",
                    "name": "Ian Magnusson",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def00",
                    "name": "Nguyen Tai",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def01",
                    "name": "Ben Bogin",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def02",
                    "name": "David Heineman",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def03",
                    "name": "Jena D. Hwang",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def04",
                    "name": "Luca Soldaini",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def05",
                    "name": "Akshita Bhagia",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def06",
                    "name": "Jiacheng Liu",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def07",
                    "name": "Dirk Groeneveld",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def08",
                    "name": "Oyvind Tafjord",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def09",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def0a",
                    "name": "Pang Wei Koh",
                    "hidden": false
                },
                {
                    "_id": "67fffbd0fd5600e97f8def0b",
                    "name": "Jesse Dodge",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62bddd0b1e22ec8427a0f27e/aehRMdMEvD-jKSr8rDVEk.png"
            ],
            "publishedAt": "2025-04-15T17:02:15.000Z",
            "submittedOnDailyAt": "2025-04-16T17:22:15.774Z",
            "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
            "submittedOnDailyBy": {
                "_id": "62bddd0b1e22ec8427a0f27e",
                "avatarUrl": "/avatars/c433d0fb2945c70d1998f1e0e98d265f.svg",
                "isPro": false,
                "fullname": "Ian Magnusson",
                "user": "IanMagnusson",
                "type": "user"
            },
            "summary": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.",
            "upvotes": 7,
            "discussionId": "67fffbd2fd5600e97f8def6e",
            "ai_keywords": [
                "large language models",
                "deduplication",
                "filtering",
                "controlled pretraining",
                "MMLU",
                "ARC",
                "HellaSwag",
                "MBPP",
                "HumanEval",
                "continuous likelihood metrics"
            ]
        },
        "publishedAt": "2025-04-15T13:02:15.000Z",
        "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments",
        "summary": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62bddd0b1e22ec8427a0f27e/aehRMdMEvD-jKSr8rDVEk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11393.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62bddd0b1e22ec8427a0f27e",
            "avatarUrl": "/avatars/c433d0fb2945c70d1998f1e0e98d265f.svg",
            "fullname": "Ian Magnusson",
            "name": "IanMagnusson",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11001",
            "authors": [
                {
                    "_id": "67ff4bdb1dc5d56fdd7a1bc4",
                    "user": {
                        "_id": "62d7b2339b629105a5d6888a",
                        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                        "isPro": false,
                        "fullname": "Alan Dao",
                        "user": "alandao",
                        "type": "user"
                    },
                    "name": "Alan Dao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-16T11:13:40.678Z",
                    "hidden": false
                },
                {
                    "_id": "67ff4bdb1dc5d56fdd7a1bc5",
                    "name": "Thinh Le",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
            ],
            "publishedAt": "2025-04-15T09:18:21.000Z",
            "submittedOnDailyAt": "2025-04-16T04:49:41.555Z",
            "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
            "submittedOnDailyBy": {
                "_id": "62d7b2339b629105a5d6888a",
                "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                "isPro": false,
                "fullname": "Alan Dao",
                "user": "alandao",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
            "upvotes": 7,
            "discussionId": "67ff4bdc1dc5d56fdd7a1c36",
            "githubRepo": "https://github.com/menloresearch/ReZero",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "Large Language Model (LLM)",
                "knowledge-intensive tasks",
                "Reinforcement Learning (RL)",
                "query formulation",
                "reasoning over results",
                "ReZero (Retry-Zero)",
                "persistence",
                "search query",
                "unsuccessful attempt",
                "alternative queries",
                "robustness",
                "information-seeking scenarios"
            ]
        },
        "publishedAt": "2025-04-15T05:18:21.000Z",
        "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
        "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "fullname": "Alan Dao",
            "name": "alandao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10342",
            "authors": [
                {
                    "_id": "67fe51cda72a71760da6d239",
                    "name": "Yueqi Song",
                    "hidden": false
                },
                {
                    "_id": "67fe51cda72a71760da6d23a",
                    "name": "Tianyue Ou",
                    "hidden": false
                },
                {
                    "_id": "67fe51cda72a71760da6d23b",
                    "name": "Yibo Kong",
                    "hidden": false
                },
                {
                    "_id": "67fe51cda72a71760da6d23c",
                    "name": "Zecheng Li",
                    "hidden": false
                },
                {
                    "_id": "67fe51cda72a71760da6d23d",
                    "name": "Graham Neubig",
                    "hidden": false
                },
                {
                    "_id": "67fe51cda72a71760da6d23e",
                    "name": "Xiang Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T15:50:39.000Z",
            "submittedOnDailyAt": "2025-04-16T11:33:01.021Z",
            "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge",
            "submittedOnDailyBy": {
                "_id": "63e7bf7be02ee67e8e53f78d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7bf7be02ee67e8e53f78d/z9j1PWrurEyIA2JXzpz0i.png",
                "isPro": false,
                "fullname": "Yueqi Song",
                "user": "yueqis",
                "type": "user"
            },
            "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.",
            "upvotes": 7,
            "discussionId": "67fe51cea72a71760da6d2d8",
            "ai_keywords": [
                "multimodal benchmarks",
                "general reasoning abilities",
                "VisualPuzzles",
                "algorithmic reasoning",
                "analogical reasoning",
                "deductive reasoning",
                "inductive reasoning",
                "spatial reasoning",
                "Chinese Civil Service Examination",
                "logical reasoning",
                "domain-specific knowledge",
                "multimodal large language models",
                "knowledge-intensive benchmarks",
                "reasoning-focused tasks",
                "knowledge-light tasks",
                "\"thinking\" modes"
            ]
        },
        "publishedAt": "2025-04-14T11:50:39.000Z",
        "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge",
        "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10342.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63e7bf7be02ee67e8e53f78d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7bf7be02ee67e8e53f78d/z9j1PWrurEyIA2JXzpz0i.png",
            "fullname": "Yueqi Song",
            "name": "yueqis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10277",
            "authors": [
                {
                    "_id": "67ff741e01428cf3b865ceb5",
                    "user": {
                        "_id": "6596ca5cce76219628b8eab4",
                        "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
                        "isPro": false,
                        "fullname": "Pierre Le Jeune",
                        "user": "pierlj",
                        "type": "user"
                    },
                    "name": "Pierre Le Jeune",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:15:33.830Z",
                    "hidden": false
                },
                {
                    "_id": "67ff741e01428cf3b865ceb6",
                    "name": "Jiaen Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff741e01428cf3b865ceb7",
                    "name": "Luca Rossi",
                    "hidden": false
                },
                {
                    "_id": "67ff741e01428cf3b865ceb8",
                    "name": "Matteo Dora",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T14:44:41.000Z",
            "submittedOnDailyAt": "2025-04-16T15:53:48.680Z",
            "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
            "submittedOnDailyBy": {
                "_id": "6596ca5cce76219628b8eab4",
                "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
                "isPro": false,
                "fullname": "Pierre Le Jeune",
                "user": "pierlj",
                "type": "user"
            },
            "summary": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications.",
            "upvotes": 7,
            "discussionId": "67ff741f01428cf3b865ceef"
        },
        "publishedAt": "2025-04-14T10:44:41.000Z",
        "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
        "summary": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10277.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6596ca5cce76219628b8eab4",
            "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
            "fullname": "Pierre Le Jeune",
            "name": "pierlj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11456",
            "authors": [
                {
                    "_id": "67ff79b3d68757d92e9c168e",
                    "name": "Zhiwei He",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c168f",
                    "name": "Tian Liang",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1690",
                    "name": "Jiahao Xu",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1691",
                    "name": "Qiuzhi Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1692",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1693",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1694",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1695",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1696",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1697",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1698",
                    "name": "Zhuosheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c1699",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c169a",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c169b",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "67ff79b3d68757d92e9c169c",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T17:59:51.000Z",
            "submittedOnDailyAt": "2025-04-16T08:09:13.393Z",
            "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
            "submittedOnDailyBy": {
                "_id": "60107b385ac3e86b3ea4fc34",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
                "isPro": true,
                "fullname": "Daniel van Strien",
                "user": "davanstrien",
                "type": "user"
            },
            "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
            "upvotes": 6,
            "discussionId": "67ff79b4d68757d92e9c16e1",
            "githubRepo": "https://github.com/zwhe99/DeepMath",
            "ai_keywords": [
                "reinforcement learning",
                "large-scale dataset",
                "mathematical problems",
                "training data",
                "verifiable answer formats",
                "decontamination",
                "benchmark",
                "curriculum learning",
                "rule-based RL",
                "supervised fine-tuning",
                "distillation",
                "generalizable reasoning",
                "AI reasoning systems"
            ]
        },
        "publishedAt": "2025-04-15T13:59:51.000Z",
        "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
        "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11456.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60107b385ac3e86b3ea4fc34",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 588
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11343",
            "authors": [
                {
                    "_id": "67ff2d4a86e7ad2b4bea1349",
                    "name": "Wei Xiong",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea134a",
                    "name": "Jiarui Yao",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea134b",
                    "name": "Yuhui Xu",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea134c",
                    "name": "Bo Pang",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea134d",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea134e",
                    "name": "Doyen Sahoo",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea134f",
                    "name": "Junnan Li",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea1350",
                    "name": "Nan Jiang",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea1351",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea1352",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "67ff2d4a86e7ad2b4bea1353",
                    "name": "Hanze Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T16:15:02.000Z",
            "submittedOnDailyAt": "2025-04-16T02:39:25.160Z",
            "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
            "submittedOnDailyBy": {
                "_id": "643e59806db6ba8c5ee123f3",
                "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
                "isPro": false,
                "fullname": "Wei Xiong",
                "user": "weqweasdas",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
            "upvotes": 6,
            "discussionId": "67ff2d4b86e7ad2b4bea1381",
            "ai_keywords": [
                "GRPO",
                "DeepSeek-R1",
                "reinforcement learning (RL)",
                "fine-tuning",
                "large language models (LLMs)",
                "complex reasoning tasks",
                "RAFT",
                "positively rewarded samples",
                "policy gradient",
                "KL efficiency"
            ]
        },
        "publishedAt": "2025-04-15T12:15:02.000Z",
        "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
        "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643e59806db6ba8c5ee123f3",
            "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
            "fullname": "Wei Xiong",
            "name": "weqweasdas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11455",
            "authors": [
                {
                    "_id": "67ffa6ab3a1962b50cbaadd9",
                    "name": "Junke Wang",
                    "hidden": false
                },
                {
                    "_id": "67ffa6ab3a1962b50cbaadda",
                    "name": "Zhi Tian",
                    "hidden": false
                },
                {
                    "_id": "67ffa6ab3a1962b50cbaaddb",
                    "name": "Xun Wang",
                    "hidden": false
                },
                {
                    "_id": "67ffa6ab3a1962b50cbaaddc",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ffa6ab3a1962b50cbaaddd",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "67ffa6ab3a1962b50cbaadde",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "67ffa6ab3a1962b50cbaaddf",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T17:59:46.000Z",
            "submittedOnDailyAt": "2025-04-16T11:18:58.258Z",
            "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL",
            "submittedOnDailyBy": {
                "_id": "642e1e7147833318f0eb3755",
                "avatarUrl": "/avatars/7ef4f5a099d3eb0fab99b589f33903fa.svg",
                "isPro": false,
                "fullname": "Junke Wang",
                "user": "Daniel0724",
                "type": "user"
            },
            "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.",
            "upvotes": 4,
            "discussionId": "67ffa6ad3a1962b50cbaae3c",
            "githubRepo": "https://github.com/wdrink/SimpleAR",
            "ai_keywords": [
                "autoregressive visual generation",
                "training optimization",
                "inference optimization",
                "parameters",
                "resolution images",
                "fidelity",
                "GenEval",
                "DPG",
                "supervised fine-tuning",
                "Group Relative Policy Optimization",
                "generation aesthetics",
                "prompt alignment",
                "inference acceleration",
                "vLLM"
            ]
        },
        "publishedAt": "2025-04-15T13:59:46.000Z",
        "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation\n  through Pretraining, SFT, and RL",
        "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11455.png",
        "numComments": 0,
        "submittedBy": {
            "_id": "642e1e7147833318f0eb3755",
            "avatarUrl": "/avatars/7ef4f5a099d3eb0fab99b589f33903fa.svg",
            "fullname": "Junke Wang",
            "name": "Daniel0724",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11447",
            "authors": [
                {
                    "_id": "67ff1026f8afab940cc23f88",
                    "name": "An Zhaol",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f89",
                    "name": "Shengyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f8a",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f8b",
                    "name": "Zejian Li",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f8c",
                    "name": "Jiale Wu",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f8d",
                    "name": "Haoran Xu",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f8e",
                    "name": "AnYang Wei",
                    "hidden": false
                },
                {
                    "_id": "67ff1026f8afab940cc23f8f",
                    "name": "Perry Pengyun GU Lingyun Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T17:57:13.000Z",
            "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
            "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
            "submittedOnDailyBy": {
                "_id": "63943c882b9483beb473ec25",
                "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
                "isPro": false,
                "fullname": "Shengyuan Zhang",
                "user": "SYZhang0805",
                "type": "user"
            },
            "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
            "upvotes": 4,
            "discussionId": "67ff1027f8afab940cc23fd4",
            "ai_keywords": [
                "diffusion models",
                "LiDAR scene completion",
                "score distillation",
                "direct policy optimization (DPO)",
                "preference alignment",
                "student model",
                "paired completion scenes",
                "LiDAR scene evaluation metrics",
                "winning and losing sample pairs",
                "score functions",
                "preference learning"
            ]
        },
        "publishedAt": "2025-04-15T13:57:13.000Z",
        "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
        "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63943c882b9483beb473ec25",
            "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
            "fullname": "Shengyuan Zhang",
            "name": "SYZhang0805",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11409",
            "authors": [
                {
                    "_id": "67fff37f995d6bb5fe72e2bb",
                    "name": "Ali Taghibakhshi",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2bc",
                    "name": "Sharath Turuvekere Sreenivas",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2bd",
                    "name": "Saurav Muralidharan",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2be",
                    "name": "Marcin Chochowski",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2bf",
                    "name": "Yashaswi Karnati",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c0",
                    "name": "Raviraj Joshi",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c1",
                    "name": "Ameya Sunil Mahabaleshwarkar",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c2",
                    "name": "Zijia Chen",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c3",
                    "name": "Yoshi Suhara",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c4",
                    "name": "Oluwatobi Olabiyi",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c5",
                    "name": "Daniel Korzekwa",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c6",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c7",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c8",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2c9",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2ca",
                    "name": "Ashwath Aithal",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2cb",
                    "name": "Nima Tajbakhsh",
                    "hidden": false
                },
                {
                    "_id": "67fff37f995d6bb5fe72e2cc",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T17:26:29.000Z",
            "submittedOnDailyAt": "2025-04-16T16:46:26.831Z",
            "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning",
            "submittedOnDailyBy": {
                "_id": "654a4bd5b9cfada0bd39536e",
                "avatarUrl": "/avatars/d7663dd476a4e263ae7a288bb6bd2994.svg",
                "isPro": false,
                "fullname": "Ali Taghibakhshi",
                "user": "jrd971000",
                "type": "user"
            },
            "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.",
            "upvotes": 4,
            "discussionId": "67fff380995d6bb5fe72e2fa",
            "ai_keywords": [
                "Hybrid LLM architectures",
                "Attention",
                "State Space Models (SSMs)",
                "compression",
                "distillation",
                "group-aware pruning strategy",
                "structural integrity",
                "sequence modeling capabilities",
                "FFN (Feedforward Neural Network)",
                "embedding dimension",
                "layer pruning",
                "knowledge distillation",
                "MINITRON technique",
                "Nemotron-H 8B Hybrid model"
            ]
        },
        "publishedAt": "2025-04-15T13:26:29.000Z",
        "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning",
        "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11409.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654a4bd5b9cfada0bd39536e",
            "avatarUrl": "/avatars/d7663dd476a4e263ae7a288bb6bd2994.svg",
            "fullname": "Ali Taghibakhshi",
            "name": "jrd971000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11326",
            "authors": [
                {
                    "_id": "67ff25b765b52d1b69c1f6c1",
                    "user": {
                        "_id": "67ff29ecbf6889a333c69c7a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
                        "isPro": false,
                        "fullname": "Henghui Ding",
                        "user": "HenghuiDing",
                        "type": "user"
                    },
                    "name": "Henghui Ding",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-16T03:55:17.825Z",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c2",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c3",
                    "name": "Nikhila Ravi",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c4",
                    "name": "Shuting He",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c5",
                    "name": "Yunchao Wei",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c6",
                    "name": "Song Bai",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c7",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c8",
                    "name": "Kehuan Song",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6c9",
                    "name": "Xinglin Xie",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6ca",
                    "name": "Kexin Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6cb",
                    "name": "Licheng Jiao",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6cc",
                    "name": "Lingling Li",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6cd",
                    "name": "Shuyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6ce",
                    "name": "Xuqiang Cao",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6cf",
                    "name": "Linnan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d0",
                    "name": "Jiaxuan Zhao",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d1",
                    "name": "Fang Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d2",
                    "name": "Mengjiao Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d3",
                    "name": "Junpei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d4",
                    "name": "Xu Liu",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d5",
                    "name": "Yuting Yang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d6",
                    "name": "Mengru Ma",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d7",
                    "name": "Hao Fang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d8",
                    "name": "Runmin Cong",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6d9",
                    "name": "Xiankai Lu",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6da",
                    "name": "Zhiyang Che",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6db",
                    "name": "Wei Zhan",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6dc",
                    "name": "Tianming Liang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6dd",
                    "name": "Haichao Jiang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6de",
                    "name": "Wei-Shi Zheng",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6df",
                    "name": "Jian-Fang Hu",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6e0",
                    "name": "Haobo Yuan",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6e1",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:42:58.429Z",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6e2",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6e3",
                    "name": "Lu Qi",
                    "hidden": false
                },
                {
                    "_id": "67ff25b765b52d1b69c1f6e4",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T16:02:47.000Z",
            "submittedOnDailyAt": "2025-04-16T02:26:49.281Z",
            "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
            "submittedOnDailyBy": {
                "_id": "67ff29ecbf6889a333c69c7a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
                "isPro": false,
                "fullname": "Henghui Ding",
                "user": "HenghuiDing",
                "type": "user"
            },
            "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
            "upvotes": 4,
            "discussionId": "67ff25b865b52d1b69c1f736"
        },
        "publishedAt": "2025-04-15T12:02:47.000Z",
        "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
        "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "fullname": "Henghui Ding",
            "name": "HenghuiDing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08846",
            "authors": [
                {
                    "_id": "67ff25f33026f8abc4c4a10d",
                    "name": "Mostafa Faghih Shojaei",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a10e",
                    "name": "Rahul Gulati",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a10f",
                    "name": "Benjamin A. Jasperson",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a110",
                    "name": "Shangshang Wang",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a111",
                    "user": {
                        "_id": "6729342804227d5ea3b283c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
                        "isPro": false,
                        "fullname": "Simone Cimolato",
                        "user": "simocimolato",
                        "type": "user"
                    },
                    "name": "Simone Cimolato",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:42:55.794Z",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a112",
                    "user": {
                        "_id": "672ad19141a93b8e140e8689",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VRMw7_PIE5QoD_eWP0hOp.png",
                        "isPro": false,
                        "fullname": "Dangli Cao",
                        "user": "Dinzhenzhenzhu",
                        "type": "user"
                    },
                    "name": "Dangli Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:23:14.535Z",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a113",
                    "name": "Willie Neiswanger",
                    "hidden": false
                },
                {
                    "_id": "67ff25f33026f8abc4c4a114",
                    "user": {
                        "_id": "67859d73e670c62966ba5767",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mx3fE-YPsjsxpYsBh_DDn.png",
                        "isPro": false,
                        "fullname": "Krishna Garikipati",
                        "user": "garikipati",
                        "type": "user"
                    },
                    "name": "Krishna Garikipati",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-16T05:32:49.978Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T01:26:34.000Z",
            "submittedOnDailyAt": "2025-04-16T07:21:37.722Z",
            "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
            "submittedOnDailyBy": {
                "_id": "6729342804227d5ea3b283c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
                "isPro": false,
                "fullname": "Simone Cimolato",
                "user": "simocimolato",
                "type": "user"
            },
            "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
            "upvotes": 4,
            "discussionId": "67ff25f43026f8abc4c4a16c",
            "projectPage": "https://my-ai-university.com",
            "githubRepo": "https://github.com/my-ai-university/finite-element-method",
            "ai_keywords": [
                "large language model (LLM)",
                "retrieval-augmented generation (RAG)",
                "finite-element-method (FEM)",
                "Low-Rank Adaptation (LoRA)",
                "RAG-based synthesis",
                "cosine similarity",
                "LLM-based assessment",
                "traceability",
                "base Llama 3.2 model"
            ]
        },
        "publishedAt": "2025-04-10T21:26:34.000Z",
        "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
        "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08846.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6729342804227d5ea3b283c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
            "fullname": "Simone Cimolato",
            "name": "simocimolato",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09454",
            "authors": [
                {
                    "_id": "67ff6f8c661b74d0050d2774",
                    "name": "Weinan Jia",
                    "hidden": false
                },
                {
                    "_id": "67ff6f8c661b74d0050d2775",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "67ff6f8c661b74d0050d2776",
                    "name": "Nan Chen",
                    "hidden": false
                },
                {
                    "_id": "67ff6f8c661b74d0050d2777",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff6f8c661b74d0050d2778",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T06:33:28.000Z",
            "submittedOnDailyAt": "2025-04-16T07:21:45.180Z",
            "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
            "submittedOnDailyBy": {
                "_id": "630636bcd37ce67e0e4d1d42",
                "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
                "isPro": false,
                "fullname": "Mengqi Huang",
                "user": "CoreloneH",
                "type": "user"
            },
            "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D^2iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT.",
            "upvotes": 3,
            "discussionId": "67ff6f8f661b74d0050d28a9",
            "ai_keywords": [
                "Diffusion models",
                "Diffusion Transformer (DiT)",
                "compression",
                "image regions",
                "information densities",
                "local realism",
                "computational complexity",
                "global consistency",
                "generated images",
                "Dynamic VAE (DVAE)",
                "hierarchical encoder",
                "downsampling rates",
                "latent codes",
                "Dynamic Diffusion Transformer (D$^2$iT)",
                "multi-grained noise",
                "coarse-grained",
                "fine-grained",
                "Dynamic Grain Transformer",
                "Dynamic Content Transformer",
                "rough prediction",
                "detailed regions correction"
            ]
        },
        "publishedAt": "2025-04-13T02:33:28.000Z",
        "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
        "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D^2iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09454.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630636bcd37ce67e0e4d1d42",
            "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
            "fullname": "Mengqi Huang",
            "name": "CoreloneH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.06949",
            "authors": [
                {
                    "_id": "67ff12ea58ed263257af79b5",
                    "name": "Zhixuan Lin",
                    "hidden": false
                },
                {
                    "_id": "67ff12ea58ed263257af79b6",
                    "name": "Johan Obando-Ceron",
                    "hidden": false
                },
                {
                    "_id": "67ff12ea58ed263257af79b7",
                    "user": {
                        "_id": "66906c4e37eadb9c577984d3",
                        "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
                        "isPro": false,
                        "fullname": "Owen He",
                        "user": "littleowen",
                        "type": "user"
                    },
                    "name": "Xu Owen He",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-16T02:16:11.020Z",
                    "hidden": false
                },
                {
                    "_id": "67ff12ea58ed263257af79b8",
                    "name": "Aaron Courville",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-09T14:57:55.000Z",
            "submittedOnDailyAt": "2025-04-16T01:28:49.703Z",
            "title": "Adaptive Computation Pruning for the Forgetting Transformer",
            "submittedOnDailyBy": {
                "_id": "6694cc1009326cb83f2d11bb",
                "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
                "isPro": false,
                "fullname": "Zhixuan Lin",
                "user": "zhixuan-lin",
                "type": "user"
            },
            "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
            "upvotes": 3,
            "discussionId": "67ff12eb58ed263257af79fc",
            "ai_keywords": [
                "Forgetting Transformer (FoX)",
                "forget gate",
                "softmax attention",
                "RoPE-based Transformer",
                "Adaptive Computation Pruning (ACP)",
                "input-output dependencies",
                "pruning threshold",
                "FLOPs",
                "training throughput",
                "pruning patterns"
            ]
        },
        "publishedAt": "2025-04-09T10:57:55.000Z",
        "title": "Adaptive Computation Pruning for the Forgetting Transformer",
        "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6694cc1009326cb83f2d11bb",
            "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
            "fullname": "Zhixuan Lin",
            "name": "zhixuan-lin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11042",
            "authors": [
                {
                    "_id": "67ff9068de173bffef0e7eae",
                    "user": {
                        "_id": "6408e9f2bf3e9a4bb2e09400",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678306212334-6408e9f2bf3e9a4bb2e09400.jpeg",
                        "isPro": false,
                        "fullname": "Sukannya Purkayastha",
                        "user": "sukannya",
                        "type": "user"
                    },
                    "name": "Sukannya Purkayastha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:57:51.596Z",
                    "hidden": false
                },
                {
                    "_id": "67ff9068de173bffef0e7eaf",
                    "name": "Zhuang Li",
                    "hidden": false
                },
                {
                    "_id": "67ff9068de173bffef0e7eb0",
                    "name": "Anne Lauscher",
                    "hidden": false
                },
                {
                    "_id": "67ff9068de173bffef0e7eb1",
                    "name": "Lizhen Qu",
                    "hidden": false
                },
                {
                    "_id": "67ff9068de173bffef0e7eb2",
                    "name": "Iryna Gurevych",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T10:07:33.000Z",
            "submittedOnDailyAt": "2025-04-16T09:42:33.166Z",
            "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
            "submittedOnDailyBy": {
                "_id": "6408e9f2bf3e9a4bb2e09400",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678306212334-6408e9f2bf3e9a4bb2e09400.jpeg",
                "isPro": false,
                "fullname": "Sukannya Purkayastha",
                "user": "sukannya",
                "type": "user"
            },
            "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)",
            "upvotes": 2,
            "discussionId": "67ff906ade173bffef0e7f10",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "zero-shot setting",
                "instruction-based fine-tuning"
            ]
        },
        "publishedAt": "2025-04-15T06:07:33.000Z",
        "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews",
        "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11042.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6408e9f2bf3e9a4bb2e09400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678306212334-6408e9f2bf3e9a4bb2e09400.jpeg",
            "fullname": "Sukannya Purkayastha",
            "name": "sukannya",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10443",
            "authors": [
                {
                    "_id": "67ff92d142dd8c5df9fcb97a",
                    "user": {
                        "_id": "669e221425f2081c6e3d8b61",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e221425f2081c6e3d8b61/OearXzKrvoIZtDKTH4kje.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Hao",
                        "user": "Hoar012",
                        "type": "user"
                    },
                    "name": "Haoran Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:57:49.451Z",
                    "hidden": false
                },
                {
                    "_id": "67ff92d142dd8c5df9fcb97b",
                    "name": "Jiaming Han",
                    "hidden": false
                },
                {
                    "_id": "67ff92d142dd8c5df9fcb97c",
                    "name": "Yiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "67ff92d142dd8c5df9fcb97d",
                    "name": "Xiangyu Yue",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:34:06.000Z",
            "submittedOnDailyAt": "2025-04-16T09:55:00.813Z",
            "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
            "submittedOnDailyBy": {
                "_id": "669e221425f2081c6e3d8b61",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e221425f2081c6e3d8b61/OearXzKrvoIZtDKTH4kje.jpeg",
                "isPro": false,
                "fullname": "Haoran Hao",
                "user": "Hoar012",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
            "upvotes": 2,
            "discussionId": "67ff92d242dd8c5df9fcb9fc",
            "projectPage": "https://hoar012.github.io/TDC-Project/",
            "githubRepo": "https://github.com/Hoar012/TDC-Video",
            "ai_keywords": [
                "Temporal Dynamic Context (TDC)",
                "semantically consistent scenes",
                "inter-frame similarities",
                "visual-audio encoders",
                "temporal context compressor",
                "query-based Transformer",
                "static frame tokens",
                "temporal context tokens",
                "training-free chain-of-thought strategy",
                "general video understanding benchmarks",
                "audio-video understanding benchmarks"
            ]
        },
        "publishedAt": "2025-04-14T13:34:06.000Z",
        "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
        "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10443.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "669e221425f2081c6e3d8b61",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669e221425f2081c6e3d8b61/OearXzKrvoIZtDKTH4kje.jpeg",
            "fullname": "Haoran Hao",
            "name": "Hoar012",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10049",
            "authors": [
                {
                    "_id": "67ff5b335cf0fe153845d1c9",
                    "user": {
                        "_id": "60d35154d7b174177faabd55",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
                        "isPro": false,
                        "fullname": "Théo Gigant",
                        "user": "gigant",
                        "type": "user"
                    },
                    "name": "Théo Gigant",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T08:42:49.462Z",
                    "hidden": false
                },
                {
                    "_id": "67ff5b335cf0fe153845d1ca",
                    "name": "Camille Guinaudeau",
                    "hidden": false
                },
                {
                    "_id": "67ff5b335cf0fe153845d1cb",
                    "name": "Frédéric Dufaux",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T09:55:01.000Z",
            "submittedOnDailyAt": "2025-04-16T05:55:17.253Z",
            "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
            "submittedOnDailyBy": {
                "_id": "60d35154d7b174177faabd55",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
                "isPro": false,
                "fullname": "Théo Gigant",
                "user": "gigant",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
            "upvotes": 2,
            "discussionId": "67ff5b355cf0fe153845d215",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "automatic summarization",
                "multimodal presentations",
                "text-heavy multimodal documents",
                "input-length budgets",
                "slides",
                "video stream",
                "raw video",
                "structured representation",
                "interleaved slides",
                "transcript",
                "cross-modal interactions"
            ]
        },
        "publishedAt": "2025-04-14T05:55:01.000Z",
        "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
        "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60d35154d7b174177faabd55",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
            "fullname": "Théo Gigant",
            "name": "gigant",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 32
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11457",
            "authors": [
                {
                    "_id": "67ffcf5874193e65a7831a17",
                    "name": "Ziqi Pang",
                    "hidden": false
                },
                {
                    "_id": "67ffcf5874193e65a7831a18",
                    "name": "Xin Xu",
                    "hidden": false
                },
                {
                    "_id": "67ffcf5874193e65a7831a19",
                    "name": "Yu-Xiong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T17:59:54.000Z",
            "submittedOnDailyAt": "2025-04-16T14:10:26.944Z",
            "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes\n  Diffusion for Visual Perception",
            "submittedOnDailyBy": {
                "_id": "642a33ea5673845d9854f458",
                "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
                "isPro": false,
                "fullname": "Ziqi Pang",
                "user": "ziqipang",
                "type": "user"
            },
            "summary": "With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP.",
            "upvotes": 0,
            "discussionId": "67ffcf5974193e65a7831a71",
            "ai_keywords": [
                "generative diffusion models",
                "pixel generation",
                "generative denoising process",
                "discriminative tasks",
                "intermediate sampling errors",
                "perception quality",
                "timestep contributions",
                "perception degradation",
                "training-denoising distribution shifts",
                "diffusion-tailored data augmentation",
                "interactivity",
                "user interfaces",
                "correctional prompts",
                "multi-round interactions",
                "depth estimation",
                "referring image segmentation",
                "generalist perception tasks"
            ]
        },
        "publishedAt": "2025-04-15T13:59:54.000Z",
        "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes\n  Diffusion for Visual Perception",
        "summary": "With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11457.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "642a33ea5673845d9854f458",
            "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
            "fullname": "Ziqi Pang",
            "name": "ziqipang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11080",
            "authors": [
                {
                    "_id": "67ff97c20816a39af5ec7c96",
                    "user": {
                        "_id": "67ff950b9e4824de182acf83",
                        "avatarUrl": "/avatars/d8bae10afdd7457a08ec9554f79c7429.svg",
                        "isPro": false,
                        "fullname": "Elman Ghazaei",
                        "user": "ElmanGhazaei",
                        "type": "user"
                    },
                    "name": "Elman Ghazaei",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-16T11:43:00.595Z",
                    "hidden": false
                },
                {
                    "_id": "67ff97c20816a39af5ec7c97",
                    "name": "Erchan Aptoula",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T11:25:10.000Z",
            "submittedOnDailyAt": "2025-04-16T10:14:03.191Z",
            "title": "Change State Space Models for Remote Sensing Change Detection",
            "submittedOnDailyBy": {
                "_id": "67ff950b9e4824de182acf83",
                "avatarUrl": "/avatars/d8bae10afdd7457a08ec9554f79c7429.svg",
                "isPro": false,
                "fullname": "Elman Ghazaei",
                "user": "ElmanGhazaei",
                "type": "user"
            },
            "summary": "Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance.",
            "upvotes": 0,
            "discussionId": "67ff97c40816a39af5ec7d15",
            "githubRepo": "https://github.com/Elman295/CSSM",
            "ai_keywords": [
                "ConvNets",
                "Vision transformers (ViT)",
                "State Space Models",
                "Vision Mamba",
                "Change State Space Model",
                "bi-temporal images",
                "network parameters",
                "computational efficiency",
                "detection performance",
                "robustness",
                "benchmark datasets"
            ]
        },
        "publishedAt": "2025-04-15T07:25:10.000Z",
        "title": "Change State Space Models for Remote Sensing Change Detection",
        "summary": "Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11080.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67ff950b9e4824de182acf83",
            "avatarUrl": "/avatars/d8bae10afdd7457a08ec9554f79c7429.svg",
            "fullname": "Elman Ghazaei",
            "name": "ElmanGhazaei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]