[
    {
        "paper": {
            "id": "2504.10514",
            "authors": [
                {
                    "_id": "67ffedb8b0c26d6ec0b608cf",
                    "name": "Yijun Liang",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d0",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d1",
                    "user": {
                        "_id": "64a8121e35fab7cd04c30ed0",
                        "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
                        "isPro": false,
                        "fullname": "Chenrui Fan",
                        "user": "Fcr09",
                        "type": "user"
                    },
                    "name": "Chenrui Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:05:05.662Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d2",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d3",
                    "name": "Dang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d4",
                    "user": {
                        "_id": "63f546e0fcf95ecac2b0ee3e",
                        "avatarUrl": "/avatars/02a401bcff91cc473d9946bbb771a985.svg",
                        "isPro": false,
                        "fullname": "Kwesi Cobbina",
                        "user": "kweCobi",
                        "type": "user"
                    },
                    "name": "Kwesi Cobbina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:08:36.225Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d5",
                    "user": {
                        "_id": "639d4b8d860db464ae35c3ab",
                        "avatarUrl": "/avatars/ec0fa3e91593a03fc9fb611e66b30553.svg",
                        "isPro": false,
                        "fullname": "Shweta Bhardwaj",
                        "user": "shweta12",
                        "type": "user"
                    },
                    "name": "Shweta Bhardwaj",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:08:29.680Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d6",
                    "user": {
                        "_id": "6393847e3e30234ae798b7be",
                        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
                        "isPro": true,
                        "fullname": "JiuhaiChen",
                        "user": "jiuhai",
                        "type": "user"
                    },
                    "name": "Jiuhai Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:08:22.786Z",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d7",
                    "name": "Fuxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "67ffedb8b0c26d6ec0b608d8",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:05:07.396Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T16:36:26.000Z",
            "submittedOnDailyAt": "2025-04-17T00:58:33.032Z",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
            "upvotes": 34,
            "discussionId": "67ffedbeb0c26d6ec0b60a5b",
            "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ColorBench",
            "githubRepo": "https://github.com/tianyi-lab/ColorBench",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "color understanding",
                "color perception",
                "color-based cues",
                "color transformations",
                "scaling law",
                "language model",
                "vision encoder",
                "CoT reasoning",
                "multimodal AI"
            ]
        },
        "publishedAt": "2025-04-10T12:36:26.000Z",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10514.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12285",
            "authors": [
                {
                    "_id": "68006e6e175c8dce4ec17f7a",
                    "user": {
                        "_id": "613f07f40153aafa379775f2",
                        "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
                        "isPro": false,
                        "fullname": "Shuming Ma",
                        "user": "shumingma",
                        "type": "user"
                    },
                    "name": "Shuming Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:13:17.676Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7b",
                    "user": {
                        "_id": "63f71771d36951307fcb4dcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                        "isPro": false,
                        "fullname": "Hongyu Wang",
                        "user": "hongyuw",
                        "type": "user"
                    },
                    "name": "Hongyu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:14:45.597Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7c",
                    "user": {
                        "_id": "632bd2f72d6a805eeb4bc601",
                        "avatarUrl": "/avatars/6e1533e8a599f3068290aa69ac82cab7.svg",
                        "isPro": false,
                        "fullname": "HUANG SHAOHAN",
                        "user": "buaahsh",
                        "type": "user"
                    },
                    "name": "Shaohan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:14:59.750Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7d",
                    "user": {
                        "_id": "64abbcff6cadc7aca584f71b",
                        "avatarUrl": "/avatars/fc6e85ad4a8befd133a37b411712c648.svg",
                        "isPro": false,
                        "fullname": "Xingxing Zhang",
                        "user": "THU-CHUNXIA",
                        "type": "user"
                    },
                    "name": "Xingxing Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:15:06.359Z",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7e",
                    "name": "Ying Hu",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f7f",
                    "name": "Ting Song",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f80",
                    "name": "Yan Xia",
                    "hidden": false
                },
                {
                    "_id": "68006e6e175c8dce4ec17f81",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:15:42.250Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T17:51:43.000Z",
            "submittedOnDailyAt": "2025-04-17T01:29:56.744Z",
            "title": "BitNet b1.58 2B4T Technical Report",
            "submittedOnDailyBy": {
                "_id": "63f71771d36951307fcb4dcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                "isPro": false,
                "fullname": "Hongyu Wang",
                "user": "hongyuw",
                "type": "user"
            },
            "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
            "upvotes": 31,
            "discussionId": "68006e70175c8dce4ec17fc0",
            "ai_keywords": [
                "BitNet b1.58 2B4T",
                "Large Language Model (LLM)",
                "1-bit Large Language Model",
                "Train",
                "Corpus",
                "4 trillion tokens",
                "Benchmarks",
                "Language understanding",
                "Mathematical reasoning",
                "Coding proficiency",
                "Conversational ability"
            ]
        },
        "publishedAt": "2025-04-16T13:51:43.000Z",
        "title": "BitNet b1.58 2B4T Technical Report",
        "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12285.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "fullname": "Hongyu Wang",
            "name": "hongyuw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11536",
            "authors": [
                {
                    "_id": "6800cc7159e20f50cc282e87",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e88",
                    "user": {
                        "_id": "64ce05c631c655ff8a2e183c",
                        "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
                        "isPro": false,
                        "fullname": "Shijue Huang",
                        "user": "JoeYing",
                        "type": "user"
                    },
                    "name": "Shijue Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:05.968Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e89",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8a",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:27.962Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8b",
                    "user": {
                        "_id": "643f37cce9d063936912048b",
                        "avatarUrl": "/avatars/25822ea5676a79b2e1ddf08d5fc2226c.svg",
                        "isPro": false,
                        "fullname": "Yujia Qin",
                        "user": "YujiaHi",
                        "type": "user"
                    },
                    "name": "Yujia Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:35.665Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8c",
                    "name": "Baoquan Zhong",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8d",
                    "user": {
                        "_id": "6698fe04a188ffb7e412deb7",
                        "avatarUrl": "/avatars/e389e72e37916b74efc14724d56a0cf1.svg",
                        "isPro": false,
                        "fullname": "Chengquan Jiang",
                        "user": "imjcqt",
                        "type": "user"
                    },
                    "name": "Chengquan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:51.371Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8e",
                    "user": {
                        "_id": "671ca8f1299315f77400863a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lB32V9C2R2D1rAXhmz73w.png",
                        "isPro": false,
                        "fullname": "Jinxin Chi",
                        "user": "chijx",
                        "type": "user"
                    },
                    "name": "Jinxin Chi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:11:57.367Z",
                    "hidden": false
                },
                {
                    "_id": "6800cc7159e20f50cc282e8f",
                    "user": {
                        "_id": "643f956635e2b54a42e7feba",
                        "avatarUrl": "/avatars/c6185f81ae8499ae866ad451c1cbf43b.svg",
                        "isPro": false,
                        "fullname": "Wanjun Zhong",
                        "user": "WanjunZhong",
                        "type": "user"
                    },
                    "name": "Wanjun Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:12:03.535Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T18:10:22.000Z",
            "submittedOnDailyAt": "2025-04-17T08:10:39.383Z",
            "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
            "upvotes": 27,
            "discussionId": "6800cc7359e20f50cc282f43",
            "ai_keywords": [
                "reinforcement learning",
                "dynamic interleaving",
                "real-time code execution",
                "natural language reasoning processes",
                "automated RL paradigm",
                "policy rollouts",
                "multi-turn real-time code execution",
                "synthetic cold-start data generation",
                "code-augmented long-form reasoning traces",
                "fine-tuning",
                "RL training",
                "task outcomes as rewards",
                "autonomous discovery",
                "optimal tool invocation patterns",
                "MATH Olympiad benchmark",
                "AIME",
                "accuracy",
                "training steps",
                "OpenAI's o1-preview",
                "code self-correction",
                "adaptive tool use",
                "hybrid neuro-symbolic systems"
            ]
        },
        "publishedAt": "2025-04-15T14:10:22.000Z",
        "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
        "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11536.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6679
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12240",
            "authors": [
                {
                    "_id": "680057b49031335df49732fc",
                    "user": {
                        "_id": "64970d3d9c3b29dca8633f87",
                        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
                        "isPro": false,
                        "fullname": "JunhaoZhuang",
                        "user": "JunhaoZhuang",
                        "type": "user"
                    },
                    "name": "Junhao Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:04:09.222Z",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df49732fd",
                    "user": {
                        "_id": "66837d3c48edefb453b0640a",
                        "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
                        "isPro": false,
                        "fullname": "Lingen Li",
                        "user": "l-li",
                        "type": "user"
                    },
                    "name": "Lingen Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:09:48.492Z",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df49732fe",
                    "user": {
                        "_id": "62d4577bc85b0fcf7fde39bb",
                        "avatarUrl": "/avatars/a3a5729e33ae89ce9ba408830db3c835.svg",
                        "isPro": false,
                        "fullname": "Xuan Ju",
                        "user": "juxuan27",
                        "type": "user"
                    },
                    "name": "Xuan Ju",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:09:55.870Z",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df49732ff",
                    "name": "Zhaoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df4973300",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "680057b49031335df4973301",
                    "user": {
                        "_id": "63ca3ddc04c979828310bfcb",
                        "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
                        "isPro": false,
                        "fullname": "Ying Shan",
                        "user": "yshan2u",
                        "type": "user"
                    },
                    "name": "Ying Shan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:12:47.014Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
            ],
            "publishedAt": "2025-04-16T16:45:19.000Z",
            "submittedOnDailyAt": "2025-04-17T00:32:24.205Z",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "submittedOnDailyBy": {
                "_id": "64970d3d9c3b29dca8633f87",
                "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
                "isPro": false,
                "fullname": "JunhaoZhuang",
                "user": "JunhaoZhuang",
                "type": "user"
            },
            "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
            "upvotes": 17,
            "discussionId": "680057b89031335df497343e",
            "projectPage": "https://zhuang2002.github.io/Cobra/",
            "githubRepo": "https://github.com/zhuang2002/Cobra",
            "ai_keywords": [
                "diffusion models",
                "line art colorization",
                "contextual image guidance",
                "color hints",
                "Causal Sparse DiT architecture",
                "positional encodings",
                "causal sparse attention",
                "Key-Value Cache",
                "long-context references",
                "color identity consistency"
            ]
        },
        "publishedAt": "2025-04-16T12:45:19.000Z",
        "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
        "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/VweYE_xmPVmixUo3dh0Wu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64970d3d9c3b29dca8633f87/J9PY5rVuEtD_-ZLibCR1W.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12240.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "fullname": "JunhaoZhuang",
            "name": "JunhaoZhuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 33
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10326",
            "authors": [
                {
                    "_id": "67ff772061373fdf16ce1d38",
                    "user": {
                        "_id": "66486ba1640bc89c93bcc8a2",
                        "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
                        "isPro": false,
                        "fullname": "Yangshen Deng",
                        "user": "YangshenDeng",
                        "type": "user"
                    },
                    "name": "Yangshen Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:57:58.302Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d39",
                    "name": "Zhengxin You",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3a",
                    "user": {
                        "_id": "647dd5a6becb41a272925b5e",
                        "avatarUrl": "/avatars/9ec9a3357ac43dcbe350fb3b72f3dbc4.svg",
                        "isPro": false,
                        "fullname": "Long Xiang",
                        "user": "BenjaminXIANG",
                        "type": "user"
                    },
                    "name": "Long Xiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T11:10:34.914Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3b",
                    "user": {
                        "_id": "66d6be0bddf54fd90923c727",
                        "avatarUrl": "/avatars/7bb82c8c339db944d79d47b3b9b35aa8.svg",
                        "isPro": false,
                        "fullname": "Li",
                        "user": "Qilong00",
                        "type": "user"
                    },
                    "name": "Qilong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:16:16.665Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3c",
                    "user": {
                        "_id": "66efe50667c4ce2c9024cd45",
                        "avatarUrl": "/avatars/7b7f650953c371f08a5beecc500b6a43.svg",
                        "isPro": false,
                        "fullname": "peiqiyuan",
                        "user": "YuanPeiqi",
                        "type": "user"
                    },
                    "name": "Peiqi Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:16:22.829Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3d",
                    "name": "Zhaoyang Hong",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3e",
                    "user": {
                        "_id": "66d6c339b61dd11022907252",
                        "avatarUrl": "/avatars/d2ed3cc003e94b2e5204ce0f8a481dcf.svg",
                        "isPro": false,
                        "fullname": "Yitao Zheng",
                        "user": "FeTieTer",
                        "type": "user"
                    },
                    "name": "Yitao Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:16:36.808Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d3f",
                    "name": "Wanting Li",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d40",
                    "user": {
                        "_id": "671a7bce2b10d343bab18637",
                        "avatarUrl": "/avatars/af1c10a59236d953b42e67d3955eecc4.svg",
                        "isPro": false,
                        "fullname": "runzhong",
                        "user": "runzhongli",
                        "type": "user"
                    },
                    "name": "Runzhong Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:17:02.031Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d41",
                    "user": {
                        "_id": "63898b61ec1f539adc0f4da2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg",
                        "isPro": false,
                        "fullname": "Haotian Liu",
                        "user": "liuhaotian",
                        "type": "user"
                    },
                    "name": "Haotian Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:17:25.728Z",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d42",
                    "name": "Kyriakos Mouratidis",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d43",
                    "name": "Man Lung Yiu",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d44",
                    "name": "Huan Li",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d45",
                    "name": "Qiaomu Shen",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d46",
                    "name": "Rui Mao",
                    "hidden": false
                },
                {
                    "_id": "67ff772061373fdf16ce1d47",
                    "user": {
                        "_id": "66b02a2642c34e7a212133c0",
                        "avatarUrl": "/avatars/737a69b095e8c427ecd08f870b173635.svg",
                        "isPro": false,
                        "fullname": "Bo Tang",
                        "user": "BO1022",
                        "type": "user"
                    },
                    "name": "Bo Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:18:01.460Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T15:34:26.000Z",
            "submittedOnDailyAt": "2025-04-17T06:00:20.705Z",
            "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
            "submittedOnDailyBy": {
                "_id": "66486ba1640bc89c93bcc8a2",
                "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
                "isPro": false,
                "fullname": "Yangshen Deng",
                "user": "YangshenDeng",
                "type": "user"
            },
            "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
            "upvotes": 17,
            "discussionId": "67ff772261373fdf16ce1d93",
            "ai_keywords": [
                "vector database",
                "long-context inference",
                "Large Language Models (LLMs)",
                "KV cache",
                "attention computation",
                "Model as a Service (MaaS)",
                "Service Level Objectives (SLOs)",
                "KV cache disaggregation",
                "retrieval-based sparse attention",
                "query processing procedure",
                "native query optimizer",
                "LLM inference benchmarks"
            ]
        },
        "publishedAt": "2025-04-14T11:34:26.000Z",
        "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
        "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10326.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66486ba1640bc89c93bcc8a2",
            "avatarUrl": "/avatars/07e07bd768339495e55b7b17f2f8bc59.svg",
            "fullname": "Yangshen Deng",
            "name": "YangshenDeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09081",
            "authors": [
                {
                    "_id": "67fdbfcccaa65039e8c3d8ae",
                    "user": {
                        "_id": "67dcd93f73e2178fe917b893",
                        "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
                        "isPro": false,
                        "fullname": "Prabhat Pandey",
                        "user": "panprabh",
                        "type": "user"
                    },
                    "name": "Prabhat Pandey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:23:52.370Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbfcccaa65039e8c3d8af",
                    "name": "Rupak Vignesh Swaminathan",
                    "hidden": false
                },
                {
                    "_id": "67fdbfcccaa65039e8c3d8b0",
                    "user": {
                        "_id": "66279810009f1bfdc6bf71bf",
                        "avatarUrl": "/avatars/97f561c91b92e1abb4fe6f0b5c688126.svg",
                        "isPro": false,
                        "fullname": "Girish",
                        "user": "vijaygirish2001",
                        "type": "user"
                    },
                    "name": "K V Vijay Girish",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:23:49.809Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbfcccaa65039e8c3d8b1",
                    "user": {
                        "_id": "66367dfb2d6b86ff193dbbe0",
                        "avatarUrl": "/avatars/0a0c230bb5fc81a28a166691146cf807.svg",
                        "isPro": false,
                        "fullname": "Arunasish Sen",
                        "user": "svinxz",
                        "type": "user"
                    },
                    "name": "Arunasish Sen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T11:58:01.767Z",
                    "hidden": false
                },
                {
                    "_id": "67fdbfcccaa65039e8c3d8b2",
                    "name": "Jian Xie",
                    "hidden": false
                },
                {
                    "_id": "67fdbfcccaa65039e8c3d8b3",
                    "name": "Grant P. Strimel",
                    "hidden": false
                },
                {
                    "_id": "67fdbfcccaa65039e8c3d8b4",
                    "name": "Andreas Schwarz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-12T04:45:48.000Z",
            "submittedOnDailyAt": "2025-04-17T07:32:43.212Z",
            "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "67dcd93f73e2178fe917b893",
                "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
                "isPro": false,
                "fullname": "Prabhat Pandey",
                "user": "panprabh",
                "type": "user"
            },
            "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
            "upvotes": 13,
            "discussionId": "67fdbfe0caa65039e8c3de4b",
            "ai_keywords": [
                "SIFT (Speech Instruction Fine-Tuning)",
                "speech-text large language models (LLMs)",
                "instruction fine-tuning",
                "pre-training",
                "speech corpora",
                "off-the-shelf expert models",
                "speech understanding",
                "controllable speech generation",
                "SIFT-LLM",
                "instruction-following benchmarks",
                "foundational speech tasks",
                "EvalSIFT"
            ]
        },
        "publishedAt": "2025-04-12T00:45:48.000Z",
        "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
        "summary": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset\ndesigned for instruction fine-tuning and pre-training of speech-text large\nlanguage models (LLMs). SIFT-50M is built from publicly available speech\ncorpora, which collectively contain 14K hours of speech, and leverages LLMs\nalong with off-the-shelf expert models. The dataset spans five languages,\nencompassing a diverse range of speech understanding as well as controllable\nspeech generation instructions. Using SIFT-50M, we train SIFT-LLM, which\noutperforms existing speech-text LLMs on instruction-following benchmarks while\nachieving competitive performance on foundational speech tasks. To support\nfurther research, we also introduce EvalSIFT, a benchmark dataset specifically\ndesigned to evaluate the instruction-following capabilities of speech-text\nLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09081.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67dcd93f73e2178fe917b893",
            "avatarUrl": "/avatars/86ec25d32c45da3937280ef9a619f19e.svg",
            "fullname": "Prabhat Pandey",
            "name": "panprabh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10483",
            "authors": [
                {
                    "_id": "67fdd3e3913c97aa32f94e9b",
                    "user": {
                        "_id": "641480554b1701c01cdb36c4",
                        "avatarUrl": "/avatars/f1f6b294e0236d76a68c099164c81f36.svg",
                        "isPro": false,
                        "fullname": "Xingjian Leng",
                        "user": "xingjianleng",
                        "type": "user"
                    },
                    "name": "Xingjian Leng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-16T09:23:36.449Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3e3913c97aa32f94e9c",
                    "name": "Jaskirat Singh",
                    "hidden": false
                },
                {
                    "_id": "67fdd3e3913c97aa32f94e9d",
                    "user": {
                        "_id": "6752870ec63bc5b670b1b27e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6752870ec63bc5b670b1b27e/3CdHxnyTKbGup-1V67nEV.jpeg",
                        "isPro": false,
                        "fullname": "Yunzhong Hou",
                        "user": "yunzhong-hou",
                        "type": "user"
                    },
                    "name": "Yunzhong Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:34:11.946Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3e3913c97aa32f94e9e",
                    "user": {
                        "_id": "63bf533f4a2beec65568f813",
                        "avatarUrl": "/avatars/bba583653b5cada8dd4a3ff2281e9dec.svg",
                        "isPro": false,
                        "fullname": "Xing",
                        "user": "Zhenchang",
                        "type": "user"
                    },
                    "name": "Zhenchang Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:34:21.190Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3e3913c97aa32f94e9f",
                    "user": {
                        "_id": "6596422646624a86ff3b3bda",
                        "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
                        "isPro": false,
                        "fullname": "Saining Xie",
                        "user": "sainx",
                        "type": "user"
                    },
                    "name": "Saining Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:34:39.700Z",
                    "hidden": false
                },
                {
                    "_id": "67fdd3e3913c97aa32f94ea0",
                    "user": {
                        "_id": "666351ebd86c026caa135e5c",
                        "avatarUrl": "/avatars/50a37f7e999f660c69f518b71577eb7d.svg",
                        "isPro": false,
                        "fullname": "Liang Zheng",
                        "user": "liangzheng06",
                        "type": "user"
                    },
                    "name": "Liang Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:34:56.041Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:59:53.000Z",
            "submittedOnDailyAt": "2025-04-17T05:08:17.990Z",
            "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
            "upvotes": 11,
            "discussionId": "67fdd3e5913c97aa32f94ee3",
            "projectPage": "https://end2end-diffusion.github.io/",
            "githubRepo": "https://github.com/End2End-Diffusion/REPA-E",
            "ai_keywords": [
                "latent diffusion models",
                "variational auto-encoder (VAE) tokenizer",
                "end-to-end training",
                "diffusion-loss",
                "representation-alignment (REPA) loss",
                "diffusion model training",
                "VAE",
                "latent space structure",
                "downstream generation performance",
                "FID",
                "ImageNet 256 x 256"
            ]
        },
        "publishedAt": "2025-04-14T13:59:53.000Z",
        "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
        "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10483.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 820
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11468",
            "authors": [
                {
                    "_id": "68008583bee5b086995074fe",
                    "user": {
                        "_id": "65112b6b8c4b535a971ad3cf",
                        "avatarUrl": "/avatars/1ef00b312d2b009c8c7aab21b4b3f258.svg",
                        "isPro": false,
                        "fullname": "guiminghardychen",
                        "user": "g-h-chen",
                        "type": "user"
                    },
                    "name": "Hardy Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:04:02.073Z",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b086995074ff",
                    "name": "Haoqin Tu",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b08699507500",
                    "name": "Fali Wang",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b08699507501",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b08699507502",
                    "name": "Xianfeng Tang",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b08699507503",
                    "name": "Xinya Du",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b08699507504",
                    "name": "Yuyin Zhou",
                    "hidden": false
                },
                {
                    "_id": "68008583bee5b08699507505",
                    "name": "Cihang Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T16:54:05.000Z",
            "submittedOnDailyAt": "2025-04-17T14:26:09.114Z",
            "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "65112b6b8c4b535a971ad3cf",
                "avatarUrl": "/avatars/1ef00b312d2b009c8c7aab21b4b3f258.svg",
                "isPro": false,
                "fullname": "guiminghardychen",
                "user": "g-h-chen",
                "type": "user"
            },
            "summary": "This work revisits the dominant supervised fine-tuning (SFT) then\nreinforcement learning (RL) paradigm for training Large Vision-Language Models\n(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent\nRL by inducing ``pseudo reasoning paths'' imitated from expert models. While\nthese paths may resemble the native reasoning paths of RL models, they often\ninvolve prolonged, hesitant, less informative steps, and incorrect reasoning.\nTo systematically study this effect, we introduce VLAA-Thinking, a new\nmultimodal dataset designed to support reasoning in LVLMs. Constructed via a\nsix-step pipeline involving captioning, reasoning distillation, answer rewrite\nand verification, VLAA-Thinking comprises high-quality, step-by-step visual\nreasoning traces for SFT, along with a more challenging RL split from the same\ndata source. Using this dataset, we conduct extensive experiments comparing\nSFT, RL and their combinations. Results show that while SFT helps models learn\nreasoning formats, it often locks aligned models into imitative, rigid\nreasoning modes that impede further learning. In contrast, building on the\nGroup Relative Policy Optimization (GRPO) with a novel mixed reward module\nintegrating both perception and cognition signals, our RL approach fosters more\ngenuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on\nQwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard\n(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)\namong 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope\nour findings provide valuable insights in developing reasoning-capable LVLMs\nand can inform future research in this area.",
            "upvotes": 10,
            "discussionId": "68008585bee5b086995075ac",
            "ai_keywords": [
                "supervised fine-tuning (SFT)",
                "reinforcement learning (RL)",
                "Large Vision-Language Models (LVLMs)",
                "VLAA-Thinking",
                "captioning",
                "reasoning distillation",
                "answer rewrite",
                "verification",
                "visual reasoning traces",
                "Group Relative Policy Optimization (GRPO)",
                "mixed reward module",
                "perception and cognition signals",
                "VLAA-Thinker",
                "Qwen2.5VL 3B",
                "Open LMM Reasoning Leaderboard",
                "4B scale LVLMs"
            ]
        },
        "publishedAt": "2025-04-10T12:54:05.000Z",
        "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large\n  Vision-Language Models",
        "summary": "This work revisits the dominant supervised fine-tuning (SFT) then\nreinforcement learning (RL) paradigm for training Large Vision-Language Models\n(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent\nRL by inducing ``pseudo reasoning paths'' imitated from expert models. While\nthese paths may resemble the native reasoning paths of RL models, they often\ninvolve prolonged, hesitant, less informative steps, and incorrect reasoning.\nTo systematically study this effect, we introduce VLAA-Thinking, a new\nmultimodal dataset designed to support reasoning in LVLMs. Constructed via a\nsix-step pipeline involving captioning, reasoning distillation, answer rewrite\nand verification, VLAA-Thinking comprises high-quality, step-by-step visual\nreasoning traces for SFT, along with a more challenging RL split from the same\ndata source. Using this dataset, we conduct extensive experiments comparing\nSFT, RL and their combinations. Results show that while SFT helps models learn\nreasoning formats, it often locks aligned models into imitative, rigid\nreasoning modes that impede further learning. In contrast, building on the\nGroup Relative Policy Optimization (GRPO) with a novel mixed reward module\nintegrating both perception and cognition signals, our RL approach fosters more\ngenuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on\nQwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard\n(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)\namong 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope\nour findings provide valuable insights in developing reasoning-capable LVLMs\nand can inform future research in this area.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11468.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65112b6b8c4b535a971ad3cf",
            "avatarUrl": "/avatars/1ef00b312d2b009c8c7aab21b4b3f258.svg",
            "fullname": "guiminghardychen",
            "name": "g-h-chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12264",
            "authors": [
                {
                    "_id": "68010305e501b14512a3f5aa",
                    "name": "Ayca Takmaz",
                    "hidden": false
                },
                {
                    "_id": "68010305e501b14512a3f5ab",
                    "name": "Cristiano Saltori",
                    "hidden": false
                },
                {
                    "_id": "68010305e501b14512a3f5ac",
                    "name": "Neehar Peri",
                    "hidden": false
                },
                {
                    "_id": "68010305e501b14512a3f5ad",
                    "name": "Tim Meinhardt",
                    "hidden": false
                },
                {
                    "_id": "68010305e501b14512a3f5ae",
                    "name": "Riccardo de Lutio",
                    "hidden": false
                },
                {
                    "_id": "68010305e501b14512a3f5af",
                    "name": "Laura Leal-Taixé",
                    "hidden": false
                },
                {
                    "_id": "68010305e501b14512a3f5b0",
                    "name": "Aljoša Ošep",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T17:21:55.000Z",
            "submittedOnDailyAt": "2025-04-17T12:08:52.551Z",
            "title": "Towards Learning to Complete Anything in Lidar",
            "submittedOnDailyBy": {
                "_id": "6499631f906b89ee66f1fb81",
                "avatarUrl": "/avatars/fe27062dfc20cf57169d596457d0ef92.svg",
                "isPro": false,
                "fullname": "Ayca Takmaz",
                "user": "aycatakmaz",
                "type": "user"
            },
            "summary": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar",
            "upvotes": 6,
            "discussionId": "68010307e501b14512a3f607",
            "projectPage": "https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar/",
            "ai_keywords": [
                "Lidar-based shape-completion",
                "Lidar-based semantic/panoptic scene completion",
                "zero-shot approach",
                "temporal context",
                "multi-modal sensor sequences",
                "object shapes",
                "semantic features",
                "Lidar-only instance-level completion and recognition model",
                "partial shape completions",
                "full object shapes",
                "standard benchmarks for Semantic and Panoptic Scene Completion",
                "amodal 3D bounding boxes"
            ]
        },
        "publishedAt": "2025-04-16T13:21:55.000Z",
        "title": "Towards Learning to Complete Anything in Lidar",
        "summary": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12264.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6499631f906b89ee66f1fb81",
            "avatarUrl": "/avatars/fe27062dfc20cf57169d596457d0ef92.svg",
            "fullname": "Ayca Takmaz",
            "name": "aycatakmaz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11092",
            "authors": [
                {
                    "_id": "6800a43e1fd95d7dc21d6b83",
                    "user": {
                        "_id": "63e367d3fae035bdc4c347fc",
                        "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
                        "isPro": false,
                        "fullname": "Jiaxin Huang",
                        "user": "JaceyH919",
                        "type": "user"
                    },
                    "name": "Jiaxin Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:03:59.476Z",
                    "hidden": false
                },
                {
                    "_id": "6800a43e1fd95d7dc21d6b84",
                    "user": {
                        "_id": "677c94b658cfcc3df8a9d80e",
                        "avatarUrl": "/avatars/a7160e3cde089aaba8f16dad42133c80.svg",
                        "isPro": false,
                        "fullname": "miao",
                        "user": "shengmiao",
                        "type": "user"
                    },
                    "name": "Sheng Miao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:13:53.013Z",
                    "hidden": false
                },
                {
                    "_id": "6800a43e1fd95d7dc21d6b85",
                    "name": "BangBnag Yang",
                    "hidden": false
                },
                {
                    "_id": "6800a43e1fd95d7dc21d6b86",
                    "user": {
                        "_id": "641a74bdf5e9c66105015385",
                        "avatarUrl": "/avatars/13f966357de0ef5bb192ae566ad85ca0.svg",
                        "isPro": false,
                        "fullname": "Zack Ma",
                        "user": "yuewenma",
                        "type": "user"
                    },
                    "name": "Yuewen Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T11:14:07.429Z",
                    "hidden": false
                },
                {
                    "_id": "6800a43e1fd95d7dc21d6b87",
                    "name": "Yiyi Liao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T11:38:14.000Z",
            "submittedOnDailyAt": "2025-04-17T07:32:38.400Z",
            "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
            "submittedOnDailyBy": {
                "_id": "63e367d3fae035bdc4c347fc",
                "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
                "isPro": false,
                "fullname": "Jiaxin Huang",
                "user": "JaceyH919",
                "type": "user"
            },
            "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
            "upvotes": 5,
            "discussionId": "6800a4441fd95d7dc21d6d46",
            "projectPage": "https://xdimlab.github.io/Vivid4D/",
            "ai_keywords": [
                "Vivid4D",
                "4D monocular video synthesis",
                "view augmentation",
                "video inpainting",
                "monocular depth priors",
                "unposed web videos",
                "synthetically generated masks",
                "iterative view augmentation strategy",
                "robust reconstruction loss"
            ]
        },
        "publishedAt": "2025-04-15T07:38:14.000Z",
        "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
        "summary": "Reconstructing 4D dynamic scenes from casually captured monocular videos is\nvaluable but highly challenging, as each timestamp is observed from a single\nviewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular\nvideo synthesis by augmenting observation views - synthesizing multi-view\nvideos from a monocular input. Unlike existing methods that either solely\nleverage geometric priors for supervision or use generative priors while\noverlooking geometry, we integrate both. This reformulates view augmentation as\na video inpainting task, where observed views are warped into new viewpoints\nbased on monocular depth priors. To achieve this, we train a video inpainting\nmodel on unposed web videos with synthetically generated masks that mimic\nwarping occlusions, ensuring spatially and temporally consistent completion of\nmissing regions. To further mitigate inaccuracies in monocular depth priors, we\nintroduce an iterative view augmentation strategy and a robust reconstruction\nloss. Experiments demonstrate that our method effectively improves monocular 4D\nscene reconstruction and completion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11092.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e367d3fae035bdc4c347fc",
            "avatarUrl": "/avatars/239da0c9287bf965348aff4890e94722.svg",
            "fullname": "Jiaxin Huang",
            "name": "JaceyH919",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11952",
            "authors": [
                {
                    "_id": "6800646e0679d4ec4b9d01a7",
                    "user": {
                        "_id": "645c60dd7d655680b57ddbff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
                        "isPro": true,
                        "fullname": "Ram Kadiyala",
                        "user": "1024m",
                        "type": "user"
                    },
                    "name": "Ram Mohan Rao Kadiyala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:04:05.627Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01a8",
                    "user": {
                        "_id": "63da8ba3f03c3d71ef32408c",
                        "avatarUrl": "/avatars/1b2e6f3ea2bac5ab35dbd53edb7f8cf2.svg",
                        "isPro": false,
                        "fullname": "Siddartha Pullakhandam",
                        "user": "Siddartha10",
                        "type": "user"
                    },
                    "name": "Siddartha Pullakhandam",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:18:54.938Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01a9",
                    "name": "Kanwal Mehreen",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01aa",
                    "user": {
                        "_id": "618c1ad1c74578e0a4a4d074",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618c1ad1c74578e0a4a4d074/8u_AkeHt4d6xtQ8hzaffU.jpeg",
                        "isPro": true,
                        "fullname": "Drishti Sharma",
                        "user": "DrishtiSharma",
                        "type": "user"
                    },
                    "name": "Drishti Sharma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:19:22.573Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01ab",
                    "name": "Siddhant Gupta",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01ac",
                    "user": {
                        "_id": "66c578770a22b2f9ab575847",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c578770a22b2f9ab575847/-zfNho1DR3yZHDazq669-.png",
                        "isPro": false,
                        "fullname": "Jebish Purbey",
                        "user": "jebish7",
                        "type": "user"
                    },
                    "name": "Jebish Purbey",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:23:00.066Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01ad",
                    "user": {
                        "_id": "653d84f13fc9c706fa755d03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653d84f13fc9c706fa755d03/F_jYbeuLLM9EX8hKXcbHD.png",
                        "isPro": false,
                        "fullname": "Ashay Srivastava",
                        "user": "ashay-sriv",
                        "type": "user"
                    },
                    "name": "Ashay Srivastava",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:23:15.588Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01ae",
                    "user": {
                        "_id": "669a745a4bbe8ad52ee287cf",
                        "avatarUrl": "/avatars/245644aa638b45a17ff71124bd5bbe0f.svg",
                        "isPro": false,
                        "fullname": "Subhasya Tippareddy",
                        "user": "subhasyar",
                        "type": "user"
                    },
                    "name": "Subhasya TippaReddy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:23:23.946Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01af",
                    "user": {
                        "_id": "669a7383c9111326dc596f5e",
                        "avatarUrl": "/avatars/8bbd307fd4bb2d7055b2c8fc9140dc81.svg",
                        "isPro": false,
                        "fullname": "Arvind Reddy Bobbili",
                        "user": "Arvindreddy",
                        "type": "user"
                    },
                    "name": "Arvind Reddy Bobbili",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:23:32.804Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01b0",
                    "name": "Suraj Telugara Chandrashekhar",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01b1",
                    "user": {
                        "_id": "668db0bfb09a05f3d7cc796f",
                        "avatarUrl": "/avatars/969c511cca4d129b99eca6252a468385.svg",
                        "isPro": false,
                        "fullname": "Modabbir Adeeb",
                        "user": "moda10",
                        "type": "user"
                    },
                    "name": "Modabbir Adeeb",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:23:46.461Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01b2",
                    "user": {
                        "_id": "641c3337f0b71a9743629985",
                        "avatarUrl": "/avatars/820b124887f173c263a675728baf99c6.svg",
                        "isPro": false,
                        "fullname": "Srinadh Vura",
                        "user": "SriV",
                        "type": "user"
                    },
                    "name": "Srinadh Vura",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-17T08:23:53.315Z",
                    "hidden": false
                },
                {
                    "_id": "6800646e0679d4ec4b9d01b3",
                    "name": "Hamza Farooq",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T10:29:30.000Z",
            "submittedOnDailyAt": "2025-04-17T00:47:10.039Z",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "submittedOnDailyBy": {
                "_id": "645c60dd7d655680b57ddbff",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
                "isPro": true,
                "fullname": "Ram Kadiyala",
                "user": "1024m",
                "type": "user"
            },
            "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
            "upvotes": 4,
            "discussionId": "680064710679d4ec4b9d0224",
            "ai_keywords": [
                "token classification",
                "adversarial inputs"
            ]
        },
        "publishedAt": "2025-04-16T06:29:30.000Z",
        "title": "Robust and Fine-Grained Detection of AI Generated Texts",
        "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11952.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645c60dd7d655680b57ddbff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645c60dd7d655680b57ddbff/MTMtkV6sy44oD-zwkGX0E.png",
            "fullname": "Ram Kadiyala",
            "name": "1024m",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09566",
            "authors": [
                {
                    "_id": "6800e4285fba288bf14ea9b1",
                    "name": "Chenghao Li",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b2",
                    "name": "Chaoning Zhang",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b3",
                    "name": "Yi Lu",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b4",
                    "name": "Jiaquan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b5",
                    "name": "Qigan Sun",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b6",
                    "name": "Xudong Wang",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b7",
                    "name": "Jiwei Wei",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b8",
                    "name": "Guoqing Wang",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9b9",
                    "name": "Yang Yang",
                    "hidden": false
                },
                {
                    "_id": "6800e4285fba288bf14ea9ba",
                    "name": "Heng Tao Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T13:35:41.000Z",
            "submittedOnDailyAt": "2025-04-17T10:01:14.931Z",
            "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
            "submittedOnDailyBy": {
                "_id": "6800e2a4737ee7ff0c077fb5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1_YqWLLnDQ1ORpsNi8BuH.png",
                "isPro": false,
                "fullname": "Chenghao Li",
                "user": "CatWorldLee",
                "type": "user"
            },
            "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.",
            "upvotes": 2,
            "discussionId": "6800e4295fba288bf14ea9fc",
            "ai_keywords": [
                "Chain-of-Thought (CoT)",
                "large language models (LLMs)",
                "reasoning chain",
                "Syzygy of Thoughts (SoT)",
                "Minimal Free Resolution (MFR)",
                "Module",
                "Betti numbers",
                "Freeness",
                "Mapping",
                "Exactness",
                "Minimality",
                "GSM8K",
                "MATH",
                "GPT-4o-mini",
                "Qwen2.5",
                "inference accuracy",
                "transparency",
                "high performance"
            ]
        },
        "publishedAt": "2025-04-13T09:35:41.000Z",
        "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
        "summary": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language\nmodels (LLMs) by decomposing problems into sequential steps, mimicking human\nlogic and reducing errors. However, complex tasks with vast solution spaces and\nvague constraints often exceed the capacity of a single reasoning chain.\nInspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic\ngeometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends\nCoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper\nlogical dependencies, enabling more robust and structured problem-solving. MFR\ndecomposes a module into a sequence of free modules with minimal rank,\nproviding a structured analytical approach to complex systems. This method\nintroduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\",\n\"Exactness\" and \"Minimality\", enabling the systematic decomposition of the\noriginal complex problem into logically complete minimal subproblems while\npreserving key problem features and reducing reasoning length. We tested SoT\nacross diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini,\nQwen2.5), achieving inference accuracy that matches or surpasses mainstream\nCoTs standards. Additionally, by aligning the sampling process with algebraic\nconstraints, our approach enhances the scalability of inference time in LLMs,\nensuring both transparent reasoning and high performance. Our code will be\npublicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09566.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6800e2a4737ee7ff0c077fb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1_YqWLLnDQ1ORpsNi8BuH.png",
            "fullname": "Chenghao Li",
            "name": "CatWorldLee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.09048",
            "authors": [
                {
                    "_id": "67fff3a82d8bee24b1c1d7d7",
                    "user": {
                        "_id": "644a8050236745531574706e",
                        "avatarUrl": "/avatars/298f1ea4bcf139e66b7ab74645799961.svg",
                        "isPro": false,
                        "fullname": "Yongchang WU",
                        "user": "SunshineWu",
                        "type": "user"
                    },
                    "name": "Yongchang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-17T08:05:03.693Z",
                    "hidden": false
                },
                {
                    "_id": "67fff3a82d8bee24b1c1d7d8",
                    "name": "Zipeng Qi",
                    "hidden": false
                },
                {
                    "_id": "67fff3a82d8bee24b1c1d7d9",
                    "name": "Zhenwei Shi",
                    "hidden": false
                },
                {
                    "_id": "67fff3a82d8bee24b1c1d7da",
                    "name": "Zhengxia Zou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-12T02:05:55.000Z",
            "submittedOnDailyAt": "2025-04-17T11:26:04.675Z",
            "title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "644a8050236745531574706e",
                "avatarUrl": "/avatars/298f1ea4bcf139e66b7ab74645799961.svg",
                "isPro": false,
                "fullname": "Yongchang WU",
                "user": "SunshineWu",
                "type": "user"
            },
            "summary": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian",
            "upvotes": 2,
            "discussionId": "67fff3aa2d8bee24b1c1d884",
            "ai_keywords": [
                "3D Gaussian Splatting (3DGS)",
                "divide-and-conquer paradigm",
                "scene partitioning",
                "optimization",
                "merging processes",
                "BlockGaussian",
                "content-aware scene partition strategy",
                "visibility-aware block optimization",
                "content-complexity variation",
                "computational load",
                "supervision mismatch",
                "auxiliary points",
                "ground-truth supervision",
                "pseudo-view geometry constraint",
                "rendering degradation",
                "airspace floaters",
                "large-scale scene reconstruction",
                "reconstruction efficiency",
                "rendering quality",
                "PSNR improvement"
            ]
        },
        "publishedAt": "2025-04-11T22:05:55.000Z",
        "title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting",
        "summary": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated\nremarkable potential in novel view synthesis tasks. The divide-and-conquer\nparadigm has enabled large-scale scene reconstruction, but significant\nchallenges remain in scene partitioning, optimization, and merging processes.\nThis paper introduces BlockGaussian, a novel framework incorporating a\ncontent-aware scene partition strategy and visibility-aware block optimization\nto achieve efficient and high-quality large-scale scene reconstruction.\nSpecifically, our approach considers the content-complexity variation across\ndifferent regions and balances computational load during scene partitioning,\nenabling efficient scene reconstruction. To tackle the supervision mismatch\nissue during independent block optimization, we introduce auxiliary points\nduring individual block optimization to align the ground-truth supervision,\nwhich enhances the reconstruction quality. Furthermore, we propose a\npseudo-view geometry constraint that effectively mitigates rendering\ndegradation caused by airspace floaters during block merging. Extensive\nexperiments on large-scale scenes demonstrate that our approach achieves\nstate-of-the-art performance in both reconstruction efficiency and rendering\nquality, with a 5x speedup in optimization and an average PSNR improvement of\n1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces\ncomputational requirements, enabling large-scale scene reconstruction on a\nsingle 24GB VRAM device. The project page is available at\nhttps://github.com/SunshineWYC/BlockGaussian",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09048.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a8050236745531574706e",
            "avatarUrl": "/avatars/298f1ea4bcf139e66b7ab74645799961.svg",
            "fullname": "Yongchang WU",
            "name": "SunshineWu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13128",
            "authors": [
                {
                    "_id": "6801a85879ba651f02e104f5",
                    "name": "Nandan Thakur",
                    "hidden": false
                },
                {
                    "_id": "6801a85879ba651f02e104f6",
                    "name": "Jimmy Lin",
                    "hidden": false
                },
                {
                    "_id": "6801a85879ba651f02e104f7",
                    "name": "Sam Havens",
                    "hidden": false
                },
                {
                    "_id": "6801a85879ba651f02e104f8",
                    "name": "Michael Carbin",
                    "hidden": false
                },
                {
                    "_id": "6801a85879ba651f02e104f9",
                    "name": "Omar Khattab",
                    "hidden": false
                },
                {
                    "_id": "6801a85879ba651f02e104fa",
                    "name": "Andrew Drozdov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:44:06.000Z",
            "submittedOnDailyAt": "2025-04-17T23:49:42.509Z",
            "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\n  Technical Documents",
            "submittedOnDailyBy": {
                "_id": "60196690dd31fde3c1062960",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1612277330660-noauth.jpeg",
                "isPro": false,
                "fullname": "Nandan Thakur",
                "user": "nthakur",
                "type": "user"
            },
            "summary": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.",
            "upvotes": 1,
            "discussionId": "6801a85979ba651f02e10540",
            "projectPage": "https://fresh-stack.github.io/",
            "ai_keywords": [
                "retrieval techniques",
                "hybrid architectures",
                "rerankers",
                "oracle approaches"
            ]
        },
        "publishedAt": "2025-04-17T13:44:06.000Z",
        "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\n  Technical Documents",
        "summary": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13128.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60196690dd31fde3c1062960",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1612277330660-noauth.jpeg",
            "fullname": "Nandan Thakur",
            "name": "nthakur",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.09702",
            "authors": [
                {
                    "_id": "6801913053f9bb6c0d90f07a",
                    "user": {
                        "_id": "64b7d9f1aa992c4ee9961012",
                        "avatarUrl": "/avatars/3f818c51d71c43cae8cca815bdb8225f.svg",
                        "isPro": false,
                        "fullname": "Yunxiang Zhang",
                        "user": "yunx-z",
                        "type": "user"
                    },
                    "name": "Yunxiang Zhang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-17T23:41:32.260Z",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f07b",
                    "name": "Muhammad Khalifa",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f07c",
                    "name": "Shitanshu Bhushan",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f07d",
                    "name": "Grant D Murphy",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f07e",
                    "name": "Lajanugen Logeswaran",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f07f",
                    "name": "Jaekyeom Kim",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f080",
                    "name": "Moontae Lee",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f081",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "6801913053f9bb6c0d90f082",
                    "name": "Lu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T19:35:43.000Z",
            "submittedOnDailyAt": "2025-04-17T22:10:54.430Z",
            "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research\n  Challenges?",
            "submittedOnDailyBy": {
                "_id": "64b7d9f1aa992c4ee9961012",
                "avatarUrl": "/avatars/3f818c51d71c43cae8cca815bdb8225f.svg",
                "isPro": false,
                "fullname": "Yunxiang Zhang",
                "user": "yunx-z",
                "type": "user"
            },
            "summary": "Existing evaluation of large language model (LLM) agents on scientific\ndiscovery lacks objective baselines and metrics to assess the viability of\ntheir proposed methods. To address this issue, we introduce MLRC-Bench, a\nbenchmark designed to quantify how effectively language agents can tackle\nchallenging Machine Learning (ML) Research Competitions. Our benchmark\nhighlights open research problems that demand novel methodologies, in contrast\nto recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's\nRE-Bench (Wijk et al., 2024), which focus on well-established research tasks\nthat are largely solvable through sufficient engineering effort. Unlike prior\nwork, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end\nagentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of\nproposing and implementing novel research methods and evaluates them with newly\nproposed rigorous protocol and objective metrics. Our curated suite of 7\ncompetition tasks reveals significant challenges for LLM agents. Even the\nbest-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a))\ncloses only 9.3% of the gap between baseline and top human participant scores.\nFurthermore, our analysis reveals a misalignment between the LLM-judged\ninnovation and their actual performance on cutting-edge ML research problems.\nMLRC-Bench is a dynamic benchmark, which is designed to continually grow with\nnew ML competitions to encourage rigorous and objective evaluations of AI's\nresearch capabilities.",
            "upvotes": 1,
            "discussionId": "6801913153f9bb6c0d90f0d8",
            "ai_keywords": [
                "large language model (LLM)",
                "MLRC-Bench",
                "Machine Learning (ML) Research Competitions",
                "MLE-Bench",
                "RE-Bench",
                "AI Scientist",
                "LLM-as-a-judge",
                "MLAB",
                "gemini-exp-1206",
                "baseline",
                "top human participant scores",
                "LLM-judged innovation"
            ]
        },
        "publishedAt": "2025-04-13T15:35:43.000Z",
        "title": "MLRC-Bench: Can Language Agents Solve Machine Learning Research\n  Challenges?",
        "summary": "Existing evaluation of large language model (LLM) agents on scientific\ndiscovery lacks objective baselines and metrics to assess the viability of\ntheir proposed methods. To address this issue, we introduce MLRC-Bench, a\nbenchmark designed to quantify how effectively language agents can tackle\nchallenging Machine Learning (ML) Research Competitions. Our benchmark\nhighlights open research problems that demand novel methodologies, in contrast\nto recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's\nRE-Bench (Wijk et al., 2024), which focus on well-established research tasks\nthat are largely solvable through sufficient engineering effort. Unlike prior\nwork, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end\nagentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of\nproposing and implementing novel research methods and evaluates them with newly\nproposed rigorous protocol and objective metrics. Our curated suite of 7\ncompetition tasks reveals significant challenges for LLM agents. Even the\nbest-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a))\ncloses only 9.3% of the gap between baseline and top human participant scores.\nFurthermore, our analysis reveals a misalignment between the LLM-judged\ninnovation and their actual performance on cutting-edge ML research problems.\nMLRC-Bench is a dynamic benchmark, which is designed to continually grow with\nnew ML competitions to encourage rigorous and objective evaluations of AI's\nresearch capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09702.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7d9f1aa992c4ee9961012",
            "avatarUrl": "/avatars/3f818c51d71c43cae8cca815bdb8225f.svg",
            "fullname": "Yunxiang Zhang",
            "name": "yunx-z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09346",
            "authors": [
                {
                    "_id": "67fe7287ebac3e14798967e0",
                    "user": {
                        "_id": "67a294ef40d06a4933cf8ee0",
                        "avatarUrl": "/avatars/151865ebc56767bc408f05138d4b2b6e.svg",
                        "isPro": false,
                        "fullname": "Shira Michel",
                        "user": "shiramichel",
                        "type": "user"
                    },
                    "name": "Shira Michel",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-15T15:04:12.728Z",
                    "hidden": false
                },
                {
                    "_id": "67fe7287ebac3e14798967e1",
                    "name": "Sufi Kaur",
                    "hidden": false
                },
                {
                    "_id": "67fe7287ebac3e14798967e2",
                    "name": "Sarah Elizabeth Gillespie",
                    "hidden": false
                },
                {
                    "_id": "67fe7287ebac3e14798967e3",
                    "name": "Jeffrey Gleason",
                    "hidden": false
                },
                {
                    "_id": "67fe7287ebac3e14798967e4",
                    "name": "Christo Wilson",
                    "hidden": false
                },
                {
                    "_id": "67fe7287ebac3e14798967e5",
                    "name": "Avijit Ghosh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-12T21:31:22.000Z",
            "submittedOnDailyAt": "2025-04-17T13:47:32.056Z",
            "title": "\"It's not a representation of me\": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services",
            "submittedOnDailyBy": {
                "_id": "6413251362e6057cbb6259bd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6413251362e6057cbb6259bd/rPwZpEXzoTCVq07w_vDYz.png",
                "isPro": false,
                "fullname": "Avijit Ghosh",
                "user": "evijit",
                "type": "user"
            },
            "summary": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.",
            "upvotes": 1,
            "discussionId": "67fe7288ebac3e147989681a"
        },
        "publishedAt": "2025-04-12T17:31:22.000Z",
        "title": "\"It's not a representation of me\": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services",
        "summary": "Recent advances in artificial intelligence (AI) speech generation and voice\ncloning technologies have produced naturalistic speech and accurate voice\nreplication, yet their influence on sociotechnical systems across diverse\naccents and linguistic traits is not fully understood. This study evaluates two\nsynthetic AI voice services (Speechify and ElevenLabs) through a mixed methods\napproach using surveys and interviews to assess technical performance and\nuncover how users' lived experiences influence their perceptions of accent\nvariations in these speech technologies. Our findings reveal technical\nperformance disparities across five regional, English-language accents and\ndemonstrate how current speech generation technologies may inadvertently\nreinforce linguistic privilege and accent-based discrimination, potentially\ncreating new forms of digital exclusion. Overall, our study highlights the need\nfor inclusive design and regulation by providing actionable insights for\ndevelopers, policymakers, and organizations to ensure equitable and socially\nresponsible AI speech technologies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09346.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6413251362e6057cbb6259bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6413251362e6057cbb6259bd/rPwZpEXzoTCVq07w_vDYz.png",
            "fullname": "Avijit Ghosh",
            "name": "evijit",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 42
        },
        "isAuthorParticipating": false
    }
]