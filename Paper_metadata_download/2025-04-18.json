[
    {
        "paper": {
            "id": "2504.13161",
            "authors": [
                {
                    "_id": "6801d661ed5fc062197db592",
                    "user": {
                        "_id": "633bd54b00732349209a18fe",
                        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                        "isPro": false,
                        "fullname": "Shizhe Diao",
                        "user": "shizhediao",
                        "type": "user"
                    },
                    "name": "Shizhe Diao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:46.555Z",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db593",
                    "name": "Yu Yang",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db594",
                    "name": "Yonggan Fu",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db595",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db596",
                    "name": "Dan Su",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db597",
                    "name": "Markus Kliegl",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db598",
                    "name": "Zijia Chen",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db599",
                    "name": "Peter Belcak",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db59a",
                    "name": "Yoshi Suhara",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db59b",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db59c",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db59d",
                    "name": "Yingyan",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db59e",
                    "name": "Lin",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db59f",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "6801d661ed5fc062197db5a0",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:58:13.000Z",
            "submittedOnDailyAt": "2025-04-18T03:05:25.298Z",
            "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
            "submittedOnDailyBy": {
                "_id": "633bd54b00732349209a18fe",
                "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                "isPro": false,
                "fullname": "Shizhe Diao",
                "user": "shizhediao",
                "type": "user"
            },
            "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/",
            "upvotes": 69,
            "discussionId": "6801d663ed5fc062197db631",
            "ai_keywords": [
                "CLIMB",
                "semantic space",
                "proxy model",
                "predictor",
                "ClimbLab",
                "ClimbMix"
            ]
        },
        "publishedAt": "2025-04-17T13:58:13.000Z",
        "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
        "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13161.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "fullname": "Shizhe Diao",
            "name": "shizhediao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13146",
            "authors": [
                {
                    "_id": "6801b77dcb758561ae26997e",
                    "user": {
                        "_id": "647c0564001553a39c38e79e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t5KF0Vp7kCk-5EZRWhG8i.jpeg",
                        "isPro": false,
                        "fullname": "Yash Savani",
                        "user": "yashsavani",
                        "type": "user"
                    },
                    "name": "Yash Savani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:03.683Z",
                    "hidden": false
                },
                {
                    "_id": "6801b77dcb758561ae26997f",
                    "name": "Asher Trockman",
                    "hidden": false
                },
                {
                    "_id": "6801b77dcb758561ae269980",
                    "name": "Zhili Feng",
                    "hidden": false
                },
                {
                    "_id": "6801b77dcb758561ae269981",
                    "name": "Avi Schwarzschild",
                    "hidden": false
                },
                {
                    "_id": "6801b77dcb758561ae269982",
                    "user": {
                        "_id": "63458a32d54fb141deda949d",
                        "avatarUrl": "/avatars/fc4b59cd009075ac7987c6cdddbe3fea.svg",
                        "isPro": false,
                        "fullname": "Alex Robey",
                        "user": "arobey1",
                        "type": "user"
                    },
                    "name": "Alexander Robey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:01.839Z",
                    "hidden": false
                },
                {
                    "_id": "6801b77dcb758561ae269983",
                    "name": "Marc Finzi",
                    "hidden": false
                },
                {
                    "_id": "6801b77dcb758561ae269984",
                    "name": "J. Zico Kolter",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:54:14.000Z",
            "submittedOnDailyAt": "2025-04-18T00:59:33.586Z",
            "title": "Antidistillation Sampling",
            "submittedOnDailyBy": {
                "_id": "6570917c0ea91e592aff0b8c",
                "avatarUrl": "/avatars/529e9713e6ac835e11599ea7070a9603.svg",
                "isPro": false,
                "fullname": "Avi Schwarzschild",
                "user": "schwarzschild",
                "type": "user"
            },
            "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
            "upvotes": 52,
            "discussionId": "6801b77ecb758561ae269a19",
            "projectPage": "https://antidistillation.com",
            "ai_keywords": [
                "antidistillation sampling",
                "next-token probability distribution",
                "reasoning traces"
            ]
        },
        "publishedAt": "2025-04-17T13:54:14.000Z",
        "title": "Antidistillation Sampling",
        "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\nAntidistillation sampling provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13146.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6570917c0ea91e592aff0b8c",
            "avatarUrl": "/avatars/529e9713e6ac835e11599ea7070a9603.svg",
            "fullname": "Avi Schwarzschild",
            "name": "schwarzschild",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13169",
            "authors": [
                {
                    "_id": "6801bcd484335da5c3e32d0b",
                    "user": {
                        "_id": "644a767044b75fd95805d232",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a767044b75fd95805d232/vHA2vI_B3CpXapdBEwspB.jpeg",
                        "isPro": false,
                        "fullname": "Patrick (Tsung-Han) Wu",
                        "user": "tsunghanwu",
                        "type": "user"
                    },
                    "name": "Tsung-Han Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:59.626Z",
                    "hidden": false
                },
                {
                    "_id": "6801bcd484335da5c3e32d0c",
                    "name": "Heekyung Lee",
                    "hidden": false
                },
                {
                    "_id": "6801bcd484335da5c3e32d0d",
                    "name": "Jiaxin Ge",
                    "hidden": false
                },
                {
                    "_id": "6801bcd484335da5c3e32d0e",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                },
                {
                    "_id": "6801bcd484335da5c3e32d0f",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "6801bcd484335da5c3e32d10",
                    "name": "David M. Chan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:59:22.000Z",
            "submittedOnDailyAt": "2025-04-18T01:16:30.770Z",
            "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
            "submittedOnDailyBy": {
                "_id": "6388f68c43d8b0797a09ff84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
                "isPro": false,
                "fullname": "David Chan",
                "user": "davidchan",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.",
            "upvotes": 26,
            "discussionId": "6801bcd684335da5c3e32db7",
            "projectPage": "https://reverse-vlm.github.io",
            "githubRepo": "https://github.com/tsunghan-wu/reverse_vlm",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "visual hallucinations",
                "generation adjustment",
                "post-hoc verification",
                "hallucination-aware training",
                "on-the-fly self-verification",
                "hallucination-verification dataset",
                "semi-synthetic samples",
                "inference-time retrospective resampling",
                "CHAIR-MSCOCO",
                "HaloQuest",
                "state-of-the-art hallucination reduction"
            ]
        },
        "publishedAt": "2025-04-17T13:59:22.000Z",
        "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models\n  with Retrospective Resampling",
        "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13169.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6388f68c43d8b0797a09ff84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
            "fullname": "David Chan",
            "name": "davidchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12369",
            "authors": [
                {
                    "_id": "6801a8453c431c2bbe3b5f94",
                    "name": "Zeqi Xiao",
                    "hidden": false
                },
                {
                    "_id": "6801a8453c431c2bbe3b5f95",
                    "name": "Yushi Lan",
                    "hidden": false
                },
                {
                    "_id": "6801a8453c431c2bbe3b5f96",
                    "name": "Yifan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6801a8453c431c2bbe3b5f97",
                    "name": "Wenqi Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6801a8453c431c2bbe3b5f98",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "6801a8453c431c2bbe3b5f99",
                    "name": "Yanhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "6801a8453c431c2bbe3b5f9a",
                    "name": "Xingang Pan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/650e37cc11f3210cf7910501/YSoQPv8_5FCfCdk2bZXxq.mp4"
            ],
            "publishedAt": "2025-04-16T17:59:30.000Z",
            "submittedOnDailyAt": "2025-04-18T00:24:00.506Z",
            "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
            "submittedOnDailyBy": {
                "_id": "650e37cc11f3210cf7910501",
                "avatarUrl": "/avatars/dab5c9d647cfa97c59f5170216673a20.svg",
                "isPro": false,
                "fullname": "zeqixiao",
                "user": "zeqixiao",
                "type": "user"
            },
            "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.",
            "upvotes": 23,
            "discussionId": "6801a8463c431c2bbe3b5fe4",
            "projectPage": "https://xizaoqu.github.io/worldmem/",
            "githubRepo": "https://github.com/xizaoqu/WorldMem",
            "ai_keywords": [
                "WorldMem",
                "memory bank",
                "memory units",
                "memory frames",
                "states",
                "poses",
                "timestamps",
                "memory attention mechanism",
                "scene generation",
                "long-term consistency",
                "3D spatial consistency",
                "dynamic evolution"
            ]
        },
        "publishedAt": "2025-04-16T13:59:30.000Z",
        "title": "WORLDMEM: Long-term Consistent World Simulation with Memory",
        "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/650e37cc11f3210cf7910501/YSoQPv8_5FCfCdk2bZXxq.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12369.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650e37cc11f3210cf7910501",
            "avatarUrl": "/avatars/dab5c9d647cfa97c59f5170216673a20.svg",
            "fullname": "zeqixiao",
            "name": "zeqixiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12626",
            "authors": [
                {
                    "_id": "6801b65181552de84a4b7e29",
                    "name": "Lvmin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6801b65181552de84a4b7e2a",
                    "name": "Maneesh Agrawala",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T04:02:31.000Z",
            "submittedOnDailyAt": "2025-04-18T00:47:56.027Z",
            "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.",
            "upvotes": 22,
            "discussionId": "6801b65281552de84a4b7e42",
            "projectPage": "https://lllyasviel.github.io/frame_pack_gitpage/",
            "githubRepo": "https://github.com/lllyasviel/FramePack",
            "ai_keywords": [
                "FramePack",
                "next-frame prediction",
                "transformer context length",
                "video diffusion",
                "computation bottleneck",
                "image diffusion",
                "anti-drifting sampling",
                "inverted temporal order",
                "exposure bias",
                "existing video diffusion models",
                "parameter-efficient fine-tuning",
                "visual quality",
                "diffusion schedulers",
                "extreme flow shift timesteps"
            ]
        },
        "publishedAt": "2025-04-17T00:02:31.000Z",
        "title": "Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation",
        "summary": "We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12626.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 48
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12322",
            "authors": [
                {
                    "_id": "6801d3de81552de84a537dd5",
                    "user": {
                        "_id": "652f9a74c22d404ebfa9f51d",
                        "avatarUrl": "/avatars/8959d312b7c4c28952d4a26bb67f82ea.svg",
                        "isPro": false,
                        "fullname": "gaoxin",
                        "user": "GX-XinGao",
                        "type": "user"
                    },
                    "name": "Xin Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:50.872Z",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537dd6",
                    "name": "Qizhi Pei",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537dd7",
                    "name": "Zinan Tang",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537dd8",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537dd9",
                    "name": "Honglin Lin",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537dda",
                    "name": "Jiang Wu",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537ddb",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "6801d3de81552de84a537ddc",
                    "name": "Lijun Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T06:13:43.000Z",
            "submittedOnDailyAt": "2025-04-18T03:58:23.748Z",
            "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
            "submittedOnDailyBy": {
                "_id": "6397f6081323f19c578f142e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
                "isPro": false,
                "fullname": "QizhiPei",
                "user": "QizhiPei",
                "type": "user"
            },
            "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
            "upvotes": 22,
            "discussionId": "6801d3df81552de84a537e20",
            "githubRepo": "https://github.com/GX-XinGao/GRA",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "multiple small LLMs involved framework",
                "GRA",
                "Generator",
                "Reviewer",
                "Adjudicator",
                "peer-review-inspired data synthesis pipeline",
                "data-level parity"
            ]
        },
        "publishedAt": "2025-04-11T02:13:43.000Z",
        "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis",
        "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12322.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6397f6081323f19c578f142e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
            "fullname": "QizhiPei",
            "name": "QizhiPei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13122",
            "authors": [
                {
                    "_id": "6801b62608d748addc187952",
                    "name": "Haojian Huang",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187953",
                    "user": {
                        "_id": "6570450a78d7aca0c361a177",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
                        "isPro": false,
                        "fullname": "Harold Chen",
                        "user": "Harold328",
                        "type": "user"
                    },
                    "name": "Haodong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:11.374Z",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187954",
                    "user": {
                        "_id": "64c139d867eff857ea51caa8",
                        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                        "isPro": false,
                        "fullname": "Shengqiong Wu",
                        "user": "ChocoWu",
                        "type": "user"
                    },
                    "name": "Shengqiong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:08.857Z",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187955",
                    "name": "Meng Luo",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187956",
                    "name": "Jinlan Fu",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187957",
                    "name": "Xinya Du",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187958",
                    "name": "Hanwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6801b62608d748addc187959",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:06.521Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:39:41.000Z",
            "submittedOnDailyAt": "2025-04-18T00:47:30.566Z",
            "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
            "submittedOnDailyBy": {
                "_id": "6570450a78d7aca0c361a177",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
                "isPro": false,
                "fullname": "Harold Chen",
                "user": "Harold328",
                "type": "user"
            },
            "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
            "upvotes": 16,
            "discussionId": "6801b62808d748addc1879b2",
            "githubRepo": "https://github.com/HaroldChen19/VistaDPO",
            "ai_keywords": [
                "Large Video Models (LVMs)",
                "Large Language Models (LLMs)",
                "Video Hierarchical Spatial-Temporal Direct Preference Optimization",
                "Instance Level",
                "Temporal Level",
                "Perceptive Level",
                "QA pairs",
                "timestamps",
                "keyframes",
                "bounding boxes",
                "Video Hallucination",
                "Video QA",
                "Captioning"
            ]
        },
        "publishedAt": "2025-04-17T13:39:41.000Z",
        "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\n  Optimization for Large Video Models",
        "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13122.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6570450a78d7aca0c361a177",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570450a78d7aca0c361a177/z0GrnXEsjK2_G-hFfQhKv.jpeg",
            "fullname": "Harold Chen",
            "name": "Harold328",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13181",
            "authors": [
                {
                    "_id": "680237af2724430ec1defd6f",
                    "name": "Daniel Bolya",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd70",
                    "name": "Po-Yao Huang",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd71",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd72",
                    "name": "Jang Hyun Cho",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd73",
                    "name": "Andrea Madotto",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd74",
                    "name": "Chen Wei",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd75",
                    "name": "Tengyu Ma",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd76",
                    "name": "Jiale Zhi",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd77",
                    "name": "Jathushan Rajasegaran",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd78",
                    "name": "Hanoona Rasheed",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd79",
                    "name": "Junke Wang",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd7a",
                    "name": "Marco Monteiro",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd7b",
                    "name": "Hu Xu",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd7c",
                    "name": "Shiyu Dong",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd7d",
                    "name": "Nikhila Ravi",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd7e",
                    "name": "Daniel Li",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd7f",
                    "name": "Piotr Doll√°r",
                    "hidden": false
                },
                {
                    "_id": "680237af2724430ec1defd80",
                    "name": "Christoph Feichtenhofer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:59:57.000Z",
            "submittedOnDailyAt": "2025-04-18T10:00:45.650Z",
            "title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.",
            "upvotes": 15,
            "discussionId": "680237b12724430ec1defdfa",
            "githubRepo": "https://github.com/facebookresearch/perception_models",
            "ai_keywords": [
                "Perception Encoder (PE)",
                "contrastive vision-language training",
                "multimodal language modeling",
                "dense prediction",
                "zero-shot image and video classification and retrieval",
                "document, image, and video Q&A",
                "detection",
                "depth estimation",
                "tracking"
            ]
        },
        "publishedAt": "2025-04-17T13:59:57.000Z",
        "title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network",
        "summary": "We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13181.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 821
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13055",
            "authors": [
                {
                    "_id": "6801eb7e3881da18a86691c3",
                    "name": "Xiangyan Liu",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691c4",
                    "name": "Jinjie Ni",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691c5",
                    "name": "Zijian Wu",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691c6",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691c7",
                    "user": {
                        "_id": "6214e4ee1e35c843d42d1f88",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
                        "isPro": true,
                        "fullname": "Longxu Dou",
                        "user": "dreamerdeo",
                        "type": "user"
                    },
                    "name": "Longxu Dou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:28.304Z",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691c8",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691c9",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "6801eb7e3881da18a86691ca",
                    "name": "Michael Qizhe Shieh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T16:10:13.000Z",
            "submittedOnDailyAt": "2025-04-18T04:35:54.079Z",
            "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
            "submittedOnDailyBy": {
                "_id": "6214e4ee1e35c843d42d1f88",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
                "isPro": true,
                "fullname": "Longxu Dou",
                "user": "dreamerdeo",
                "type": "user"
            },
            "summary": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.",
            "upvotes": 12,
            "discussionId": "6801eb7f3881da18a8669226",
            "githubRepo": "https://github.com/John-AI-Lab/NoisyRollout",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "vision-language models (VLMs)",
                "policy exploration",
                "NoisyRollout",
                "visual perception",
                "reasoning process",
                "trajectories",
                "clean and moderately distorted images",
                "targeted diversity",
                "convolutional manner",
                "inductive bias",
                "noise annealing schedule",
                "training samples",
                "out-of-domain benchmarks",
                "reasoning tasks",
                "perception tasks",
                "in-domain performance"
            ]
        },
        "publishedAt": "2025-04-17T12:10:13.000Z",
        "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
        "summary": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13055.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "fullname": "Longxu Dou",
            "name": "dreamerdeo",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12364",
            "authors": [
                {
                    "_id": "6801d913a0cf74448f93d5c8",
                    "user": {
                        "_id": "649e7693a83143427691769c",
                        "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
                        "isPro": false,
                        "fullname": "Tianhui Song",
                        "user": "sthuihui",
                        "type": "user"
                    },
                    "name": "Tianhui Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:44.267Z",
                    "hidden": false
                },
                {
                    "_id": "6801d913a0cf74448f93d5c9",
                    "name": "Weixin Feng",
                    "hidden": false
                },
                {
                    "_id": "6801d913a0cf74448f93d5ca",
                    "user": {
                        "_id": "66615c855fd9d736e670e0a9",
                        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                        "isPro": false,
                        "fullname": "wangshuai",
                        "user": "wangsssssss",
                        "type": "user"
                    },
                    "name": "Shuai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:42.164Z",
                    "hidden": false
                },
                {
                    "_id": "6801d913a0cf74448f93d5cb",
                    "name": "Xubin Li",
                    "hidden": false
                },
                {
                    "_id": "6801d913a0cf74448f93d5cc",
                    "name": "Tiezheng Ge",
                    "hidden": false
                },
                {
                    "_id": "6801d913a0cf74448f93d5cd",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "6801d913a0cf74448f93d5ce",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T15:09:45.000Z",
            "submittedOnDailyAt": "2025-04-18T03:19:21.541Z",
            "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging",
            "submittedOnDailyBy": {
                "_id": "649e7693a83143427691769c",
                "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
                "isPro": false,
                "fullname": "Tianhui Song",
                "user": "sthuihui",
                "type": "user"
            },
            "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.",
            "upvotes": 12,
            "discussionId": "6801d91ba0cf74448f93d7f5",
            "githubRepo": "https://github.com/MCG-NJU/DMM",
            "ai_keywords": [
                "text-to-image (T2I) generation models",
                "model checkpoints",
                "fine-tuned",
                "parameter redundancy",
                "storage cost",
                "model merging",
                "static linear interpolation",
                "parameter space",
                "style mixing",
                "style-promptable image generation pipeline",
                "style vectors",
                "score distillation",
                "compressed models",
                "versatile T2I model",
                "merging goals",
                "evaluation protocols",
                "teacher models",
                "controllable arbitrary-style generation"
            ]
        },
        "publishedAt": "2025-04-16T11:09:45.000Z",
        "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based\n  Model Merging",
        "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12364.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "649e7693a83143427691769c",
            "avatarUrl": "/avatars/8eecb0423e36797ee592b35407aa7978.svg",
            "fullname": "Tianhui Song",
            "name": "sthuihui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.05506",
            "authors": [
                {
                    "_id": "6801a137a199bc0f78da6930",
                    "user": {
                        "_id": "63efd75a5c2ceb16fc6e98fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Masry",
                        "user": "ahmed-masry",
                        "type": "user"
                    },
                    "name": "Ahmed Masry",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:16.613Z",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6931",
                    "user": {
                        "_id": "624eb4c9058568da72ac0964",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649325306950-624eb4c9058568da72ac0964.png",
                        "isPro": false,
                        "fullname": "Saidul Islam",
                        "user": "38saidul",
                        "type": "user"
                    },
                    "name": "Mohammed Saidul Islam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:14.462Z",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6932",
                    "name": "Mahir Ahmed",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6933",
                    "name": "Aayush Bajaj",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6934",
                    "name": "Firoz Kabir",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6935",
                    "name": "Aaryaman Kartha",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6936",
                    "name": "Md Tahmid Rahman Laskar",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6937",
                    "name": "Mizanur Rahman",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6938",
                    "name": "Shadikur Rahman",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da6939",
                    "name": "Mehrad Shahmohammadi",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da693a",
                    "name": "Megh Thakkar",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da693b",
                    "name": "Md Rizwan Parvez",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da693c",
                    "name": "Enamul Hoque",
                    "hidden": false
                },
                {
                    "_id": "6801a137a199bc0f78da693d",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-07T21:05:06.000Z",
            "submittedOnDailyAt": "2025-04-18T00:21:30.532Z",
            "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
            "submittedOnDailyBy": {
                "_id": "63efd75a5c2ceb16fc6e98fc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
                "isPro": true,
                "fullname": "Ahmed Masry",
                "user": "ahmed-masry",
                "type": "user"
            },
            "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
            "upvotes": 11,
            "discussionId": "6801a147a199bc0f78da6d4a",
            "githubRepo": "https://github.com/vis-nlp/ChartQAPro",
            "ai_keywords": [
                "Chart Question Answering (CQA)",
                "visual representations",
                "large vision-language models (LVLMs)",
                "ChartQA",
                "ChartQAPro",
                "infographics",
                "dashboards",
                "multiple-choice",
                "conversational",
                "hypothetical",
                "unanswerable questions",
                "Claude Sonnet 3.5",
                "error analyses",
                "ablation studies",
                "chart reasoning"
            ]
        },
        "publishedAt": "2025-04-07T17:05:06.000Z",
        "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question\n  Answering",
        "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05506.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63efd75a5c2ceb16fc6e98fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63efd75a5c2ceb16fc6e98fc/qoA4LKuLTEr7hx90i90UK.jpeg",
            "fullname": "Ahmed Masry",
            "name": "ahmed-masry",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 69
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13180",
            "authors": [
                {
                    "_id": "6802c455f2384edf1c8ee068",
                    "name": "Jang Hyun Cho",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee069",
                    "name": "Andrea Madotto",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee06a",
                    "name": "Effrosyni Mavroudi",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee06b",
                    "name": "Triantafyllos Afouras",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee06c",
                    "name": "Tushar Nagarajan",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee06d",
                    "name": "Muhammad Maaz",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee06e",
                    "name": "Yale Song",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee06f",
                    "name": "Tengyu Ma",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee070",
                    "name": "Shuming Hu",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee071",
                    "name": "Suyog Jain",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee072",
                    "name": "Miguel Martin",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee073",
                    "name": "Huiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee074",
                    "name": "Hanoona Rasheed",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee075",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee076",
                    "name": "Po-Yao Huang",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee077",
                    "name": "Daniel Bolya",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee078",
                    "name": "Nikhila Ravi",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee079",
                    "name": "Shashank Jain",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee07a",
                    "name": "Tammy Stark",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee07b",
                    "name": "Shane Moon",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee07c",
                    "name": "Babak Damavandi",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee07d",
                    "name": "Vivian Lee",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee07e",
                    "name": "Andrew Westbury",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee07f",
                    "name": "Salman Khan",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee080",
                    "name": "Philipp Kr√§henb√ºhl",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee081",
                    "name": "Piotr Doll√°r",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee082",
                    "name": "Lorenzo Torresani",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee083",
                    "name": "Kristen Grauman",
                    "hidden": false
                },
                {
                    "_id": "6802c455f2384edf1c8ee084",
                    "name": "Christoph Feichtenhofer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-18T20:07:57.228Z",
            "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "647a4ae6c6077fb66fbd0036",
                "avatarUrl": "/avatars/ee08791366311da83bd5395012f3b1c2.svg",
                "isPro": false,
                "fullname": "Jang Hyun (Vincent) Cho",
                "user": "janghyuncho7",
                "type": "user"
            },
            "summary": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.",
            "upvotes": 10,
            "discussionId": "6802c45bf2384edf1c8ee1d0",
            "projectPage": "https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding/",
            "githubRepo": "https://github.com/facebookresearch/perception_models/",
            "ai_keywords": [
                "Perception Language Model (PLM)",
                "distillation",
                "training pipelines",
                "large-scale synthetic data",
                "video question-answer pairs",
                "spatio-temporally grounded video captions",
                "PLM-VideoBench",
                "video understanding tasks"
            ]
        },
        "publishedAt": "2025-04-17T13:59:56.000Z",
        "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding",
        "summary": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13180.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647a4ae6c6077fb66fbd0036",
            "avatarUrl": "/avatars/ee08791366311da83bd5395012f3b1c2.svg",
            "fullname": "Jang Hyun (Vincent) Cho",
            "name": "janghyuncho7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12395",
            "authors": [
                {
                    "_id": "6801f92ac9953c32ecda5c12",
                    "name": "Jiale Tao",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c13",
                    "name": "Yanbing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c14",
                    "name": "Qixun Wang",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c15",
                    "name": "Yiji Cheng",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c16",
                    "user": {
                        "_id": "637745113a63a2983ffbde13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                        "isPro": false,
                        "fullname": "Haofan Wang",
                        "user": "wanghaofan",
                        "type": "user"
                    },
                    "name": "Haofan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:16.438Z",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c17",
                    "name": "Xu Bai",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c18",
                    "name": "Zhengguang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c19",
                    "name": "Ruihuang Li",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c1a",
                    "name": "Linqing Wang",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c1b",
                    "name": "Chunyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c1c",
                    "name": "Qin Lin",
                    "hidden": false
                },
                {
                    "_id": "6801f92ac9953c32ecda5c1d",
                    "name": "Qinglin Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/d00ZBp12WFtZTsnetcf4u.png"
            ],
            "publishedAt": "2025-04-16T18:01:59.000Z",
            "submittedOnDailyAt": "2025-04-18T05:34:45.132Z",
            "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
            "submittedOnDailyBy": {
                "_id": "637745113a63a2983ffbde13",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                "isPro": false,
                "fullname": "Haofan Wang",
                "user": "wanghaofan",
                "type": "user"
            },
            "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.",
            "upvotes": 10,
            "discussionId": "6801f92ec9953c32ecda5d95",
            "projectPage": "https://instantcharacter.github.io/",
            "githubRepo": "https://github.com/Tencent/InstantCharacter",
            "ai_keywords": [
                "foundation diffusion transformer",
                "InstantCharacter",
                "high-fidelity",
                "scalable adapter",
                "stacked transformer encoders",
                "latent space",
                "textual controllability",
                "open-domain personalization",
                "character appearances",
                "poses",
                "styles",
                "large-scale character dataset",
                "paired (multi-view character)",
                "unpaired (text-image combinations)",
                "identity consistency",
                "textual editability",
                "high-fidelity images",
                "character-driven image generation"
            ]
        },
        "publishedAt": "2025-04-16T14:01:59.000Z",
        "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion\n  Transformer Framework",
        "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/637745113a63a2983ffbde13/d00ZBp12WFtZTsnetcf4u.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12395.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "fullname": "Haofan Wang",
            "name": "wanghaofan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 75
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13145",
            "authors": [
                {
                    "_id": "6801cbcc382250483109ddd4",
                    "name": "Li-Cheng Lan",
                    "hidden": false
                },
                {
                    "_id": "6801cbcc382250483109ddd5",
                    "name": "Andrew Bai",
                    "hidden": false
                },
                {
                    "_id": "6801cbcc382250483109ddd6",
                    "name": "Minhao Cheng",
                    "hidden": false
                },
                {
                    "_id": "6801cbcc382250483109ddd7",
                    "name": "Ruochen Wang",
                    "hidden": false
                },
                {
                    "_id": "6801cbcc382250483109ddd8",
                    "name": "Cho-Jui Hsieh",
                    "hidden": false
                },
                {
                    "_id": "6801cbcc382250483109ddd9",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:55.704Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:53:54.000Z",
            "submittedOnDailyAt": "2025-04-18T02:26:03.331Z",
            "title": "Exploring Expert Failures Improves LLM Agent Tuning",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
            "upvotes": 9,
            "discussionId": "6801cbcd382250483109de04",
            "ai_keywords": [
                "Rejection Sampling Fine-Tuning (RFT)",
                "expert-generated successful trajectories",
                "self-generated trajectories",
                "out-of-distribution (OOD)",
                "agent exploration efficiency",
                "acquisition of critical skills",
                "Exploring Expert Failures (EEF)",
                "beneficial actions",
                "harmful actions",
                "agent tuning performance",
                "WebShop",
                "SciWorld"
            ]
        },
        "publishedAt": "2025-04-17T13:53:54.000Z",
        "title": "Exploring Expert Failures Improves LLM Agent Tuning",
        "summary": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13145.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13171",
            "authors": [
                {
                    "_id": "680210b4b2ae01ba08b04189",
                    "name": "Kevin Lin",
                    "hidden": false
                },
                {
                    "_id": "680210b4b2ae01ba08b0418a",
                    "name": "Charlie Snell",
                    "hidden": false
                },
                {
                    "_id": "680210b4b2ae01ba08b0418b",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "680210b4b2ae01ba08b0418c",
                    "name": "Charles Packer",
                    "hidden": false
                },
                {
                    "_id": "680210b4b2ae01ba08b0418d",
                    "name": "Sarah Wooders",
                    "hidden": false
                },
                {
                    "_id": "680210b4b2ae01ba08b0418e",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "680210b4b2ae01ba08b0418f",
                    "name": "Joseph E. Gonzalez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:59:25.000Z",
            "submittedOnDailyAt": "2025-04-18T07:14:31.051Z",
            "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
            "submittedOnDailyBy": {
                "_id": "65097423e64ee37323bd2def",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65097423e64ee37323bd2def/PTEwbfafNI88gdX1VVmIn.jpeg",
                "isPro": false,
                "fullname": "Hao Jiang",
                "user": "TechxGenus",
                "type": "user"
            },
            "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
            "upvotes": 7,
            "discussionId": "680210b6b2ae01ba08b04219",
            "ai_keywords": [
                "large language models (LLMs)",
                "sleep-time compute",
                "anticipation",
                "pre-computing",
                "test-time compute",
                "Stateful GSM-Symbolic",
                "Stateful AIME",
                "Multi-Query GSM-Symbolic",
                "amortization",
                "predictability"
            ]
        },
        "publishedAt": "2025-04-17T13:59:25.000Z",
        "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time",
        "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13171.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65097423e64ee37323bd2def",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65097423e64ee37323bd2def/PTEwbfafNI88gdX1VVmIn.jpeg",
            "fullname": "Hao Jiang",
            "name": "TechxGenus",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 58
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11651",
            "authors": [
                {
                    "_id": "6801228e8b6f40490fb179f8",
                    "user": {
                        "_id": "66a3196b98016fc5e1731bfb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a3196b98016fc5e1731bfb/Kx3IdfxIXOl4U-ox7CoxT.png",
                        "isPro": false,
                        "fullname": "Tianyi Zhang",
                        "user": "LeanQuant",
                        "type": "user"
                    },
                    "name": "Tianyi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:39:45.219Z",
                    "hidden": false
                },
                {
                    "_id": "6801228e8b6f40490fb179f9",
                    "name": "Yang Sui",
                    "hidden": false
                },
                {
                    "_id": "6801228e8b6f40490fb179fa",
                    "name": "Shaochen Zhong",
                    "hidden": false
                },
                {
                    "_id": "6801228e8b6f40490fb179fb",
                    "name": "Vipin Chaudhary",
                    "hidden": false
                },
                {
                    "_id": "6801228e8b6f40490fb179fc",
                    "name": "Xia Hu",
                    "hidden": false
                },
                {
                    "_id": "6801228e8b6f40490fb179fd",
                    "name": "Anshumali Shrivastava",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/y08UoFEvo4xcchekhDNkW.png",
                "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/lOMLxThCYxgE5LK48FbWK.png",
                "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/R5ZVq9db_Zsw2nUwVcUiL.png",
                "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/XTgtlYlz1GkFiGCSkitxk.png",
                "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/42KF3yzj_D_0FNHw0PVBX.png"
            ],
            "publishedAt": "2025-04-15T22:38:38.000Z",
            "submittedOnDailyAt": "2025-04-18T17:50:43.713Z",
            "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU\n  Inference via Dynamic-Length Float",
            "submittedOnDailyBy": {
                "_id": "66a3196b98016fc5e1731bfb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a3196b98016fc5e1731bfb/Kx3IdfxIXOl4U-ox7CoxT.png",
                "isPro": false,
                "fullname": "Tianyi Zhang",
                "user": "LeanQuant",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have grown rapidly in size, creating significant\nchallenges for efficient deployment on resource-constrained hardware. In this\npaper, we introduce Dynamic-Length Float (DFloat11), a lossless compression\nframework that reduces LLM size by 30% while preserving outputs that are\nbit-for-bit identical to the original model. DFloat11 is motivated by the low\nentropy in the BFloat16 weight representation of LLMs, which reveals\nsignificant inefficiency in existing storage format. By applying entropy\ncoding, DFloat11 assigns dynamic-length encodings to weights based on\nfrequency, achieving near information-optimal compression without any loss of\nprecision. To facilitate efficient inference with dynamic-length encodings, we\ndevelop a custom GPU kernel for fast online decompression. Our design\nincorporates the following: (i) decomposition of memory-intensive lookup tables\n(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for\ncoordinating thread read/write positions using lightweight auxiliary variables,\nand (iii) transformer-block-level decompression to minimize latency.\nExperiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,\nvalidates our hypothesis that DFloat11 achieves around 30% model size reduction\nwhile preserving bit-for-bit exact outputs. Compared to a potential alternative\nof offloading parts of an uncompressed model to the CPU to meet memory\nconstraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.\nWith a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context\nlengths than uncompressed models. Notably, our method enables lossless\ninference of Llama-3.1-405B, an 810GB model, on a single node equipped with\n8x80GB GPUs. Our code and models are available at\nhttps://github.com/LeanModels/DFloat11.",
            "upvotes": 7,
            "discussionId": "6801228f8b6f40490fb17a18",
            "githubRepo": "https://github.com/LeanModels/DFloat11",
            "ai_keywords": [
                "Dynamic-Length Float (DFloat11)",
                "BFloat16",
                "entropy coding",
                "lookup tables (LUTs)",
                "GPU SRAM",
                "transformer-block-level decompression",
                "token generation",
                "context length",
                "lossless inference"
            ]
        },
        "publishedAt": "2025-04-15T18:38:38.000Z",
        "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU\n  Inference via Dynamic-Length Float",
        "summary": "Large Language Models (LLMs) have grown rapidly in size, creating significant\nchallenges for efficient deployment on resource-constrained hardware. In this\npaper, we introduce Dynamic-Length Float (DFloat11), a lossless compression\nframework that reduces LLM size by 30% while preserving outputs that are\nbit-for-bit identical to the original model. DFloat11 is motivated by the low\nentropy in the BFloat16 weight representation of LLMs, which reveals\nsignificant inefficiency in existing storage format. By applying entropy\ncoding, DFloat11 assigns dynamic-length encodings to weights based on\nfrequency, achieving near information-optimal compression without any loss of\nprecision. To facilitate efficient inference with dynamic-length encodings, we\ndevelop a custom GPU kernel for fast online decompression. Our design\nincorporates the following: (i) decomposition of memory-intensive lookup tables\n(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for\ncoordinating thread read/write positions using lightweight auxiliary variables,\nand (iii) transformer-block-level decompression to minimize latency.\nExperiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,\nvalidates our hypothesis that DFloat11 achieves around 30% model size reduction\nwhile preserving bit-for-bit exact outputs. Compared to a potential alternative\nof offloading parts of an uncompressed model to the CPU to meet memory\nconstraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.\nWith a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context\nlengths than uncompressed models. Notably, our method enables lossless\ninference of Llama-3.1-405B, an 810GB model, on a single node equipped with\n8x80GB GPUs. Our code and models are available at\nhttps://github.com/LeanModels/DFloat11.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/y08UoFEvo4xcchekhDNkW.png",
            "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/lOMLxThCYxgE5LK48FbWK.png",
            "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/R5ZVq9db_Zsw2nUwVcUiL.png",
            "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/XTgtlYlz1GkFiGCSkitxk.png",
            "https://cdn-uploads.huggingface.co/production/uploads/66a3196b98016fc5e1731bfb/42KF3yzj_D_0FNHw0PVBX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11651.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a3196b98016fc5e1731bfb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a3196b98016fc5e1731bfb/Kx3IdfxIXOl4U-ox7CoxT.png",
            "fullname": "Tianyi Zhang",
            "name": "LeanQuant",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.07959",
            "authors": [
                {
                    "_id": "6801eaa05246a16677d1f2d9",
                    "user": {
                        "_id": "645dcc0da19f3e64bbf36492",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dcc0da19f3e64bbf36492/SPGOweEnV5syFFpf40niQ.png",
                        "isPro": false,
                        "fullname": "Dongyoung Kim",
                        "user": "dongyong2",
                        "type": "user"
                    },
                    "name": "Dongyoung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:39.520Z",
                    "hidden": false
                },
                {
                    "_id": "6801eaa05246a16677d1f2da",
                    "name": "Mahmoud Afifi",
                    "hidden": false
                },
                {
                    "_id": "6801eaa05246a16677d1f2db",
                    "name": "Dongyun Kim",
                    "hidden": false
                },
                {
                    "_id": "6801eaa05246a16677d1f2dc",
                    "name": "Michael S. Brown",
                    "hidden": false
                },
                {
                    "_id": "6801eaa05246a16677d1f2dd",
                    "name": "Seon Joo Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-10T17:59:31.000Z",
            "submittedOnDailyAt": "2025-04-18T08:18:48.467Z",
            "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy",
            "submittedOnDailyBy": {
                "_id": "645dcc0da19f3e64bbf36492",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dcc0da19f3e64bbf36492/SPGOweEnV5syFFpf40niQ.png",
                "isPro": false,
                "fullname": "Dongyoung Kim",
                "user": "dongyong2",
                "type": "user"
            },
            "summary": "Computational color constancy, or white balancing, is a key module in a\ncamera's image signal processor (ISP) that corrects color casts from scene\nlighting. Because this operation occurs in the camera-specific raw color space,\nwhite balance algorithms must adapt to different cameras. This paper introduces\na learning-based method for cross-camera color constancy that generalizes to\nnew cameras without retraining. Our method leverages pre-calibrated color\ncorrection matrices (CCMs) available on ISPs that map the camera's raw color\nspace to a standard space (e.g., CIE XYZ). Our method uses these CCMs to\ntransform predefined illumination colors (i.e., along the Planckian locus) into\nthe test camera's raw space. The mapped illuminants are encoded into a compact\ncamera fingerprint embedding (CFE) that enables the network to adapt to unseen\ncameras. To prevent overfitting due to limited cameras and CCMs during\ntraining, we introduce a data augmentation technique that interpolates between\ncameras and their CCMs. Experimental results across multiple datasets and\nbackbones show that our method achieves state-of-the-art cross-camera color\nconstancy while remaining lightweight and relying only on data readily\navailable in camera ISPs.",
            "upvotes": 7,
            "discussionId": "6801eaa25246a16677d1f37f",
            "projectPage": "https://www.dykim.me/projects/ccmnet",
            "ai_keywords": [
                "computational color constancy",
                "white balancing",
                "image signal processor (ISP)",
                "color casts",
                "camera-specific raw color space",
                "learning-based method",
                "cross-camera color constancy",
                "pre-calibrated color correction matrices (CCMs)",
                "CIE XYZ",
                "Planckian locus",
                "camera fingerprint embedding (CFE)",
                "data augmentation",
                "interpolation"
            ]
        },
        "publishedAt": "2025-04-10T13:59:31.000Z",
        "title": "CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera\n  Color Constancy",
        "summary": "Computational color constancy, or white balancing, is a key module in a\ncamera's image signal processor (ISP) that corrects color casts from scene\nlighting. Because this operation occurs in the camera-specific raw color space,\nwhite balance algorithms must adapt to different cameras. This paper introduces\na learning-based method for cross-camera color constancy that generalizes to\nnew cameras without retraining. Our method leverages pre-calibrated color\ncorrection matrices (CCMs) available on ISPs that map the camera's raw color\nspace to a standard space (e.g., CIE XYZ). Our method uses these CCMs to\ntransform predefined illumination colors (i.e., along the Planckian locus) into\nthe test camera's raw space. The mapped illuminants are encoded into a compact\ncamera fingerprint embedding (CFE) that enables the network to adapt to unseen\ncameras. To prevent overfitting due to limited cameras and CCMs during\ntraining, we introduce a data augmentation technique that interpolates between\ncameras and their CCMs. Experimental results across multiple datasets and\nbackbones show that our method achieves state-of-the-art cross-camera color\nconstancy while remaining lightweight and relying only on data readily\navailable in camera ISPs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07959.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645dcc0da19f3e64bbf36492",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dcc0da19f3e64bbf36492/SPGOweEnV5syFFpf40niQ.png",
            "fullname": "Dongyoung Kim",
            "name": "dongyong2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12157",
            "authors": [
                {
                    "_id": "6801d4446d2188af01a9b6b6",
                    "name": "Xiaojun Ye",
                    "hidden": false
                },
                {
                    "_id": "6801d4446d2188af01a9b6b7",
                    "name": "Chun Wang",
                    "hidden": false
                },
                {
                    "_id": "6801d4446d2188af01a9b6b8",
                    "name": "Yiren Song",
                    "hidden": false
                },
                {
                    "_id": "6801d4446d2188af01a9b6b9",
                    "name": "Sheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6801d4446d2188af01a9b6ba",
                    "name": "Liangcheng Li",
                    "hidden": false
                },
                {
                    "_id": "6801d4446d2188af01a9b6bb",
                    "name": "Jiajun Bu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T15:04:14.000Z",
            "submittedOnDailyAt": "2025-04-18T02:56:22.746Z",
            "title": "FocusedAD: Character-centric Movie Audio Description",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": false,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .",
            "upvotes": 6,
            "discussionId": "6801d4466d2188af01a9b756",
            "ai_keywords": [
                "Character Perception Module (CPM)",
                "Dynamic Prior Module (DPM)",
                "Focused Caption Module (FCM)",
                "soft prompts",
                "zero-shot results",
                "MAD-eval-Named",
                "Cinepile-AD dataset"
            ]
        },
        "publishedAt": "2025-04-16T11:04:14.000Z",
        "title": "FocusedAD: Character-centric Movie Audio Description",
        "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12157.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13079",
            "authors": [
                {
                    "_id": "6801c2a379ba651f02e807ba",
                    "user": {
                        "_id": "617df9bb402d4d8f8eee3737",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Han Wang",
                        "user": "HanNight",
                        "type": "user"
                    },
                    "name": "Han Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:57.694Z",
                    "hidden": false
                },
                {
                    "_id": "6801c2a379ba651f02e807bb",
                    "name": "Archiki Prasad",
                    "hidden": false
                },
                {
                    "_id": "6801c2a379ba651f02e807bc",
                    "name": "Elias Stengel-Eskin",
                    "hidden": false
                },
                {
                    "_id": "6801c2a379ba651f02e807bd",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T16:46:11.000Z",
            "submittedOnDailyAt": "2025-04-18T01:51:26.288Z",
            "title": "Retrieval-Augmented Generation with Conflicting Evidence",
            "submittedOnDailyBy": {
                "_id": "617df9bb402d4d8f8eee3737",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
                "isPro": false,
                "fullname": "Han Wang",
                "user": "HanNight",
                "type": "user"
            },
            "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
            "upvotes": 5,
            "discussionId": "6801c2a479ba651f02e807df",
            "githubRepo": "https://github.com/HanNight/RAMDocs",
            "ai_keywords": [
                "retrieval-augmented generation (RAG)",
                "RAMDocs",
                "MADAM-RAG",
                "multi-agent approach",
                "aggregator",
                "disambiguated entities",
                "AmbigDocs",
                "FaithEval",
                "Llama3.3-70B-Instruct",
                "exact match score"
            ]
        },
        "publishedAt": "2025-04-17T12:46:11.000Z",
        "title": "Retrieval-Augmented Generation with Conflicting Evidence",
        "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13079.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "617df9bb402d4d8f8eee3737",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635645820205-noauth.jpeg",
            "fullname": "Han Wang",
            "name": "HanNight",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13143",
            "authors": [
                {
                    "_id": "6802d28a5c4268810bdd148e",
                    "name": "Siwei Yang",
                    "hidden": false
                },
                {
                    "_id": "6802d28a5c4268810bdd148f",
                    "name": "Mude Hui",
                    "hidden": false
                },
                {
                    "_id": "6802d28a5c4268810bdd1490",
                    "name": "Bingchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "6802d28a5c4268810bdd1491",
                    "name": "Yuyin Zhou",
                    "hidden": false
                },
                {
                    "_id": "6802d28a5c4268810bdd1492",
                    "name": "Nataniel Ruiz",
                    "hidden": false
                },
                {
                    "_id": "6802d28a5c4268810bdd1493",
                    "name": "Cihang Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:51:59.000Z",
            "submittedOnDailyAt": "2025-04-18T21:01:16.673Z",
            "title": "Complex-Edit: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark",
            "submittedOnDailyBy": {
                "_id": "645eb61da3c5cd8a16efffff",
                "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
                "isPro": false,
                "fullname": "Cihang Xie",
                "user": "cihangxie",
                "type": "user"
            },
            "summary": "We introduce Complex-Edit, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.",
            "upvotes": 3,
            "discussionId": "6802d28c5c4268810bdd152f",
            "projectPage": "https://ucsc-vlaa.github.io/Complex-Edit/",
            "githubRepo": "https://github.com/UCSC-VLAA/Complex-Edit",
            "ai_keywords": [
                "Complex-Edit",
                "Chain-of-Edit",
                "editing instructions",
                "VLM-based",
                "auto-evaluation pipeline",
                "atomic editing tasks"
            ]
        },
        "publishedAt": "2025-04-17T13:51:59.000Z",
        "title": "Complex-Edit: CoT-Like Instruction Generation for\n  Complexity-Controllable Image Editing Benchmark",
        "summary": "We introduce Complex-Edit, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13143.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645eb61da3c5cd8a16efffff",
            "avatarUrl": "/avatars/9112bfeed598dfabf9e077e69e09ecc9.svg",
            "fullname": "Cihang Xie",
            "name": "cihangxie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12782",
            "authors": [
                {
                    "_id": "6801cd2966aeef19a5cec2a4",
                    "name": "Leyang Li",
                    "hidden": false
                },
                {
                    "_id": "6801cd2966aeef19a5cec2a5",
                    "user": {
                        "_id": "631c4a23aa346997917bcb89",
                        "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
                        "isPro": false,
                        "fullname": "Shilin Lu",
                        "user": "Shilin-LU",
                        "type": "user"
                    },
                    "name": "Shilin Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:53.383Z",
                    "hidden": false
                },
                {
                    "_id": "6801cd2966aeef19a5cec2a6",
                    "name": "Yan Ren",
                    "hidden": false
                },
                {
                    "_id": "6801cd2966aeef19a5cec2a7",
                    "name": "Adams Wai-Kin Kong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T09:29:30.000Z",
            "submittedOnDailyAt": "2025-04-18T02:25:50.855Z",
            "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
            "submittedOnDailyBy": {
                "_id": "631c4a23aa346997917bcb89",
                "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
                "isPro": false,
                "fullname": "Shilin Lu",
                "user": "Shilin-LU",
                "type": "user"
            },
            "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
            "upvotes": 3,
            "discussionId": "6801cd2b66aeef19a5cec330",
            "ai_keywords": [
                "ANT",
                "deNoising Trajectories",
                "classifier-free guidance",
                "score function field",
                "natural image manifold",
                "augmentation-enhanced weight saliency map",
                "trajectory-aware objective"
            ]
        },
        "publishedAt": "2025-04-17T05:29:30.000Z",
        "title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep\n  Unwanted Concepts",
        "summary": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631c4a23aa346997917bcb89",
            "avatarUrl": "/avatars/4a562ba78e6be289049027c27798cc6e.svg",
            "fullname": "Shilin Lu",
            "name": "Shilin-LU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12563",
            "authors": [
                {
                    "_id": "68025a2274429c60ea9d501b",
                    "user": {
                        "_id": "6463739b51fa6e63060491b1",
                        "avatarUrl": "/avatars/f8ee64b3768ef46e847cfa15fcd689f0.svg",
                        "isPro": false,
                        "fullname": "Haris Riaz",
                        "user": "hriaz",
                        "type": "user"
                    },
                    "name": "Haris Riaz",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-18T14:19:12.226Z",
                    "hidden": false
                },
                {
                    "_id": "68025a2274429c60ea9d501c",
                    "name": "Sourav Bhabesh",
                    "hidden": false
                },
                {
                    "_id": "68025a2274429c60ea9d501d",
                    "name": "Vinayak Arannil",
                    "hidden": false
                },
                {
                    "_id": "68025a2274429c60ea9d501e",
                    "name": "Miguel Ballesteros",
                    "hidden": false
                },
                {
                    "_id": "68025a2274429c60ea9d501f",
                    "name": "Graham Horwood",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6463739b51fa6e63060491b1/nu18Byh5c3NJm31cTHWbQ.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6463739b51fa6e63060491b1/BtUOUklFhnD3C4E6zl69d.png"
            ],
            "publishedAt": "2025-04-17T01:25:15.000Z",
            "submittedOnDailyAt": "2025-04-18T13:36:11.907Z",
            "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic\n  Data Generation",
            "submittedOnDailyBy": {
                "_id": "6463739b51fa6e63060491b1",
                "avatarUrl": "/avatars/f8ee64b3768ef46e847cfa15fcd689f0.svg",
                "isPro": false,
                "fullname": "Haris Riaz",
                "user": "hriaz",
                "type": "user"
            },
            "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.",
            "upvotes": 1,
            "discussionId": "68025a2774429c60ea9d5193",
            "ai_keywords": [
                "language models",
                "synthetic data",
                "meta-prompting",
                "expert LLM agents",
                "domain adaptation",
                "diversity",
                "automated metrics",
                "pre-training corpora",
                "in-context exemplars"
            ]
        },
        "publishedAt": "2025-04-16T21:25:15.000Z",
        "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic\n  Data Generation",
        "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6463739b51fa6e63060491b1/nu18Byh5c3NJm31cTHWbQ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6463739b51fa6e63060491b1/BtUOUklFhnD3C4E6zl69d.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12563.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6463739b51fa6e63060491b1",
            "avatarUrl": "/avatars/f8ee64b3768ef46e847cfa15fcd689f0.svg",
            "fullname": "Haris Riaz",
            "name": "hriaz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09228",
            "authors": [
                {
                    "_id": "680204d6989701bd175cd761",
                    "user": {
                        "_id": "66babcdde5904d821f3f9e6b",
                        "avatarUrl": "/avatars/ba2247ad3248c99ce52b2a18fe47b626.svg",
                        "isPro": false,
                        "fullname": "you wu",
                        "user": "WY123L",
                        "type": "user"
                    },
                    "name": "You Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-18T09:38:14.581Z",
                    "hidden": false
                },
                {
                    "_id": "680204d6989701bd175cd762",
                    "name": "Xucheng Wang",
                    "hidden": false
                },
                {
                    "_id": "680204d6989701bd175cd763",
                    "name": "Xiangyang Yang",
                    "hidden": false
                },
                {
                    "_id": "680204d6989701bd175cd764",
                    "name": "Mengyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "680204d6989701bd175cd765",
                    "name": "Dan Zeng",
                    "hidden": false
                },
                {
                    "_id": "680204d6989701bd175cd766",
                    "name": "Hengzhou Ye",
                    "hidden": false
                },
                {
                    "_id": "680204d6989701bd175cd767",
                    "name": "Shuiwang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-12T14:06:50.000Z",
            "submittedOnDailyAt": "2025-04-18T23:25:45.519Z",
            "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
            "submittedOnDailyBy": {
                "_id": "66babcdde5904d821f3f9e6b",
                "avatarUrl": "/avatars/ba2247ad3248c99ce52b2a18fe47b626.svg",
                "isPro": false,
                "fullname": "you wu",
                "user": "WY123L",
                "type": "user"
            },
            "summary": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack.",
            "upvotes": 1,
            "discussionId": "680204d7989701bd175cd7b0",
            "githubRepo": "https://github.com/wuyou3474/ORTrack",
            "ai_keywords": [
                "Vision Transformer (ViT)",
                "occlusions",
                "ORR (Occlusion-Robust Representations)",
                "spatial Cox process",
                "random masking",
                "ORTrack",
                "Adaptive Feature-Based Knowledge Distillation (AFKD)",
                "ORTrack-D"
            ]
        },
        "publishedAt": "2025-04-12T10:06:50.000Z",
        "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
        "summary": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09228.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66babcdde5904d821f3f9e6b",
            "avatarUrl": "/avatars/ba2247ad3248c99ce52b2a18fe47b626.svg",
            "fullname": "you wu",
            "name": "WY123L",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]