[
    {
        "paper": {
            "id": "2504.13837",
            "authors": [
                {
                    "_id": "6805b9ec7c5fa8020f595642",
                    "name": "Yang Yue",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595643",
                    "name": "Zhiqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595644",
                    "name": "Rui Lu",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595645",
                    "name": "Andrew Zhao",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595646",
                    "name": "Zhaokai Wang",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595647",
                    "name": "Yang Yue",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595648",
                    "name": "Shiji Song",
                    "hidden": false
                },
                {
                    "_id": "6805b9ec7c5fa8020f595649",
                    "name": "Gao Huang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
            ],
            "publishedAt": "2025-04-18T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-21T01:54:36.096Z",
            "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
            "submittedOnDailyBy": {
                "_id": "649d475111592b1a765ac1a3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
                "isPro": false,
                "fullname": "Yang Yue",
                "user": "Yang130",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
            "upvotes": 63,
            "discussionId": "6805b9ed7c5fa8020f59568c",
            "projectPage": "https://limit-of-rlvr.github.io/",
            "githubRepo": "https://github.com/LeapLabTHU/limit-of-RLVR",
            "ai_keywords": [
                "Reinforcement Learning",
                "Verifiable Rewards",
                "RLVR",
                "LLMs (Large Language Models)",
                "reasoning capabilities",
                "mathematics",
                "programming tasks",
                "pass@\\textit{k}",
                "benchmark",
                "RL-trained models",
                "base models",
                "reasoning paths",
                "sampling distribution",
                "performance",
                "biasing",
                "output distribution",
                "visual reasoning tasks",
                "distillation"
            ]
        },
        "publishedAt": "2025-04-18T13:59:56.000Z",
        "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13837.png",
        "numComments": 12,
        "submittedBy": {
            "_id": "649d475111592b1a765ac1a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
            "fullname": "Yang Yue",
            "name": "Yang130",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13835",
            "authors": [
                {
                    "_id": "6805b38355d3c792e1a9d0dd",
                    "name": "Yicheng Chen",
                    "hidden": false
                },
                {
                    "_id": "6805b38355d3c792e1a9d0de",
                    "name": "Yining Li",
                    "hidden": false
                },
                {
                    "_id": "6805b38355d3c792e1a9d0df",
                    "name": "Kai Hu",
                    "hidden": false
                },
                {
                    "_id": "6805b38355d3c792e1a9d0e0",
                    "name": "Zerun Ma",
                    "hidden": false
                },
                {
                    "_id": "6805b38355d3c792e1a9d0e1",
                    "name": "Haochen Ye",
                    "hidden": false
                },
                {
                    "_id": "6805b38355d3c792e1a9d0e2",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T17:59:46.000Z",
            "submittedOnDailyAt": "2025-04-21T01:39:08.191Z",
            "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
            "submittedOnDailyBy": {
                "_id": "649988726677f66c2b486392",
                "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
                "isPro": false,
                "fullname": "Yining Li",
                "user": "ly015",
                "type": "user"
            },
            "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
            "upvotes": 31,
            "discussionId": "6805b38555d3c792e1a9d155",
            "projectPage": "https://yichengchen24.github.io/projects/mig",
            "githubRepo": "https://github.com/yichengchen24/MIG",
            "ai_keywords": [
                "label graph",
                "semantic space",
                "information content",
                "Maximize the Information Gain (MIG)"
            ]
        },
        "publishedAt": "2025-04-18T13:59:46.000Z",
        "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
        "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13835.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "649988726677f66c2b486392",
            "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
            "fullname": "Yining Li",
            "name": "ly015",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11544",
            "authors": [
                {
                    "_id": "6804ca9fd8538baa1c39ca93",
                    "name": "Tianyang Xu",
                    "hidden": false
                },
                {
                    "_id": "6804ca9fd8538baa1c39ca94",
                    "name": "Haojie Zheng",
                    "hidden": false
                },
                {
                    "_id": "6804ca9fd8538baa1c39ca95",
                    "name": "Chengze Li",
                    "hidden": false
                },
                {
                    "_id": "6804ca9fd8538baa1c39ca96",
                    "name": "Haoxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "6804ca9fd8538baa1c39ca97",
                    "name": "Yixin Liu",
                    "hidden": false
                },
                {
                    "_id": "6804ca9fd8538baa1c39ca98",
                    "name": "Ruoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "6804ca9fd8538baa1c39ca99",
                    "name": "Lichao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T18:24:00.000Z",
            "submittedOnDailyAt": "2025-04-21T01:38:46.380Z",
            "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
            "submittedOnDailyBy": {
                "_id": "6610fb736504d9bed5890d58",
                "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
                "isPro": false,
                "fullname": "Tianyang Xu",
                "user": "TerryXu666",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
            "upvotes": 28,
            "discussionId": "6804caa0d8538baa1c39cac2",
            "projectPage": "https://terry-xu-666.github.io/NodeRAG_web/",
            "githubRepo": "https://github.com/Terry-Xu-666/NodeRAG",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "external and private corpus",
                "factually consistent responses",
                "knowledge graph index",
                "graph-based RAG methods",
                "heterogeneous graph structures",
                "seamless and holistic integration",
                "end-to-end process",
                "question-answering performance",
                "multi-hop benchmarks",
                "open-ended head-to-head evaluations",
                "retrieval tokens"
            ]
        },
        "publishedAt": "2025-04-15T14:24:00.000Z",
        "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
        "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11544.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6610fb736504d9bed5890d58",
            "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
            "fullname": "Tianyang Xu",
            "name": "TerryXu666",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.11833",
            "authors": [
                {
                    "_id": "6805bb01747a412bca737b53",
                    "name": "Changjiang Gao",
                    "hidden": false
                },
                {
                    "_id": "6805bb01747a412bca737b54",
                    "name": "Xu Huang",
                    "hidden": false
                },
                {
                    "_id": "6805bb01747a412bca737b55",
                    "name": "Wenhao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6805bb01747a412bca737b56",
                    "name": "Shujian Huang",
                    "hidden": false
                },
                {
                    "_id": "6805bb01747a412bca737b57",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "6805bb01747a412bca737b58",
                    "name": "Fei Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T07:45:10.000Z",
            "submittedOnDailyAt": "2025-04-21T01:57:09.327Z",
            "title": "Could Thinking Multilingually Empower LLM Reasoning?",
            "submittedOnDailyBy": {
                "_id": "65fed45b08d35929362dd651",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
                "isPro": false,
                "fullname": "FeiYuan",
                "user": "FeYuan",
                "type": "user"
            },
            "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
            "upvotes": 16,
            "discussionId": "6805bb02747a412bca737b7e",
            "githubRepo": "https://github.com/CONE-MT/multilingual_reasoning"
        },
        "publishedAt": "2025-04-16T03:45:10.000Z",
        "title": "Could Thinking Multilingually Empower LLM Reasoning?",
        "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11833.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fed45b08d35929362dd651",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
            "fullname": "FeiYuan",
            "name": "FeYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13173",
            "authors": [
                {
                    "_id": "6805c4dab15a57fcb59b6f08",
                    "name": "Ali Behrouz",
                    "hidden": false
                },
                {
                    "_id": "6805c4dab15a57fcb59b6f09",
                    "name": "Meisam Razaviyayn",
                    "hidden": false
                },
                {
                    "_id": "6805c4dab15a57fcb59b6f0a",
                    "name": "Peilin Zhong",
                    "hidden": false
                },
                {
                    "_id": "6805c4dab15a57fcb59b6f0b",
                    "name": "Vahab Mirrokni",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T17:59:33.000Z",
            "submittedOnDailyAt": "2025-04-21T02:39:28.607Z",
            "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
            "submittedOnDailyBy": {
                "_id": "65cccd5134a5d74cbaa9446c",
                "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
                "isPro": false,
                "fullname": "Ali Behrouz",
                "user": "AliBehrouz",
                "type": "user"
            },
            "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
            "upvotes": 12,
            "discussionId": "6805c4dbb15a57fcb59b6f3d",
            "ai_keywords": [
                "Transformers",
                "Titans",
                "linear recurrent neural networks",
                "associative memory modules",
                "attentional bias",
                "dot-product similarity",
                "L2 regression",
                "retention regularization",
                "forget gates",
                "Miras",
                "Moneta",
                "Yaad",
                "Memora",
                "parallelizable training process",
                "language modeling",
                "commonsense reasoning",
                "recall intensive tasks"
            ]
        },
        "publishedAt": "2025-04-17T13:59:33.000Z",
        "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
        "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13173.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65cccd5134a5d74cbaa9446c",
            "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
            "fullname": "Ali Behrouz",
            "name": "AliBehrouz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10823",
            "authors": [
                {
                    "_id": "68003da4c771c307fec04da7",
                    "user": {
                        "_id": "66d079d94b3b38cefaf1dc4e",
                        "avatarUrl": "/avatars/f680a19fb7b8e52c811eb6df218a2cea.svg",
                        "isPro": false,
                        "fullname": "Lee",
                        "user": "Ayoung01",
                        "type": "user"
                    },
                    "name": "Ayoung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-20T15:03:37.028Z",
                    "hidden": false
                },
                {
                    "_id": "68003da4c771c307fec04da8",
                    "name": "Ryan Sungmo Kwon",
                    "hidden": false
                },
                {
                    "_id": "68003da4c771c307fec04da9",
                    "name": "Peter Railton",
                    "hidden": false
                },
                {
                    "_id": "68003da4c771c307fec04daa",
                    "name": "Lu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T02:54:16.000Z",
            "submittedOnDailyAt": "2025-04-21T11:29:39.399Z",
            "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives",
            "submittedOnDailyBy": {
                "_id": "66d079d94b3b38cefaf1dc4e",
                "avatarUrl": "/avatars/f680a19fb7b8e52c811eb6df218a2cea.svg",
                "isPro": false,
                "fullname": "Lee",
                "user": "Ayoung01",
                "type": "user"
            },
            "summary": "Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing.",
            "upvotes": 11,
            "discussionId": "68003da5c771c307fec04df1",
            "ai_keywords": [
                "large language models (LLMs)",
                "CLASH (Character perspective-based LLM Assessments in Situations with High-stakes)",
                "decision ambivalence",
                "psychological discomfort",
                "value shifts"
            ]
        },
        "publishedAt": "2025-04-14T22:54:16.000Z",
        "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives",
        "summary": "Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10823.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d079d94b3b38cefaf1dc4e",
            "avatarUrl": "/avatars/f680a19fb7b8e52c811eb6df218a2cea.svg",
            "fullname": "Lee",
            "name": "Ayoung01",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13157",
            "authors": [
                {
                    "_id": "6804392129303a3402c4f38e",
                    "user": {
                        "_id": "631bfb21f6bc4be4a6592afc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
                        "isPro": false,
                        "fullname": "Khiem Vuong",
                        "user": "kvuong2711",
                        "type": "user"
                    },
                    "name": "Khiem Vuong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-20T15:01:35.224Z",
                    "hidden": false
                },
                {
                    "_id": "6804392129303a3402c4f38f",
                    "name": "Anurag Ghosh",
                    "hidden": false
                },
                {
                    "_id": "6804392129303a3402c4f390",
                    "name": "Deva Ramanan",
                    "hidden": false
                },
                {
                    "_id": "6804392129303a3402c4f391",
                    "name": "Srinivasa Narasimhan",
                    "hidden": false
                },
                {
                    "_id": "6804392129303a3402c4f392",
                    "name": "Shubham Tulsiani",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
            ],
            "publishedAt": "2025-04-17T17:57:05.000Z",
            "submittedOnDailyAt": "2025-04-21T01:28:10.375Z",
            "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
            "submittedOnDailyBy": {
                "_id": "631bfb21f6bc4be4a6592afc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
                "isPro": false,
                "fullname": "Khiem Vuong",
                "user": "kvuong2711",
                "type": "user"
            },
            "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
            "upvotes": 10,
            "discussionId": "6804392329303a3402c4f3e8",
            "projectPage": "https://aerial-megadepth.github.io/",
            "githubRepo": "https://github.com/kvuong2711/aerial-megadepth",
            "ai_keywords": [
                "geometric reconstruction",
                "learning-based approaches",
                "extreme viewpoint variation",
                "co-registered",
                "aerial-ground datasets",
                "pseudo-synthetic renderings",
                "3D city-wide meshes",
                "crowd-sourced images",
                "visual fidelity",
                "domain gap",
                "mesh-based renderings",
                "fine-tuning",
                "DUSt3R",
                "camera rotation error",
                "scene reconstruction",
                "novel-view synthesis",
                "downstream tasks"
            ]
        },
        "publishedAt": "2025-04-17T13:57:05.000Z",
        "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
        "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13157.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631bfb21f6bc4be4a6592afc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
            "fullname": "Khiem Vuong",
            "name": "kvuong2711",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.09621",
            "authors": [
                {
                    "_id": "6800ef5509eaa9d1d87a6eaf",
                    "name": "Jiuchen Chen",
                    "hidden": false
                },
                {
                    "_id": "6800ef5509eaa9d1d87a6eb0",
                    "user": {
                        "_id": "6672c01fa6eb488f049ecb80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
                        "isPro": false,
                        "fullname": "Xinyu Yan",
                        "user": "fengyanzi",
                        "type": "user"
                    },
                    "name": "Xinyu Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-19T15:17:09.181Z",
                    "hidden": false
                },
                {
                    "_id": "6800ef5509eaa9d1d87a6eb1",
                    "name": "Qizhi Xu",
                    "hidden": false
                },
                {
                    "_id": "6800ef5509eaa9d1d87a6eb2",
                    "name": "Kaiqi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-13T15:41:25.000Z",
            "submittedOnDailyAt": "2025-04-21T05:45:47.747Z",
            "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
            "submittedOnDailyBy": {
                "_id": "6672c01fa6eb488f049ecb80",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
                "isPro": false,
                "fullname": "Xinyu Yan",
                "user": "fengyanzi",
                "type": "user"
            },
            "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
            "upvotes": 6,
            "discussionId": "6800ef5709eaa9d1d87a6f76",
            "projectPage": "https://castlechen339.github.io/DehazeXL.github.io/",
            "githubRepo": "https://github.com/CastleChen339/DehazeXL",
            "ai_keywords": [
                "haze removal",
                "image slicing",
                "downsampling",
                "DehazeXL",
                "global context",
                "local feature extraction",
                "end-to-end modeling",
                "visual attribution",
                "8KDehaze",
                "ultra-high-resolution haze removal dataset",
                "remote sensing images"
            ]
        },
        "publishedAt": "2025-04-13T11:41:25.000Z",
        "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
        "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09621.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6672c01fa6eb488f049ecb80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
            "fullname": "Xinyu Yan",
            "name": "fengyanzi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13828",
            "authors": [
                {
                    "_id": "680634b8e4e8571bc15e09af",
                    "name": "Shijie Xia",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b0",
                    "name": "Yiwei Qin",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b1",
                    "name": "Xuefeng Li",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b2",
                    "name": "Yan Ma",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b3",
                    "name": "Run-Ze Fan",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b4",
                    "name": "Steffi Chern",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b5",
                    "name": "Haoyang Zou",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b6",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b7",
                    "name": "Xiangkun Hu",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b8",
                    "name": "Jiahe Jin",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09b9",
                    "name": "Yanheng He",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09ba",
                    "name": "Yixin Ye",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09bb",
                    "name": "Yixiu Liu",
                    "hidden": false
                },
                {
                    "_id": "680634b8e4e8571bc15e09bc",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T17:55:58.000Z",
            "submittedOnDailyAt": "2025-04-21T12:40:47.597Z",
            "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering",
            "submittedOnDailyBy": {
                "_id": "65900d4ff5a209eeac08b463",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg",
                "isPro": false,
                "fullname": "shijie xia",
                "user": "seven-cat",
                "type": "user"
            },
            "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering",
            "upvotes": 5,
            "discussionId": "680634bae4e8571bc15e0a0b",
            "projectPage": "https://gair-nlp.github.io/cognition-engineering/",
            "githubRepo": "https://github.com/GAIR-NLP/cognition-engineering",
            "ai_keywords": [
                "Large Language Models",
                "prompt engineering",
                "knowledge-retrieval systems",
                "latent space",
                "test-time scaling",
                "thought-construction engines",
                "cognition engineering"
            ]
        },
        "publishedAt": "2025-04-18T13:55:58.000Z",
        "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering",
        "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13828.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65900d4ff5a209eeac08b463",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg",
            "fullname": "shijie xia",
            "name": "seven-cat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13626",
            "authors": [
                {
                    "_id": "6805fa66fddd500b98039425",
                    "name": "Yule Liu",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b98039426",
                    "name": "Jingyi Zheng",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b98039427",
                    "name": "Zhen Sun",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b98039428",
                    "name": "Zifan Peng",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b98039429",
                    "name": "Wenhan Dong",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b9803942a",
                    "name": "Zeyang Sha",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b9803942b",
                    "name": "Shiwen Cui",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b9803942c",
                    "name": "Weiqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "6805fa66fddd500b9803942d",
                    "name": "Xinlei He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T11:07:19.000Z",
            "submittedOnDailyAt": "2025-04-21T06:29:28.134Z",
            "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "63da3d7ae697e5898cb86854",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
                "isPro": false,
                "fullname": "Talha Rzgar Akku",
                "user": "Q-bert",
                "type": "user"
            },
            "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token (<think> and </think>) can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.",
            "upvotes": 5,
            "discussionId": "6805fa67fddd500b98039461",
            "ai_keywords": [
                "large reasoning models (LRMs)",
                "overthinking problems",
                "fine-tuning",
                "thinking token",
                "external CoTs (Chain of Thought)",
                "ThoughtMani",
                "unnecessary intermediate steps",
                "computational costs",
                "LiveBench/Code dataset",
                "output token counts",
                "safety alignment"
            ]
        },
        "publishedAt": "2025-04-18T07:07:19.000Z",
        "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
        "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token (<think> and </think>) can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13626.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63da3d7ae697e5898cb86854",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
            "fullname": "Talha Rzgar Akku",
            "name": "Q-bert",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 89
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13072",
            "authors": [
                {
                    "_id": "6805bfc5e332a61dd90160b0",
                    "name": "Wenqi Dong",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b1",
                    "name": "Bangbang Yang",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b2",
                    "name": "Zesong Yang",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b3",
                    "name": "Yuan Li",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b4",
                    "name": "Tao Hu",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b5",
                    "name": "Hujun Bao",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b6",
                    "name": "Yuewen Ma",
                    "hidden": false
                },
                {
                    "_id": "6805bfc5e332a61dd90160b7",
                    "name": "Zhaopeng Cui",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
            ],
            "publishedAt": "2025-04-17T16:33:39.000Z",
            "submittedOnDailyAt": "2025-04-21T03:26:56.809Z",
            "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
            "submittedOnDailyBy": {
                "_id": "63d748ff6f49aa82306b7e48",
                "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
                "isPro": false,
                "fullname": "BB Yang",
                "user": "ybbbbt",
                "type": "user"
            },
            "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
            "upvotes": 5,
            "discussionId": "6805bfc9e332a61dd901618b",
            "ai_keywords": [
                "hierarchical framework",
                "2D image generation",
                "3D object generation",
                "video-diffusion-based amodal completion",
                "occlusions",
                "shadows",
                "shape prior injection",
                "spatial coherence",
                "natural object arrangements",
                "complete object instances",
                "interactive applications",
                "physical plausibility",
                "user inputs"
            ]
        },
        "publishedAt": "2025-04-17T12:33:39.000Z",
        "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
        "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13072.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d748ff6f49aa82306b7e48",
            "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
            "fullname": "BB Yang",
            "name": "ybbbbt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13816",
            "authors": [
                {
                    "_id": "6806548d7593cbf4c6c2ca2e",
                    "name": "Chenghao Xiao",
                    "hidden": false
                },
                {
                    "_id": "6806548d7593cbf4c6c2ca2f",
                    "name": "Hou Pong Chan",
                    "hidden": false
                },
                {
                    "_id": "6806548d7593cbf4c6c2ca30",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6806548d7593cbf4c6c2ca31",
                    "name": "Mahani Aljunied",
                    "hidden": false
                },
                {
                    "_id": "6806548d7593cbf4c6c2ca32",
                    "name": "Lidong Bing",
                    "hidden": false
                },
                {
                    "_id": "6806548d7593cbf4c6c2ca33",
                    "name": "Noura Al Moubayed",
                    "hidden": false
                },
                {
                    "_id": "6806548d7593cbf4c6c2ca34",
                    "name": "Yu Rong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T17:44:12.000Z",
            "submittedOnDailyAt": "2025-04-21T12:58:34.648Z",
            "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations",
            "submittedOnDailyBy": {
                "_id": "63108cc834c7d77420b0fd68",
                "avatarUrl": "/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg",
                "isPro": false,
                "fullname": "chenghao xiao",
                "user": "gowitheflow",
                "type": "user"
            },
            "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
            "upvotes": 3,
            "discussionId": "680654927593cbf4c6c2cb9c",
            "ai_keywords": [
                "LLMs",
                "knowledge boundaries",
                "internal representations",
                "middle to middle-upper layers",
                "training-free alignment method",
                "bilingual question pair translation",
                "multilingual evaluation suite"
            ]
        },
        "publishedAt": "2025-04-18T13:44:12.000Z",
        "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations",
        "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13816.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63108cc834c7d77420b0fd68",
            "avatarUrl": "/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg",
            "fullname": "chenghao xiao",
            "name": "gowitheflow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13359",
            "authors": [
                {
                    "_id": "680690f08a4cbdf2a11ba1ce",
                    "name": "Mehmet Hamza Erol",
                    "hidden": false
                },
                {
                    "_id": "680690f08a4cbdf2a11ba1cf",
                    "name": "Batu El",
                    "hidden": false
                },
                {
                    "_id": "680690f08a4cbdf2a11ba1d0",
                    "name": "Mirac Suzgun",
                    "hidden": false
                },
                {
                    "_id": "680690f08a4cbdf2a11ba1d1",
                    "name": "Mert Yuksekgonul",
                    "hidden": false
                },
                {
                    "_id": "680690f08a4cbdf2a11ba1d2",
                    "name": "James Zou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T21:58:29.000Z",
            "submittedOnDailyAt": "2025-04-21T17:55:36.170Z",
            "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models",
            "submittedOnDailyBy": {
                "_id": "6404a5c40ab5e22719f3e105",
                "avatarUrl": "/avatars/f683bd957502162bad10039c28c8c244.svg",
                "isPro": false,
                "fullname": "Mehmet Hamza Erol",
                "user": "mhamzaerol",
                "type": "user"
            },
            "summary": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment.",
            "upvotes": 2,
            "discussionId": "680690f18a4cbdf2a11ba1f8",
            "githubRepo": "https://github.com/mhamzaerol/Cost-of-Pass"
        },
        "publishedAt": "2025-04-17T17:58:29.000Z",
        "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models",
        "summary": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13359.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6404a5c40ab5e22719f3e105",
            "avatarUrl": "/avatars/f683bd957502162bad10039c28c8c244.svg",
            "fullname": "Mehmet Hamza Erol",
            "name": "mhamzaerol",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.12083",
            "authors": [
                {
                    "_id": "68005c2d8da26a09080784c1",
                    "user": {
                        "_id": "6483b3d52193a1768c00c5ff",
                        "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
                        "isPro": false,
                        "fullname": "Pritam Sarkar",
                        "user": "pritamqu",
                        "type": "user"
                    },
                    "name": "Pritam Sarkar",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-17T01:59:05.956Z",
                    "hidden": false
                },
                {
                    "_id": "68005c2d8da26a09080784c2",
                    "name": "Ali Etemad",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T13:43:56.000Z",
            "submittedOnDailyAt": "2025-04-21T17:18:19.285Z",
            "title": "Self-alignment of Large Video Language Models with Refined Regularized\n  Preference Optimization",
            "submittedOnDailyBy": {
                "_id": "6483b3d52193a1768c00c5ff",
                "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
                "isPro": false,
                "fullname": "Pritam Sarkar",
                "user": "pritamqu",
                "type": "user"
            },
            "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning.",
            "upvotes": 2,
            "discussionId": "68005c2e8da26a0908078502",
            "projectPage": "https://pritamsarkar.com/RRPO/",
            "githubRepo": "https://github.com/pritamqu/RRPO",
            "ai_keywords": [
                "Large Video Language Models (LVLMs)",
                "self-alignment framework",
                "preferred and non-preferred response pairs",
                "spatio-temporal understanding",
                "spurious correlations",
                "Refined Regularized Preference Optimization (RRPO)",
                "sub-sequence-level refined rewards",
                "token-wise KL regularization",
                "Direct Preference Optimization (DPO)",
                "video hallucination",
                "short- and long-video understanding",
                "fine-grained temporal reasoning"
            ]
        },
        "publishedAt": "2025-04-16T09:43:56.000Z",
        "title": "Self-alignment of Large Video Language Models with Refined Regularized\n  Preference Optimization",
        "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12083.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6483b3d52193a1768c00c5ff",
            "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
            "fullname": "Pritam Sarkar",
            "name": "pritamqu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13677",
            "authors": [
                {
                    "_id": "6806a3464020c8351ddd6b90",
                    "name": "Andrea Santilli",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b91",
                    "name": "Adam Golinski",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b92",
                    "name": "Michael Kirchhof",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b93",
                    "name": "Federico Danieli",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b94",
                    "name": "Arno Blaas",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b95",
                    "name": "Miao Xiong",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b96",
                    "name": "Luca Zappella",
                    "hidden": false
                },
                {
                    "_id": "6806a3464020c8351ddd6b97",
                    "name": "Sinead Williamson",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T13:13:42.000Z",
            "submittedOnDailyAt": "2025-04-21T18:29:41.361Z",
            "title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results",
            "submittedOnDailyBy": {
                "_id": "5e8ef1f14957053f606489e6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
                "isPro": false,
                "fullname": "Andrea Santilli",
                "user": "teelinsan",
                "type": "user"
            },
            "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.",
            "upvotes": 0,
            "discussionId": "6806a3474020c8351ddd6bc7",
            "ai_keywords": [
                "uncertainty quantification (UQ)",
                "language models (LMs)",
                "AUROC",
                "negative sequence probabilities",
                "task correctness functions",
                "ROUGE-L",
                "lexical-based metrics",
                "embedding-based metrics",
                "LLM-as-a-judge approaches",
                "length biases"
            ]
        },
        "publishedAt": "2025-04-18T09:13:42.000Z",
        "title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results",
        "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "fullname": "Andrea Santilli",
            "name": "teelinsan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.13519",
            "authors": [
                {
                    "_id": "68066bce27a3e396583a2b13",
                    "user": {
                        "_id": "6423b89623b98d5ee6ebbae4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423b89623b98d5ee6ebbae4/WiotUlYYZpVM9aH5sFAst.jpeg",
                        "isPro": false,
                        "fullname": "Yipeng Sun",
                        "user": "yipengsun",
                        "type": "user"
                    },
                    "name": "Yipeng Sun",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-21T16:01:32.331Z",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b14",
                    "name": "Linda-Sophie Schneider",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b15",
                    "name": "Mingxuan Gu",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b16",
                    "name": "Siyuan Mei",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b17",
                    "name": "Chengze Ye",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b18",
                    "name": "Fabian Wagner",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b19",
                    "name": "Siming Bayer",
                    "hidden": false
                },
                {
                    "_id": "68066bce27a3e396583a2b1a",
                    "name": "Andreas Maier",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T07:15:27.000Z",
            "submittedOnDailyAt": "2025-04-21T14:34:07.442Z",
            "title": "Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for\n  Low-Dose CT with Attention-Guided Bilateral Filtering",
            "submittedOnDailyBy": {
                "_id": "6423b89623b98d5ee6ebbae4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423b89623b98d5ee6ebbae4/WiotUlYYZpVM9aH5sFAst.jpeg",
                "isPro": false,
                "fullname": "Yipeng Sun",
                "user": "yipengsun",
                "type": "user"
            },
            "summary": "Effective denoising is crucial in low-dose CT to enhance subtle structures\nand low-contrast lesions while preventing diagnostic errors. Supervised methods\nstruggle with limited paired datasets, and self-supervised approaches often\nrequire multiple noisy images and rely on deep networks like U-Net, offering\nlittle insight into the denoising mechanism. To address these challenges, we\npropose an interpretable self-supervised single-image denoising framework --\nFilter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral\nFilter that adapted to each noisy input through a lightweight module that\npredicts spatially varying filter parameters, which can be visualized and\nadjusted post-training for user-controlled denoising in specific regions of\ninterest. To enable single-image training, we introduce a novel downsampling\nshuffle strategy with a new self-supervised loss function that extends the\nconcept of Noise2Noise to a single image and addresses spatially correlated\nnoise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading\nself-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving\ntransparency, user control, and parametric efficiency. These features provide\nkey advantages for medical applications that require precise and interpretable\nnoise reduction. Our code is demonstrated at\nhttps://github.com/sypsyp97/Filter2Noise.git .",
            "upvotes": 0,
            "discussionId": "68066bd127a3e396583a2c45",
            "githubRepo": "https://github.com/sypsyp97/Filter2Noise",
            "ai_keywords": [
                "Attention-Guided Bilateral Filter",
                "Noise2Noise",
                "downsampling shuffle strategy",
                "self-supervised loss function",
                "spatially correlated noise",
                "PSNR",
                "parametric efficiency"
            ]
        },
        "publishedAt": "2025-04-18T03:15:27.000Z",
        "title": "Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for\n  Low-Dose CT with Attention-Guided Bilateral Filtering",
        "summary": "Effective denoising is crucial in low-dose CT to enhance subtle structures\nand low-contrast lesions while preventing diagnostic errors. Supervised methods\nstruggle with limited paired datasets, and self-supervised approaches often\nrequire multiple noisy images and rely on deep networks like U-Net, offering\nlittle insight into the denoising mechanism. To address these challenges, we\npropose an interpretable self-supervised single-image denoising framework --\nFilter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral\nFilter that adapted to each noisy input through a lightweight module that\npredicts spatially varying filter parameters, which can be visualized and\nadjusted post-training for user-controlled denoising in specific regions of\ninterest. To enable single-image training, we introduce a novel downsampling\nshuffle strategy with a new self-supervised loss function that extends the\nconcept of Noise2Noise to a single image and addresses spatially correlated\nnoise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading\nself-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving\ntransparency, user control, and parametric efficiency. These features provide\nkey advantages for medical applications that require precise and interpretable\nnoise reduction. Our code is demonstrated at\nhttps://github.com/sypsyp97/Filter2Noise.git .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13519.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6423b89623b98d5ee6ebbae4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6423b89623b98d5ee6ebbae4/WiotUlYYZpVM9aH5sFAst.jpeg",
            "fullname": "Yipeng Sun",
            "name": "yipengsun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]