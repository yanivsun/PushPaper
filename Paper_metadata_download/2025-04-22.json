[
    {
        "paper": {
            "id": "2504.14945",
            "authors": [
                {
                    "_id": "6806fdeda296cac1cf860505",
                    "user": {
                        "_id": "6086838b19137b3a6ba760e7",
                        "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
                        "isPro": false,
                        "fullname": "Jianhao Yan",
                        "user": "Elliott",
                        "type": "user"
                    },
                    "name": "Jianhao Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:23.332Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860506",
                    "user": {
                        "_id": "63f3502a520c14618925825a",
                        "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
                        "isPro": false,
                        "fullname": "Yafu Li",
                        "user": "yaful",
                        "type": "user"
                    },
                    "name": "Yafu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:32.087Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860507",
                    "user": {
                        "_id": "662de927b10f82aa4f46557f",
                        "avatarUrl": "/avatars/19878db42ecf8e35456378571eae4643.svg",
                        "isPro": false,
                        "fullname": "Zican Hu",
                        "user": "huzican",
                        "type": "user"
                    },
                    "name": "Zican Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:39.458Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860508",
                    "name": "Zhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf860509",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:43:50.636Z",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf86050a",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf86050b",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6806fdeda296cac1cf86050c",
                    "name": "Yue Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T08:09:13.000Z",
            "submittedOnDailyAt": "2025-04-22T00:57:31.276Z",
            "title": "Learning to Reason under Off-Policy Guidance",
            "submittedOnDailyBy": {
                "_id": "6086838b19137b3a6ba760e7",
                "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
                "isPro": false,
                "fullname": "Jianhao Yan",
                "user": "Elliott",
                "type": "user"
            },
            "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
            "upvotes": 58,
            "discussionId": "6806fdeea296cac1cf860553",
            "githubRepo": "https://github.com/ElliottYan/LUFFY",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "zero-RL",
                "on-policy",
                "off-policy",
                "LUFFY",
                "imitation learning",
                "on-policy rollouts",
                "policy shaping",
                "regularized importance sampling",
                "mixed-policy training",
                "math benchmarks",
                "out-of-distribution tasks",
                "imitation-based supervised fine-tuning (SFT)",
                "generalizable reasoning models"
            ]
        },
        "publishedAt": "2025-04-21T04:09:13.000Z",
        "title": "Learning to Reason under Off-Policy Guidance",
        "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14945.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6086838b19137b3a6ba760e7",
            "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg",
            "fullname": "Jianhao Yan",
            "name": "Elliott",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15271",
            "authors": [
                {
                    "_id": "680748d088578d9444349293",
                    "user": {
                        "_id": "6392c73390b8e99a6779a7b0",
                        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
                        "isPro": false,
                        "fullname": "Guo Chen",
                        "user": "cg1177",
                        "type": "user"
                    },
                    "name": "Guo Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:46:58.348Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349294",
                    "user": {
                        "_id": "6582d86e58df0a2e21db80b8",
                        "avatarUrl": "/avatars/a8245b1644183bd3ee7dc06b218c6e47.svg",
                        "isPro": true,
                        "fullname": "ZhiqiLi",
                        "user": "RealZhiqiLi",
                        "type": "user"
                    },
                    "name": "Zhiqi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:20.873Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349295",
                    "name": "Shihao Wang",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349296",
                    "name": "Jindong Jiang",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349297",
                    "name": "Yicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349298",
                    "user": {
                        "_id": "64a3de701698ad2985277148",
                        "avatarUrl": "/avatars/09eebadbbea53ed2800591564ff5c931.svg",
                        "isPro": false,
                        "fullname": "lulidong",
                        "user": "lulidong",
                        "type": "user"
                    },
                    "name": "Lidong Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:51:00.432Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d9444349299",
                    "user": {
                        "_id": "641d1c5ec3983aa94915c162",
                        "avatarUrl": "/avatars/127985b837ecf61e43c835deee578b5e.svg",
                        "isPro": false,
                        "fullname": "De-An Huang",
                        "user": "deahuang",
                        "type": "user"
                    },
                    "name": "De-An Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:50:49.235Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929a",
                    "user": {
                        "_id": "67da13552f60f9492fb908c6",
                        "avatarUrl": "/avatars/342ad1a5d3ab33a483ed74d6734f4b03.svg",
                        "isPro": false,
                        "fullname": "Wonmin Byeon",
                        "user": "WonminByeon",
                        "type": "user"
                    },
                    "name": "Wonmin Byeon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:50:43.675Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929b",
                    "name": "Matthieu Le",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929c",
                    "user": {
                        "_id": "67d3a14adfd19a63b25f0871",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fwYFVlhNGgJo3sSxWDfoP.png",
                        "isPro": false,
                        "fullname": "Tuomas Rintamaki",
                        "user": "trintamaki",
                        "type": "user"
                    },
                    "name": "Tuomas Rintamaki",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:49:24.223Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929d",
                    "user": {
                        "_id": "67d0d2d85f0fcc0c38872acc",
                        "avatarUrl": "/avatars/27bdff7f058a515261b9bec654941ca0.svg",
                        "isPro": false,
                        "fullname": "Tyler Poon",
                        "user": "tpoon-nvidia",
                        "type": "user"
                    },
                    "name": "Tyler Poon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:49:41.245Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929e",
                    "name": "Max Ehrlich",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d944434929f",
                    "name": "Tuomas Rintamaki",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a0",
                    "name": "Tyler Poon",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a1",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a2",
                    "user": {
                        "_id": "62c77f4352d8ae531f5511f9",
                        "avatarUrl": "/avatars/50198ccb02ccd286975a4613fbabee28.svg",
                        "isPro": false,
                        "fullname": "Limin Wang",
                        "user": "lmwang",
                        "type": "user"
                    },
                    "name": "Limin Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:58.836Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a3",
                    "user": {
                        "_id": "6311021788942700629e6247",
                        "avatarUrl": "/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg",
                        "isPro": false,
                        "fullname": "Bryan Catanzaro",
                        "user": "ctnzr",
                        "type": "user"
                    },
                    "name": "Bryan Catanzaro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:38.596Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a4",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a5",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a6",
                    "user": {
                        "_id": "66c8037c737ba92ae3fe0322",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg",
                        "isPro": false,
                        "fullname": "Zhiding Yu",
                        "user": "Zhiding",
                        "type": "user"
                    },
                    "name": "Zhiding Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:23.522Z",
                    "hidden": false
                },
                {
                    "_id": "680748d088578d94443492a7",
                    "user": {
                        "_id": "6656eb16e50d7c40881a14f0",
                        "avatarUrl": "/avatars/c6822a51c8d5918debf6ee1d25fe1825.svg",
                        "isPro": false,
                        "fullname": "GuilinLiu",
                        "user": "GuilinLiu",
                        "type": "user"
                    },
                    "name": "Guilin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:48:17.611Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:57:28.000Z",
            "submittedOnDailyAt": "2025-04-22T06:14:17.808Z",
            "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6392c73390b8e99a6779a7b0",
                "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
                "isPro": false,
                "fullname": "Guo Chen",
                "user": "cg1177",
                "type": "user"
            },
            "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
            "upvotes": 49,
            "discussionId": "680748d188578d9444349311",
            "projectPage": "https://nvlabs.github.io/EAGLE/",
            "githubRepo": "https://github.com/NVlabs/EAGLE",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "long-context multimodal learning",
                "long video comprehension",
                "high-resolution image understanding",
                "Automatic Degrade Sampling",
                "Image Area Preservation",
                "efficiency optimizations",
                "long-context data training",
                "Eagle-Video-110K",
                "story-level annotations",
                "clip-level annotations",
                "long-video understanding",
                "multimodal benchmarks",
                "Video-MME"
            ]
        },
        "publishedAt": "2025-04-21T13:57:28.000Z",
        "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
        "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15271.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "fullname": "Guo Chen",
            "name": "cg1177",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15257",
            "authors": [
                {
                    "_id": "68070ee593d1301c2f2ade99",
                    "user": {
                        "_id": "62728f4f6253fe2068da1021",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                        "isPro": false,
                        "fullname": "Hongcheng Gao",
                        "user": "HongchengGao",
                        "type": "user"
                    },
                    "name": "Hongcheng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:45:06.014Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9a",
                    "user": {
                        "_id": "6650c77a74664a42ddfb9187",
                        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                        "isPro": false,
                        "fullname": "yueliu1999",
                        "user": "yueliu1999",
                        "type": "user"
                    },
                    "name": "Yue Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:44:41.556Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9b",
                    "name": "Yufei He",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9c",
                    "user": {
                        "_id": "6214e4ee1e35c843d42d1f88",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
                        "isPro": false,
                        "fullname": "Longxu Dou",
                        "user": "dreamerdeo",
                        "type": "user"
                    },
                    "name": "Longxu Dou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:46.818Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9d",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9e",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2ade9f",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:45:53.602Z",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2adea0",
                    "name": "Min Lin",
                    "hidden": false
                },
                {
                    "_id": "68070ee593d1301c2f2adea1",
                    "user": {
                        "_id": "63d91b6d255ef6add20e1b38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "P2333",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:46:22.858Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:35:42.000Z",
            "submittedOnDailyAt": "2025-04-22T02:12:27.161Z",
            "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
            "submittedOnDailyBy": {
                "_id": "6650c77a74664a42ddfb9187",
                "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                "isPro": false,
                "fullname": "yueliu1999",
                "user": "yueliu1999",
                "type": "user"
            },
            "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
            "upvotes": 36,
            "discussionId": "68070ee693d1301c2f2aded1",
            "ai_keywords": [
                "DeepSeek R1",
                "reinforcement learning",
                "reward",
                "performance",
                "complexity",
                "efficiency",
                "deliberative reasoning"
            ]
        },
        "publishedAt": "2025-04-21T13:35:42.000Z",
        "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
        "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15257.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "fullname": "yueliu1999",
            "name": "yueliu1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13958",
            "authors": [
                {
                    "_id": "6806fe1bec9fb3764b875ea0",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea1",
                    "user": {
                        "_id": "63888d3fd68e37abd599f428",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                        "isPro": true,
                        "fullname": "emre can",
                        "user": "emrecanacikgoz",
                        "type": "user"
                    },
                    "name": "Emre Can Acikgoz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:57.521Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea2",
                    "name": "Qi He",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea3",
                    "user": {
                        "_id": "65f906e5c3dbdcae83ff7aac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                        "isPro": false,
                        "fullname": "Hongru Wang",
                        "user": "Merlin-Hongru",
                        "type": "user"
                    },
                    "name": "Hongru Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:19.954Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea4",
                    "user": {
                        "_id": "6270ff726417aed8a7340c8b",
                        "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
                        "isPro": false,
                        "fullname": "Xiusi Chen",
                        "user": "XtremSup",
                        "type": "user"
                    },
                    "name": "Xiusi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:51:58.709Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea5",
                    "name": "Dilek Hakkani-Tür",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea6",
                    "user": {
                        "_id": "640f687c06c3b5ca88412985",
                        "avatarUrl": "/avatars/6c1e538b78f52ce45ff124b0a5ec3369.svg",
                        "isPro": false,
                        "fullname": "gokhan tur",
                        "user": "Gokhantur",
                        "type": "user"
                    },
                    "name": "Gokhan Tur",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:51:42.581Z",
                    "hidden": false
                },
                {
                    "_id": "6806fe1bec9fb3764b875ea7",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
            ],
            "publishedAt": "2025-04-16T21:45:32.000Z",
            "submittedOnDailyAt": "2025-04-22T01:37:59.400Z",
            "title": "ToolRL: Reward is All Tool Learning Needs",
            "submittedOnDailyBy": {
                "_id": "63888d3fd68e37abd599f428",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
                "isPro": true,
                "fullname": "emre can",
                "user": "emrecanacikgoz",
                "type": "user"
            },
            "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
            "upvotes": 33,
            "discussionId": "6806fe1dec9fb3764b875f0c",
            "githubRepo": "https://github.com/qiancheng0/ToolRL",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "R1-like models",
                "reasoning and generalization",
                "tool selection",
                "tool application",
                "reward design",
                "finegrained feedback",
                "reward strategies",
                "Group Relative Policy Optimization (GRPO)"
            ]
        },
        "publishedAt": "2025-04-16T17:45:32.000Z",
        "title": "ToolRL: Reward is All Tool Learning Needs",
        "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/v-Pmd0mDnq-v5479tRcgC.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/pCPBgXm4IAZdhYjc-ZSrw.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/0c5XM8ZGXnieGz3IzuYTh.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/omlm8xwODvorHTUkfFOgR.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63888d3fd68e37abd599f428/1n73QmTy-PA5FfFQJMTej.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13958.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63888d3fd68e37abd599f428",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63888d3fd68e37abd599f428/YaNyxG_oM6IgrHTkFZ6Eq.jpeg",
            "fullname": "emre can",
            "name": "emrecanacikgoz",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13203",
            "authors": [
                {
                    "_id": "68070197c1bd0fc00c7d90a7",
                    "user": {
                        "_id": "625913bd5f80a3c1aad074b6",
                        "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
                        "isPro": false,
                        "fullname": "Salman Rahman",
                        "user": "salmannyu",
                        "type": "user"
                    },
                    "name": "Salman Rahman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:54:36.037Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90a8",
                    "user": {
                        "_id": "5efe5c89ff69163f6f59e628",
                        "avatarUrl": "/avatars/c7ea475c50fcce528c9b748f279058eb.svg",
                        "isPro": false,
                        "fullname": "Liwei Jiang",
                        "user": "liweijiang",
                        "type": "user"
                    },
                    "name": "Liwei Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:54:42.089Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90a9",
                    "name": "James Shiffer",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90aa",
                    "user": {
                        "_id": "64881deb8e004bb92b0f4845",
                        "avatarUrl": "/avatars/30a1e016d469bf7eb42c713351a9f65c.svg",
                        "isPro": false,
                        "fullname": "Genglin Liu",
                        "user": "genglinliu",
                        "type": "user"
                    },
                    "name": "Genglin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:54:53.094Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90ab",
                    "name": "Sheriff Issaka",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90ac",
                    "user": {
                        "_id": "65ae1c4468139e3c42973fe4",
                        "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
                        "isPro": false,
                        "fullname": "Md Rizwan Parvez",
                        "user": "mparvez",
                        "type": "user"
                    },
                    "name": "Md Rizwan Parvez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:55:13.627Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90ad",
                    "user": {
                        "_id": "6189ddd0992df2640e3e7d40",
                        "avatarUrl": "/avatars/925f2308fc412ae352b57a1b71815028.svg",
                        "isPro": false,
                        "fullname": "Hamid Palangi",
                        "user": "hamidpalangi",
                        "type": "user"
                    },
                    "name": "Hamid Palangi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:55:19.780Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90ae",
                    "user": {
                        "_id": "60b7b9d71b90c5d07c23fbd0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1622653364258-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai-Wei Chang",
                        "user": "kaiweichang",
                        "type": "user"
                    },
                    "name": "Kai-Wei Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:55:26.346Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90af",
                    "user": {
                        "_id": "64d42729f63b01b7f676b176",
                        "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                        "isPro": false,
                        "fullname": "Yejin Choi",
                        "user": "yejinchoinka",
                        "type": "user"
                    },
                    "name": "Yejin Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:55:32.976Z",
                    "hidden": false
                },
                {
                    "_id": "68070197c1bd0fc00c7d90b0",
                    "name": "Saadia Gabriel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T16:11:28.000Z",
            "submittedOnDailyAt": "2025-04-22T01:13:32.158Z",
            "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
            "submittedOnDailyBy": {
                "_id": "625913bd5f80a3c1aad074b6",
                "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
                "isPro": false,
                "fullname": "Salman Rahman",
                "user": "salmannyu",
                "type": "user"
            },
            "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.",
            "upvotes": 25,
            "discussionId": "68070198c1bd0fc00c7d9101",
            "projectPage": "https://x-teaming.github.io/",
            "githubRepo": "https://github.com/salman-lui/x-teaming",
            "ai_keywords": [
                "multi-turn interactions",
                "language models (LMs)",
                "safety risks",
                "harmful intent",
                "single-turn safety",
                "adaptability",
                "diversity",
                "X-Teaming",
                "collaborative agents",
                "attack scenarios",
                "jailbreak effectiveness",
                "jailbreak success rates",
                "Claude 3.7 Sonnet model",
                "multi-turn jailbreak",
                "XGuard-Train",
                "safety training dataset",
                "interactive jailbreaks",
                "multi-turn safety alignment"
            ]
        },
        "publishedAt": "2025-04-15T12:11:28.000Z",
        "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
        "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13203.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "625913bd5f80a3c1aad074b6",
            "avatarUrl": "/avatars/745e4d3c916a0f0fced72ac702ff677d.svg",
            "fullname": "Salman Rahman",
            "name": "salmannyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.14870",
            "authors": [
                {
                    "_id": "68070933142b618d3357d2bc",
                    "name": "Hongru Wang",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2bd",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2be",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2bf",
                    "name": "Xiusi Chen",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2c0",
                    "name": "Jiahao Qiu",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2c1",
                    "name": "Shijue Huang",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2c2",
                    "name": "Bowen Jin",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2c3",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2c4",
                    "name": "Kam-Fai Wong",
                    "hidden": false
                },
                {
                    "_id": "68070933142b618d3357d2c5",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T05:40:05.000Z",
            "submittedOnDailyAt": "2025-04-22T14:43:11.034Z",
            "title": "OTC: Optimal Tool Calls via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65f906e5c3dbdcae83ff7aac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                "isPro": false,
                "fullname": "Hongru Wang",
                "user": "Merlin-Hongru",
                "type": "user"
            },
            "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.",
            "upvotes": 23,
            "discussionId": "68070934142b618d3357d318",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "tool-integrated reasoning (TIR)",
                "large language models (LLMs)",
                "search engines",
                "code interpreters",
                "optimization",
                "final answer correctness",
                "efficiency",
                "cost",
                "suboptimal behavior",
                "excessive tool calls",
                "computational overhead",
                "financial overhead",
                "insufficient tool use",
                "answer quality",
                "Optimal Tool Call-controlled Policy Optimization (OTC-PO)",
                "tool-integrated reward",
                "correctness",
                "tool efficiency",
                "high tool productivity",
                "Proximal Policy Optimization (PPO)",
                "Group Relative Preference Optimization (GRPO)",
                "OTC-PPO",
                "OTC-GRPO",
                "Qwen-2.5",
                "Qwen-Math",
                "QA benchmarks",
                "tool calls",
                "tool productivity",
                "answer accuracy"
            ]
        },
        "publishedAt": "2025-04-21T01:40:05.000Z",
        "title": "OTC: Optimal Tool Calls via Reinforcement Learning",
        "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14870.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "fullname": "Hongru Wang",
            "name": "Merlin-Hongru",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.14603",
            "authors": [
                {
                    "_id": "6806fab1b89f3c89c81afbe0",
                    "user": {
                        "_id": "654dbac9938fbf1e696be8aa",
                        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                        "isPro": false,
                        "fullname": "Chaoyun Zhang",
                        "user": "vyokky",
                        "type": "user"
                    },
                    "name": "Chaoyun Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:25.217Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe1",
                    "name": "He Huang",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe2",
                    "name": "Chiming Ni",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe3",
                    "name": "Jian Mu",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe4",
                    "name": "Si Qin",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe5",
                    "user": {
                        "_id": "62c6df026a092eda1f1ab6e5",
                        "avatarUrl": "/avatars/d58fff1a157b189ce2617889ef5f6e2f.svg",
                        "isPro": false,
                        "fullname": "Shilin He",
                        "user": "shilhe",
                        "type": "user"
                    },
                    "name": "Shilin He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:57:38.634Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe6",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe7",
                    "name": "Fangkai Yang",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe8",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbe9",
                    "user": {
                        "_id": "632407c892e07e3ca20aca28",
                        "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
                        "isPro": false,
                        "fullname": "Chao Du",
                        "user": "duchao",
                        "type": "user"
                    },
                    "name": "Chao Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:57:25.179Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbea",
                    "user": {
                        "_id": "666933c97bf97e24f7b5266e",
                        "avatarUrl": "/avatars/283961b37d463a386b08ad33dacca0f4.svg",
                        "isPro": false,
                        "fullname": "Liqun Li",
                        "user": "liqul",
                        "type": "user"
                    },
                    "name": "Liqun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:57:10.526Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbeb",
                    "name": "Yu Kang",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbec",
                    "name": "Zhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbed",
                    "name": "Suzhen Zheng",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbee",
                    "user": {
                        "_id": "6430e3d1cd31d174a9f9f097",
                        "avatarUrl": "/avatars/132d28d4336ed93b676123f3fb15bd61.svg",
                        "isPro": false,
                        "fullname": "rujiawang",
                        "user": "rujiawang",
                        "type": "user"
                    },
                    "name": "Rujia Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:56:40.486Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbef",
                    "name": "Jiaxu Qian",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbf0",
                    "user": {
                        "_id": "653b52098abd634b83f4fd38",
                        "avatarUrl": "/avatars/c9c5023b030e94c1b1abfb7a1c1dfaf3.svg",
                        "isPro": false,
                        "fullname": "MingHua Ma",
                        "user": "Gezelligheid520",
                        "type": "user"
                    },
                    "name": "Minghua Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:56:25.955Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbf1",
                    "user": {
                        "_id": "6554338ba4de813482e91ad5",
                        "avatarUrl": "/avatars/bee879fa4a818fd6c070ee001769ce05.svg",
                        "isPro": false,
                        "fullname": "Jian-Guang Lou",
                        "user": "substill",
                        "type": "user"
                    },
                    "name": "Jian-Guang Lou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:56:18.953Z",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbf2",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbf3",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "6806fab1b89f3c89c81afbf4",
                    "user": {
                        "_id": "66473d2c7abe6ad66e81a3dd",
                        "avatarUrl": "/avatars/82f40244806c06ffeaa1c4265e9725ea.svg",
                        "isPro": false,
                        "fullname": "ZHANGDONGMEI",
                        "user": "ZDM6426",
                        "type": "user"
                    },
                    "name": "Dongmei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:56:01.402Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-20T13:04:43.000Z",
            "submittedOnDailyAt": "2025-04-22T00:41:48.404Z",
            "title": "UFO2: The Desktop AgentOS",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": false,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
            "upvotes": 23,
            "discussionId": "6806fabdb89f3c89c81affb5",
            "projectPage": "https://microsoft.github.io/UFO/",
            "githubRepo": "https://github.com/microsoft/UFO",
            "ai_keywords": [
                "multimodal large language models",
                "task decomposition",
                "coordination",
                "application-specialized AppAgent",
                "native APIs",
                "domain-specific knowledge",
                "unified GUI--API action layer",
                "hybrid control detection pipeline",
                "Windows UI Automation (UIA)",
                "vision-based parsing",
                "speculative multi-action planning",
                "Picture-in-Picture (PiP) interface",
                "runtime efficiency"
            ]
        },
        "publishedAt": "2025-04-20T09:04:43.000Z",
        "title": "UFO2: The Desktop AgentOS",
        "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14603.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "fullname": "Chaoyun Zhang",
            "name": "vyokky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.14396",
            "authors": [
                {
                    "_id": "6806fcc4f349e60f6c1b928c",
                    "user": {
                        "_id": "630461624ec2dfa82a5ad7e7",
                        "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
                        "isPro": false,
                        "fullname": "Minho Park",
                        "user": "mpark",
                        "type": "user"
                    },
                    "name": "Minho Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:59.880Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b928d",
                    "user": {
                        "_id": "679ecfd537a46d35e42f068d",
                        "avatarUrl": "/avatars/d3e891401105b5ef7196cb057ad92be0.svg",
                        "isPro": false,
                        "fullname": "TaewoongKang",
                        "user": "TaewoongKang",
                        "type": "user"
                    },
                    "name": "Taewoong Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:46.869Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b928e",
                    "user": {
                        "_id": "6369f693bf21b20c5692937b",
                        "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
                        "isPro": false,
                        "fullname": "Jooyeol Yun",
                        "user": "YeolJoo",
                        "type": "user"
                    },
                    "name": "Jooyeol Yun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:02.918Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b928f",
                    "user": {
                        "_id": "642fcfc0a043f0ac7deeaae0",
                        "avatarUrl": "/avatars/6cc46dd480cdc0d86c7a509e22782a13.svg",
                        "isPro": false,
                        "fullname": "Sungwon Hwang",
                        "user": "sungwon95",
                        "type": "user"
                    },
                    "name": "Sungwon Hwang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:52.332Z",
                    "hidden": false
                },
                {
                    "_id": "6806fcc4f349e60f6c1b9290",
                    "user": {
                        "_id": "64be3127805e5b64572da65c",
                        "avatarUrl": "/avatars/edd3e94ba9e375827cc75b164602bcac.svg",
                        "isPro": false,
                        "fullname": "Jaegul Choo",
                        "user": "joyfull78",
                        "type": "user"
                    },
                    "name": "Jaegul Choo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:52:58.097Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-19T19:59:11.000Z",
            "submittedOnDailyAt": "2025-04-22T00:50:52.270Z",
            "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
            "submittedOnDailyBy": {
                "_id": "630461624ec2dfa82a5ad7e7",
                "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
                "isPro": false,
                "fullname": "Minho Park",
                "user": "mpark",
                "type": "user"
            },
            "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
            "upvotes": 23,
            "discussionId": "6806fcc7f349e60f6c1b93ab",
            "projectPage": "https://pmh9960.github.io/research/SphereDiff/",
            "githubRepo": "https://github.com/pmh9960/SphereDiff",
            "ai_keywords": [
                "SphereDiff",
                "diffusion models",
                "equirectangular projection (ERP)",
                "spherical latent representation",
                "MultiDiffusion",
                "spherical latent space",
                "spherical latent sampling method",
                "distortion-aware weighted averaging",
                "high-fidelity",
                "immersive AR/VR applications"
            ]
        },
        "publishedAt": "2025-04-19T15:59:11.000Z",
        "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
        "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14396.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630461624ec2dfa82a5ad7e7",
            "avatarUrl": "/avatars/6696e21069494552b81a28a899a28fd1.svg",
            "fullname": "Minho Park",
            "name": "mpark",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15281",
            "authors": [
                {
                    "_id": "68072d05362af0cf18fb4b0c",
                    "name": "Cailin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b0d",
                    "name": "Yaoqi Hu",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b0e",
                    "user": {
                        "_id": "6675854966c4fa6d0cee4d50",
                        "avatarUrl": "/avatars/aa6041a97985078e82cc89bfbade9828.svg",
                        "isPro": false,
                        "fullname": "xuanyang zhang",
                        "user": "xuanyangz",
                        "type": "user"
                    },
                    "name": "Xuanyang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:53:34.744Z",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b0f",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:44.772Z",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b10",
                    "user": {
                        "_id": "6592960e0f4519bfc2dd4968",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-9JT886u9hepkN3IX-875.png",
                        "isPro": false,
                        "fullname": "Jia Cheng Bao",
                        "user": "unpackableorange",
                        "type": "user"
                    },
                    "name": "Jiacheng Bao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:53:41.233Z",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b11",
                    "name": "Shengqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b12",
                    "user": {
                        "_id": "67da6acc05101e8e1d2c20a2",
                        "avatarUrl": "/avatars/1cfa3a1f59687db58af4e1b4a8767bfd.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Yiying12",
                        "type": "user"
                    },
                    "name": "Yiying Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:42.183Z",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b13",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b14",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "68072d05362af0cf18fb4b15",
                    "name": "Ming Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
            ],
            "publishedAt": "2025-04-21T17:59:55.000Z",
            "submittedOnDailyAt": "2025-04-22T04:18:58.848Z",
            "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
            "submittedOnDailyBy": {
                "_id": "64b914c8ace99c0723ad83a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                "isPro": false,
                "fullname": "Wei Cheng",
                "user": "wchengad",
                "type": "user"
            },
            "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
            "upvotes": 21,
            "discussionId": "68072d08362af0cf18fb4c7b",
            "projectPage": "https://styleme3d.github.io/",
            "githubRepo": "https://github.com/AIGCResearch/styleme3d",
            "ai_keywords": [
                "Gaussian Splatting",
                "StyleMe3D",
                "multi-modal style conditioning",
                "multi-level semantic alignment",
                "perceptual quality enhancement",
                "Dynamic Style Score Distillation",
                "Stable Diffusion",
                "latent space",
                "Contrastive Style Descriptor",
                "localized, content-aware texture transfer",
                "Simultaneously Optimized Scale",
                "3D Gaussian Quality Assessment",
                "differentiable aesthetic prior",
                "NeRF synthetic dataset",
                "tandt db",
                "preserving geometric details",
                "stylistic consistency",
                "real-time rendering"
            ]
        },
        "publishedAt": "2025-04-21T13:59:55.000Z",
        "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
        "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/EeggKsuKNbiiygcWDKxQZ.webm"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15281.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "fullname": "Wei Cheng",
            "name": "wchengad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13367",
            "authors": [
                {
                    "_id": "68072cae860bab681fa8c2c9",
                    "name": "Xiao Pu",
                    "hidden": false
                },
                {
                    "_id": "68072cae860bab681fa8c2ca",
                    "user": {
                        "_id": "6002c1db698168af3bb9f4a5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1610793426015-noauth.png",
                        "isPro": false,
                        "fullname": "Michael Saxon",
                        "user": "saxon",
                        "type": "user"
                    },
                    "name": "Michael Saxon",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-22T05:44:17.040Z",
                    "hidden": false
                },
                {
                    "_id": "68072cae860bab681fa8c2cb",
                    "name": "Wenyue Hua",
                    "hidden": false
                },
                {
                    "_id": "68072cae860bab681fa8c2cc",
                    "name": "William Yang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T22:16:30.000Z",
            "submittedOnDailyAt": "2025-04-22T16:37:07.701Z",
            "title": "THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating\n  Overthinking in Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "6002c1db698168af3bb9f4a5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1610793426015-noauth.png",
                "isPro": false,
                "fullname": "Michael Saxon",
                "user": "saxon",
                "type": "user"
            },
            "summary": "Reasoning models have demonstrated impressive performance on difficult tasks\nthat traditional language models struggle at. However, many are plagued with\nthe problem of overthinking--generating large amounts of unnecessary tokens\nwhich don't improve accuracy on a question. We introduce approximate measures\nof problem-level difficulty and demonstrate that a clear relationship between\nproblem difficulty and optimal token spend exists, and evaluate how well\ncalibrated a variety of reasoning models are in terms of efficiently allocating\nthe optimal token count. We find that in general, reasoning models are poorly\ncalibrated, particularly on easy problems. To evaluate calibration on easy\nquestions we introduce DUMB500, a dataset of extremely easy math, reasoning,\ncode, and task problems, and jointly evaluate reasoning model on these simple\nexamples and extremely difficult examples from existing frontier benchmarks on\nthe same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free\nblack box decoding technique that significantly improves reasoning model\ncalibration.",
            "upvotes": 18,
            "discussionId": "68072cb1860bab681fa8c380",
            "ai_keywords": [
                "reasoning models",
                "overthinking",
                "unnecessary tokens",
                "problem-level difficulty",
                "optimal token spend",
                "efficient allocation",
                "calibrated",
                "DUMB500",
                "extreme easy math",
                "extreme easy reasoning",
                "extreme easy code",
                "extreme easy task problems",
                "frontier benchmarks",
                "THOUGHTTERMINATOR",
                "training-free",
                "black box decoding"
            ]
        },
        "publishedAt": "2025-04-17T18:16:30.000Z",
        "title": "THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating\n  Overthinking in Reasoning Models",
        "summary": "Reasoning models have demonstrated impressive performance on difficult tasks\nthat traditional language models struggle at. However, many are plagued with\nthe problem of overthinking--generating large amounts of unnecessary tokens\nwhich don't improve accuracy on a question. We introduce approximate measures\nof problem-level difficulty and demonstrate that a clear relationship between\nproblem difficulty and optimal token spend exists, and evaluate how well\ncalibrated a variety of reasoning models are in terms of efficiently allocating\nthe optimal token count. We find that in general, reasoning models are poorly\ncalibrated, particularly on easy problems. To evaluate calibration on easy\nquestions we introduce DUMB500, a dataset of extremely easy math, reasoning,\ncode, and task problems, and jointly evaluate reasoning model on these simple\nexamples and extremely difficult examples from existing frontier benchmarks on\nthe same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free\nblack box decoding technique that significantly improves reasoning model\ncalibration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13367.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6002c1db698168af3bb9f4a5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1610793426015-noauth.png",
            "fullname": "Michael Saxon",
            "name": "saxon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15133",
            "authors": [
                {
                    "_id": "68073a2d19a9fa6096218691",
                    "user": {
                        "_id": "6549caee44e75a7de4fee2fa",
                        "avatarUrl": "/avatars/5aea69671eb1299aaaa948d888b4b64f.svg",
                        "isPro": false,
                        "fullname": "Xu Ziwen",
                        "user": "xzwnlp",
                        "type": "user"
                    },
                    "name": "Ziwen Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:02:20.981Z",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218692",
                    "user": {
                        "_id": "66d270dc5ae47374c27c9e9a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d270dc5ae47374c27c9e9a/41UfzGQsFl42579N-Kbd4.jpeg",
                        "isPro": false,
                        "fullname": "Shuxun Wang",
                        "user": "Saberlve",
                        "type": "user"
                    },
                    "name": "Shuxun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:02:26.748Z",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218693",
                    "user": {
                        "_id": "67026ef05ce58dd0c3fc0d1c",
                        "avatarUrl": "/avatars/94d907941a00ddc9a8030b5c6772bc59.svg",
                        "isPro": false,
                        "fullname": "xukewei",
                        "user": "xukewei",
                        "type": "user"
                    },
                    "name": "Kewei Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:02:36.614Z",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218694",
                    "user": {
                        "_id": "66f4bdbdc51768d9d4498d16",
                        "avatarUrl": "/avatars/0f6ded5fd9cf4e6f0b180b5aa329ea33.svg",
                        "isPro": false,
                        "fullname": "Haoming Xu",
                        "user": "HaomingXu",
                        "type": "user"
                    },
                    "name": "Haoming Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:02:43.867Z",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218695",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218696",
                    "user": {
                        "_id": "65cad52fd6c974694fc20b8e",
                        "avatarUrl": "/avatars/8232a7c5db590ed26751a47c45d481b8.svg",
                        "isPro": false,
                        "fullname": "Xinle Deng",
                        "user": "Linear-Matrix-Probability",
                        "type": "user"
                    },
                    "name": "Xinle Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:02:50.386Z",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218697",
                    "user": {
                        "_id": "6122fbe636a4c36a99dbea7b",
                        "avatarUrl": "/avatars/c0cd2c1ef58e315d9adda9d26000f625.svg",
                        "isPro": false,
                        "fullname": "Yunzhi Yao",
                        "user": "cowTodd",
                        "type": "user"
                    },
                    "name": "Yunzhi Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:03:04.285Z",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218698",
                    "name": "Guozhou Zheng",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa6096218699",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "68073a2d19a9fa609621869a",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:02:05.015Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T14:33:55.000Z",
            "submittedOnDailyAt": "2025-04-22T05:12:13.220Z",
            "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
            "upvotes": 16,
            "discussionId": "68073a2e19a9fa60962186db",
            "projectPage": "https://zjunlp.github.io/project/EasyEdit2/",
            "githubRepo": "https://github.com/zjunlp/EasyEdit",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "test-time interventions",
                "steering vector generator",
                "steering vector applier",
                "seamless model steering",
                "model steering performance"
            ]
        },
        "publishedAt": "2025-04-21T10:33:55.000Z",
        "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models",
        "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15133.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15280",
            "authors": [
                {
                    "_id": "680703e4c0ce7eea9ba1a548",
                    "user": {
                        "_id": "6499eca0685215f7247bd5ce",
                        "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
                        "isPro": false,
                        "fullname": "Chun-Hsiao Yeh",
                        "user": "danielchyeh",
                        "type": "user"
                    },
                    "name": "Chun-Hsiao Yeh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:54.878Z",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a549",
                    "user": {
                        "_id": "641bd539aebaa27e07540613",
                        "avatarUrl": "/avatars/000e8057e7280b9eba9bec5116d14718.svg",
                        "isPro": false,
                        "fullname": "chenyu wang",
                        "user": "ch-chenyu",
                        "type": "user"
                    },
                    "name": "Chenyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:52.696Z",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a54a",
                    "name": "Shengbang Tong",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a54b",
                    "user": {
                        "_id": "6442b024e255a338677d78fa",
                        "avatarUrl": "/avatars/516769c6c3bc7a0d416924181c6866dc.svg",
                        "isPro": false,
                        "fullname": "Ta-Ying Cheng",
                        "user": "chengtim",
                        "type": "user"
                    },
                    "name": "Ta-Ying Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:58:20.913Z",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a54c",
                    "name": "Rouyu Wang",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a54d",
                    "name": "Tianzhe Chu",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a54e",
                    "name": "Yuexiang Zhai",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a54f",
                    "user": {
                        "_id": "65dd08d38c103eb3d93346bd",
                        "avatarUrl": "/avatars/6f84dba62d93d21e48212951f54f7e09.svg",
                        "isPro": false,
                        "fullname": "Yubei Chen",
                        "user": "yubei",
                        "type": "user"
                    },
                    "name": "Yubei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:59:03.407Z",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a550",
                    "name": "Shenghua Gao",
                    "hidden": false
                },
                {
                    "_id": "680703e4c0ce7eea9ba1a551",
                    "name": "Yi Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:59:53.000Z",
            "submittedOnDailyAt": "2025-04-22T01:49:06.289Z",
            "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
            "submittedOnDailyBy": {
                "_id": "637c7420f219c71f93ec8f81",
                "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
                "isPro": false,
                "fullname": "frog",
                "user": "frog123123123123",
                "type": "user"
            },
            "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.",
            "upvotes": 13,
            "discussionId": "680703e6c0ce7eea9ba1a63e",
            "projectPage": "https://danielchyeh.github.io/All-Angles-Bench/",
            "githubRepo": "https://github.com/Chenyu-Wang567/All-Angles-Bench/tree/main",
            "ai_keywords": [
                "Multi-Modal Large Language Models (MLLMs)",
                "multi-view geometric consistency",
                "cross-view correspondence",
                "All-Angles Bench",
                "Gemini-2.0-Flash",
                "Claude-3.7-Sonnet",
                "GPT-4o",
                "geometric correspondence",
                "camera pose estimation",
                "cross-view correspondence",
                "partially occluded views",
                "coarse camera poses",
                "multi-view awareness"
            ]
        },
        "publishedAt": "2025-04-21T13:59:53.000Z",
        "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
        "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15280.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637c7420f219c71f93ec8f81",
            "avatarUrl": "/avatars/969b72bd4320423af89e6a5d0ffa03cc.svg",
            "fullname": "frog",
            "name": "frog123123123123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.14655",
            "authors": [
                {
                    "_id": "68070991b0cb768cae20c585",
                    "name": "Yunhui Xia",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c586",
                    "user": {
                        "_id": "6468823272d9180d4ac90bdf",
                        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                        "isPro": false,
                        "fullname": "Wei Shen",
                        "user": "Swtheking",
                        "type": "user"
                    },
                    "name": "Wei Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T10:59:51.173Z",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c587",
                    "name": "Yan Wang",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c588",
                    "user": {
                        "_id": "6731c04d5f55903a1d8c307c",
                        "avatarUrl": "/avatars/704b1d628c55e3141194b08736f21267.svg",
                        "isPro": false,
                        "fullname": "Jason Klein Liu",
                        "user": "jasonkleinlove",
                        "type": "user"
                    },
                    "name": "Jason Klein Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c589",
                    "name": "Huifeng Sun",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c58a",
                    "name": "Siyue Wu",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c58b",
                    "user": {
                        "_id": "63f6c04ac96958470d1e9043",
                        "avatarUrl": "/avatars/da46cdd9e21498e120ca91b67bfbfb5e.svg",
                        "isPro": false,
                        "fullname": "Jian Hu",
                        "user": "chuyi777",
                        "type": "user"
                    },
                    "name": "Jian Hu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-22T03:14:26.859Z",
                    "hidden": false
                },
                {
                    "_id": "68070991b0cb768cae20c58c",
                    "name": "Xiaolong Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-20T15:28:16.000Z",
            "submittedOnDailyAt": "2025-04-22T01:46:36.359Z",
            "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
            "submittedOnDailyBy": {
                "_id": "6468823272d9180d4ac90bdf",
                "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                "isPro": false,
                "fullname": "Wei Shen",
                "user": "Swtheking",
                "type": "user"
            },
            "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.",
            "upvotes": 13,
            "discussionId": "68070992b0cb768cae20c5ca",
            "ai_keywords": [
                "LeetCodeDataset",
                "code-generation models",
                "LLM research",
                "evaluation",
                "supervised fine-tuning (SFT)",
                "reasoning models"
            ]
        },
        "publishedAt": "2025-04-20T11:28:16.000Z",
        "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
        "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14655.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6468823272d9180d4ac90bdf",
            "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
            "fullname": "Wei Shen",
            "name": "Swtheking",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.14899",
            "authors": [
                {
                    "_id": "68075a2bdadb28dddc13dfc6",
                    "user": {
                        "_id": "64892bc82eafb7b91182bec5",
                        "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
                        "isPro": false,
                        "fullname": "chenjie cao",
                        "user": "ewrfcas",
                        "type": "user"
                    },
                    "name": "Chenjie Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:03:42.731Z",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfc7",
                    "user": {
                        "_id": "6434caa64b34368fdb07da48",
                        "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
                        "isPro": false,
                        "fullname": "Jingkai Zhou",
                        "user": "theFoxofSky",
                        "type": "user"
                    },
                    "name": "Jingkai Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:03:50.199Z",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfc8",
                    "name": "Shikai Li",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfc9",
                    "name": "Jingyun Liang",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfca",
                    "user": {
                        "_id": "6805e0a6a0ec02e5d8bb31f8",
                        "avatarUrl": "/avatars/dec2296e1f2d39399a8b99777ff4c5ad.svg",
                        "isPro": false,
                        "fullname": "CHAO-HUI-YU",
                        "user": "Alex-snow",
                        "type": "user"
                    },
                    "name": "Chaohui Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:04:08.656Z",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfcb",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfcc",
                    "name": "Xiangyang Xue",
                    "hidden": false
                },
                {
                    "_id": "68075a2bdadb28dddc13dfcd",
                    "user": {
                        "_id": "6409fcc8f3dabf93824c84c6",
                        "avatarUrl": "/avatars/dd8fd579630e50ba3058c3829604478e.svg",
                        "isPro": false,
                        "fullname": "YANWEI",
                        "user": "yanweifuture",
                        "type": "user"
                    },
                    "name": "Yanwei Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:04:23.211Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/ldOrEdwnbNQcTKZBcT4Oq.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/g5LyXAgsA-XhlQDd-q-_S.mp4"
            ],
            "publishedAt": "2025-04-21T07:10:41.000Z",
            "submittedOnDailyAt": "2025-04-22T07:30:21.972Z",
            "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation",
            "submittedOnDailyBy": {
                "_id": "64892bc82eafb7b91182bec5",
                "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
                "isPro": false,
                "fullname": "chenjie cao",
                "user": "ewrfcas",
                "type": "user"
            },
            "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.",
            "upvotes": 11,
            "discussionId": "68075a2edadb28dddc13e0ac",
            "projectPage": "https://ewrfcas.github.io/Uni3C/",
            "ai_keywords": [
                "plug-and-play control module",
                "PCDController",
                "unprojected point clouds",
                "monocular depth",
                "3D priors",
                "video foundational models",
                "jointly aligned 3D world guidance",
                "scenery point clouds",
                "SMPL-X characters",
                "camera controllability",
                "human motion quality"
            ]
        },
        "publishedAt": "2025-04-21T03:10:41.000Z",
        "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation",
        "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/ldOrEdwnbNQcTKZBcT4Oq.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64892bc82eafb7b91182bec5/g5LyXAgsA-XhlQDd-q-_S.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14899.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64892bc82eafb7b91182bec5",
            "avatarUrl": "/avatars/c9eed91e60f3ea227b35e111d3fc4200.svg",
            "fullname": "chenjie cao",
            "name": "ewrfcas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.14239",
            "authors": [
                {
                    "_id": "6806e9e819a9fa609609fe65",
                    "name": "Yuhang Liu",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe66",
                    "user": {
                        "_id": "64245f2c089d5fae56b4549a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                        "isPro": false,
                        "fullname": "Pengxiang Li",
                        "user": "pengxiang",
                        "type": "user"
                    },
                    "name": "Pengxiang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:50.804Z",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe67",
                    "user": {
                        "_id": "629084f3a391a907d5e0484e",
                        "avatarUrl": "/avatars/3a1d6d5aa90b3b8372505d13ccf8f2dd.svg",
                        "isPro": false,
                        "fullname": "unk",
                        "user": "xieck13",
                        "type": "user"
                    },
                    "name": "Congkai Xie",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-22T00:59:21.913Z",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe68",
                    "name": "Xavier Hu",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe69",
                    "user": {
                        "_id": "650dde4ce14eeb01d42b37a1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dde4ce14eeb01d42b37a1/n5Yv24uofZ2XJjXdYCrKd.png",
                        "isPro": false,
                        "fullname": "Xiaotian Han",
                        "user": "xiaotianhan",
                        "type": "user"
                    },
                    "name": "Xiaotian Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:27.543Z",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe6a",
                    "name": "Shengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe6b",
                    "name": "Hongxia Yang",
                    "hidden": false
                },
                {
                    "_id": "6806e9e819a9fa609609fe6c",
                    "name": "Fei Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-19T09:25:55.000Z",
            "submittedOnDailyAt": "2025-04-22T02:26:02.459Z",
            "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
            "submittedOnDailyBy": {
                "_id": "64245f2c089d5fae56b4549a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                "isPro": false,
                "fullname": "Pengxiang Li",
                "user": "pengxiang",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
            "upvotes": 11,
            "discussionId": "6806e9e919a9fa609609fea1",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "Graphical User Interface (GUI) Agents",
                "Spatial Reasoning Distillation",
                "Actor2Reasoner framework",
                "Reasoning Injection",
                "Deliberation Enhancement",
                "Sub-goal Guidance",
                "Error Recovery Scenario Construction",
                "GUI grounding",
                "trajectory tasks"
            ]
        },
        "publishedAt": "2025-04-19T05:25:55.000Z",
        "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
        "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14239.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13805",
            "authors": [
                {
                    "_id": "680662b97415e191e3579b9e",
                    "user": {
                        "_id": "64d761b98ebc40443831f82a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
                        "isPro": false,
                        "fullname": "lgy0404",
                        "user": "lgy0404",
                        "type": "user"
                    },
                    "name": "Guangyi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:52:04.194Z",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579b9f",
                    "user": {
                        "_id": "65a088f4300957620ba45c70",
                        "avatarUrl": "/avatars/56ed45e10d3455531979f30881b2d3f9.svg",
                        "isPro": false,
                        "fullname": "pengxiang zhao",
                        "user": "Pengxiangzhao",
                        "type": "user"
                    },
                    "name": "Pengxiang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:52:01.390Z",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba0",
                    "name": "Liang Liu",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba1",
                    "user": {
                        "_id": "67c26500918bb8d89b01d6a3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OJROx6Q4Z2rnW5eOiqvm3.png",
                        "isPro": false,
                        "fullname": "Zhiming Chen",
                        "user": "zhimingc",
                        "type": "user"
                    },
                    "name": "Zhiming Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:01:08.419Z",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba2",
                    "user": {
                        "_id": "6458ce236fa580137af5aa95",
                        "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Chai",
                        "user": "Yuxiang007",
                        "type": "user"
                    },
                    "name": "Yuxiang Chai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:01:01.122Z",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba3",
                    "name": "Shuai Ren",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba4",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba5",
                    "user": {
                        "_id": "66604211253289136b6c42d6",
                        "avatarUrl": "/avatars/b2ea4870d45efea01d4c2f34a2cd96bd.svg",
                        "isPro": false,
                        "fullname": "SHIBO HE",
                        "user": "dkeeeee",
                        "type": "user"
                    },
                    "name": "Shibo He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:00:42.823Z",
                    "hidden": false
                },
                {
                    "_id": "680662b97415e191e3579ba6",
                    "name": "Wenchao Meng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T17:13:34.000Z",
            "submittedOnDailyAt": "2025-04-22T01:14:18.820Z",
            "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
            "submittedOnDailyBy": {
                "_id": "6458ce236fa580137af5aa95",
                "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
                "isPro": false,
                "fullname": "Yuxiang Chai",
                "user": "Yuxiang007",
                "type": "user"
            },
            "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
            "upvotes": 8,
            "discussionId": "680662bd7415e191e3579c7d",
            "ai_keywords": [
                "LearnGUI",
                "LearnAct",
                "DemoParser",
                "KnowSeeker",
                "ActExecutor",
                "demonstration-based learning",
                "mobile GUI agents",
                "human demonstrations",
                "multi-agent framework"
            ]
        },
        "publishedAt": "2025-04-18T13:13:34.000Z",
        "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
        "summary": "Mobile GUI agents show promise in automating tasks but face generalization\nchallenges in diverse real-world scenarios. Traditional approaches using\npre-training or fine-tuning with massive datasets struggle with the diversity\nof mobile applications and user-specific tasks. We propose enhancing mobile GUI\nagent capabilities through human demonstrations, focusing on improving\nperformance in unseen scenarios rather than pursuing universal generalization\nthrough larger datasets. To realize this paradigm, we introduce LearnGUI, the\nfirst comprehensive dataset specifically designed for studying\ndemonstration-based learning in mobile GUI agents, comprising 2,252 offline\ntasks and 101 online tasks with high-quality human demonstrations. We further\ndevelop LearnAct, a sophisticated multi-agent framework that automatically\nextracts knowledge from demonstrations to enhance task completion. This\nframework integrates three specialized agents: DemoParser for knowledge\nextraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for\ndemonstration-enhanced task execution. Our experimental results show\nsignificant performance gains in both offline and online evaluations. In\noffline assessments, a single demonstration improves model performance,\nincreasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online\nevaluations, our framework enhances UI-TARS-7B-SFT's task success rate from\n18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish\ndemonstration-based learning as a promising direction for more adaptable,\npersonalized, and deployable mobile GUI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13805.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6458ce236fa580137af5aa95",
            "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
            "fullname": "Yuxiang Chai",
            "name": "Yuxiang007",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.08902",
            "authors": [
                {
                    "_id": "680730598ce2be7f5450bd77",
                    "user": {
                        "_id": "64e3950d9ec4cf50009ce960",
                        "avatarUrl": "/avatars/8aade78bdde8423bd811ca8394cb08c5.svg",
                        "isPro": false,
                        "fullname": "Pascal Chang",
                        "user": "pascalchang87",
                        "type": "user"
                    },
                    "name": "Pascal Chang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:31.652Z",
                    "hidden": false
                },
                {
                    "_id": "680730598ce2be7f5450bd78",
                    "user": {
                        "_id": "66a38c9e053b7e9c907fba72",
                        "avatarUrl": "/avatars/59588845e21ec6ad63631c892066f4ad.svg",
                        "isPro": false,
                        "fullname": "Sergio Sancho",
                        "user": "ssancho",
                        "type": "user"
                    },
                    "name": "Sergio Sancho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:28.796Z",
                    "hidden": false
                },
                {
                    "_id": "680730598ce2be7f5450bd79",
                    "name": "Jingwei Tang",
                    "hidden": false
                },
                {
                    "_id": "680730598ce2be7f5450bd7a",
                    "user": {
                        "_id": "67c051dcb07e7a089db0cc99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a6mfBXZYBDyq1VqLxbcfi.png",
                        "isPro": false,
                        "fullname": "Markus Grosse",
                        "user": "coffeeweb2907",
                        "type": "user"
                    },
                    "name": "Markus Gross",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:01:42.279Z",
                    "hidden": false
                },
                {
                    "_id": "680730598ce2be7f5450bd7b",
                    "name": "Vinicius C. Azevedo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-11T18:12:01.000Z",
            "submittedOnDailyAt": "2025-04-22T04:59:10.950Z",
            "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
            "submittedOnDailyBy": {
                "_id": "66863bd107019f3fe48a21ab",
                "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
                "isPro": false,
                "fullname": "Manuel Kansy",
                "user": "manuelkansy",
                "type": "user"
            },
            "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions.",
            "upvotes": 7,
            "discussionId": "6807305f8ce2be7f5450bf69",
            "ai_keywords": [
                "latent rectified flow models",
                "Laplacian Pyramid Warping",
                "frequency-aware image warping",
                "generative perceptual illusions"
            ]
        },
        "publishedAt": "2025-04-11T14:12:01.000Z",
        "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
        "summary": "Anamorphosis refers to a category of images that are intentionally distorted,\nmaking them unrecognizable when viewed directly. Their true form only reveals\nitself when seen from a specific viewpoint, which can be through some\ncatadioptric device like a mirror or a lens. While the construction of these\nmathematical devices can be traced back to as early as the 17th century, they\nare only interpretable when viewed from a specific vantage point and tend to\nlose meaning when seen normally. In this paper, we revisit these famous optical\nillusions with a generative twist. With the help of latent rectified flow\nmodels, we propose a method to create anamorphic images that still retain a\nvalid interpretation when viewed directly. To this end, we introduce Laplacian\nPyramid Warping, a frequency-aware image warping technique key to generating\nhigh-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to\nlatent space models and to a wider range of spatial transforms, enabling the\ncreation of novel generative perceptual illusions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08902.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "66863bd107019f3fe48a21ab",
            "avatarUrl": "/avatars/3297e18e43d40e902b9554a077a34a8a.svg",
            "fullname": "Manuel Kansy",
            "name": "manuelkansy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15270",
            "authors": [
                {
                    "_id": "68074ea84a1c690663814e56",
                    "name": "Ji Qi",
                    "hidden": false
                },
                {
                    "_id": "68074ea84a1c690663814e57",
                    "name": "Yuan Yao",
                    "hidden": false
                },
                {
                    "_id": "68074ea84a1c690663814e58",
                    "user": {
                        "_id": "64ed568ccf6118a9379a61b8",
                        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                        "isPro": false,
                        "fullname": "Yushi Bai",
                        "user": "bys0318",
                        "type": "user"
                    },
                    "name": "Yushi Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:05:19.003Z",
                    "hidden": false
                },
                {
                    "_id": "68074ea84a1c690663814e59",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "68074ea84a1c690663814e5a",
                    "name": "Juanzi Li",
                    "hidden": false
                },
                {
                    "_id": "68074ea84a1c690663814e5b",
                    "user": {
                        "_id": "6310a3cd531cc21f9e06de6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
                        "isPro": false,
                        "fullname": "Zhiyuan Liu",
                        "user": "acharkq",
                        "type": "user"
                    },
                    "name": "Zhiyuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:06:02.057Z",
                    "hidden": false
                },
                {
                    "_id": "68074ea84a1c690663814e5c",
                    "user": {
                        "_id": "6570ae84c4993b8fb96f41a8",
                        "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
                        "isPro": false,
                        "fullname": "Tat-Seng Chua",
                        "user": "chuats",
                        "type": "user"
                    },
                    "name": "Tat-Seng Chua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:05:41.211Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:57:21.000Z",
            "submittedOnDailyAt": "2025-04-22T06:39:45.004Z",
            "title": "An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes",
            "submittedOnDailyBy": {
                "_id": "64ed568ccf6118a9379a61b8",
                "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                "isPro": false,
                "fullname": "Yushi Bai",
                "user": "bys0318",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present Quicksviewer, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45times compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.",
            "upvotes": 6,
            "discussionId": "68074eab4a1c690663814ef7",
            "projectPage": "https://quicksviewer.github.io/",
            "githubRepo": "https://github.com/quicksviewer/quicksviewer",
            "ai_keywords": [
                "LMMs (Large Multimodal Models)",
                "Quicksviewer",
                "Gumbel Softmax",
                "spatiotemporal redundancy",
                "large receptive field",
                "language backbone",
                "Video-MME",
                "power law",
                "cubing network"
            ]
        },
        "publishedAt": "2025-04-21T13:57:21.000Z",
        "title": "An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes",
        "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present Quicksviewer, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45times compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15270.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ed568ccf6118a9379a61b8",
            "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
            "fullname": "Yushi Bai",
            "name": "bys0318",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15217",
            "authors": [
                {
                    "_id": "68070e1895e06498588c4497",
                    "user": {
                        "_id": "63239bd492e07e3ca2068a16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63239bd492e07e3ca2068a16/OFESFFWQVHC1EtJpnGOWv.jpeg",
                        "isPro": true,
                        "fullname": "Yatong Bai",
                        "user": "Bai-YT",
                        "type": "user"
                    },
                    "name": "Yatong Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:50:50.514Z",
                    "hidden": false
                },
                {
                    "_id": "68070e1895e06498588c4498",
                    "name": "Jonah Casebeer",
                    "hidden": false
                },
                {
                    "_id": "68070e1895e06498588c4499",
                    "name": "Somayeh Sojoudi",
                    "hidden": false
                },
                {
                    "_id": "68070e1895e06498588c449a",
                    "user": {
                        "_id": "648119ee7a741c7f33f49f25",
                        "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
                        "isPro": false,
                        "fullname": "Nicholas J. Bryan",
                        "user": "Njb",
                        "type": "user"
                    },
                    "name": "Nicholas J. Bryan",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-22T03:35:40.134Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
            ],
            "publishedAt": "2025-04-21T16:41:40.000Z",
            "submittedOnDailyAt": "2025-04-22T02:07:09.310Z",
            "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
            "submittedOnDailyBy": {
                "_id": "648119ee7a741c7f33f49f25",
                "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
                "isPro": false,
                "fullname": "Nicholas J. Bryan",
                "user": "Njb",
                "type": "user"
            },
            "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.",
            "upvotes": 5,
            "discussionId": "68070e1f95e06498588c467f",
            "ai_keywords": [
                "Distributional RewArds for Generative OptimizatioN (DRAGON)",
                "fine-tuning",
                "media generation models",
                "reinforcement learning with human feedback (RLHF)",
                "direct preference optimization (DPO)",
                "reward functions",
                "exemplar distribution",
                "cross-modality encoders",
                "CLAP",
                "online and on-policy generations",
                "positive demonstration set",
                "negative set",
                "contrast",
                "audio-domain text-to-music diffusion model",
                "music aesthetics model",
                "Vendi diversity",
                "Frechet audio distance (FAD)",
                "full-dataset FAD",
                "human-voted music quality"
            ]
        },
        "publishedAt": "2025-04-21T12:41:40.000Z",
        "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
        "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648119ee7a741c7f33f49f25/V_gAc821aOtnXsoaUFvCQ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15217.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648119ee7a741c7f33f49f25",
            "avatarUrl": "/avatars/183d2143ba20e1cc8712c63c055aadd7.svg",
            "fullname": "Nicholas J. Bryan",
            "name": "Njb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13941",
            "authors": [
                {
                    "_id": "6806f6ff67a715240a5ab9f8",
                    "user": {
                        "_id": "6338dd1776421c0543150467",
                        "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
                        "isPro": false,
                        "fullname": "Syeda Nahida Akter",
                        "user": "SieraL",
                        "type": "user"
                    },
                    "name": "Syeda Nahida Akter",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:06:38.568Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9f9",
                    "user": {
                        "_id": "66980b9c9baa4382e1678809",
                        "avatarUrl": "/avatars/1a516bb7aa7871834c19de708cdd853a.svg",
                        "isPro": false,
                        "fullname": "Shrimai Prabhumoye",
                        "user": "shrimai19",
                        "type": "user"
                    },
                    "name": "Shrimai Prabhumoye",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-22T01:55:12.561Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9fa",
                    "name": "Matvei Novikov",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9fb",
                    "name": "Seungju Han",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9fc",
                    "name": "Ying Lin",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9fd",
                    "user": {
                        "_id": "64ecf888961565b46783bf08",
                        "avatarUrl": "/avatars/0c4ac90b7fa8a6091a67b0216dada7db.svg",
                        "isPro": false,
                        "fullname": "Evelina Bakhturina",
                        "user": "ekmb",
                        "type": "user"
                    },
                    "name": "Evelina Bakhturi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:06:52.587Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9fe",
                    "user": {
                        "_id": "651b3cff66fe9bc55b2828d0",
                        "avatarUrl": "/avatars/1552a97628364537ae3db9fdb30b35c9.svg",
                        "isPro": false,
                        "fullname": "Eric Nyberg",
                        "user": "ericnyberg",
                        "type": "user"
                    },
                    "name": "Eric Nyberg",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:06:58.156Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5ab9ff",
                    "user": {
                        "_id": "64d42729f63b01b7f676b176",
                        "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                        "isPro": false,
                        "fullname": "Yejin Choi",
                        "user": "yejinchoinka",
                        "type": "user"
                    },
                    "name": "Yejin Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:07:04.801Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5aba00",
                    "user": {
                        "_id": "630544b09d2531fabd156fd3",
                        "avatarUrl": "/avatars/7b5374244a887577834fb4524ff76d01.svg",
                        "isPro": false,
                        "fullname": "Mostofa Patwary",
                        "user": "mpatwary",
                        "type": "user"
                    },
                    "name": "Mostofa Patwary",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:07:11.089Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5aba01",
                    "user": {
                        "_id": "6641544c695975af2cbd0da6",
                        "avatarUrl": "/avatars/0ad3c18dcba585259b064fe9b00a07ce.svg",
                        "isPro": false,
                        "fullname": "Mohammad Shoeybi",
                        "user": "shoeybi",
                        "type": "user"
                    },
                    "name": "Mohammad Shoeybi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:07:16.923Z",
                    "hidden": false
                },
                {
                    "_id": "6806f6ff67a715240a5aba02",
                    "user": {
                        "_id": "6311021788942700629e6247",
                        "avatarUrl": "/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg",
                        "isPro": false,
                        "fullname": "Bryan Catanzaro",
                        "user": "ctnzr",
                        "type": "user"
                    },
                    "name": "Bryan Catanzaro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:07:23.267Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-15T21:37:13.000Z",
            "submittedOnDailyAt": "2025-04-22T00:28:47.499Z",
            "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
            "submittedOnDailyBy": {
                "_id": "6338dd1776421c0543150467",
                "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
                "isPro": false,
                "fullname": "Syeda Nahida Akter",
                "user": "SieraL",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
            "upvotes": 5,
            "discussionId": "6806f70067a715240a5aba4c",
            "ai_keywords": [
                "Reinforcement Learning",
                "NEMOTRON-CROSSTHINK",
                "multi-domain corpora",
                "structured templates",
                "answer-space complexity",
                "verifiable answers",
                "data blending strategies",
                "reward modeling",
                "MATH-500",
                "AMC23",
                "MMLU-PRO",
                "GPQA-DIAMOND",
                "AGIEVAL",
                "SUPERGPQA",
                "response efficiency",
                "tokens"
            ]
        },
        "publishedAt": "2025-04-15T17:37:13.000Z",
        "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
        "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13941.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6338dd1776421c0543150467",
            "avatarUrl": "/avatars/4539dcec644e40be33f4a0d419fa66cb.svg",
            "fullname": "Syeda Nahida Akter",
            "name": "SieraL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15047",
            "authors": [
                {
                    "_id": "6806fb8e27dabde0a109776d",
                    "user": {
                        "_id": "645b663eca5d8a297712f2e1",
                        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
                        "isPro": false,
                        "fullname": "Quy-Anh Dang",
                        "user": "quyanh",
                        "type": "user"
                    },
                    "name": "Quy-Anh Dang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:09.578Z",
                    "hidden": false
                },
                {
                    "_id": "6806fb8e27dabde0a109776e",
                    "user": {
                        "_id": "63105f7463b70252b4775783",
                        "avatarUrl": "/avatars/89cfe71302cde6c2834f58d44bbcdbd5.svg",
                        "isPro": false,
                        "fullname": "Chris Ngo",
                        "user": "tnngo2",
                        "type": "user"
                    },
                    "name": "Chris Ngo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-22T09:51:07.511Z",
                    "hidden": false
                },
                {
                    "_id": "6806fb8e27dabde0a109776f",
                    "name": "Truong-Son Hy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
                "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
            ],
            "publishedAt": "2025-04-21T12:04:57.000Z",
            "submittedOnDailyAt": "2025-04-22T00:47:26.080Z",
            "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
            "submittedOnDailyBy": {
                "_id": "645b663eca5d8a297712f2e1",
                "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
                "isPro": false,
                "fullname": "Quy-Anh Dang",
                "user": "quyanh",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score approx 0.84), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
            "upvotes": 4,
            "discussionId": "6806fb9127dabde0a1097849",
            "githubRepo": "https://github.com/knoveleng/rainbowplus",
            "ai_keywords": [
                "adversarial prompts",
                "vulnerabilities",
                "unsafe outputs",
                "biased outputs",
                "red-teaming methods",
                "scalability challenges",
                "resource-intensive requirements",
                "quality-diversity (QD) search",
                "evolutionary computation",
                "MAP-Elites",
                "adaptive quality-diversity (QD) search",
                "multi-element archive",
                "fitness function",
                "single-prompt archives",
                "pairwise comparisons",
                "attack success rate (ASR)",
                "Diverse-Score",
                "superior attack success rate (ASR)",
                "diversity",
                "unique prompts",
                "benchmark datasets",
                "open-source LLMS",
                "HarmBench",
                "closed-source",
                "AutoDAN-Turbo",
                "vulnerability assessment"
            ]
        },
        "publishedAt": "2025-04-21T08:04:57.000Z",
        "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
        "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score approx 0.84), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/XbUmMPAitDuX-g7n50bTa.png",
            "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/a16jiYkgx_UtIyM9BiBc2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15047.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "fullname": "Quy-Anh Dang",
            "name": "quyanh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.14717",
            "authors": [
                {
                    "_id": "680720e066b60d551b653f1b",
                    "name": "Bowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "680720e066b60d551b653f1c",
                    "user": {
                        "_id": "6258a6455ea3a0a9b6de3f22",
                        "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
                        "isPro": false,
                        "fullname": "Lei Ke",
                        "user": "lkeab",
                        "type": "user"
                    },
                    "name": "Lei Ke",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:05:00.815Z",
                    "hidden": false
                },
                {
                    "_id": "680720e066b60d551b653f1d",
                    "user": {
                        "_id": "6314ff478be84e1ab412155a",
                        "avatarUrl": "/avatars/5a0c0aedc99aac6731e0652faf98b2e6.svg",
                        "isPro": false,
                        "fullname": "Adam W Harley",
                        "user": "aharley",
                        "type": "user"
                    },
                    "name": "Adam W. Harley",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:04:46.261Z",
                    "hidden": false
                },
                {
                    "_id": "680720e066b60d551b653f1e",
                    "name": "Katerina Fragkiadaki",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
            ],
            "publishedAt": "2025-04-20T19:09:43.000Z",
            "submittedOnDailyAt": "2025-04-22T03:25:44.916Z",
            "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
            "submittedOnDailyBy": {
                "_id": "6258a6455ea3a0a9b6de3f22",
                "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
                "isPro": false,
                "fullname": "Lei Ke",
                "user": "lkeab",
                "type": "user"
            },
            "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
            "upvotes": 4,
            "discussionId": "680720e166b60d551b653f7d",
            "projectPage": "https://tapip3d.github.io/",
            "githubRepo": "https://github.com/zbw001/TAPIP3D",
            "ai_keywords": [
                "camera-stabilized",
                "spatio-temporal feature clouds",
                "depth and camera motion information",
                "2D video features",
                "3D world space",
                "multi-frame 3D motion estimates",
                "Local Pair Attention mechanism",
                "3D contextualization strategy",
                "spatial relationships in 3D",
                "feature neighborhoods",
                "3D trajectory estimation",
                "3D point tracking",
                "2D tracker",
                "inference in both camera coordinates",
                "world coordinates",
                "3D point tracking benchmarks",
                "2D square correlation neighborhoods"
            ]
        },
        "publishedAt": "2025-04-20T15:09:43.000Z",
        "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
        "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/D3KLUoyXGD0mILRMznzGG.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6258a6455ea3a0a9b6de3f22/qsRjn3Gr351OY_A8w0tGc.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14717.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6258a6455ea3a0a9b6de3f22",
            "avatarUrl": "/avatars/6eeed72a97fb24465e5e65583fbe50cf.svg",
            "fullname": "Lei Ke",
            "name": "lkeab",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13099",
            "authors": [
                {
                    "_id": "6807aa5868e39b28126fa252",
                    "name": "Ranjan Sapkota",
                    "hidden": false
                },
                {
                    "_id": "6807aa5868e39b28126fa253",
                    "name": "Rahul Harsha Cheppally",
                    "hidden": false
                },
                {
                    "_id": "6807aa5868e39b28126fa254",
                    "name": "Ajay Sharda",
                    "hidden": false
                },
                {
                    "_id": "6807aa5868e39b28126fa255",
                    "name": "Manoj Karkee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/A_POI7fe9fs2MrRl0AW_-.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/DUhk0VbWeEJHQ3YhcP52S.jpeg"
            ],
            "publishedAt": "2025-04-17T17:08:11.000Z",
            "submittedOnDailyAt": "2025-04-22T13:15:44.991Z",
            "title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and\n  CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection\n  in Complex Orchard Environments Under Label Ambiguity",
            "submittedOnDailyBy": {
                "_id": "67ddd80896ac367438d400a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                "isPro": false,
                "fullname": "Ranjan Sapkota",
                "user": "RanjanSapkota",
                "type": "user"
            },
            "summary": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs",
            "upvotes": 3,
            "discussionId": "6807aa5968e39b28126fa2a2",
            "ai_keywords": [
                "DINOv2",
                "deformable attention",
                "CNN-based attention",
                "mean Average Precision (mAP50)",
                "mAP@50:95",
                "training dynamics",
                "transformer-based architectures"
            ]
        },
        "publishedAt": "2025-04-17T13:08:11.000Z",
        "title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and\n  CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection\n  in Complex Orchard Environments Under Label Ambiguity",
        "summary": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/A_POI7fe9fs2MrRl0AW_-.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/DUhk0VbWeEJHQ3YhcP52S.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13099.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ddd80896ac367438d400a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
            "fullname": "Ranjan Sapkota",
            "name": "RanjanSapkota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.14738",
            "authors": [
                {
                    "_id": "68074e6b6a30fb43d9503b11",
                    "user": {
                        "_id": "65d28835f8ab7fc6697d854b",
                        "avatarUrl": "/avatars/c0077cbf930c5eee11cf4c2eace6f502.svg",
                        "isPro": false,
                        "fullname": "Reya Vir",
                        "user": "reyavir",
                        "type": "user"
                    },
                    "name": "Reya Vir",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-22T16:05:27.348Z",
                    "hidden": false
                },
                {
                    "_id": "68074e6b6a30fb43d9503b12",
                    "user": {
                        "_id": "63f53f733aa49d8cb97b15e8",
                        "avatarUrl": "/avatars/2b23950c845a1ad3b4b82b437ea52562.svg",
                        "isPro": false,
                        "fullname": "Shreya Shankar",
                        "user": "shreyashankar",
                        "type": "user"
                    },
                    "name": "Shreya Shankar",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-22T08:08:12.241Z",
                    "hidden": false
                },
                {
                    "_id": "68074e6b6a30fb43d9503b13",
                    "name": "Harrison Chase",
                    "hidden": false
                },
                {
                    "_id": "68074e6b6a30fb43d9503b14",
                    "name": "Will Fu-Hinthorn",
                    "hidden": false
                },
                {
                    "_id": "68074e6b6a30fb43d9503b15",
                    "name": "Aditya Parameswaran",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-20T21:04:23.000Z",
            "submittedOnDailyAt": "2025-04-22T18:24:58.263Z",
            "title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom\n  Production Large Language Model Pipelines",
            "submittedOnDailyBy": {
                "_id": "65d28835f8ab7fc6697d854b",
                "avatarUrl": "/avatars/c0077cbf930c5eee11cf4c2eace6f502.svg",
                "isPro": false,
                "fullname": "Reya Vir",
                "user": "reyavir",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering.",
            "upvotes": 2,
            "discussionId": "68074e6c6a30fb43d9503b46",
            "ai_keywords": [
                "PROMPTEVALS",
                "LLM pipeline prompts",
                "assertion criteria",
                "fine-tuned Mistral",
                "fine-tuned Llama 3",
                "GPT-4o",
                "relevant assertions",
                "LLM reliability",
                "LLM alignment",
                "prompt engineering"
            ]
        },
        "publishedAt": "2025-04-20T17:04:23.000Z",
        "title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom\n  Production Large Language Model Pipelines",
        "summary": "Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14738.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d28835f8ab7fc6697d854b",
            "avatarUrl": "/avatars/c0077cbf930c5eee11cf4c2eace6f502.svg",
            "fullname": "Reya Vir",
            "name": "reyavir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12186",
            "authors": [
                {
                    "_id": "680764930b2f7fda706f6c03",
                    "name": "Alejandro Newell",
                    "hidden": false
                },
                {
                    "_id": "680764930b2f7fda706f6c04",
                    "user": {
                        "_id": "66c0bff5453a7ef6c5b94cad",
                        "avatarUrl": "/avatars/8d56e44718f7178e4578f6c17a697678.svg",
                        "isPro": false,
                        "fullname": "Peiyun Hu",
                        "user": "peiyun-hu-apple",
                        "type": "user"
                    },
                    "name": "Peiyun Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:07:36.922Z",
                    "hidden": false
                },
                {
                    "_id": "680764930b2f7fda706f6c05",
                    "user": {
                        "_id": "65de060e8ad379149e3dbac6",
                        "avatarUrl": "/avatars/ff48c3407547dd5fc51dcb1b961a5e4a.svg",
                        "isPro": true,
                        "fullname": "Lahav Lipson",
                        "user": "lahavl",
                        "type": "user"
                    },
                    "name": "Lahav Lipson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:07:46.486Z",
                    "hidden": false
                },
                {
                    "_id": "680764930b2f7fda706f6c06",
                    "name": "Stephan R. Richter",
                    "hidden": false
                },
                {
                    "_id": "680764930b2f7fda706f6c07",
                    "user": {
                        "_id": "6707b1a9356f027ca986d6b1",
                        "avatarUrl": "/avatars/d32eae81b27dc4f7090196d410e96876.svg",
                        "isPro": false,
                        "fullname": "Vladlen Koltun",
                        "user": "vkoltun",
                        "type": "user"
                    },
                    "name": "Vladlen Koltun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-22T11:08:01.500Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T15:40:15.000Z",
            "submittedOnDailyAt": "2025-04-22T08:13:12.160Z",
            "title": "CoMotion: Concurrent Multi-person 3D Motion",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion",
            "upvotes": 2,
            "discussionId": "680764950b2f7fda706f6ca7",
            "ai_keywords": [
                "3D poses",
                "monocular camera",
                "temporally coherent predictions",
                "occlusions",
                "pose update",
                "online tracking",
                "pseudo-labeled annotations",
                "state-of-the-art systems",
                "3D pose estimation accuracy"
            ]
        },
        "publishedAt": "2025-04-16T11:40:15.000Z",
        "title": "CoMotion: Concurrent Multi-person 3D Motion",
        "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12186.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 829
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.15266",
            "authors": [
                {
                    "_id": "680826892b4aaa243886d2eb",
                    "name": "Vaishnavh Nagarajan",
                    "hidden": false
                },
                {
                    "_id": "680826892b4aaa243886d2ec",
                    "name": "Chen Henry Wu",
                    "hidden": false
                },
                {
                    "_id": "680826892b4aaa243886d2ed",
                    "name": "Charles Ding",
                    "hidden": false
                },
                {
                    "_id": "680826892b4aaa243886d2ee",
                    "name": "Aditi Raghunathan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:47:46.000Z",
            "submittedOnDailyAt": "2025-04-22T22:03:45.049Z",
            "title": "Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction",
            "submittedOnDailyBy": {
                "_id": "636577efe7a78348d820a4cc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667594347446-636577efe7a78348d820a4cc.jpeg",
                "isPro": true,
                "fullname": "Chen Wu",
                "user": "ChenWu98",
                "type": "user"
            },
            "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity",
            "upvotes": 1,
            "discussionId": "6808268b2b4aaa243886d34d",
            "ai_keywords": [
                "next-token learning",
                "multi-token approaches",
                "teacherless training",
                "diffusion models",
                "open-ended real-world tasks",
                "abstract knowledge graph",
                "stochastic planning",
                "hash-conditioning",
                "softmax-based sampling"
            ]
        },
        "publishedAt": "2025-04-21T13:47:46.000Z",
        "title": "Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction",
        "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15266.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636577efe7a78348d820a4cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667594347446-636577efe7a78348d820a4cc.jpeg",
            "fullname": "Chen Wu",
            "name": "ChenWu98",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.14032",
            "authors": [
                {
                    "_id": "6807f86ad9efafd30d1ae9ff",
                    "name": "Haiwen Huang",
                    "hidden": false
                },
                {
                    "_id": "6807f86ad9efafd30d1aea00",
                    "name": "Anpei Chen",
                    "hidden": false
                },
                {
                    "_id": "6807f86ad9efafd30d1aea01",
                    "name": "Volodymyr Havrylov",
                    "hidden": false
                },
                {
                    "_id": "6807f86ad9efafd30d1aea02",
                    "name": "Andreas Geiger",
                    "hidden": false
                },
                {
                    "_id": "6807f86ad9efafd30d1aea03",
                    "name": "Dan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-18T18:46:08.000Z",
            "submittedOnDailyAt": "2025-04-22T18:43:45.930Z",
            "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision\n  Foundation Models",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved\nimpressive results on various downstream tasks, but their limited feature\nresolution hampers performance in applications requiring pixel-level\nunderstanding. Feature upsampling offers a promising direction to address this\nchallenge. In this work, we identify two critical factors for enhancing feature\nupsampling: the upsampler architecture and the training objective. For the\nupsampler architecture, we introduce a coordinate-based cross-attention\ntransformer that integrates the high-resolution images with coordinates and\nlow-resolution VFM features to generate sharp, high-quality features. For the\ntraining objective, we propose constructing high-resolution pseudo-groundtruth\nfeatures by leveraging class-agnostic masks and self-distillation. Our approach\neffectively captures fine-grained details and adapts flexibly to various input\nand feature resolutions. Through experiments, we demonstrate that our approach\nsignificantly outperforms existing feature upsampling techniques across various\ndownstream tasks. Our code is released at https://github.com/andrehuang/loftup.",
            "upvotes": 1,
            "discussionId": "6807f86fd9efafd30d1aeb12",
            "ai_keywords": [
                "coordinate-based cross-attention transformer",
                "high-resolution images",
                "low-resolution VFM features",
                "high-resolution pseudo-groundtruth features",
                "class-agnostic masks",
                "self-distillation",
                "fine-grained details"
            ]
        },
        "publishedAt": "2025-04-18T14:46:08.000Z",
        "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision\n  Foundation Models",
        "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved\nimpressive results on various downstream tasks, but their limited feature\nresolution hampers performance in applications requiring pixel-level\nunderstanding. Feature upsampling offers a promising direction to address this\nchallenge. In this work, we identify two critical factors for enhancing feature\nupsampling: the upsampler architecture and the training objective. For the\nupsampler architecture, we introduce a coordinate-based cross-attention\ntransformer that integrates the high-resolution images with coordinates and\nlow-resolution VFM features to generate sharp, high-quality features. For the\ntraining objective, we propose constructing high-resolution pseudo-groundtruth\nfeatures by leveraging class-agnostic masks and self-distillation. Our approach\neffectively captures fine-grained details and adapts flexibly to various input\nand feature resolutions. Through experiments, we demonstrate that our approach\nsignificantly outperforms existing feature upsampling techniques across various\ndownstream tasks. Our code is released at https://github.com/andrehuang/loftup.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14032.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 829
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.10642",
            "authors": [
                {
                    "_id": "6807ac6258cd41b34071c428",
                    "name": "Tan-Hanh Pham",
                    "hidden": false
                },
                {
                    "_id": "6807ac6258cd41b34071c429",
                    "name": "Chris Ngo",
                    "hidden": false
                },
                {
                    "_id": "6807ac6258cd41b34071c42a",
                    "name": "Trong-Duong Bui",
                    "hidden": false
                },
                {
                    "_id": "6807ac6258cd41b34071c42b",
                    "name": "Minh Luu Quang",
                    "hidden": false
                },
                {
                    "_id": "6807ac6258cd41b34071c42c",
                    "name": "Tan-Huong Pham",
                    "hidden": false
                },
                {
                    "_id": "6807ac6258cd41b34071c42d",
                    "name": "Truong-Son Hy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T18:51:37.000Z",
            "submittedOnDailyAt": "2025-04-22T13:19:33.177Z",
            "title": "SilVar-Med: A Speech-Driven Visual Language Model for Explainable\n  Abnormality Detection in Medical Imaging",
            "submittedOnDailyBy": {
                "_id": "63105f7463b70252b4775783",
                "avatarUrl": "/avatars/89cfe71302cde6c2834f58d44bbcdbd5.svg",
                "isPro": false,
                "fullname": "Chris Ngo",
                "user": "tnngo2",
                "type": "user"
            },
            "summary": "Medical Visual Language Models have shown great potential in various\nhealthcare applications, including medical image captioning and diagnostic\nassistance. However, most existing models rely on text-based instructions,\nlimiting their usability in real-world clinical environments especially in\nscenarios such as surgery, text-based interaction is often impractical for\nphysicians. In addition, current medical image analysis models typically lack\ncomprehensive reasoning behind their predictions, which reduces their\nreliability for clinical decision-making. Given that medical diagnosis errors\ncan have life-changing consequences, there is a critical need for interpretable\nand rational medical assistance. To address these challenges, we introduce an\nend-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image\nassistant that integrates speech interaction with VLMs, pioneering the task of\nvoice-based communication for medical image analysis. In addition, we focus on\nthe interpretation of the reasoning behind each prediction of medical\nabnormalities with a proposed reasoning dataset. Through extensive experiments,\nwe demonstrate a proof-of-concept study for reasoning-driven medical image\ninterpretation with end-to-end speech interaction. We believe this work will\nadvance the field of medical AI by fostering more transparent, interactive, and\nclinically viable diagnostic support systems. Our code and dataset are publicly\navailable at SiVar-Med.",
            "upvotes": 1,
            "discussionId": "6807ac6358cd41b34071c485",
            "ai_keywords": [
                "speech-driven",
                "multimodal",
                "VLM",
                "voice-based communication",
                "medical image analysis",
                "reasoning-driven",
                "medical image interpretation"
            ]
        },
        "publishedAt": "2025-04-14T14:51:37.000Z",
        "title": "SilVar-Med: A Speech-Driven Visual Language Model for Explainable\n  Abnormality Detection in Medical Imaging",
        "summary": "Medical Visual Language Models have shown great potential in various\nhealthcare applications, including medical image captioning and diagnostic\nassistance. However, most existing models rely on text-based instructions,\nlimiting their usability in real-world clinical environments especially in\nscenarios such as surgery, text-based interaction is often impractical for\nphysicians. In addition, current medical image analysis models typically lack\ncomprehensive reasoning behind their predictions, which reduces their\nreliability for clinical decision-making. Given that medical diagnosis errors\ncan have life-changing consequences, there is a critical need for interpretable\nand rational medical assistance. To address these challenges, we introduce an\nend-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image\nassistant that integrates speech interaction with VLMs, pioneering the task of\nvoice-based communication for medical image analysis. In addition, we focus on\nthe interpretation of the reasoning behind each prediction of medical\nabnormalities with a proposed reasoning dataset. Through extensive experiments,\nwe demonstrate a proof-of-concept study for reasoning-driven medical image\ninterpretation with end-to-end speech interaction. We believe this work will\nadvance the field of medical AI by fostering more transparent, interactive, and\nclinically viable diagnostic support systems. Our code and dataset are publicly\navailable at SiVar-Med.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10642.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63105f7463b70252b4775783",
            "avatarUrl": "/avatars/89cfe71302cde6c2834f58d44bbcdbd5.svg",
            "fullname": "Chris Ngo",
            "name": "tnngo2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]