[
    {
        "paper": {
            "id": "2504.15279",
            "authors": [
                {
                    "_id": "68070d3b5035e6d88636ae13",
                    "user": {
                        "_id": "649e5ee29420f68cf1c1470e",
                        "avatarUrl": "/avatars/7f6d1ec4fb3f85351e88044016d8ab42.svg",
                        "isPro": false,
                        "fullname": "Xu Wayen",
                        "user": "wilye",
                        "type": "user"
                    },
                    "name": "Weiye Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:11:19.332Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae14",
                    "user": {
                        "_id": "664b4a748dd1bfb5a3a970fe",
                        "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
                        "isPro": false,
                        "fullname": "Jiahao Wang",
                        "user": "GenuineWWD",
                        "type": "user"
                    },
                    "name": "Jiahao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:11:17.358Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae15",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:20:27.920Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae16",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae17",
                    "name": "Wengang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae18",
                    "name": "Aijun Yang",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae19",
                    "user": {
                        "_id": "65ead3ea908526a39082e641",
                        "avatarUrl": "/avatars/dcf870695fd56b06ca03d82f831e9019.svg",
                        "isPro": false,
                        "fullname": "Lewei Lu",
                        "user": "luotto",
                        "type": "user"
                    },
                    "name": "Lewei Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:20:51.831Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae1a",
                    "name": "Houqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae1b",
                    "name": "Xiaohua Wang",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae1c",
                    "user": {
                        "_id": "64ae2359179421d320b1694b",
                        "avatarUrl": "/avatars/c387a75191005bcaa473091de5383a10.svg",
                        "isPro": false,
                        "fullname": "Xizhou Zhu",
                        "user": "Einsiedler",
                        "type": "user"
                    },
                    "name": "Xizhou Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:21:11.459Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae1d",
                    "user": {
                        "_id": "64d1c560c0c627dfa71bdbe0",
                        "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
                        "isPro": false,
                        "fullname": "wenhai.wang",
                        "user": "wangwhcore",
                        "type": "user"
                    },
                    "name": "Wenhai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:21:24.337Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae1e",
                    "user": {
                        "_id": "64686f7172d9180d4ac8b4e4",
                        "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
                        "isPro": false,
                        "fullname": "Jifeng Dai",
                        "user": "daijifeng",
                        "type": "user"
                    },
                    "name": "Jifeng Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:21:31.315Z",
                    "hidden": false
                },
                {
                    "_id": "68070d3b5035e6d88636ae1f",
                    "name": "Jinguo Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:59:53.000Z",
            "submittedOnDailyAt": "2025-04-24T01:22:54.990Z",
            "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "664b4a748dd1bfb5a3a970fe",
                "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
                "isPro": false,
                "fullname": "Jiahao Wang",
                "user": "GenuineWWD",
                "type": "user"
            },
            "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.",
            "upvotes": 56,
            "discussionId": "68070d3f5035e6d88636af56",
            "projectPage": "https://visulogic-benchmark.github.io/VisuLogic/",
            "githubRepo": "https://github.com/VisuLogic-Benchmark/VisuLogic-Eval",
            "ai_keywords": [
                "Visual reasoning",
                "multimodal large language models (MLLMs)",
                "text descriptions",
                "language-based reasoning shortcuts",
                "genuine vision-centric reasoning",
                "VisuLogic",
                "human-verified problems",
                "quantitative shifts",
                "spatial relations",
                "attribute comparisons",
                "supplementary training dataset",
                "reinforcement-learning baseline"
            ]
        },
        "publishedAt": "2025-04-21T13:59:53.000Z",
        "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
        "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664b4a748dd1bfb5a3a970fe",
            "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
            "fullname": "Jiahao Wang",
            "name": "GenuineWWD",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.14509",
            "authors": [
                {
                    "_id": "6809dd092e04f68a3f5baa66",
                    "user": {
                        "_id": "6339029a76421c0543167075",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
                        "isPro": false,
                        "fullname": "fulong ye",
                        "user": "Alon77777",
                        "type": "user"
                    },
                    "name": "Fulong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:21:52.931Z",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa67",
                    "user": {
                        "_id": "6309807c03837fbcf0856059",
                        "avatarUrl": "/avatars/fcafe6726e33094fed463a48f04f044f.svg",
                        "isPro": false,
                        "fullname": "miaohua",
                        "user": "miaohua",
                        "type": "user"
                    },
                    "name": "Miao Hua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:21:59.688Z",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa68",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa69",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:23:09.434Z",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa6a",
                    "user": {
                        "_id": "666a6e144a6c703a09d787d5",
                        "avatarUrl": "/avatars/ae5dfa87e352a3561cd0cc53d5e3a91a.svg",
                        "isPro": false,
                        "fullname": "Qichao Sun",
                        "user": "giruhc9gj",
                        "type": "user"
                    },
                    "name": "Qichao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:22:46.844Z",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa6b",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa6c",
                    "user": {
                        "_id": "645dcad7a19f3e64bbf35e6c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rV1uHDSnZv7jAvFq4ftj4.jpeg",
                        "isPro": false,
                        "fullname": "Qian He",
                        "user": "heqian",
                        "type": "user"
                    },
                    "name": "Qian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:22:32.939Z",
                    "hidden": false
                },
                {
                    "_id": "6809dd092e04f68a3f5baa6d",
                    "user": {
                        "_id": "67bc6b515d9470ec64bdcc33",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dttL8Fb3bKCVG5zjg_02q.png",
                        "isPro": false,
                        "fullname": "Xinglong Wu",
                        "user": "Xingzhe-xlwu",
                        "type": "user"
                    },
                    "name": "Xinglong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:22:21.048Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-20T06:53:00.000Z",
            "submittedOnDailyAt": "2025-04-24T05:26:05.811Z",
            "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
            "submittedOnDailyBy": {
                "_id": "6339029a76421c0543167075",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
                "isPro": false,
                "fullname": "fulong ye",
                "user": "Alon77777",
                "type": "user"
            },
            "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
            "upvotes": 37,
            "discussionId": "6809dd102e04f68a3f5babf5",
            "projectPage": "https://superhero-7.github.io/DreamID/",
            "githubRepo": "https://github.com/superhero-7/DreamID",
            "ai_keywords": [
                "diffusion-based model",
                "Triplet ID Group",
                "diffusion models",
                "image-space loss functions",
                "SD Turbo",
                "SwapNet",
                "FaceNet",
                "ID Adapter",
                "face swapping",
                "explicit supervision",
                "identity similarity",
                "attribute preservation",
                "image fidelity",
                "pose preservation",
                "expression preservation",
                "high-quality face swapping"
            ]
        },
        "publishedAt": "2025-04-20T02:53:00.000Z",
        "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
        "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14509.png",
        "numComments": 8,
        "submittedBy": {
            "_id": "6339029a76421c0543167075",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
            "fullname": "fulong ye",
            "name": "Alon77777",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15431",
            "authors": [
                {
                    "_id": "680879ead6dc8bf64565c975",
                    "user": {
                        "_id": "67aaee60a8192c1ba3d7d42b",
                        "avatarUrl": "/avatars/9544923076835a606f061cb71e84ef65.svg",
                        "isPro": false,
                        "fullname": "Sungjun Han",
                        "user": "sungjunhan-trl",
                        "type": "user"
                    },
                    "name": "Sungjun Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:23:37.021Z",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c976",
                    "user": {
                        "_id": "6138cc1306dd10833d2db64b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
                        "isPro": false,
                        "fullname": "Juyoung Suk",
                        "user": "scottsuk0306",
                        "type": "user"
                    },
                    "name": "Juyoung Suk",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-23T08:08:21.257Z",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c977",
                    "name": "Suyeong An",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c978",
                    "name": "Hyungguk Kim",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c979",
                    "user": {
                        "_id": "6729c4baae4312f3ab001a90",
                        "avatarUrl": "/avatars/5d454f1695a0a8574e730fd4d884243b.svg",
                        "isPro": false,
                        "fullname": "Kyuseok Kim",
                        "user": "kyudolski",
                        "type": "user"
                    },
                    "name": "Kyuseok Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:24:00.523Z",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c97a",
                    "name": "Wonsuk Yang",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c97b",
                    "user": {
                        "_id": "6257adfdb98dcaa7e0de7ab4",
                        "avatarUrl": "/avatars/ddfc2135104895d09cfce0cd6f10e5fb.svg",
                        "isPro": false,
                        "fullname": "Seungtaek Choi",
                        "user": "hist0613",
                        "type": "user"
                    },
                    "name": "Seungtaek Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:11:03.864Z",
                    "hidden": false
                },
                {
                    "_id": "680879ead6dc8bf64565c97c",
                    "user": {
                        "_id": "6188cf3293317afcd1c2df7f",
                        "avatarUrl": "/avatars/95621801d5b3f3c1a681f1ad6cc66c6a.svg",
                        "isPro": false,
                        "fullname": "Jay Shin",
                        "user": "jshin49",
                        "type": "user"
                    },
                    "name": "Jamin Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T12:41:14.275Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T20:54:44.000Z",
            "submittedOnDailyAt": "2025-04-24T01:09:32.264Z",
            "title": "Trillion 7B Technical Report",
            "submittedOnDailyBy": {
                "_id": "6138cc1306dd10833d2db64b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
                "isPro": false,
                "fullname": "Juyoung Suk",
                "user": "scottsuk0306",
                "type": "user"
            },
            "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.",
            "upvotes": 26,
            "discussionId": "680879ebd6dc8bf64565c9bb",
            "ai_keywords": [
                "Trillion-7B",
                "Cross-lingual Document Attention (XLDA)",
                "language-specific filtering",
                "tailored tokenizer construction",
                "multilingual data",
                "multilingual performance",
                "cross-lingual consistency"
            ]
        },
        "publishedAt": "2025-04-21T16:54:44.000Z",
        "title": "Trillion 7B Technical Report",
        "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15431.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6138cc1306dd10833d2db64b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
            "fullname": "Juyoung Suk",
            "name": "scottsuk0306",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15777",
            "authors": [
                {
                    "_id": "6808452d16c1c427ac727816",
                    "user": {
                        "_id": "67469d6a8407f929491dce06",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
                        "isPro": true,
                        "fullname": "Shangshang Wang",
                        "user": "upup-ashton-wang",
                        "type": "user"
                    },
                    "name": "Shangshang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:11:10.259Z",
                    "hidden": false
                },
                {
                    "_id": "6808452d16c1c427ac727817",
                    "name": "Julian Asilis",
                    "hidden": false
                },
                {
                    "_id": "6808452d16c1c427ac727818",
                    "name": "Ömer Faruk Akgül",
                    "hidden": false
                },
                {
                    "_id": "6808452d16c1c427ac727819",
                    "name": "Enes Burak Bilgin",
                    "hidden": false
                },
                {
                    "_id": "6808452d16c1c427ac72781a",
                    "name": "Ollie Liu",
                    "hidden": false
                },
                {
                    "_id": "6808452d16c1c427ac72781b",
                    "name": "Willie Neiswanger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T10:38:00.000Z",
            "submittedOnDailyAt": "2025-04-24T06:51:34.876Z",
            "title": "Tina: Tiny Reasoning Models via LoRA",
            "submittedOnDailyBy": {
                "_id": "67469d6a8407f929491dce06",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
                "isPro": true,
                "fullname": "Shangshang Wang",
                "user": "upup-ashton-wang",
                "type": "user"
            },
            "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.",
            "upvotes": 25,
            "discussionId": "6808452f16c1c427ac7278b1",
            "projectPage": "https://shangshangwang.notion.site/tina",
            "githubRepo": "https://github.com/shangshang-wang/Tina",
            "ai_keywords": [
                "parameter-efficient updates",
                "reinforcement learning (RL)",
                "low-rank adaptation (LoRA)",
                "traininng logs",
                "checkpoints"
            ]
        },
        "publishedAt": "2025-04-22T06:38:00.000Z",
        "title": "Tina: Tiny Reasoning Models via LoRA",
        "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15777.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "fullname": "Shangshang Wang",
            "name": "upup-ashton-wang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16929",
            "authors": [
                {
                    "_id": "6809ba7976a4f4f7268546a7",
                    "name": "Shaden Alshammari",
                    "hidden": false
                },
                {
                    "_id": "6809ba7976a4f4f7268546a8",
                    "name": "John Hershey",
                    "hidden": false
                },
                {
                    "_id": "6809ba7976a4f4f7268546a9",
                    "user": {
                        "_id": "63124ad25e2531edb9edca45",
                        "avatarUrl": "/avatars/9cc2e352ba3f79347e588a3e4a814987.svg",
                        "isPro": false,
                        "fullname": "Axel Feldmann",
                        "user": "axelf",
                        "type": "user"
                    },
                    "name": "Axel Feldmann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:27:03.416Z",
                    "hidden": false
                },
                {
                    "_id": "6809ba7976a4f4f7268546aa",
                    "user": {
                        "_id": "654e866a85a5e608059a9b4f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/O0T97EupMHH_iXcv1xYFr.png",
                        "isPro": false,
                        "fullname": "William Freeman",
                        "user": "mrpuppt",
                        "type": "user"
                    },
                    "name": "William T. Freeman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:27:14.536Z",
                    "hidden": false
                },
                {
                    "_id": "6809ba7976a4f4f7268546ab",
                    "user": {
                        "_id": "62dae3734398e21bf7f53443",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Mark Hamilton",
                        "user": "mhamilton723",
                        "type": "user"
                    },
                    "name": "Mark Hamilton",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:27:36.305Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
            ],
            "publishedAt": "2025-04-23T17:59:01.000Z",
            "submittedOnDailyAt": "2025-04-24T02:45:09.509Z",
            "title": "I-Con: A Unifying Framework for Representation Learning",
            "submittedOnDailyBy": {
                "_id": "62dae3734398e21bf7f53443",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
                "isPro": false,
                "fullname": "Mark Hamilton",
                "user": "mhamilton723",
                "type": "user"
            },
            "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.",
            "upvotes": 24,
            "discussionId": "6809ba7d76a4f4f72685478a",
            "ai_keywords": [
                "information-theoretic equation",
                "KL divergence",
                "conditional distributions",
                "supervisory representations",
                "learned representations",
                "information geometry",
                "clustering",
                "spectral methods",
                "dimensionality reduction",
                "contrastive learning",
                "supervised learning",
                "I-Con",
                "debiasing methods",
                "contrastive representation learners"
            ]
        },
        "publishedAt": "2025-04-23T13:59:01.000Z",
        "title": "I-Con: A Unifying Framework for Representation Learning",
        "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16929.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62dae3734398e21bf7f53443",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
            "fullname": "Mark Hamilton",
            "name": "mhamilton723",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16074",
            "authors": [
                {
                    "_id": "680a12f28ea8fd5e81332f3e",
                    "name": "Shi Qiu",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f3f",
                    "name": "Shaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f40",
                    "name": "Zhuo-Yang Song",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f41",
                    "name": "Yunbo Sun",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f42",
                    "name": "Zeyu Cai",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f43",
                    "name": "Jiashen Wei",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f44",
                    "name": "Tianyu Luo",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f45",
                    "name": "Yixuan Yin",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f46",
                    "name": "Haoxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f47",
                    "name": "Yi Hu",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f48",
                    "name": "Chenyang Wang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f49",
                    "name": "Chencheng Tang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f4a",
                    "name": "Haoling Chang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f4b",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f4c",
                    "name": "Ziheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f4d",
                    "name": "Tianyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f4e",
                    "name": "Jingtian Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f4f",
                    "name": "Zhangyi Liu",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f50",
                    "name": "Minghao Li",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f51",
                    "name": "Yuku Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f52",
                    "name": "Boxuan Jing",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f53",
                    "name": "Xianqi Yin",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f54",
                    "name": "Yutong Ren",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f55",
                    "name": "Zizhuo Fu",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f56",
                    "name": "Weike Wang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f57",
                    "name": "Xudong Tian",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f58",
                    "name": "Anqi Lv",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f59",
                    "name": "Laifu Man",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f5a",
                    "name": "Jianxiang Li",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f5b",
                    "name": "Feiyu Tao",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f5c",
                    "name": "Qihua Sun",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f5d",
                    "name": "Zhou Liang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f5e",
                    "name": "Yushu Mu",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f5f",
                    "name": "Zhongxuan Li",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f60",
                    "name": "Jing-Jun Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f61",
                    "name": "Shutao Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f62",
                    "name": "Xiaotian Li",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f63",
                    "name": "Xingqi Xia",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f64",
                    "name": "Jiawei Lin",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f65",
                    "name": "Zheyu Shen",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f66",
                    "name": "Jiahang Chen",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f67",
                    "name": "Qiuhao Xiong",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f68",
                    "name": "Binran Wang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f69",
                    "name": "Fengyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f6a",
                    "name": "Ziyang Ni",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f6b",
                    "name": "Bohan Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f6c",
                    "name": "Fan Cui",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f6d",
                    "name": "Changkun Shao",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f6e",
                    "name": "Qing-Hong Cao",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f6f",
                    "name": "Ming-xing Luo",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f70",
                    "name": "Muhan Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a12f28ea8fd5e81332f71",
                    "name": "Hua Xing Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T17:53:29.000Z",
            "submittedOnDailyAt": "2025-04-24T13:54:06.040Z",
            "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6644bb2c9bdbd85493074411",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6644bb2c9bdbd85493074411/iug8dkP1zjID-kXzNxDjD.jpeg",
                "isPro": false,
                "fullname": "SHI QIU",
                "user": "StarThomas1002",
                "type": "user"
            },
            "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.",
            "upvotes": 17,
            "discussionId": "680a12f58ea8fd5e81333086",
            "projectPage": "https://phybench-official.github.io/phybench-demo/",
            "githubRepo": "https://github.com/phybench-official",
            "ai_keywords": [
                "large language models (LLMs)",
                "physics problems",
                "real-world physical scenarios",
                "mechanics",
                "electromagnetism",
                "thermodynamics",
                "optics",
                "modern physics",
                "advanced physics",
                "Expression Edit Distance (EED) Score",
                "edit distance between mathematical expressions",
                "complex physical reasoning scenarios"
            ]
        },
        "publishedAt": "2025-04-22T13:53:29.000Z",
        "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models",
        "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16074.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6644bb2c9bdbd85493074411",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6644bb2c9bdbd85493074411/iug8dkP1zjID-kXzNxDjD.jpeg",
            "fullname": "SHI QIU",
            "name": "StarThomas1002",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.15843",
            "authors": [
                {
                    "_id": "6809948944114def75aaeb7d",
                    "name": "Junshu Pan",
                    "hidden": false
                },
                {
                    "_id": "6809948944114def75aaeb7e",
                    "user": {
                        "_id": "6468823272d9180d4ac90bdf",
                        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                        "isPro": false,
                        "fullname": "Wei Shen",
                        "user": "Swtheking",
                        "type": "user"
                    },
                    "name": "Wei Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:26:03.812Z",
                    "hidden": false
                },
                {
                    "_id": "6809948944114def75aaeb7f",
                    "name": "Shulin Huang",
                    "hidden": false
                },
                {
                    "_id": "6809948944114def75aaeb80",
                    "name": "Qiji Zhou",
                    "hidden": false
                },
                {
                    "_id": "6809948944114def75aaeb81",
                    "name": "Yue Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T12:39:30.000Z",
            "submittedOnDailyAt": "2025-04-24T00:02:36.679Z",
            "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
            "submittedOnDailyBy": {
                "_id": "6468823272d9180d4ac90bdf",
                "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
                "isPro": false,
                "fullname": "Wei Shen",
                "user": "Swtheking",
                "type": "user"
            },
            "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
            "upvotes": 16,
            "discussionId": "6809948a44114def75aaebab",
            "ai_keywords": [
                "reinforcement learning from human feedback (RLHF)",
                "large language models (LLMs)",
                "Direct Preference Optimization (DPO)",
                "human preferences",
                "reference model",
                "data weight adjuster",
                "Simple Preference Optimization (SimPO)",
                "catastrophic forgetting",
                "Pre-DPO",
                "guiding reference model",
                "AlpacaEval 2.0",
                "Arena-Hard v0.1"
            ]
        },
        "publishedAt": "2025-04-22T08:39:30.000Z",
        "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
        "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15843.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6468823272d9180d4ac90bdf",
            "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
            "fullname": "Wei Shen",
            "name": "Swtheking",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16801",
            "authors": [
                {
                    "_id": "6809bebd0f6dfd7bd5159b76",
                    "user": {
                        "_id": "6545f8922a2a483042ebc8b3",
                        "avatarUrl": "/avatars/ab00da8aa841694f3f11093a9148e4c5.svg",
                        "isPro": false,
                        "fullname": "xiaoxing2001",
                        "user": "xiaoxing2001",
                        "type": "user"
                    },
                    "name": "Xiaoxing Hu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-24T04:32:01.063Z",
                    "hidden": false
                },
                {
                    "_id": "6809bebd0f6dfd7bd5159b77",
                    "user": {
                        "_id": "63e202f352b7578dba448ab5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                        "isPro": false,
                        "fullname": "Yang",
                        "user": "Kaichengalex",
                        "type": "user"
                    },
                    "name": "Kaicheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:10:22.302Z",
                    "hidden": false
                },
                {
                    "_id": "6809bebd0f6dfd7bd5159b78",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "6809bebd0f6dfd7bd5159b79",
                    "user": {
                        "_id": "61384b860317b0a5c10877d3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631080954171-61384b860317b0a5c10877d3.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Xu",
                        "user": "haoranxu",
                        "type": "user"
                    },
                    "name": "Haoran Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:26:36.470Z",
                    "hidden": false
                },
                {
                    "_id": "6809bebd0f6dfd7bd5159b7a",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "6809bebd0f6dfd7bd5159b7b",
                    "name": "Yupei Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T15:20:53.000Z",
            "submittedOnDailyAt": "2025-04-24T03:04:11.389Z",
            "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "63e202f352b7578dba448ab5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                "isPro": false,
                "fullname": "Yang",
                "user": "Kaichengalex",
                "type": "user"
            },
            "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
            "upvotes": 14,
            "discussionId": "6809bec10f6dfd7bd5159c38",
            "projectPage": "https://xiaoxing2001.github.io/DeGLA.github.io/",
            "githubRepo": "https://github.com/xiaoxing2001/DeGLA",
            "ai_keywords": [
                "Decoupled Global-Local Alignment (DeGLA)",
                "self-distillation mechanism",
                "learnable image-text encoder",
                "frozen teacher model",
                "exponential moving average",
                "catastrophic forgetting",
                "in-context learning",
                "Large Language Models (LLMs)",
                "high-quality negative captions",
                "Image-Grounded Contrast (IGC) loss",
                "Text-Grounded Contrast (TGC) loss",
                "vision-language compositionally",
                "VALSE",
                "SugarCrepe",
                "ARO benchmarks",
                "zero-shot classification tasks"
            ]
        },
        "publishedAt": "2025-04-23T11:20:53.000Z",
        "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
        "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16801.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "fullname": "Yang",
            "name": "Kaichengalex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16915",
            "authors": [
                {
                    "_id": "6809b14111003e54bd204d99",
                    "name": "Chong Mou",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204d9a",
                    "user": {
                        "_id": "639709c2be8a14bb9eeea8f6",
                        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
                        "isPro": false,
                        "fullname": "Yanze Wu",
                        "user": "yanze",
                        "type": "user"
                    },
                    "name": "Yanze Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:10:24.617Z",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204d9b",
                    "name": "Wenxu Wu",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204d9c",
                    "user": {
                        "_id": "66b2e5f5523bf90aa7057467",
                        "avatarUrl": "/avatars/ccdb58c2e56cf861e9dcec50c85d7778.svg",
                        "isPro": false,
                        "fullname": "Guo",
                        "user": "Zinan123212",
                        "type": "user"
                    },
                    "name": "Zinan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:28:30.677Z",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204d9d",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204d9e",
                    "name": "Yufeng Cheng",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204d9f",
                    "name": "Yiming Luo",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da0",
                    "name": "Fei Ding",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da1",
                    "user": {
                        "_id": "64b7ef3af902508f0d8bf76d",
                        "avatarUrl": "/avatars/c3185cc11456a4575e19c62f1b50b41f.svg",
                        "isPro": false,
                        "fullname": "Shiwen Zhang",
                        "user": "shiwenzh",
                        "type": "user"
                    },
                    "name": "Shiwen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:32:47.326Z",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da2",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:32:57.170Z",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da3",
                    "user": {
                        "_id": "6805bdfb344d6d8a8fd5b07a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png",
                        "isPro": false,
                        "fullname": "Mengtian Li",
                        "user": "LemonSky1995",
                        "type": "user"
                    },
                    "name": "Mengtian Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:33:22.554Z",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da4",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da5",
                    "name": "Jian Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da6",
                    "name": "Qian He",
                    "hidden": false
                },
                {
                    "_id": "6809b14111003e54bd204da7",
                    "user": {
                        "_id": "67bc6b515d9470ec64bdcc33",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dttL8Fb3bKCVG5zjg_02q.png",
                        "isPro": false,
                        "fullname": "Xinglong Wu",
                        "user": "Xingzhe-xlwu",
                        "type": "user"
                    },
                    "name": "Xinglong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:29:20.661Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T17:41:44.000Z",
            "submittedOnDailyAt": "2025-04-24T02:18:39.286Z",
            "title": "DreamO: A Unified Framework for Image Customization",
            "submittedOnDailyBy": {
                "_id": "639709c2be8a14bb9eeea8f6",
                "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
                "isPro": false,
                "fullname": "Yanze Wu",
                "user": "yanze",
                "type": "user"
            },
            "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
            "upvotes": 13,
            "discussionId": "6809b14411003e54bd204e51",
            "projectPage": "https://mc-e.github.io/project/DreamO/",
            "githubRepo": "https://github.com/bytedance/DreamO",
            "ai_keywords": [
                "diffusion transformer (DiT)",
                "feature routing constraint",
                "placeholder strategy",
                "progressive training strategy",
                "baseline consistency",
                "customization capabilities",
                "quality alignment stage"
            ]
        },
        "publishedAt": "2025-04-23T13:41:44.000Z",
        "title": "DreamO: A Unified Framework for Image Customization",
        "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16915.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "fullname": "Yanze Wu",
            "name": "yanze",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 139
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16891",
            "authors": [
                {
                    "_id": "6809ba729786ec0ed3afe6af",
                    "name": "Ivan Moshkov",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b0",
                    "name": "Darragh Hanley",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b1",
                    "name": "Ivan Sorokin",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b2",
                    "name": "Shubham Toshniwal",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b3",
                    "name": "Christof Henkel",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b4",
                    "name": "Benedikt Schifferer",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b5",
                    "name": "Wei Du",
                    "hidden": false
                },
                {
                    "_id": "6809ba729786ec0ed3afe6b6",
                    "name": "Igor Gitman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T17:13:04.000Z",
            "submittedOnDailyAt": "2025-04-24T14:23:13.442Z",
            "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning dataset",
            "submittedOnDailyBy": {
                "_id": "6356f834e75acf88c902b538",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666644004634-noauth.jpeg",
                "isPro": false,
                "fullname": "Igor Gitman",
                "user": "igitman",
                "type": "user"
            },
            "summary": "This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license.",
            "upvotes": 9,
            "discussionId": "6809ba739786ec0ed3afe6f5",
            "projectPage": "https://nvidia.github.io/NeMo-Skills/openmathreasoning1/",
            "githubRepo": "https://github.com/NVIDIA/NeMo-Skills"
        },
        "publishedAt": "2025-04-23T13:13:04.000Z",
        "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning dataset",
        "summary": "This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6356f834e75acf88c902b538",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666644004634-noauth.jpeg",
            "fullname": "Igor Gitman",
            "name": "igitman",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.15585",
            "authors": [
                {
                    "_id": "6809c1f389b7cade55b32a6c",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a6d",
                    "name": "Guibin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a6e",
                    "name": "Zhenhong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a6f",
                    "name": "Jiahao Wu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a70",
                    "name": "Miao Yu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a71",
                    "name": "Shiqian Zhao",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a72",
                    "name": "Chenlong Yin",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a73",
                    "user": {
                        "_id": "66dedaae6c821863fc78637f",
                        "avatarUrl": "/avatars/d1ef954f7669c16465243d426e414c4d.svg",
                        "isPro": false,
                        "fullname": "Jinhu Fu",
                        "user": "Fred456",
                        "type": "user"
                    },
                    "name": "Jinhu Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:59:47.159Z",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a74",
                    "name": "Yibo Yan",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a75",
                    "user": {
                        "_id": "6635d16a46904d5cfdda21cb",
                        "avatarUrl": "/avatars/6c6dcad65186b2eaedfabb9a484a9641.svg",
                        "isPro": false,
                        "fullname": "Hanjun Luo",
                        "user": "Atarogic",
                        "type": "user"
                    },
                    "name": "Hanjun Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:59:57.410Z",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a76",
                    "name": "Liang Lin",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a77",
                    "name": "Zhihao Xu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a78",
                    "name": "Haolang Lu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a79",
                    "name": "Xinye Cao",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a7a",
                    "name": "Xinyun Zhou",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a7b",
                    "name": "Weifei Jin",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a7c",
                    "name": "Fanci Meng",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a7d",
                    "name": "Junyuan Mao",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a7e",
                    "name": "Hao Wu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a7f",
                    "user": {
                        "_id": "6482c303954578a9d1e66f91",
                        "avatarUrl": "/avatars/7bccbd8b82f154a350de5011659ac341.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "minghewang",
                        "type": "user"
                    },
                    "name": "Minghe Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T12:41:08.753Z",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a80",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a81",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a82",
                    "name": "Chengwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a83",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a84",
                    "name": "Qiankun Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a85",
                    "name": "Chongye Guo",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a86",
                    "name": "Yalan Qin",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a87",
                    "name": "Yi Ding",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a88",
                    "name": "Donghai Hong",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a89",
                    "name": "Jiaming Ji",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a8a",
                    "name": "Xinfeng Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a8b",
                    "name": "Yifan Jiang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a8c",
                    "name": "Dongxia Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a8d",
                    "name": "Yihao Huang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a8e",
                    "name": "Yufei Guo",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a8f",
                    "name": "Jen-tse Huang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a90",
                    "name": "Yanwei Yue",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a91",
                    "name": "Wenke Huang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a92",
                    "name": "Guancheng Wan",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a93",
                    "name": "Tianlin Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a94",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a95",
                    "name": "Jie Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a96",
                    "name": "Qing Guo",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a97",
                    "name": "Jingyi Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a98",
                    "name": "Tianlong Chen",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a99",
                    "name": "Joey Tianyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a9a",
                    "name": "Xiaojun Jia",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a9b",
                    "name": "Weisong Sun",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a9c",
                    "name": "Cong Wu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a9d",
                    "name": "Jing Chen",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a9e",
                    "name": "Xuming Hu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32a9f",
                    "name": "Yiming Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa0",
                    "name": "Xiao Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa1",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:10:18.968Z",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa2",
                    "name": "Luu Anh Tuan",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa3",
                    "name": "Guowen Xu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa4",
                    "name": "Tianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa5",
                    "name": "Xingjun Ma",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa6",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa7",
                    "name": "Bo An",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa8",
                    "name": "Jun Sun",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aa9",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aaa",
                    "name": "Shirui Pan",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aab",
                    "name": "Yuval Elovici",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aac",
                    "name": "Bhavya Kailkhura",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aad",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aae",
                    "name": "Yaodong Yang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aaf",
                    "name": "Hongwei Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab0",
                    "name": "Wenyuan Xu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab1",
                    "name": "Yizhou Sun",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab2",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab3",
                    "name": "Qing Li",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab4",
                    "name": "Ke Tang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab5",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab6",
                    "name": "Felix Juefei-Xu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab7",
                    "name": "Hui Xiong",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab8",
                    "name": "Xiaofeng Wang",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32ab9",
                    "name": "Shuicheng Yan",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32aba",
                    "name": "Dacheng Tao",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32abb",
                    "name": "Philip S. Yu",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32abc",
                    "name": "Qingsong Wen",
                    "hidden": false
                },
                {
                    "_id": "6809c1f389b7cade55b32abd",
                    "name": "Yang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T05:02:49.000Z",
            "submittedOnDailyAt": "2025-04-24T03:15:54.692Z",
            "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
            "upvotes": 9,
            "discussionId": "6809c1f789b7cade55b32bf4"
        },
        "publishedAt": "2025-04-22T01:02:49.000Z",
        "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
        "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15585.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.11919",
            "authors": [
                {
                    "_id": "6809ae47c6faa064f324307d",
                    "user": {
                        "_id": "652f979c61ce8120849bb72f",
                        "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
                        "isPro": false,
                        "fullname": "Qianjin Yu",
                        "user": "USTCYu",
                        "type": "user"
                    },
                    "name": "Qianjin Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:10:26.728Z",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f324307e",
                    "name": "Keyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f324307f",
                    "name": "Zihan Chen",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243080",
                    "user": {
                        "_id": "66c31c91f1685a3adefef31d",
                        "avatarUrl": "/avatars/efbd2e6f813408b4092621efe8254e35.svg",
                        "isPro": false,
                        "fullname": "zhangchushu",
                        "user": "zcs1234",
                        "type": "user"
                    },
                    "name": "Chushu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:58:35.159Z",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243081",
                    "user": {
                        "_id": "672ad3ee751a2b4a6f7e064d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/672ad3ee751a2b4a6f7e064d/xSKXBmXIXCSBmMJ1dhols.jpeg",
                        "isPro": false,
                        "fullname": "MeiManlin",
                        "user": "MeiManlin",
                        "type": "user"
                    },
                    "name": "Manlin Mei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:58:13.997Z",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243082",
                    "name": "Lingjun Huang",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243083",
                    "name": "Fang Tan",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243084",
                    "name": "Yongsheng Du",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243085",
                    "user": {
                        "_id": "64be320fcf4f379eebb8903c",
                        "avatarUrl": "/avatars/3e423729c7081235a51de5f0144eef4b.svg",
                        "isPro": false,
                        "fullname": "Liu",
                        "user": "Kunlinliu2",
                        "type": "user"
                    },
                    "name": "Kunlin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:57:32.644Z",
                    "hidden": false
                },
                {
                    "_id": "6809ae47c6faa064f3243086",
                    "name": "Yurui Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T09:55:34.000Z",
            "submittedOnDailyAt": "2025-04-24T08:12:14.834Z",
            "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
            "submittedOnDailyBy": {
                "_id": "652f979c61ce8120849bb72f",
                "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
                "isPro": false,
                "fullname": "Qianjin Yu",
                "user": "USTCYu",
                "type": "user"
            },
            "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
            "upvotes": 9,
            "discussionId": "6809ae49c6faa064f32430d1",
            "ai_keywords": [
                "chain-of-thought (CoT) data",
                "LLM-Adaptive questiondifficulty levels",
                "LLM-Adaptive question database",
                "model supervised fine-tuning (SFT)"
            ]
        },
        "publishedAt": "2025-04-16T05:55:34.000Z",
        "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
        "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11919.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "652f979c61ce8120849bb72f",
            "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
            "fullname": "Qianjin Yu",
            "name": "USTCYu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15707",
            "authors": [
                {
                    "_id": "6809f6b03e1d48a9bb7f5719",
                    "user": {
                        "_id": "650971dbce83a0c12a851000",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
                        "isPro": false,
                        "fullname": "Yannic Neuhaus",
                        "user": "YanNeu",
                        "type": "user"
                    },
                    "name": "Yannic Neuhaus",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:09:36.629Z",
                    "hidden": false
                },
                {
                    "_id": "6809f6b03e1d48a9bb7f571a",
                    "name": "Matthias Hein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T08:47:59.000Z",
            "submittedOnDailyAt": "2025-04-24T07:02:13.584Z",
            "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
            "submittedOnDailyBy": {
                "_id": "650971dbce83a0c12a851000",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
                "isPro": false,
                "fullname": "Yannic Neuhaus",
                "user": "YanNeu",
                "type": "user"
            },
            "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .",
            "upvotes": 8,
            "discussionId": "6809f6b13e1d48a9bb7f5790",
            "githubRepo": "https://github.com/YanNeu/RePOPE",
            "ai_keywords": [
                "MSCOCO",
                "object hallucination",
                "POPE",
                "RePOPE"
            ]
        },
        "publishedAt": "2025-04-22T04:47:59.000Z",
        "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
        "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15707.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650971dbce83a0c12a851000",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
            "fullname": "Yannic Neuhaus",
            "name": "YanNeu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.10419",
            "authors": [
                {
                    "_id": "68026f762e2023f6cf7f0daa",
                    "user": {
                        "_id": "680268a7fd1fae58d58a2b49",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
                        "isPro": false,
                        "fullname": "Michał Turski",
                        "user": "mturski",
                        "type": "user"
                    },
                    "name": "Michał Turski",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-18T15:27:52.927Z",
                    "hidden": false
                },
                {
                    "_id": "68026f762e2023f6cf7f0dab",
                    "user": {
                        "_id": "6728d349a30b64dfcb517270",
                        "avatarUrl": "/avatars/78083b511eedb76ee85f9c92faecdc78.svg",
                        "isPro": false,
                        "fullname": "Mateusz Chilinski",
                        "user": "sf-mchilinski",
                        "type": "user"
                    },
                    "name": "Mateusz Chiliński",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:53:46.684Z",
                    "hidden": false
                },
                {
                    "_id": "68026f762e2023f6cf7f0dac",
                    "user": {
                        "_id": "600b381d3cc3b87db94bc0ce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
                        "isPro": false,
                        "fullname": "Łukasz Borchmann",
                        "user": "Borchmann",
                        "type": "user"
                    },
                    "name": "Łukasz Borchmann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:53:52.942Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-14T17:06:59.000Z",
            "submittedOnDailyAt": "2025-04-24T03:55:04.111Z",
            "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
            "submittedOnDailyBy": {
                "_id": "680268a7fd1fae58d58a2b49",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
                "isPro": false,
                "fullname": "Michał Turski",
                "user": "mturski",
                "type": "user"
            },
            "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
            "upvotes": 4,
            "discussionId": "68026f782e2023f6cf7f0e05"
        },
        "publishedAt": "2025-04-14T13:06:59.000Z",
        "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
        "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10419.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "680268a7fd1fae58d58a2b49",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
            "fullname": "Michał Turski",
            "name": "mturski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15254",
            "authors": [
                {
                    "_id": "6807bf3e70a0cec724b8a011",
                    "user": {
                        "_id": "6697abd4be7ce6de07140e72",
                        "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
                        "isPro": false,
                        "fullname": "Anirudh Khatry",
                        "user": "anirudhkhatry",
                        "type": "user"
                    },
                    "name": "Anirudh Khatry",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-23T08:30:04.897Z",
                    "hidden": false
                },
                {
                    "_id": "6807bf3e70a0cec724b8a012",
                    "name": "Robert Zhang",
                    "hidden": false
                },
                {
                    "_id": "6807bf3e70a0cec724b8a013",
                    "name": "Jia Pan",
                    "hidden": false
                },
                {
                    "_id": "6807bf3e70a0cec724b8a014",
                    "name": "Ziteng Wang",
                    "hidden": false
                },
                {
                    "_id": "6807bf3e70a0cec724b8a015",
                    "name": "Qiaochu Chen",
                    "hidden": false
                },
                {
                    "_id": "6807bf3e70a0cec724b8a016",
                    "user": {
                        "_id": "65be9918b54ab5b37d1b67a7",
                        "avatarUrl": "/avatars/8237eb89aa1f9847e7f55c11925a108a.svg",
                        "isPro": false,
                        "fullname": "Greg Durrett",
                        "user": "gregdurrett",
                        "type": "user"
                    },
                    "name": "Greg Durrett",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-24T11:56:46.592Z",
                    "hidden": false
                },
                {
                    "_id": "6807bf3e70a0cec724b8a017",
                    "name": "Isil Dillig",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T17:33:33.000Z",
            "submittedOnDailyAt": "2025-04-24T05:15:24.237Z",
            "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
            "submittedOnDailyBy": {
                "_id": "6697abd4be7ce6de07140e72",
                "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
                "isPro": false,
                "fullname": "Anirudh Khatry",
                "user": "anirudhkhatry",
                "type": "user"
            },
            "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
            "upvotes": 3,
            "discussionId": "6807bf3f70a0cec724b8a044",
            "githubRepo": "https://github.com/anirudhkhatry/CRUST-bench"
        },
        "publishedAt": "2025-04-21T13:33:33.000Z",
        "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
        "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15254.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6697abd4be7ce6de07140e72",
            "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
            "fullname": "Anirudh Khatry",
            "name": "anirudhkhatry",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16145",
            "authors": [
                {
                    "_id": "680a164533e15279df86adec",
                    "user": {
                        "_id": "65eaa07cb6c760d77468b4b6",
                        "avatarUrl": "/avatars/4a1aae58986b40444351e0a167ca807c.svg",
                        "isPro": false,
                        "fullname": "Jingchao Wang",
                        "user": "jcwang0602",
                        "type": "user"
                    },
                    "name": "Jingchao Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T11:25:46.213Z",
                    "hidden": false
                },
                {
                    "_id": "680a164533e15279df86aded",
                    "name": "Hong Wang",
                    "hidden": false
                },
                {
                    "_id": "680a164533e15279df86adee",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "680a164533e15279df86adef",
                    "name": "Kunhua Ji",
                    "hidden": false
                },
                {
                    "_id": "680a164533e15279df86adf0",
                    "name": "Dingjiang Huang",
                    "hidden": false
                },
                {
                    "_id": "680a164533e15279df86adf1",
                    "name": "Yefeng Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T12:48:12.000Z",
            "submittedOnDailyAt": "2025-04-24T09:58:00.646Z",
            "title": "Progressive Language-guided Visual Learning for Multi-Task Visual\n  Grounding",
            "submittedOnDailyBy": {
                "_id": "65eaa07cb6c760d77468b4b6",
                "avatarUrl": "/avatars/4a1aae58986b40444351e0a167ca807c.svg",
                "isPro": false,
                "fullname": "Jingchao Wang",
                "user": "jcwang0602",
                "type": "user"
            },
            "summary": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring\nExpression Comprehension (REC) and Referring Expression Segmentation (RES). The\nexisting representative approaches generally follow the research pipeline which\nmainly consists of three core procedures, including independent feature\nextraction for visual and linguistic modalities, respectively, cross-modal\ninteraction module, and independent prediction heads for different sub-tasks.\nAlbeit achieving remarkable performance, this research line has two\nlimitations: 1) The linguistic content has not been fully injected into the\nentire visual backbone for boosting more effective visual feature extraction\nand it needs an extra cross-modal interaction module; 2) The relationship\nbetween REC and RES tasks is not effectively exploited to help the\ncollaborative prediction for more accurate output. To deal with these problems,\nin this paper, we propose a Progressive Language-guided Visual Learning\nframework for multi-task visual grounding, called PLVL, which not only finely\nmine the inherent feature expression of the visual modality itself but also\nprogressively inject the language information to help learn linguistic-related\nvisual features. In this manner, our PLVL does not need additional cross-modal\nfusion module while fully introducing the language guidance. Furthermore, we\nanalyze that the localization center for REC would help identify the\nto-be-segmented object region for RES to some extent. Inspired by this\ninvestigation, we design a multi-task head to accomplish collaborative\npredictions for these two sub-tasks. Extensive experiments conducted on several\nbenchmark datasets comprehensively substantiate that our PLVL obviously\noutperforms the representative methods in both REC and RES tasks.\nhttps://github.com/jcwang0602/PLVL",
            "upvotes": 1,
            "discussionId": "680a164633e15279df86ae39",
            "githubRepo": "https://github.com/jcwang0602/PLVL",
            "ai_keywords": [
                "Multi-task visual grounding (MTVG)",
                "Referring Expression Comprehension (REC)",
                "Referring Expression Segmentation (RES)",
                "Visual backbone",
                "Linguistic modalities",
                "Cross-modal interaction module",
                "Prediction heads",
                "Progressive Language-guided Visual Learning framework (PLVL)",
                "Visual feature extraction",
                "Language information",
                "Linguistic-related visual features",
                "Cross-modal fusion module",
                "Localization center",
                "To-be-segmented object region",
                "Multi-task head",
                "Collaborative predictions"
            ]
        },
        "publishedAt": "2025-04-22T08:48:12.000Z",
        "title": "Progressive Language-guided Visual Learning for Multi-Task Visual\n  Grounding",
        "summary": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring\nExpression Comprehension (REC) and Referring Expression Segmentation (RES). The\nexisting representative approaches generally follow the research pipeline which\nmainly consists of three core procedures, including independent feature\nextraction for visual and linguistic modalities, respectively, cross-modal\ninteraction module, and independent prediction heads for different sub-tasks.\nAlbeit achieving remarkable performance, this research line has two\nlimitations: 1) The linguistic content has not been fully injected into the\nentire visual backbone for boosting more effective visual feature extraction\nand it needs an extra cross-modal interaction module; 2) The relationship\nbetween REC and RES tasks is not effectively exploited to help the\ncollaborative prediction for more accurate output. To deal with these problems,\nin this paper, we propose a Progressive Language-guided Visual Learning\nframework for multi-task visual grounding, called PLVL, which not only finely\nmine the inherent feature expression of the visual modality itself but also\nprogressively inject the language information to help learn linguistic-related\nvisual features. In this manner, our PLVL does not need additional cross-modal\nfusion module while fully introducing the language guidance. Furthermore, we\nanalyze that the localization center for REC would help identify the\nto-be-segmented object region for RES to some extent. Inspired by this\ninvestigation, we design a multi-task head to accomplish collaborative\npredictions for these two sub-tasks. Extensive experiments conducted on several\nbenchmark datasets comprehensively substantiate that our PLVL obviously\noutperforms the representative methods in both REC and RES tasks.\nhttps://github.com/jcwang0602/PLVL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16145.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65eaa07cb6c760d77468b4b6",
            "avatarUrl": "/avatars/4a1aae58986b40444351e0a167ca807c.svg",
            "fullname": "Jingchao Wang",
            "name": "jcwang0602",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.13263",
            "authors": [
                {
                    "_id": "6808977e51649749b20850f0",
                    "user": {
                        "_id": "652344742470ec6d1aa5d57b",
                        "avatarUrl": "/avatars/3d9c4fda428b10cefb4e13a086ce14fb.svg",
                        "isPro": true,
                        "fullname": "Xinyue Wang",
                        "user": "XinyueWangg",
                        "type": "user"
                    },
                    "name": "Xinyue Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T11:25:40.443Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f1",
                    "user": {
                        "_id": "66f4fcbc29b10ae4c990a2e0",
                        "avatarUrl": "/avatars/5d6df3aa5792a031c28274d428c46d84.svg",
                        "isPro": false,
                        "fullname": "Kun Zhou",
                        "user": "FrancisKunZhou",
                        "type": "user"
                    },
                    "name": "Kun Zhou",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-23T07:58:43.275Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f2",
                    "user": {
                        "_id": "672aac577ec01ef4af3367d5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uNgXH3uMBFt-hI3cCeQ-5.jpeg",
                        "isPro": false,
                        "fullname": "Wenyi Wu",
                        "user": "WenyiWU0111",
                        "type": "user"
                    },
                    "name": "Wenyi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-24T09:11:01.109Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f3",
                    "name": "Har Simrat Singh",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f4",
                    "user": {
                        "_id": "6735024daef9e8b4380d0a3b",
                        "avatarUrl": "/avatars/7d4f49dda1781b88f2765c86787569dd.svg",
                        "isPro": false,
                        "fullname": "Fang Nan",
                        "user": "fangn06",
                        "type": "user"
                    },
                    "name": "Fang Nan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-23T07:32:15.188Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f5",
                    "name": "Songyao Jin",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f6",
                    "user": {
                        "_id": "67d25f775785e2093c4bf51e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/JciZvNg0UaSF19RISb6Hu.png",
                        "isPro": false,
                        "fullname": "Aryan Philip",
                        "user": "arphilip123",
                        "type": "user"
                    },
                    "name": "Aryan Philip",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-23T07:32:15.188Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f7",
                    "user": {
                        "_id": "669bc5e4119595d21bf06299",
                        "avatarUrl": "/avatars/fa136933db8f24f24ecf76bd21f88468.svg",
                        "isPro": false,
                        "fullname": "Saloni Patnaik",
                        "user": "sapatnaik",
                        "type": "user"
                    },
                    "name": "Saloni Patnaik",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-24T20:13:27.175Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f8",
                    "name": "Hou Zhu",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850f9",
                    "name": "Shivam Singh",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850fa",
                    "user": {
                        "_id": "671561f4d1cf4a8ca40fb91a",
                        "avatarUrl": "/avatars/10e4818a27513fe395d4ae8fbca78feb.svg",
                        "isPro": false,
                        "fullname": "PARJANYA PRAJAKTA PRASHANT",
                        "user": "parjanya20",
                        "type": "user"
                    },
                    "name": "Parjanya Prashant",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-23T07:32:15.188Z",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850fb",
                    "name": "Qian Shen",
                    "hidden": false
                },
                {
                    "_id": "6808977e51649749b20850fc",
                    "name": "Biwei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-17T18:05:39.000Z",
            "submittedOnDailyAt": "2025-04-24T15:08:38.168Z",
            "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
            "submittedOnDailyBy": {
                "_id": "672aac577ec01ef4af3367d5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uNgXH3uMBFt-hI3cCeQ-5.jpeg",
                "isPro": false,
                "fullname": "Wenyi Wu",
                "user": "WenyiWU0111",
                "type": "user"
            },
            "summary": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/.",
            "upvotes": 0,
            "discussionId": "6808977f51649749b208514a",
            "ai_keywords": [
                "causal analysis",
                "causal discovery",
                "causal inference",
                "algorithm selection",
                "hyperparameter optimization",
                "result interpretation",
                "actionable insights",
                "large language model",
                "tabular data",
                "time-series data",
                "causal theory",
                "empirical evaluations"
            ]
        },
        "publishedAt": "2025-04-17T14:05:39.000Z",
        "title": "Causal-Copilot: An Autonomous Causal Analysis Agent",
        "summary": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis. A live interactive demo of Causal-Copilot is\navailable at https://causalcopilot.com/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13263.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672aac577ec01ef4af3367d5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uNgXH3uMBFt-hI3cCeQ-5.jpeg",
            "fullname": "Wenyi Wu",
            "name": "WenyiWU0111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]