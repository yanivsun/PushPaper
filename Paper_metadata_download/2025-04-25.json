[
    {
        "paper": {
            "id": "2504.17761",
            "authors": [
                {
                    "_id": "680af2df3b93130c9b2b90a7",
                    "name": "Shiyu Liu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90a8",
                    "name": "Yucheng Han",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90a9",
                    "user": {
                        "_id": "649f7b742bf2e21d955c1067",
                        "avatarUrl": "/avatars/270026405d5640886a86f961a001b057.svg",
                        "isPro": false,
                        "fullname": "xing",
                        "user": "xingpng",
                        "type": "user"
                    },
                    "name": "Peng Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T12:27:18.944Z",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90aa",
                    "name": "Fukun Yin",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ab",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ac",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:36.757Z",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ad",
                    "name": "Jiaqi Liao",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ae",
                    "name": "Yingming Wang",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90af",
                    "name": "Honghao Fu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b0",
                    "name": "Chunrui Han",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b1",
                    "name": "Guopeng Li",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b2",
                    "name": "Yuang Peng",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b3",
                    "name": "Quan Sun",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b4",
                    "name": "Jingwei Wu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b5",
                    "name": "Yan Cai",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b6",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b7",
                    "name": "Ranchen Ming",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b8",
                    "name": "Lei Xia",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90b9",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90ba",
                    "name": "Yibo Zhu",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90bb",
                    "name": "Binxing Jiao",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90bc",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90bd",
                    "user": {
                        "_id": "63417332c5565a4b8d43a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
                        "isPro": false,
                        "fullname": "Gang Yu",
                        "user": "skicy",
                        "type": "user"
                    },
                    "name": "Gang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:34.650Z",
                    "hidden": false
                },
                {
                    "_id": "680af2df3b93130c9b2b90be",
                    "name": "Daxin Jiang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
            ],
            "publishedAt": "2025-04-24T17:25:12.000Z",
            "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
            "title": "Step1X-Edit: A Practical Framework for General Image Editing",
            "submittedOnDailyBy": {
                "_id": "64b914c8ace99c0723ad83a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
                "isPro": false,
                "fullname": "Wei Cheng",
                "user": "wchengad",
                "type": "user"
            },
            "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
            "upvotes": 55,
            "discussionId": "680af2e13b93130c9b2b9132",
            "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
            "ai_keywords": [
                "Multimodal LLM",
                "latent embedding",
                "diffusion image decoder",
                "data generation pipeline",
                "GEdit-Bench",
                "real-world user instructions"
            ]
        },
        "publishedAt": "2025-04-24T13:25:12.000Z",
        "title": "Step1X-Edit: A Practical Framework for General Image Editing",
        "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "fullname": "Wei Cheng",
            "name": "wchengad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17192",
            "authors": [
                {
                    "_id": "680aee7bcf67477f2c00ca53",
                    "user": {
                        "_id": "64f7bf0c7565a69eb693ad1f",
                        "avatarUrl": "/avatars/aba6910aa39a3437a7f0df3f5cd49e6d.svg",
                        "isPro": false,
                        "fullname": "minju",
                        "user": "iaminju",
                        "type": "user"
                    },
                    "name": "Minju Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:41.304Z",
                    "hidden": false
                },
                {
                    "_id": "680aee7bcf67477f2c00ca54",
                    "user": {
                        "_id": "63036b6c5c70c21d0ea79d48",
                        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                        "isPro": false,
                        "fullname": "Jinheon Baek",
                        "user": "jinheon",
                        "type": "user"
                    },
                    "name": "Jinheon Baek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:38.982Z",
                    "hidden": false
                },
                {
                    "_id": "680aee7bcf67477f2c00ca55",
                    "name": "Seongyun Lee",
                    "hidden": false
                },
                {
                    "_id": "680aee7bcf67477f2c00ca56",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T01:57:01.000Z",
            "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
            "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6550c4f27bbfce1878f5f280",
                "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
                "isPro": false,
                "fullname": "seongyun_lee",
                "user": "Seongyun",
                "type": "user"
            },
            "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
            "upvotes": 50,
            "discussionId": "680aee7dcf67477f2c00ca96",
            "githubRepo": "https://github.com/going-doer/Paper2Code"
        },
        "publishedAt": "2025-04-23T21:57:01.000Z",
        "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
        "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6550c4f27bbfce1878f5f280",
            "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
            "fullname": "seongyun_lee",
            "name": "Seongyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17502",
            "authors": [
                {
                    "_id": "680b44fb426b7d5bc2018c75",
                    "user": {
                        "_id": "631da07f6d6a5870f3d2c375",
                        "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
                        "isPro": false,
                        "fullname": "Aviv Slobodkin",
                        "user": "lovodkin93",
                        "type": "user"
                    },
                    "name": "Aviv Slobodkin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-25T08:17:03.471Z",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c76",
                    "name": "Hagai Taitelbaum",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c77",
                    "name": "Yonatan Bitton",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c78",
                    "name": "Brian Gordon",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c79",
                    "name": "Michal Sokolik",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7a",
                    "name": "Nitzan Bitton Guetta",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7b",
                    "name": "Almog Gueta",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7c",
                    "name": "Royi Rassin",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7d",
                    "name": "Itay Laish",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7e",
                    "name": "Dani Lischinski",
                    "hidden": false
                },
                {
                    "_id": "680b44fb426b7d5bc2018c7f",
                    "name": "Idan Szpektor",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T12:44:51.000Z",
            "submittedOnDailyAt": "2025-04-25T06:50:21.552Z",
            "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "631da07f6d6a5870f3d2c375",
                "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
                "isPro": false,
                "fullname": "Aviv Slobodkin",
                "user": "lovodkin93",
                "type": "user"
            },
            "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
            "upvotes": 47,
            "discussionId": "680b44ff426b7d5bc2018d85",
            "ai_keywords": [
                "RefVNLI",
                "video-reasoning benchmarks",
                "image perturbations"
            ]
        },
        "publishedAt": "2025-04-24T08:44:51.000Z",
        "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
        "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631da07f6d6a5870f3d2c375",
            "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
            "fullname": "Aviv Slobodkin",
            "name": "lovodkin93",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17432",
            "authors": [
                {
                    "_id": "680adfbe464a44cea0b843c1",
                    "name": "Tiancheng Gu",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c2",
                    "user": {
                        "_id": "63e202f352b7578dba448ab5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                        "isPro": false,
                        "fullname": "Yang",
                        "user": "Kaichengalex",
                        "type": "user"
                    },
                    "name": "Kaicheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:46.935Z",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c3",
                    "name": "Ziyong Feng",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c4",
                    "name": "Xingjun Wang",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c5",
                    "name": "Yanzhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c6",
                    "name": "Dingkun Long",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c7",
                    "name": "Yingda Chen",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c8",
                    "name": "Weidong Cai",
                    "hidden": false
                },
                {
                    "_id": "680adfbe464a44cea0b843c9",
                    "name": "Jiankang Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T10:51:52.000Z",
            "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
            "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "63e202f352b7578dba448ab5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
                "isPro": false,
                "fullname": "Yang",
                "user": "Kaichengalex",
                "type": "user"
            },
            "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
            "upvotes": 28,
            "discussionId": "680adfbf464a44cea0b8440f",
            "projectPage": "https://garygutc.github.io/UniME/",
            "githubRepo": "https://github.com/deepglint/UniME",
            "ai_keywords": [
                "Contrastive Language-Image Pre-training (CLIP)",
                "Multimodal Large Language Models (MLLMs)",
                "Generalized vision-language understanding",
                "UniME (Universal Multimodal Embedding)",
                "Discriminative representations",
                "Textual discriminative knowledge distillation",
                "LLM-based teacher model",
                "Hard negative enhanced instruction tuning",
                "False negative contamination",
                "Challenging samples",
                "Discriminative power",
                "Instruction-following ability",
                "MMEB benchmark",
                "Short caption retrieval",
                "Long caption retrieval",
                "Compositional retrieval"
            ]
        },
        "publishedAt": "2025-04-24T06:51:52.000Z",
        "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
        "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "fullname": "Yang",
            "name": "Kaichengalex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17207",
            "authors": [
                {
                    "_id": "680af2bf2fa10fbf21684bde",
                    "name": "Phillip Y. Lee",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684bdf",
                    "name": "Jihyeon Je",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be0",
                    "name": "Chanho Park",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be1",
                    "name": "Mikaela Angelina Uy",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be2",
                    "name": "Leonidas Guibas",
                    "hidden": false
                },
                {
                    "_id": "680af2bf2fa10fbf21684be3",
                    "name": "Minhyuk Sung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T02:41:34.000Z",
            "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
            "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
            "submittedOnDailyBy": {
                "_id": "6342796a0875f2c99cfd313b",
                "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
                "isPro": false,
                "fullname": "Yuseung \"Phillip\" Lee",
                "user": "phillipinseoul",
                "type": "user"
            },
            "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
            "upvotes": 18,
            "discussionId": "680af2c02fa10fbf21684c1f",
            "ai_keywords": [
                "vision-language models (VLMs)",
                "mental imagery simulation",
                "perspective-taking",
                "visual understanding",
                "environmental interaction",
                "autonomous agents",
                "spatial reasoning",
                "perspective-aware reasoning capabilities",
                "egocentric interpretations",
                "mental imagery",
                "scene abstractions",
                "perspective transformations",
                "object detection",
                "segmentation",
                "orientation estimation",
                "synthetic benchmarks",
                "real-image benchmarks",
                "fine-tuned spatial reasoning models",
                "novel-view-synthesis-based approaches"
            ]
        },
        "publishedAt": "2025-04-23T22:41:34.000Z",
        "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
        "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6342796a0875f2c99cfd313b",
            "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
            "fullname": "Yuseung \"Phillip\" Lee",
            "name": "phillipinseoul",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.16511",
            "authors": [
                {
                    "_id": "680b2a95c94724c1465c20dd",
                    "name": "Fengze Liu",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20de",
                    "name": "Weidong Zhou",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20df",
                    "name": "Binbin Liu",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e0",
                    "name": "Zhimiao Yu",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e1",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e2",
                    "name": "Haobin Lin",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e3",
                    "name": "Yifeng Yu",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e4",
                    "name": "Xiaohuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e5",
                    "name": "Taifeng Wang",
                    "hidden": false
                },
                {
                    "_id": "680b2a95c94724c1465c20e6",
                    "name": "Yong Cao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T08:36:50.000Z",
            "submittedOnDailyAt": "2025-04-25T04:55:18.773Z",
            "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
            "submittedOnDailyBy": {
                "_id": "668f5875b5b3081d776e4094",
                "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
                "isPro": false,
                "fullname": "Xiaohuan Zhou",
                "user": "XiaohuanZhou",
                "type": "user"
            },
            "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
            "upvotes": 14,
            "discussionId": "680b2a97c94724c1465c21a3",
            "ai_keywords": [
                "large language models (LLMs)",
                "QuaDMix",
                "data selection framework",
                "parameterized data sampling function",
                "domain classification",
                "LightGBM",
                "RegMix"
            ]
        },
        "publishedAt": "2025-04-23T04:36:50.000Z",
        "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
        "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668f5875b5b3081d776e4094",
            "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
            "fullname": "Xiaohuan Zhou",
            "name": "XiaohuanZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17789",
            "authors": [
                {
                    "_id": "680b318bbbebf87944bc9595",
                    "name": "Xu Ma",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc9596",
                    "name": "Peize Sun",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc9597",
                    "name": "Haoyu Ma",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc9598",
                    "name": "Hao Tang",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc9599",
                    "name": "Chih-Yao Ma",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc959a",
                    "name": "Jialiang Wang",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc959b",
                    "name": "Kunpeng Li",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc959c",
                    "name": "Xiaoliang Dai",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc959d",
                    "name": "Yujun Shi",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc959e",
                    "name": "Xuan Ju",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc959f",
                    "name": "Yushi Hu",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a0",
                    "name": "Artsiom Sanakoyeu",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a1",
                    "name": "Felix Juefei-Xu",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a2",
                    "name": "Ji Hou",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a3",
                    "name": "Junjiao Tian",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a4",
                    "name": "Tao Xu",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a5",
                    "name": "Tingbo Hou",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a6",
                    "name": "Yen-Cheng Liu",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a7",
                    "name": "Zecheng He",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a8",
                    "name": "Zijian He",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95a9",
                    "name": "Matt Feiszli",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95aa",
                    "name": "Peizhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95ab",
                    "name": "Peter Vajda",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95ac",
                    "name": "Sam Tsai",
                    "hidden": false
                },
                {
                    "_id": "680b318bbbebf87944bc95ad",
                    "name": "Yun Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-25T05:28:51.493Z",
            "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
            "upvotes": 8,
            "discussionId": "680b3191bbebf87944bc9739",
            "ai_keywords": [
                "autoregressive (AR) models",
                "image synthesis",
                "diffusion-based models",
                "image tokens",
                "training and inference efficiency",
                "Transformer",
                "dimensional redundancy",
                "visual vocabularies",
                "Multimodal Large Language Models (MLLMs)",
                "visual encoder",
                "high-dimensional language vocabularies",
                "token-shuffle",
                "spatially local tokens",
                "channel dimension",
                "token-unshuffle",
                "spatial arrangement",
                "unified next-token prediction",
                "text-to-image generation",
                "resolution",
                "generation performance",
                "GenAI-benchmark",
                "textual prompts",
                "pretrained text-encoder",
                "text-alignment",
                "visual flaw",
                "visual appearance"
            ]
        },
        "publishedAt": "2025-04-24T13:59:56.000Z",
        "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
        "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6719
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17040",
            "authors": [
                {
                    "_id": "680af0c4175842e433ae348e",
                    "name": "Zhenhailong Wang",
                    "hidden": false
                },
                {
                    "_id": "680af0c4175842e433ae348f",
                    "name": "Senthil Purushwalkam",
                    "hidden": false
                },
                {
                    "_id": "680af0c4175842e433ae3490",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "680af0c4175842e433ae3491",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "680af0c4175842e433ae3492",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "680af0c4175842e433ae3493",
                    "name": "Ran Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T18:38:18.000Z",
            "submittedOnDailyAt": "2025-04-25T06:12:13.135Z",
            "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
            "upvotes": 8,
            "discussionId": "680af0c7175842e433ae3544",
            "ai_keywords": [
                "Dynamic Token Merging (DToMe)",
                "Virtual Token Unmerging (VTU)",
                "vision transformers",
                "token compression",
                "attention dynamics",
                "visual encoders",
                "image complexity",
                "computational costs"
            ]
        },
        "publishedAt": "2025-04-23T14:38:18.000Z",
        "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
        "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6719
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.16828",
            "authors": [
                {
                    "_id": "680a831b49e5373a4f6c2cfe",
                    "name": "Muhammad Khalifa",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2cff",
                    "name": "Rishabh Agarwal",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2d00",
                    "name": "Lajanugen Logeswaran",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2d01",
                    "name": "Jaekyeom Kim",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2d02",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2d03",
                    "name": "Moontae Lee",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2d04",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "680a831b49e5373a4f6c2d05",
                    "name": "Lu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T15:44:54.000Z",
            "submittedOnDailyAt": "2025-04-25T10:28:47.902Z",
            "title": "Process Reward Models That Think",
            "submittedOnDailyBy": {
                "_id": "5f350fe67e5835433862161b",
                "avatarUrl": "/avatars/8aecfdea5202968b1099412800a152e0.svg",
                "isPro": false,
                "fullname": "Muhammad Khalifa",
                "user": "mkhalifa",
                "type": "user"
            },
            "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.",
            "upvotes": 6,
            "discussionId": "680a831c49e5373a4f6c2d32",
            "ai_keywords": [
                "step-wise reward models",
                "verification chain-of-thought (CoT)",
                "ThinkPRM",
                "long CoT verifier",
                "ProcessBench",
                "MATH-500",
                "AIME '24",
                "GPQA-Diamond",
                "LiveCodeBench",
                "LLM-as-a-Judge",
                "discriminative verifiers",
                "generative, long CoT PRMs"
            ]
        },
        "publishedAt": "2025-04-23T11:44:54.000Z",
        "title": "Process Reward Models That Think",
        "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16828.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f350fe67e5835433862161b",
            "avatarUrl": "/avatars/8aecfdea5202968b1099412800a152e0.svg",
            "fullname": "Muhammad Khalifa",
            "name": "mkhalifa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.16921",
            "authors": [
                {
                    "_id": "680b40774d69b6950c4eabda",
                    "user": {
                        "_id": "613f7e43e9cc36f11c7715b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631558612397-613f7e43e9cc36f11c7715b1.png",
                        "isPro": false,
                        "fullname": "Jos ngel Gonzlez",
                        "user": "jogonba2",
                        "type": "user"
                    },
                    "name": "Jos ngel Gonzlez",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T12:27:15.281Z",
                    "hidden": false
                },
                {
                    "_id": "680b40774d69b6950c4eabdb",
                    "name": "Ian Borrego Obrador",
                    "hidden": false
                },
                {
                    "_id": "680b40774d69b6950c4eabdc",
                    "name": "lvaro Romo Herrero",
                    "hidden": false
                },
                {
                    "_id": "680b40774d69b6950c4eabdd",
                    "name": "Areg Mikael Sarvazyan",
                    "hidden": false
                },
                {
                    "_id": "680b40774d69b6950c4eabde",
                    "user": {
                        "_id": "60f95c8fda0985b973d59d77",
                        "avatarUrl": "/avatars/5606f0191b9f86e6b55f7e5ab6cc8bb6.svg",
                        "isPro": false,
                        "fullname": "Mara Chinea Rios",
                        "user": "mchinea",
                        "type": "user"
                    },
                    "name": "Mara Chinea-Ros",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-25T07:58:32.606Z",
                    "hidden": false
                },
                {
                    "_id": "680b40774d69b6950c4eabdf",
                    "name": "Angelo Basile",
                    "hidden": false
                },
                {
                    "_id": "680b40774d69b6950c4eabe0",
                    "name": "Marc Franco-Salvador",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T17:48:25.000Z",
            "submittedOnDailyAt": "2025-04-25T06:28:29.231Z",
            "title": "IberBench: LLM Evaluation on Iberian Languages",
            "submittedOnDailyBy": {
                "_id": "62308d13e4673fe4985d7fc9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
                "isPro": false,
                "fullname": "Areg Mikael Sarvazyan",
                "user": "asarvazyan",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
            "upvotes": 5,
            "discussionId": "680b407a4d69b6950c4eac96",
            "projectPage": "https://huggingface.co/spaces/iberbench/leaderboard",
            "githubRepo": "https://github.com/IberBench",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "sentiment and emotion analysis",
                "toxicity detection",
                "summarization",
                "IberBench",
                "dataset normalization",
                "incremental evaluation",
                "leaderboard"
            ]
        },
        "publishedAt": "2025-04-23T13:48:25.000Z",
        "title": "IberBench: LLM Evaluation on Iberian Languages",
        "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62308d13e4673fe4985d7fc9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
            "fullname": "Areg Mikael Sarvazyan",
            "name": "asarvazyan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.16064",
            "authors": [
                {
                    "_id": "680b494c6bd146aa35ab2e1c",
                    "name": "Theodoros Kouzelis",
                    "hidden": false
                },
                {
                    "_id": "680b494c6bd146aa35ab2e1d",
                    "user": {
                        "_id": "677272184d148b904333e874",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
                        "isPro": false,
                        "fullname": "Efstathios Karypidis",
                        "user": "Sta8is",
                        "type": "user"
                    },
                    "name": "Efstathios Karypidis",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T12:27:13.393Z",
                    "hidden": false
                },
                {
                    "_id": "680b494c6bd146aa35ab2e1e",
                    "user": {
                        "_id": "661ba524bd9243bf7e598355",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661ba524bd9243bf7e598355/i77yD4XgJn2vUbn_mIsT8.jpeg",
                        "isPro": false,
                        "fullname": "Ioannis Kakogeorgiou",
                        "user": "gkakogeorgiou",
                        "type": "user"
                    },
                    "name": "Ioannis Kakogeorgiou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T12:27:11.569Z",
                    "hidden": false
                },
                {
                    "_id": "680b494c6bd146aa35ab2e1f",
                    "name": "Spyros Gidaris",
                    "hidden": false
                },
                {
                    "_id": "680b494c6bd146aa35ab2e20",
                    "name": "Nikos Komodakis",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
            ],
            "publishedAt": "2025-04-22T17:41:42.000Z",
            "submittedOnDailyAt": "2025-04-25T07:32:33.466Z",
            "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
            "submittedOnDailyBy": {
                "_id": "677272184d148b904333e874",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
                "isPro": false,
                "fullname": "Efstathios Karypidis",
                "user": "Sta8is",
                "type": "user"
            },
            "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
            "upvotes": 5,
            "discussionId": "680b494e6bd146aa35ab2e97",
            "githubRepo": "https://github.com/zelaki/ReDi",
            "ai_keywords": [
                "latent diffusion models (LDMs)",
                "generative image modeling",
                "diffusion model",
                "low-level image latents",
                "variational autoencoder",
                "high-level semantic features",
                "pretrained self-supervised encoder",
                "DINO",
                "latent-semantic diffusion",
                "coherent image-feature pairs",
                "generative quality",
                "training efficiency",
                "Diffusion Transformer architectures",
                "complex distillation objectives",
                "Representation Guidance",
                "image quality",
                "training convergence speed",
                "representation-aware generative modeling"
            ]
        },
        "publishedAt": "2025-04-22T13:41:42.000Z",
        "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
        "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16064.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "677272184d148b904333e874",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
            "fullname": "Efstathios Karypidis",
            "name": "Sta8is",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17414",
            "authors": [
                {
                    "_id": "680b3f12c131c3be24f80ce0",
                    "name": "Min Wei",
                    "hidden": false
                },
                {
                    "_id": "680b3f12c131c3be24f80ce1",
                    "name": "Chaohui Yu",
                    "hidden": false
                },
                {
                    "_id": "680b3f12c131c3be24f80ce2",
                    "name": "Jingkai Zhou",
                    "hidden": false
                },
                {
                    "_id": "680b3f12c131c3be24f80ce3",
                    "name": "Fan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T10:12:40.000Z",
            "submittedOnDailyAt": "2025-04-25T06:22:09.592Z",
            "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
            "upvotes": 4,
            "discussionId": "680b3f15c131c3be24f80d65",
            "ai_keywords": [
                "diffusion-based framework",
                "animatable textured 3D meshes",
                "frame-level guidance",
                "motion coherence",
                "garment texture movements",
                "adaptive pipeline",
                "keyframe",
                "2D image try-on",
                "textured 3D mesh",
                "synchronized with original video poses",
                "rectangular masking strategy",
                "artifact propagation",
                "HR-VVT",
                "high-resolution benchmark dataset"
            ]
        },
        "publishedAt": "2025-04-24T06:12:40.000Z",
        "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
        "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17414.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6719
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17343",
            "authors": [
                {
                    "_id": "680b24471c5fbd15909bf1f9",
                    "name": "Linli Yao",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf1fa",
                    "name": "Yicheng Li",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf1fb",
                    "name": "Yuancheng Wei",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf1fc",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf1fd",
                    "name": "Shuhuai Ren",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf1fe",
                    "name": "Yuanxin Liu",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf1ff",
                    "name": "Kun Ouyang",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf200",
                    "name": "Lean Wang",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf201",
                    "name": "Shicheng Li",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf202",
                    "name": "Sida Li",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf203",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf204",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf205",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "680b24471c5fbd15909bf206",
                    "name": "Xu Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T07:59:46.000Z",
            "submittedOnDailyAt": "2025-04-25T06:34:54.517Z",
            "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
            "submittedOnDailyBy": {
                "_id": "655ca347f426a304c6b393a1",
                "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
                "isPro": false,
                "fullname": "Linli Yao",
                "user": "yaolily",
                "type": "user"
            },
            "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.",
            "upvotes": 4,
            "discussionId": "680b244a1c5fbd15909bf2ff",
            "ai_keywords": [
                "Video Large Language Models (VideoLLMs)",
                "Differential Token Drop (DTD)",
                "Change Blindness phenomenon",
                "TimeChat-Online",
                "TimeChat-Online-139K",
                "StreamingBench",
                "OvOBench",
                "Video-MME",
                "MLVU",
                "Proactive Response",
                "real-time video interaction",
                "continuous video streams",
                "user queries",
                "visual redundancy",
                "dense, redundant frames",
                "visual content",
                "video tokens",
                "meaningful temporal changes",
                "static, redundant content",
                "backward-tracing",
                "current-perception",
                "future-responding scenarios"
            ]
        },
        "publishedAt": "2025-04-24T03:59:46.000Z",
        "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
        "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17343.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655ca347f426a304c6b393a1",
            "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
            "fullname": "Linli Yao",
            "name": "yaolily",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17069",
            "authors": [
                {
                    "_id": "680b1b33388bb2cfd497ebdb",
                    "user": {
                        "_id": "62bb84f82ada492aa5775709",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb84f82ada492aa5775709/jv4yKL75t8QzHDHLbhBPT.png",
                        "isPro": false,
                        "fullname": "Rishav Pramanik",
                        "user": "rishavpramanik",
                        "type": "user"
                    },
                    "name": "Rishav Pramanik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:27.724Z",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebdc",
                    "name": "Antoine Poupon",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebdd",
                    "user": {
                        "_id": "63a614d264f470027818b066",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
                        "isPro": false,
                        "fullname": "Juan A. Rodriguez",
                        "user": "joanrodai",
                        "type": "user"
                    },
                    "name": "Juan A. Rodriguez",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T12:27:17.091Z",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebde",
                    "name": "Masih Aminbeidokhti",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebdf",
                    "name": "David Vazquez",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebe0",
                    "name": "Christopher Pal",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebe1",
                    "name": "Zhaozheng Yin",
                    "hidden": false
                },
                {
                    "_id": "680b1b33388bb2cfd497ebe2",
                    "name": "Marco Pedersoli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
            ],
            "publishedAt": "2025-04-23T19:33:58.000Z",
            "submittedOnDailyAt": "2025-04-25T03:50:09.534Z",
            "title": "Distilling semantically aware orders for autoregressive image generation",
            "submittedOnDailyBy": {
                "_id": "63a614d264f470027818b066",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
                "isPro": false,
                "fullname": "Juan A. Rodriguez",
                "user": "joanrodai",
                "type": "user"
            },
            "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
            "upvotes": 4,
            "discussionId": "680b1b35388bb2cfd497ec76",
            "ai_keywords": [
                "autoregressive patch-based image generation",
                "Vision-Language models",
                "raster-scan order",
                "causality",
                "any-given-order",
                "patch content",
                "patch location",
                "fine-tuning"
            ]
        },
        "publishedAt": "2025-04-23T15:33:58.000Z",
        "title": "Distilling semantically aware orders for autoregressive image generation",
        "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a614d264f470027818b066",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
            "fullname": "Juan A. Rodriguez",
            "name": "joanrodai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.15921",
            "authors": [
                {
                    "_id": "680ab2f808464b525df64b07",
                    "name": "Jian Hu",
                    "hidden": false
                },
                {
                    "_id": "680ab2f808464b525df64b08",
                    "name": "Dimitrios Korkinof",
                    "hidden": false
                },
                {
                    "_id": "680ab2f808464b525df64b09",
                    "name": "Shaogang Gong",
                    "hidden": false
                },
                {
                    "_id": "680ab2f808464b525df64b0a",
                    "name": "Mariano Beguerisse-Diaz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T14:06:01.000Z",
            "submittedOnDailyAt": "2025-04-25T06:39:24.280Z",
            "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
            "submittedOnDailyBy": {
                "_id": "65e1b6e9501590df0173cbd3",
                "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
                "isPro": false,
                "fullname": "Jian Hu",
                "user": "lwpyh",
                "type": "user"
            },
            "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
            "upvotes": 4,
            "discussionId": "680ab2fb08464b525df64bd2",
            "ai_keywords": [
                "LLMs",
                "ViSMap",
                "Unsupervised Video Summarisation",
                "Meta Prompting",
                "long-form video understanding",
                "supervised hierarchical training",
                "pseudo-summaries",
                "short clip descriptions",
                "meta-prompting strategy",
                "generator prompt"
            ]
        },
        "publishedAt": "2025-04-22T10:06:01.000Z",
        "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
        "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e1b6e9501590df0173cbd3",
            "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
            "fullname": "Jian Hu",
            "name": "lwpyh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17601",
            "authors": [
                {
                    "_id": "680b2b8c6bd146aa35a48222",
                    "user": {
                        "_id": "64d496b04ab89be0de7fb1a9",
                        "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
                        "isPro": false,
                        "fullname": "Erik Bergh",
                        "user": "erikbergh",
                        "type": "user"
                    },
                    "name": "Erik Bergh",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-04-25T06:39:03.785Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T14:26:42.000Z",
            "submittedOnDailyAt": "2025-04-25T05:00:56.733Z",
            "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
            "submittedOnDailyBy": {
                "_id": "64d496b04ab89be0de7fb1a9",
                "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
                "isPro": false,
                "fullname": "Erik Bergh",
                "user": "erikbergh",
                "type": "user"
            },
            "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
            "upvotes": 2,
            "discussionId": "680b2b8d6bd146aa35a48252",
            "projectPage": "https://github.com/erikbergh/interpretable_dim_reduction/",
            "githubRepo": "https://github.com/erikbergh/interpretable_dim_reduction/",
            "ai_keywords": [
                "t-SNE",
                "PCA",
                "non-linear mapping",
                "Gaussian functions",
                "linear transformations",
                "interpretability",
                "dimensionality reduction",
                "suppressed dimensions",
                "geometric relationships"
            ]
        },
        "publishedAt": "2025-04-24T10:26:42.000Z",
        "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
        "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d496b04ab89be0de7fb1a9",
            "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
            "fullname": "Erik Bergh",
            "name": "erikbergh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17788",
            "authors": [
                {
                    "_id": "680af390cbada0176f4978ee",
                    "name": "Chris Rockwell",
                    "hidden": false
                },
                {
                    "_id": "680af390cbada0176f4978ef",
                    "name": "Joseph Tung",
                    "hidden": false
                },
                {
                    "_id": "680af390cbada0176f4978f0",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "680af390cbada0176f4978f1",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "680af390cbada0176f4978f2",
                    "name": "David F. Fouhey",
                    "hidden": false
                },
                {
                    "_id": "680af390cbada0176f4978f3",
                    "name": "Chen-Hsuan Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T17:59:56.000Z",
            "submittedOnDailyAt": "2025-04-25T20:02:24.703Z",
            "title": "Dynamic Camera Poses and Where to Find Them",
            "submittedOnDailyBy": {
                "_id": "645297ba1a0c17bb7d5975d6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645297ba1a0c17bb7d5975d6/oY7kd0-UcfzYub9ewjAJC.jpeg",
                "isPro": false,
                "fullname": "Chris Rockwell",
                "user": "crockwell",
                "type": "user"
            },
            "summary": "Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.",
            "upvotes": 1,
            "discussionId": "680af393cbada0176f497a07",
            "projectPage": "https://research.nvidia.com/labs/dir/dynpose-100k/",
            "ai_keywords": [
                "DynPose-100K",
                "point tracking",
                "dynamic masking",
                "structure-from-motion"
            ]
        },
        "publishedAt": "2025-04-24T13:59:56.000Z",
        "title": "Dynamic Camera Poses and Where to Find Them",
        "summary": "Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17788.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645297ba1a0c17bb7d5975d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645297ba1a0c17bb7d5975d6/oY7kd0-UcfzYub9ewjAJC.jpeg",
            "fullname": "Chris Rockwell",
            "name": "crockwell",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17670",
            "authors": [
                {
                    "_id": "680af5c5e5f0b6a3068bc842",
                    "user": {
                        "_id": "65d61d8fd484956b5acc89fe",
                        "avatarUrl": "/avatars/47954232c90780ffe898a5a445f7fb0a.svg",
                        "isPro": false,
                        "fullname": "jiang",
                        "user": "LutaoJiang",
                        "type": "user"
                    },
                    "name": "Lutao Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:32.497Z",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc843",
                    "name": "Jiantao Lin",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc844",
                    "name": "Kanghao Chen",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc845",
                    "name": "Wenhang Ge",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc846",
                    "name": "Xin Yang",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc847",
                    "name": "Yifan Jiang",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc848",
                    "name": "Yuanhuiyi Lyu",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc849",
                    "name": "Xu Zheng",
                    "hidden": false
                },
                {
                    "_id": "680af5c5e5f0b6a3068bc84a",
                    "name": "Yingcong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T15:39:20.000Z",
            "submittedOnDailyAt": "2025-04-25T14:24:32.260Z",
            "title": "DiMeR: Disentangled Mesh Reconstruction Model",
            "submittedOnDailyBy": {
                "_id": "65d61d8fd484956b5acc89fe",
                "avatarUrl": "/avatars/47954232c90780ffe898a5a445f7fb0a.svg",
                "isPro": false,
                "fullname": "jiang",
                "user": "LutaoJiang",
                "type": "user"
            },
            "summary": "With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.",
            "upvotes": 1,
            "discussionId": "680af5c8e5f0b6a3068bc8d4",
            "projectPage": "https://lutao2021.github.io/DiMeR_page/",
            "githubRepo": "https://github.com/lutao2021/DiMeR",
            "ai_keywords": [
                "feed-forward 3D generative models",
                "Large Reconstruction Model (LRM)",
                "inductive biases",
                "mesh reconstruction",
                "DiMeR",
                "disentangled dual-stream",
                "Principle of Occam's Razor",
                "normal maps",
                "geometry branch",
                "texture branch",
                "3D ground truth supervision",
                "sparse-view reconstruction",
                "single-image-to-3D",
                "text-to-3D",
                "Chamfer Distance",
                "GSO dataset",
                "OmniObject3D dataset"
            ]
        },
        "publishedAt": "2025-04-24T11:39:20.000Z",
        "title": "DiMeR: Disentangled Mesh Reconstruction Model",
        "summary": "With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17670.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d61d8fd484956b5acc89fe",
            "avatarUrl": "/avatars/47954232c90780ffe898a5a445f7fb0a.svg",
            "fullname": "jiang",
            "name": "LutaoJiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]