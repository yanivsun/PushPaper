[
    {
        "paper": {
            "id": "2504.15376",
            "authors": [
                {
                    "_id": "680bda9c34c8d0bd08e01a25",
                    "user": {
                        "_id": "64c170190bfb901b04399295",
                        "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
                        "isPro": false,
                        "fullname": "Zhiqiu Lin",
                        "user": "zhiqiulin",
                        "type": "user"
                    },
                    "name": "Zhiqiu Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a26",
                    "user": {
                        "_id": "65f82fb0de5e636ca20184fa",
                        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
                        "isPro": false,
                        "fullname": "Alan",
                        "user": "syCen",
                        "type": "user"
                    },
                    "name": "Siyuan Cen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a27",
                    "name": "Daniel Jiang",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a28",
                    "user": {
                        "_id": "65b92084327a7e54db8289ae",
                        "avatarUrl": "/avatars/a53b6cda119ed195bd87ba2c1c7a5e13.svg",
                        "isPro": false,
                        "fullname": "Jay Karhade",
                        "user": "JayKarhade",
                        "type": "user"
                    },
                    "name": "Jay Karhade",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:06.066Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a29",
                    "user": {
                        "_id": "67b2db158904ba09ca8feb79",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
                        "isPro": false,
                        "fullname": "Hewei Wang",
                        "user": "Stephen624",
                        "type": "user"
                    },
                    "name": "Hewei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2a",
                    "user": {
                        "_id": "6484bce8d86bf0201eaff647",
                        "avatarUrl": "/avatars/a824216890c0fcd1fcff4a398c4c7cc6.svg",
                        "isPro": false,
                        "fullname": "Chancharik Mitra",
                        "user": "chancharikm",
                        "type": "user"
                    },
                    "name": "Chancharik Mitra",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:12.932Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2b",
                    "name": "Tiffany Ling",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2c",
                    "name": "Yuhan Huang",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2d",
                    "name": "Sifan Liu",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2e",
                    "name": "Mingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a2f",
                    "name": "Rushikesh Zawar",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a30",
                    "name": "Xue Bai",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a31",
                    "user": {
                        "_id": "63c9bd445fdc575773c732fe",
                        "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
                        "isPro": false,
                        "fullname": "Yilun Du",
                        "user": "yilundu",
                        "type": "user"
                    },
                    "name": "Yilun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:43.497Z",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a32",
                    "name": "Chuang Gan",
                    "hidden": false
                },
                {
                    "_id": "680bda9c34c8d0bd08e01a33",
                    "user": {
                        "_id": "6337151b0267ebcf02640eb6",
                        "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
                        "isPro": false,
                        "fullname": "Deva Ramanan",
                        "user": "devakramanan",
                        "type": "user"
                    },
                    "name": "Deva Ramanan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-28T13:52:26.336Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-21T18:34:57.000Z",
            "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
            "title": "Towards Understanding Camera Motions in Any Video",
            "submittedOnDailyBy": {
                "_id": "65f82fb0de5e636ca20184fa",
                "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
                "isPro": false,
                "fullname": "Alan",
                "user": "syCen",
                "type": "user"
            },
            "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
            "upvotes": 124,
            "discussionId": "680bda9e34c8d0bd08e01ae9",
            "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
            "githubRepo": "https://github.com/sy77777en/CameraBench",
            "ai_keywords": [
                "Structure-from-Motion (SfM)",
                "Video-Language Models (VLMs)",
                "semantic primitives",
                "geometric primitives",
                "generative VLM",
                "motion-augmented captioning",
                "video question answering",
                "video-text retrieval"
            ]
        },
        "publishedAt": "2025-04-21T14:34:57.000Z",
        "title": "Towards Understanding Camera Motions in Any Video",
        "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f82fb0de5e636ca20184fa",
            "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
            "fullname": "Alan",
            "name": "syCen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16656",
            "authors": [
                {
                    "_id": "6809a4ac81a95c83f0c81c83",
                    "name": "Chris",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c84",
                    "name": "Yichen Wei",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c85",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c86",
                    "name": "Xiaokun Wang",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c87",
                    "name": "Weijie Qiu",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c88",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c89",
                    "user": {
                        "_id": "63fdb1aa27abbe6b3ce098f5",
                        "avatarUrl": "/avatars/c22e3a77ff84b3b87c16cff2469f6d3d.svg",
                        "isPro": false,
                        "fullname": "xietian",
                        "user": "sealical",
                        "type": "user"
                    },
                    "name": "Tianyidan Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:50:25.445Z",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8a",
                    "name": "Jiangbo Pei",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8b",
                    "name": "Jianhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8c",
                    "name": "Yunzhuo Hao",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8d",
                    "user": {
                        "_id": "6462b241b438438da3c25a5d",
                        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
                        "isPro": false,
                        "fullname": "Xuchen Song",
                        "user": "xuchensong",
                        "type": "user"
                    },
                    "name": "Xuchen Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8e",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6809a4ac81a95c83f0c81c8f",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T12:24:10.000Z",
            "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
            "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
            "submittedOnDailyBy": {
                "_id": "6462b241b438438da3c25a5d",
                "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
                "isPro": false,
                "fullname": "Xuchen Song",
                "user": "xuchensong",
                "type": "user"
            },
            "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
            "upvotes": 43,
            "discussionId": "6809a4ae81a95c83f0c81cda",
            "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
            "ai_keywords": [
                "reinforcement learning",
                "reward-model guidance",
                "rule-based strategies",
                "Selective Sample Buffer (SSB)",
                "Vanishing Advantages",
                "Group Relative Policy Optimization (GRPO)",
                "visual hallucinations",
                "calibrated reward thresholds",
                "benchmark-leading performances",
                "OlympiadBench",
                "AIME2024",
                "LiveCodeBench",
                "MMMU",
                "Skywork R1V2-38B"
            ]
        },
        "publishedAt": "2025-04-23T08:24:10.000Z",
        "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
        "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6462b241b438438da3c25a5d",
            "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
            "fullname": "Xuchen Song",
            "name": "xuchensong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.18415",
            "authors": [
                {
                    "_id": "680ef1549cc294f617fb14b4",
                    "name": "Hongyu Wang",
                    "hidden": false
                },
                {
                    "_id": "680ef1549cc294f617fb14b5",
                    "name": "Shuming Ma",
                    "hidden": false
                },
                {
                    "_id": "680ef1549cc294f617fb14b6",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-25T15:17:52.000Z",
            "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
            "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
            "submittedOnDailyBy": {
                "_id": "63f71771d36951307fcb4dcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                "isPro": false,
                "fullname": "Hongyu Wang",
                "user": "hongyuw",
                "type": "user"
            },
            "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
            "upvotes": 24,
            "discussionId": "680ef1559cc294f617fb1536",
            "ai_keywords": [
                "BitNet v2",
                "1-bit Large Language Models (LLMs)",
                "activation outliers",
                "quantization",
                "4-bit activation quantization",
                "H-BitLinear",
                "Hadamard transformation",
                "activation distributions",
                "Gaussian-like forms",
                "low-bit representation",
                "8-bit activations",
                "BitNet b1.58",
                "batched inference"
            ]
        },
        "publishedAt": "2025-04-25T11:17:52.000Z",
        "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
        "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "fullname": "Hongyu Wang",
            "name": "hongyuw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17821",
            "authors": [
                {
                    "_id": "680f56b8da9639d22c64443f",
                    "user": {
                        "_id": "64d9da538767727dff1e8f19",
                        "avatarUrl": "/avatars/aaad38795007c6cbcb94c7eed1706e51.svg",
                        "isPro": false,
                        "fullname": "Yu",
                        "user": "Ghaser",
                        "type": "user"
                    },
                    "name": "Xinyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:50:03.092Z",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644440",
                    "name": "Yunxin Li",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644441",
                    "name": "Haoyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644442",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644443",
                    "name": "Wenhan Luo",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644444",
                    "name": "Yaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "680f56b8da9639d22c644445",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T13:47:30.000Z",
            "submittedOnDailyAt": "2025-04-28T09:15:13.533Z",
            "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
            "submittedOnDailyBy": {
                "_id": "62fdb01bc1588e1d4c6c1a7c",
                "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                "isPro": false,
                "fullname": "Yunxin Li",
                "user": "YunxinLi",
                "type": "user"
            },
            "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
            "upvotes": 19,
            "discussionId": "680f56bdda9639d22c64456b",
            "projectPage": "https://videovista-culturallingo.github.io/",
            "githubRepo": "https://github.com/HITsz-TMG/VideoVista"
        },
        "publishedAt": "2025-04-23T09:47:30.000Z",
        "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
        "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "fullname": "Yunxin Li",
            "name": "YunxinLi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16427",
            "authors": [
                {
                    "_id": "680c48805ec65044c2861a6a",
                    "user": {
                        "_id": "669090c01e3f5b16ce22b535",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
                        "isPro": false,
                        "fullname": "Hanlei Zhang",
                        "user": "HanleiZhang",
                        "type": "user"
                    },
                    "name": "Hanlei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6b",
                    "user": {
                        "_id": "680ddad00a5976c000344004",
                        "avatarUrl": "/avatars/82913c7239fdb56e5830d5ddabfa90c1.svg",
                        "isPro": false,
                        "fullname": "Zhuohang Li",
                        "user": "lzh0275",
                        "type": "user"
                    },
                    "name": "Zhuohang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:33:19.643Z",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6c",
                    "name": "Yeshuang Zhu",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6d",
                    "name": "Hua Xu",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6e",
                    "name": "Peiwu Wang",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a6f",
                    "name": "Haige Zhu",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a70",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "680c48805ec65044c2861a71",
                    "name": "Jinchao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T05:25:13.000Z",
            "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
            "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
            "submittedOnDailyBy": {
                "_id": "669090c01e3f5b16ce22b535",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
                "isPro": false,
                "fullname": "Hanlei Zhang",
                "user": "HanleiZhang",
                "type": "user"
            },
            "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
            "upvotes": 11,
            "discussionId": "680c48825ec65044c2861ac4",
            "githubRepo": "https://github.com/thuiar/MMLA",
            "ai_keywords": [
                "multimodal language models (MLLMs)",
                "MMLA (Multimodal Language Analysis)",
                "multimodal utterances",
                "intent",
                "emotion",
                "dialogue act",
                "sentiment",
                "speaking style",
                "communication behavior",
                "zero-shot inference",
                "supervised fine-tuning",
                "instruction tuning",
                "large language models (LLMs)"
            ]
        },
        "publishedAt": "2025-04-23T01:25:13.000Z",
        "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
        "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669090c01e3f5b16ce22b535",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
            "fullname": "Hanlei Zhang",
            "name": "HanleiZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17816",
            "authors": [
                {
                    "_id": "680ed2679e529f7799a0689f",
                    "user": {
                        "_id": "636b20591340f879a2eb98d0",
                        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
                        "isPro": false,
                        "fullname": "Daneul Kim",
                        "user": "carpedkm",
                        "type": "user"
                    },
                    "name": "Daneul Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T07:38:37.502Z",
                    "hidden": false
                },
                {
                    "_id": "680ed2679e529f7799a068a0",
                    "name": "Jingxu Zhang",
                    "hidden": false
                },
                {
                    "_id": "680ed2679e529f7799a068a1",
                    "name": "Wonjoon Jin",
                    "hidden": false
                },
                {
                    "_id": "680ed2679e529f7799a068a2",
                    "name": "Sunghyun Cho",
                    "hidden": false
                },
                {
                    "_id": "680ed2679e529f7799a068a3",
                    "user": {
                        "_id": "65115c00a588fdb36558b673",
                        "avatarUrl": "/avatars/1f36263dc4bfaf696a4aa959a6aab1e1.svg",
                        "isPro": false,
                        "fullname": "Qi Dai",
                        "user": "daiqi",
                        "type": "user"
                    },
                    "name": "Qi Dai",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
                    "hidden": false
                },
                {
                    "_id": "680ed2679e529f7799a068a4",
                    "name": "Jaesik Park",
                    "hidden": false
                },
                {
                    "_id": "680ed2679e529f7799a068a5",
                    "user": {
                        "_id": "676a328148d749b7086782d0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Tt7u8l8f_1oVBWmBp7tkm.png",
                        "isPro": false,
                        "fullname": "Chong Luo",
                        "user": "cluo-ms",
                        "type": "user"
                    },
                    "name": "Chong Luo",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
            ],
            "publishedAt": "2025-04-23T06:48:31.000Z",
            "submittedOnDailyAt": "2025-04-28T07:02:51.113Z",
            "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
            "submittedOnDailyBy": {
                "_id": "636b20591340f879a2eb98d0",
                "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
                "isPro": false,
                "fullname": "Daneul Kim",
                "user": "carpedkm",
                "type": "user"
            },
            "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
            "upvotes": 9,
            "discussionId": "680ed2689e529f7799a06907",
            "projectPage": "https://carpedkm.github.io/projects/disentangled_sub/",
            "githubRepo": "https://github.com/carpedkm/disentangled-subject-to-vid",
            "ai_keywords": [
                "subject-specific learning",
                "temporal dynamics",
                "image customization dataset",
                "identity injection",
                "temporal modeling",
                "image-to-video training method",
                "random image token dropping",
                "randomized image initialization",
                "image-to-video fine-tuning",
                "stochastic switching",
                "joint optimization",
                "catastrophic forgetting",
                "subject consistency",
                "zero-shot settings"
            ]
        },
        "publishedAt": "2025-04-23T02:48:31.000Z",
        "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
        "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17816.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "fullname": "Daneul Kim",
            "name": "carpedkm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.17768",
            "authors": [
                {
                    "_id": "680f2668db85fd31cd5080ff",
                    "name": "Piotr Nawrot",
                    "hidden": false
                },
                {
                    "_id": "680f2668db85fd31cd508100",
                    "name": "Robert Li",
                    "hidden": false
                },
                {
                    "_id": "680f2668db85fd31cd508101",
                    "name": "Renjie Huang",
                    "hidden": false
                },
                {
                    "_id": "680f2668db85fd31cd508102",
                    "name": "Sebastian Ruder",
                    "hidden": false
                },
                {
                    "_id": "680f2668db85fd31cd508103",
                    "name": "Kelly Marchisio",
                    "hidden": false
                },
                {
                    "_id": "680f2668db85fd31cd508104",
                    "user": {
                        "_id": "60809ad44ad99100d63ce36a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619040921084-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Edoardo Maria Ponti",
                        "user": "ducdauge",
                        "type": "user"
                    },
                    "name": "Edoardo M. Ponti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:50:11.195Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T17:39:25.000Z",
            "submittedOnDailyAt": "2025-04-28T05:26:26.185Z",
            "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
            "submittedOnDailyBy": {
                "_id": "640deb5d3c82bd463ee44735",
                "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
                "isPro": false,
                "fullname": "Piotr Nawrot",
                "user": "pnawrot",
                "type": "user"
            },
            "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
            "upvotes": 8,
            "discussionId": "680f2669db85fd31cd50815e",
            "ai_keywords": [
                "Sparse attention",
                "Transformer LLMs",
                "Training-free",
                "IsoFLOPS analysis",
                "Sequence lengths",
                "Sparsity levels",
                "Long-sequence tasks",
                "Natural language",
                "Accuracy preservation",
                "Decoding",
                "Prefilling",
                "Budget adaptivity",
                "Performance degradation",
                "Scaling laws"
            ]
        },
        "publishedAt": "2025-04-24T13:39:25.000Z",
        "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
        "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "640deb5d3c82bd463ee44735",
            "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
            "fullname": "Piotr Nawrot",
            "name": "pnawrot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.18425",
            "authors": [
                {
                    "_id": "680f790454e7aa920953b2c2",
                    "name": "KimiTeam",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c3",
                    "name": "Ding Ding",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c4",
                    "name": "Zeqian Ju",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c5",
                    "name": "Yichong Leng",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c6",
                    "name": "Songxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c7",
                    "name": "Tong Liu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c8",
                    "name": "Zeyu Shang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2c9",
                    "name": "Kai Shen",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2ca",
                    "name": "Wei Song",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2cb",
                    "name": "Xu Tan",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2cc",
                    "name": "Heyi Tang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2cd",
                    "name": "Zhengtao Wang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2ce",
                    "name": "Chu Wei",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2cf",
                    "name": "Yifei Xin",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d0",
                    "name": "Xinran Xu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d1",
                    "name": "Jianwei Yu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d2",
                    "name": "Yutao Zhang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d3",
                    "name": "Xinyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d4",
                    "name": "Y. Charles",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d5",
                    "name": "Jun Chen",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d6",
                    "name": "Yanru Chen",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d7",
                    "name": "Yulun Du",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d8",
                    "name": "Weiran He",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2d9",
                    "name": "Zhenxing Hu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2da",
                    "name": "Guokun Lai",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2db",
                    "name": "Qingcheng Li",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2dc",
                    "name": "Yangyang Liu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2dd",
                    "name": "Weidong Sun",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2de",
                    "name": "Jianzhou Wang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2df",
                    "name": "Yuzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e0",
                    "name": "Yuefeng Wu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e1",
                    "name": "Yuxin Wu",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e2",
                    "name": "Dongchao Yang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e3",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e4",
                    "name": "Ying Yang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e5",
                    "name": "Zhilin Yang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e6",
                    "name": "Aoxiong Yin",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e7",
                    "name": "Ruibin Yuan",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e8",
                    "name": "Yutong Zhang",
                    "hidden": false
                },
                {
                    "_id": "680f790454e7aa920953b2e9",
                    "name": "Zaida Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-25T15:31:46.000Z",
            "submittedOnDailyAt": "2025-04-28T11:19:12.806Z",
            "title": "Kimi-Audio Technical Report",
            "submittedOnDailyBy": {
                "_id": "5f1040b6e9d71719e3be71d2",
                "avatarUrl": "/avatars/a2f28940236ae625ed3810ad62e343ff.svg",
                "isPro": false,
                "fullname": "Xu Tan",
                "user": "xutan",
                "type": "user"
            },
            "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.",
            "upvotes": 7,
            "discussionId": "680f790554e7aa920953b32e",
            "ai_keywords": [
                "audio tokenizer",
                "LLM-based architecture",
                "continuous features",
                "discrete tokens",
                "chunk-wise streaming detokenizer",
                "flow matching",
                "pre-training dataset",
                "audio data",
                "speech",
                "sound",
                "music",
                "high-quality post-training data",
                "continual pre-training",
                "fine-tuned",
                "audio-related tasks",
                "state-of-the-art performance",
                "speech recognition",
                "audio understanding",
                "audio question answering",
                "speech conversation"
            ]
        },
        "publishedAt": "2025-04-25T11:31:46.000Z",
        "title": "Kimi-Audio Technical Report",
        "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18425.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1040b6e9d71719e3be71d2",
            "avatarUrl": "/avatars/a2f28940236ae625ed3810ad62e343ff.svg",
            "fullname": "Xu Tan",
            "name": "xutan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.15716",
            "authors": [
                {
                    "_id": "680dcc5d3478de07603a8036",
                    "user": {
                        "_id": "642656cbad1e3b0e6e91b752",
                        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
                        "isPro": false,
                        "fullname": "Jie Zhu",
                        "user": "amazingj",
                        "type": "user"
                    },
                    "name": "Jie Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T07:39:02.713Z",
                    "hidden": false
                },
                {
                    "_id": "680dcc5d3478de07603a8037",
                    "name": "Qian Chen",
                    "hidden": false
                },
                {
                    "_id": "680dcc5d3478de07603a8038",
                    "name": "Huaixia Dou",
                    "hidden": false
                },
                {
                    "_id": "680dcc5d3478de07603a8039",
                    "name": "Junhui Li",
                    "hidden": false
                },
                {
                    "_id": "680dcc5d3478de07603a803a",
                    "name": "Lifan Guo",
                    "hidden": false
                },
                {
                    "_id": "680dcc5d3478de07603a803b",
                    "name": "Feng Chen",
                    "hidden": false
                },
                {
                    "_id": "680dcc5d3478de07603a803c",
                    "name": "Chi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T09:01:04.000Z",
            "submittedOnDailyAt": "2025-04-28T06:16:26.234Z",
            "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "642656cbad1e3b0e6e91b752",
                "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
                "isPro": false,
                "fullname": "Jie Zhu",
                "user": "amazingj",
                "type": "user"
            },
            "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
            "upvotes": 7,
            "discussionId": "680dcc5e3478de07603a807e",
            "ai_keywords": [
                "reasoning-enhanced framework",
                "reasoning-augmented supervision",
                "reinforcement learning",
                "DianJin-R1-Data",
                "CFLUE",
                "FinQA",
                "Chinese Compliance Check (CCC)",
                "high-quality dataset",
                "DianJin-R1-7B",
                "DianJin-R1-32B",
                "Qwen2.5-7B-Instruct",
                "Qwen2.5-32B-Instruct",
                "structured format",
                "reasoning steps",
                "Group Relative Policy Optimization (GRPO)",
                "dual reward signals",
                "structured outputs",
                "answer correctness",
                "MATH-500",
                "GPQA-Diamond",
                "financial datasets",
                "single-call reasoning models",
                "multi-agent systems",
                "real-world applications"
            ]
        },
        "publishedAt": "2025-04-22T05:01:04.000Z",
        "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
        "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "fullname": "Jie Zhu",
            "name": "amazingj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.12080",
            "authors": [
                {
                    "_id": "680afc5f2c4b584e1d786eee",
                    "name": "Mengshi Qi",
                    "hidden": false
                },
                {
                    "_id": "680afc5f2c4b584e1d786eef",
                    "user": {
                        "_id": "66a8c8e4f5cda7b8690205ef",
                        "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
                        "isPro": false,
                        "fullname": "Pengfei Zhu",
                        "user": "zaplm",
                        "type": "user"
                    },
                    "name": "Pengfei Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-25T08:34:30.195Z",
                    "hidden": false
                },
                {
                    "_id": "680afc5f2c4b584e1d786ef0",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "680afc5f2c4b584e1d786ef1",
                    "name": "Xiaoyang Bi",
                    "hidden": false
                },
                {
                    "_id": "680afc5f2c4b584e1d786ef2",
                    "name": "Lu Qi",
                    "hidden": false
                },
                {
                    "_id": "680afc5f2c4b584e1d786ef3",
                    "name": "Huadong Ma",
                    "hidden": false
                },
                {
                    "_id": "680afc5f2c4b584e1d786ef4",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-16T13:41:59.000Z",
            "submittedOnDailyAt": "2025-04-28T02:11:37.182Z",
            "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
            "submittedOnDailyBy": {
                "_id": "66a8c8e4f5cda7b8690205ef",
                "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
                "isPro": false,
                "fullname": "Pengfei Zhu",
                "user": "zaplm",
                "type": "user"
            },
            "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
            "upvotes": 6,
            "discussionId": "680afc622c4b584e1d786f9e",
            "ai_keywords": [
                "prompt-tuning",
                "prompt encoder",
                "mask prior",
                "cycle-consistent cross-attention",
                "dual-branch design",
                "discriminative positive prompts",
                "negative prompts",
                "mask-tube",
                "In-Context Video Object Segmentation (IC-VOS)",
                "mIoU"
            ]
        },
        "publishedAt": "2025-04-16T09:41:59.000Z",
        "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
        "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a8c8e4f5cda7b8690205ef",
            "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
            "fullname": "Pengfei Zhu",
            "name": "zaplm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.18225",
            "authors": [
                {
                    "_id": "680fa50d618d5374f2c39ca9",
                    "name": "Pierre-Carl Langlais",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39caa",
                    "name": "Pavel Chizhov",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39cab",
                    "name": "Mattia Nee",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39cac",
                    "name": "Carlos Rosas Hinostroza",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39cad",
                    "name": "Matthieu Delsart",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39cae",
                    "name": "Irne Girard",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39caf",
                    "name": "Othman Hicheur",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39cb0",
                    "name": "Anastasia Stasenko",
                    "hidden": false
                },
                {
                    "_id": "680fa50d618d5374f2c39cb1",
                    "name": "Ivan P. Yamshchikov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-25T10:17:04.000Z",
            "submittedOnDailyAt": "2025-04-28T14:26:51.850Z",
            "title": "Even Small Reasoners Should Quote Their Sources: Introducing the\n  Pleias-RAG Model Family",
            "submittedOnDailyBy": {
                "_id": "64ce091a9e9ca8123d7a42b0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ce091a9e9ca8123d7a42b0/OEPggp82RwigxNLL35LgT.jpeg",
                "isPro": false,
                "fullname": "Pierre-Carl Langlais",
                "user": "Pclanglais",
                "type": "user"
            },
            "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI.",
            "upvotes": 4,
            "discussionId": "680fa50e618d5374f2c39d11"
        },
        "publishedAt": "2025-04-25T06:17:04.000Z",
        "title": "Even Small Reasoners Should Quote Their Sources: Introducing the\n  Pleias-RAG Model Family",
        "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18225.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ce091a9e9ca8123d7a42b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ce091a9e9ca8123d7a42b0/OEPggp82RwigxNLL35LgT.jpeg",
            "fullname": "Pierre-Carl Langlais",
            "name": "Pclanglais",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 225
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17025",
            "authors": [
                {
                    "_id": "680cb47facf3c58ba7e13b73",
                    "name": "Luca Moroni",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b74",
                    "name": "Giovanni Puccetti",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b75",
                    "name": "Pere-Lluis Huguet Cabot",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b76",
                    "name": "Andrei Stefan Bejgu",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b77",
                    "name": "Edoardo Barba",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b78",
                    "user": {
                        "_id": "62e3fe792a8df5b22feed15e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3fe792a8df5b22feed15e/iy8J4oGhW8Nr9QAgoH8ES.jpeg",
                        "isPro": false,
                        "fullname": "Alessio Miaschi",
                        "user": "alemiaschi",
                        "type": "user"
                    },
                    "name": "Alessio Miaschi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-28T12:50:18.302Z",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b79",
                    "name": "Felice Dell'Orletta",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b7a",
                    "name": "Andrea Esuli",
                    "hidden": false
                },
                {
                    "_id": "680cb47facf3c58ba7e13b7b",
                    "name": "Roberto Navigli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-23T18:12:27.000Z",
            "submittedOnDailyAt": "2025-04-28T10:38:16.730Z",
            "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing\n  Efficiency Through Vocabulary Adaptation",
            "submittedOnDailyBy": {
                "_id": "62e3fe792a8df5b22feed15e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3fe792a8df5b22feed15e/iy8J4oGhW8Nr9QAgoH8ES.jpeg",
                "isPro": false,
                "fullname": "Alessio Miaschi",
                "user": "alemiaschi",
                "type": "user"
            },
            "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks.",
            "upvotes": 3,
            "discussionId": "680cb480acf3c58ba7e13bc8",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "language contamination",
                "multilingual pretraining data",
                "semantic alignment",
                "vocabulary adaptation",
                "neural mapping",
                "vocabulary substitution",
                "token fertility",
                "Mistral-7b-v0.1",
                "Llama-3.1-8B",
                "parameter reduction",
                "continual training",
                "grounded alignment strategies"
            ]
        },
        "publishedAt": "2025-04-23T14:12:27.000Z",
        "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing\n  Efficiency Through Vocabulary Adaptation",
        "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17025.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62e3fe792a8df5b22feed15e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3fe792a8df5b22feed15e/iy8J4oGhW8Nr9QAgoH8ES.jpeg",
            "fullname": "Alessio Miaschi",
            "name": "alemiaschi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    }
]