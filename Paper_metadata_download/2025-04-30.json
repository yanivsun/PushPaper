[
    {
        "paper": {
            "id": "2504.20571",
            "authors": [
                {
                    "_id": "681187ddda5ce4cbd7556714",
                    "user": {
                        "_id": "653586fae778506c5b38a3f1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653586fae778506c5b38a3f1/GL_RShZhAkEZmIinA5_8E.jpeg",
                        "isPro": false,
                        "fullname": "Yiping Wang",
                        "user": "ypwang61",
                        "type": "user"
                    },
                    "name": "Yiping Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:58:59.486Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556715",
                    "user": {
                        "_id": "673a83b99e6f1c0d81a771fc",
                        "avatarUrl": "/avatars/f3d8e1bf7d4c36b21adee632ea12ffe0.svg",
                        "isPro": false,
                        "fullname": "Qing Yang",
                        "user": "hushqyang",
                        "type": "user"
                    },
                    "name": "Qing Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:17.953Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556716",
                    "user": {
                        "_id": "64a85e23b6512b8328f9d9e2",
                        "avatarUrl": "/avatars/4a6b35752d3f76cb03278f52b3b43426.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Zeng",
                        "user": "ZhiyuanZeng",
                        "type": "user"
                    },
                    "name": "Zhiyuan Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:12.620Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556717",
                    "user": {
                        "_id": "63815eff4761ddfa00903762",
                        "avatarUrl": "/avatars/3419b239d42e091586f1c51b526d88e5.svg",
                        "isPro": false,
                        "fullname": "Liliang Ren",
                        "user": "renll",
                        "type": "user"
                    },
                    "name": "Liliang Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:18.310Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556718",
                    "name": "Lucas Liu",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556719",
                    "user": {
                        "_id": "61942296d5c2ba6daa290357",
                        "avatarUrl": "/avatars/594021cc183c4922d48b46f43772a062.svg",
                        "isPro": false,
                        "fullname": "Baolin Peng",
                        "user": "Baolin",
                        "type": "user"
                    },
                    "name": "Baolin Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:45.735Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671a",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671b",
                    "user": {
                        "_id": "6310493158d83e8f64dc8c55",
                        "avatarUrl": "/avatars/5f91ac4dfec0d6a5bf7bad6094f0fd0f.svg",
                        "isPro": false,
                        "fullname": "Xuehai He",
                        "user": "Xuehai",
                        "type": "user"
                    },
                    "name": "Xuehai He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:52.344Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671c",
                    "user": {
                        "_id": "633523b131a2be3938ca1016",
                        "avatarUrl": "/avatars/06a18f80927289bb949d9f19ffdc4bda.svg",
                        "isPro": false,
                        "fullname": "Kuan Wang",
                        "user": "Keynes",
                        "type": "user"
                    },
                    "name": "Kuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T09:59:58.392Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671d",
                    "user": {
                        "_id": "641904caf9d6f1d772ec7af7",
                        "avatarUrl": "/avatars/4a63eac71eb30f70b1a0e9d4708f26c1.svg",
                        "isPro": false,
                        "fullname": "Jianfeng Gao",
                        "user": "wyngjf",
                        "type": "user"
                    },
                    "name": "Jianfeng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:04.685Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671e",
                    "user": {
                        "_id": "64da876370446182be5b608d",
                        "avatarUrl": "/avatars/e412fdc71404ecdf638e416846e3ebfb.svg",
                        "isPro": false,
                        "fullname": "Weizhu Chen",
                        "user": "chenweizhu",
                        "type": "user"
                    },
                    "name": "Weizhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:10.823Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd755671f",
                    "user": {
                        "_id": "6463b2247572c66a8e625a57",
                        "avatarUrl": "/avatars/7722fb5649d42d966ce1e478946d5f8f.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Shuohang",
                        "type": "user"
                    },
                    "name": "Shuohang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:19.855Z",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556720",
                    "name": "Simon Shaolei Du",
                    "hidden": false
                },
                {
                    "_id": "681187ddda5ce4cbd7556721",
                    "user": {
                        "_id": "6454c337a13edf669cd5d8ea",
                        "avatarUrl": "/avatars/a383a0dda7c2ef6a0d6c3c64651f42ff.svg",
                        "isPro": false,
                        "fullname": "Yelong Shen",
                        "user": "uuu6",
                        "type": "user"
                    },
                    "name": "Yelong Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:00:33.179Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T09:24:30.000Z",
            "submittedOnDailyAt": "2025-04-30T00:46:23.617Z",
            "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
            "upvotes": 48,
            "discussionId": "681187ddda5ce4cbd7556754",
            "githubRepo": "https://github.com/ypwang61/One-Shot-RLVR",
            "ai_keywords": [
                "reinforcement learning with verifiable reward (RLVR)",
                "1-shot RLVR",
                "large language models (LLMs)",
                "Qwen2.5-Math-1.5B",
                "MATH500",
                "mathematical reasoning benchmarks",
                "Qwen2.5-Math-7B",
                "Llama3.2-3B-Instruct",
                "DeepSeek-R1-Distill-Qwen-1.5B",
                "GRPO",
                "PPO",
                "cross-domain generalization",
                "self-reflection",
                "post-saturation generalization",
                "policy gradient loss",
                "entropic exploration",
                "entropy loss"
            ]
        },
        "publishedAt": "2025-04-29T05:24:30.000Z",
        "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
        "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20571.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6751
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20734",
            "authors": [
                {
                    "_id": "6811966ae20ba7d0683b8adc",
                    "user": {
                        "_id": "66d30f5fad293ffc4b7672bc",
                        "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
                        "isPro": false,
                        "fullname": "Woongyeong Yeo",
                        "user": "wgcyeo",
                        "type": "user"
                    },
                    "name": "Woongyeong Yeo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:03.853Z",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8add",
                    "user": {
                        "_id": "66ed7737f2f27a5dfd81ef09",
                        "avatarUrl": "/avatars/f45eea356e92ac7b3db23c2c92dec9fa.svg",
                        "isPro": false,
                        "fullname": "Kangsan Kim",
                        "user": "KangsanKim71",
                        "type": "user"
                    },
                    "name": "Kangsan Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:00.948Z",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8ade",
                    "name": "Soyeong Jeong",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8adf",
                    "user": {
                        "_id": "63036b6c5c70c21d0ea79d48",
                        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                        "isPro": false,
                        "fullname": "Jinheon Baek",
                        "user": "jinheon",
                        "type": "user"
                    },
                    "name": "Jinheon Baek",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:07.351Z",
                    "hidden": false
                },
                {
                    "_id": "6811966ae20ba7d0683b8ae0",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T13:18:58.000Z",
            "submittedOnDailyAt": "2025-04-30T01:50:28.624Z",
            "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
            "submittedOnDailyBy": {
                "_id": "66d30f5fad293ffc4b7672bc",
                "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
                "isPro": false,
                "fullname": "Woongyeong Yeo",
                "user": "wgcyeo",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
            "upvotes": 42,
            "discussionId": "6811966ae20ba7d0683b8b0e",
            "projectPage": "https://universalrag.github.io",
            "githubRepo": "https://github.com/wgcyeo/UniversalRAG",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "factual accuracy",
                "external knowledge",
                "text-only corpus",
                "modality-specific corpus",
                "heterogenous sources",
                "diverse modalities",
                "granularities",
                "modality gap",
                "modality-aware routing mechanism",
                "targeted retrieval",
                "granularity levels",
                "fine-tuned retrieval",
                "multi-modal benchmarks",
                "modality-specific baselines",
                "unified baselines"
            ]
        },
        "publishedAt": "2025-04-29T09:18:58.000Z",
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
        "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20734.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66d30f5fad293ffc4b7672bc",
            "avatarUrl": "/avatars/6f164d813b947940a088820f8fd4dbe8.svg",
            "fullname": "Woongyeong Yeo",
            "name": "wgcyeo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20595",
            "authors": [
                {
                    "_id": "68118a9f4570c2ba44bf4418",
                    "user": {
                        "_id": "6334a0bd31a2be3938c59537",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334a0bd31a2be3938c59537/kSetFUWAmJbPQ1KSlNKBr.jpeg",
                        "isPro": false,
                        "fullname": "Rulin Shao",
                        "user": "rulins",
                        "type": "user"
                    },
                    "name": "Rulin Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:02:20.688Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4419",
                    "user": {
                        "_id": "64ff618c35ec9717626d1431",
                        "avatarUrl": "/avatars/941befd75925d6b691133f84cce525f9.svg",
                        "isPro": false,
                        "fullname": "Rui Qiao",
                        "user": "volpato30",
                        "type": "user"
                    },
                    "name": "Rui Qiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:02:05.204Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441a",
                    "name": "Varsha Kishore",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441b",
                    "user": {
                        "_id": "5f1eb362eec0ad2a071ad6e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
                        "isPro": false,
                        "fullname": "Niklas Muennighoff",
                        "user": "Muennighoff",
                        "type": "user"
                    },
                    "name": "Niklas Muennighoff",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:02:27.811Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441c",
                    "name": "Xi Victoria Lin",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441d",
                    "name": "Daniela Rus",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441e",
                    "name": "Bryan Kian Hsiang Low",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf441f",
                    "user": {
                        "_id": "63a76d0de27a6dbd485fe863",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg",
                        "isPro": false,
                        "fullname": "Sewon Min",
                        "user": "sewon",
                        "type": "user"
                    },
                    "name": "Sewon Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:01:46.233Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4420",
                    "name": "Wen-tau Yih",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4421",
                    "user": {
                        "_id": "641b4263abfce26bcf7b27de",
                        "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg",
                        "isPro": false,
                        "fullname": "Pang Wei Koh",
                        "user": "pangwei",
                        "type": "user"
                    },
                    "name": "Pang Wei Koh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:01:57.373Z",
                    "hidden": false
                },
                {
                    "_id": "68118a9f4570c2ba44bf4422",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T09:49:28.000Z",
            "submittedOnDailyAt": "2025-04-30T00:58:16.950Z",
            "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
            "upvotes": 36,
            "discussionId": "68118aa44570c2ba44bf457b",
            "ai_keywords": [
                "retriever",
                "ReasonIR-8B",
                "general reasoning tasks",
                "synthetic data generation pipeline",
                "hard negative",
                "nDCG@10",
                "BRIGHT",
                "information retrieval (IR) benchmark",
                "RAG tasks",
                "MMLU",
                "GPQA",
                "closed-book baseline",
                "LLM reranker",
                "test-time compute",
                "rewritten queries",
                "LLM"
            ]
        },
        "publishedAt": "2025-04-29T05:49:28.000Z",
        "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
        "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20595.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6751
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.20879",
            "authors": [
                {
                    "_id": "6811ae6b7f4f553788e905b8",
                    "user": {
                        "_id": "62c2175d756039dd0dd20509",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677753847837-62c2175d756039dd0dd20509.jpeg",
                        "isPro": false,
                        "fullname": "Shivalika Singh",
                        "user": "shivalikasingh",
                        "type": "user"
                    },
                    "name": "Shivalika Singh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:19:56.166Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905b9",
                    "user": {
                        "_id": "66ad9237091a147c4f154dbb",
                        "avatarUrl": "/avatars/c80258e877bafc08452f241a6ecfec04.svg",
                        "isPro": false,
                        "fullname": "Yiyang Nan",
                        "user": "olivernan",
                        "type": "user"
                    },
                    "name": "Yiyang Nan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:27.669Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905ba",
                    "user": {
                        "_id": "62fdf36a594a7b92e671e3b5",
                        "avatarUrl": "/avatars/d3e0b109f4fc7da45873d599659ae5b5.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "AlexWang",
                        "type": "user"
                    },
                    "name": "Alex Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:38.051Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bb",
                    "user": {
                        "_id": "6658011eaba105a066e37e1b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
                        "isPro": false,
                        "fullname": "Daniel D'souza",
                        "user": "dsouzadaniel",
                        "type": "user"
                    },
                    "name": "Daniel D'Souza",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:45.388Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bc",
                    "user": {
                        "_id": "630906fde3246e2be88bfab3",
                        "avatarUrl": "/avatars/bca882c027d17f7feba837baae71ec2d.svg",
                        "isPro": false,
                        "fullname": "Sayash Kapoor",
                        "user": "sayashk",
                        "type": "user"
                    },
                    "name": "Sayash Kapoor",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:51.466Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bd",
                    "user": {
                        "_id": "60d35d7ad7b174177faabd5b",
                        "avatarUrl": "/avatars/4e5403b9d4a845a2d21e8217dc3c16d2.svg",
                        "isPro": false,
                        "fullname": "Ahmet Üstün",
                        "user": "ahmetu",
                        "type": "user"
                    },
                    "name": "Ahmet Üstün",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:20:58.423Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905be",
                    "user": {
                        "_id": "64931e7e2da595588288f161",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64931e7e2da595588288f161/4jOhJOFsU7RVFMgGk5kO7.jpeg",
                        "isPro": false,
                        "fullname": "Sanmi Koyejo",
                        "user": "sanmikoyejo",
                        "type": "user"
                    },
                    "name": "Sanmi Koyejo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:04.000Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905bf",
                    "user": {
                        "_id": "63081e15a670ed10f9d44229",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
                        "isPro": true,
                        "fullname": "Yuntian Deng",
                        "user": "yuntian-deng",
                        "type": "user"
                    },
                    "name": "Yuntian Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T09:56:42.033Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c0",
                    "user": {
                        "_id": "61f4283a81c4d30f58140242",
                        "avatarUrl": "/avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg",
                        "isPro": false,
                        "fullname": "Shayne Longpre",
                        "user": "Shayne",
                        "type": "user"
                    },
                    "name": "Shayne Longpre",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:11.011Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c1",
                    "name": "Noah Smith",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c2",
                    "user": {
                        "_id": "6634e8c750ea73bfbc59223e",
                        "avatarUrl": "/avatars/9f0a7bcdfd82d5344932437d458fce9f.svg",
                        "isPro": false,
                        "fullname": "Beyza Ermis",
                        "user": "beyzaermis",
                        "type": "user"
                    },
                    "name": "Beyza Ermis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:23.787Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c3",
                    "user": {
                        "_id": "6441042d5d600fb0951a5f99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6441042d5d600fb0951a5f99/4CbOaYcEz99BtVAQvnGTn.jpeg",
                        "isPro": false,
                        "fullname": "Marzieh Fadaee",
                        "user": "MarziehFadaee",
                        "type": "user"
                    },
                    "name": "Marzieh Fadaee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:31.242Z",
                    "hidden": false
                },
                {
                    "_id": "6811ae6b7f4f553788e905c4",
                    "user": {
                        "_id": "63434eb76f59b79da07dbddf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63434eb76f59b79da07dbddf/BEwmVjqPNYlqmutXG0G6e.jpeg",
                        "isPro": false,
                        "fullname": "Sara Hooker",
                        "user": "sarahooker",
                        "type": "user"
                    },
                    "name": "Sara Hooker",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:37.932Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T15:48:49.000Z",
            "submittedOnDailyAt": "2025-04-30T03:36:53.331Z",
            "title": "The Leaderboard Illusion",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
            "upvotes": 34,
            "discussionId": "6811ae6c7f4f553788e905fc"
        },
        "publishedAt": "2025-04-29T11:48:49.000Z",
        "title": "The Leaderboard Illusion",
        "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20879.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 77
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.20157",
            "authors": [
                {
                    "_id": "68119750ff0764f3840a7f93",
                    "user": {
                        "_id": "61e0c5053a1781f66b4e9aed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642120523097-61e0c5053a1781f66b4e9aed.jpeg",
                        "isPro": false,
                        "fullname": "Zae Myung Kim",
                        "user": "zaemyung",
                        "type": "user"
                    },
                    "name": "Zae Myung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:55:58.262Z",
                    "hidden": false
                },
                {
                    "_id": "68119750ff0764f3840a7f94",
                    "name": "Chanwoo Park",
                    "hidden": false
                },
                {
                    "_id": "68119750ff0764f3840a7f95",
                    "user": {
                        "_id": "60985a0547dc3dbf8a976607",
                        "avatarUrl": "/avatars/3c37bf4b7c9db83a46af7c473ee4eb86.svg",
                        "isPro": false,
                        "fullname": "Vipul Raheja",
                        "user": "machineteacher",
                        "type": "user"
                    },
                    "name": "Vipul Raheja",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:03:14.631Z",
                    "hidden": false
                },
                {
                    "_id": "68119750ff0764f3840a7f96",
                    "user": {
                        "_id": "64356b40a4bd75c62cbc5926",
                        "avatarUrl": "/avatars/5f4c603464e9c8ad613a3a25fa4cacbf.svg",
                        "isPro": false,
                        "fullname": "Dongyeop Kang",
                        "user": "dykang",
                        "type": "user"
                    },
                    "name": "Dongyeop Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:03:26.612Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
            ],
            "publishedAt": "2025-04-28T18:02:35.000Z",
            "submittedOnDailyAt": "2025-04-30T02:04:16.540Z",
            "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
            "submittedOnDailyBy": {
                "_id": "6434b6619bd5a84b5dcfa4de",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
                "isPro": true,
                "fullname": "Young-Jun Lee",
                "user": "passing2961",
                "type": "user"
            },
            "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
            "upvotes": 28,
            "discussionId": "68119751ff0764f3840a7fc5",
            "ai_keywords": [
                "Meta Policy Optimization (MPO)",
                "meta-reward model",
                "reward hacking",
                "prompt engineering",
                "policy optimization",
                "adaptive reward signal",
                "meta-learning approach",
                "prompt design",
                "reward-based RL alignment",
                "question answering",
                "mathematical reasoning"
            ]
        },
        "publishedAt": "2025-04-28T14:02:35.000Z",
        "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving\n  Reward Models",
        "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/tHS8gWUK0ptmNTs6lZck6.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6434b6619bd5a84b5dcfa4de/uMD9av8pogPwYTW-KNFJ2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20157.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "6434b6619bd5a84b5dcfa4de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434b6619bd5a84b5dcfa4de/h8Q6kPNjFNc03wmdboHzq.jpeg",
            "fullname": "Young-Jun Lee",
            "name": "passing2961",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20995",
            "authors": [
                {
                    "_id": "68118c049c2765c9323de70b",
                    "user": {
                        "_id": "6437c7dae282b4a48eaf065e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c7dae282b4a48eaf065e/AxodKQXyrviTFQRyjnL01.jpeg",
                        "isPro": false,
                        "fullname": "Haoyu Zhen",
                        "user": "anyeZHY",
                        "type": "user"
                    },
                    "name": "Haoyu Zhen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:04:19.238Z",
                    "hidden": false
                },
                {
                    "_id": "68118c049c2765c9323de70c",
                    "name": "Qiao Sun",
                    "hidden": false
                },
                {
                    "_id": "68118c049c2765c9323de70d",
                    "name": "Hongxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68118c049c2765c9323de70e",
                    "name": "Junyan Li",
                    "hidden": false
                },
                {
                    "_id": "68118c049c2765c9323de70f",
                    "name": "Siyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68118c049c2765c9323de710",
                    "user": {
                        "_id": "63c9bd445fdc575773c732fe",
                        "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
                        "isPro": false,
                        "fullname": "Yilun Du",
                        "user": "yilundu",
                        "type": "user"
                    },
                    "name": "Yilun Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T10:03:42.041Z",
                    "hidden": false
                },
                {
                    "_id": "68118c049c2765c9323de711",
                    "name": "Chuang Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T17:59:30.000Z",
            "submittedOnDailyAt": "2025-04-30T01:05:28.658Z",
            "title": "TesserAct: Learning 4D Embodied World Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
            "upvotes": 11,
            "discussionId": "68118c089c2765c9323de81d",
            "ai_keywords": [
                "embodied world models",
                "4D world models",
                "RGB-DN (RGB, Depth, and Normal) videos",
                "video generation model",
                "inverse dynamic models",
                "robotic manipulation video datasets",
                "temporal coherence",
                "spatial coherence",
                "novel view synthesis",
                "policy learning"
            ]
        },
        "publishedAt": "2025-04-29T13:59:30.000Z",
        "title": "TesserAct: Learning 4D Embodied World Models",
        "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20995.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6751
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.20998",
            "authors": [
                {
                    "_id": "6811899ba6198824c5589ed7",
                    "name": "Thao Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6811899ba6198824c5589ed8",
                    "name": "Krishna Kumar Singh",
                    "hidden": false
                },
                {
                    "_id": "6811899ba6198824c5589ed9",
                    "name": "Jing Shi",
                    "hidden": false
                },
                {
                    "_id": "6811899ba6198824c5589eda",
                    "name": "Trung Bui",
                    "hidden": false
                },
                {
                    "_id": "6811899ba6198824c5589edb",
                    "user": {
                        "_id": "649f41ee70a478f8b36b2984",
                        "avatarUrl": "/avatars/1c9a76717a450ac4aeb62a1e823d2e4a.svg",
                        "isPro": false,
                        "fullname": "Yong Jae Lee",
                        "user": "yjlee0222",
                        "type": "user"
                    },
                    "name": "Yong Jae Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:19:30.281Z",
                    "hidden": false
                },
                {
                    "_id": "6811899ba6198824c5589edc",
                    "user": {
                        "_id": "63043c140547362a22a91137",
                        "avatarUrl": "/avatars/d817f4e5f65c3fab7cef2fd6a3c72632.svg",
                        "isPro": false,
                        "fullname": "Yuheng Li",
                        "user": "Yuheng",
                        "type": "user"
                    },
                    "name": "Yuheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:19:20.416Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T17:59:57.000Z",
            "submittedOnDailyAt": "2025-04-30T00:54:06.806Z",
            "title": "YoChameleon: Personalized Vision and Language Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
            "upvotes": 9,
            "discussionId": "6811899ca6198824c5589f45",
            "ai_keywords": [
                "soft-prompt tuning",
                "subject-specific information",
                "self-prompting optimization mechanism",
                "soft-positive image generation approach"
            ]
        },
        "publishedAt": "2025-04-29T13:59:57.000Z",
        "title": "YoChameleon: Personalized Vision and Language Generation",
        "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20998.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6751
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.20630",
            "authors": [
                {
                    "_id": "6811ea47f8ca0d9acb45374b",
                    "user": {
                        "_id": "66569729ea21cfae5f5797c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
                        "isPro": false,
                        "fullname": "Yu Zhang",
                        "user": "AaronZ345",
                        "type": "user"
                    },
                    "name": "Yu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T09:56:39.194Z",
                    "hidden": false
                },
                {
                    "_id": "6811ea47f8ca0d9acb45374c",
                    "user": {
                        "_id": "66692faa83408bb0da40f8f5",
                        "avatarUrl": "/avatars/09f81a8b0bb130c8fa72dbb6526ac4c4.svg",
                        "isPro": false,
                        "fullname": "wenxiang guo",
                        "user": "verstar",
                        "type": "user"
                    },
                    "name": "Wenxiang Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:22:24.333Z",
                    "hidden": false
                },
                {
                    "_id": "6811ea47f8ca0d9acb45374d",
                    "user": {
                        "_id": "66568060c6a8cb4e884be331",
                        "avatarUrl": "/avatars/fe8de503648dda54307fec90cace7c07.svg",
                        "isPro": false,
                        "fullname": "PanChanghao",
                        "user": "DavidPigeon",
                        "type": "user"
                    },
                    "name": "Changhao Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:22:36.396Z",
                    "hidden": false
                },
                {
                    "_id": "6811ea47f8ca0d9acb45374e",
                    "user": {
                        "_id": "66984c6a03421b737aa78f24",
                        "avatarUrl": "/avatars/9c92b65469c0ef5f05a4c3c45a6dec97.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Zhu",
                        "user": "zhiyuanzhu2019",
                        "type": "user"
                    },
                    "name": "Zhiyuan Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:22:42.999Z",
                    "hidden": false
                },
                {
                    "_id": "6811ea47f8ca0d9acb45374f",
                    "name": "Tao Jin",
                    "hidden": false
                },
                {
                    "_id": "6811ea47f8ca0d9acb453750",
                    "name": "Zhou Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T10:56:44.000Z",
            "submittedOnDailyAt": "2025-04-30T07:54:10.120Z",
            "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
            "submittedOnDailyBy": {
                "_id": "66569729ea21cfae5f5797c4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
                "isPro": false,
                "fullname": "Yu Zhang",
                "user": "AaronZ345",
                "type": "user"
            },
            "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo.",
            "upvotes": 9,
            "discussionId": "6811ea48f8ca0d9acb4537cf",
            "projectPage": "https://aaronz345.github.io/ISDramaDemo/",
            "ai_keywords": [
                "multimodal inputs",
                "binaural speech",
                "dramatic prosody",
                "multimodal prompts",
                "multimodal recorded dataset",
                "contrastive learning",
                "Doppler effect",
                "Multimodal Pose Encoder",
                "flow-based model",
                "mamba-transformer",
                "Drama-MOE",
                "classifier-free guidance",
                "context-consistent guidance"
            ]
        },
        "publishedAt": "2025-04-29T06:56:44.000Z",
        "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
        "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20630.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66569729ea21cfae5f5797c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66569729ea21cfae5f5797c4/IguwJzljFN3QiEd1bn5BP.jpeg",
            "fullname": "Yu Zhang",
            "name": "AaronZ345",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16046",
            "authors": [
                {
                    "_id": "68119c70e8a3493171fadce2",
                    "user": {
                        "_id": "62fb40b59af1d16bc0ac60f4",
                        "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
                        "isPro": false,
                        "fullname": "Jack Zhang",
                        "user": "jackzhang",
                        "type": "user"
                    },
                    "name": "Jingyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:55:55.481Z",
                    "hidden": false
                },
                {
                    "_id": "68119c70e8a3493171fadce3",
                    "name": "Jiacan Yu",
                    "hidden": false
                },
                {
                    "_id": "68119c70e8a3493171fadce4",
                    "user": {
                        "_id": "63e410e88083f19a218be964",
                        "avatarUrl": "/avatars/ce271d1686f6e5ee1f5b2d429cb60de6.svg",
                        "isPro": false,
                        "fullname": "Marc Marone",
                        "user": "mmarone",
                        "type": "user"
                    },
                    "name": "Marc Marone",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:21:59.233Z",
                    "hidden": false
                },
                {
                    "_id": "68119c70e8a3493171fadce5",
                    "name": "Benjamin Van Durme",
                    "hidden": false
                },
                {
                    "_id": "68119c70e8a3493171fadce6",
                    "user": {
                        "_id": "5f6540c65e78cc6b0ed3199d",
                        "avatarUrl": "/avatars/0280d4df417855965a0964d22766c012.svg",
                        "isPro": false,
                        "fullname": "Daniel Khashabi",
                        "user": "danyaljj",
                        "type": "user"
                    },
                    "name": "Daniel Khashabi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:22:11.443Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T17:16:53.000Z",
            "submittedOnDailyAt": "2025-04-30T02:15:12.625Z",
            "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
            "submittedOnDailyBy": {
                "_id": "62fb40b59af1d16bc0ac60f4",
                "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
                "isPro": false,
                "fullname": "Jack Zhang",
                "user": "jackzhang",
                "type": "user"
            },
            "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
            "upvotes": 9,
            "discussionId": "68119c70e8a3493171fadd11",
            "ai_keywords": [
                "BloomScrub",
                "Bloom filters",
                "quotation detection",
                "rewriting techniques",
                "copyright screening",
                "adaptive abstention"
            ]
        },
        "publishedAt": "2025-04-22T13:16:53.000Z",
        "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement",
        "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fb40b59af1d16bc0ac60f4",
            "avatarUrl": "/avatars/03ff66a419db8f2bc8e89a3b47aaaeac.svg",
            "fullname": "Jack Zhang",
            "name": "jackzhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20996",
            "authors": [
                {
                    "_id": "6811c55384adfa26b82abd76",
                    "user": {
                        "_id": "637c94d3f219c71f93eda9ad",
                        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
                        "isPro": false,
                        "fullname": "Sicheng Mo",
                        "user": "Sichengmo",
                        "type": "user"
                    },
                    "name": "Sicheng Mo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:22:58.085Z",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd77",
                    "name": "Thao Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd78",
                    "name": "Xun Huang",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd79",
                    "name": "Siddharth Srinivasan Iyer",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd7a",
                    "name": "Yijun Li",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd7b",
                    "name": "Yuchen Liu",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd7c",
                    "name": "Abhishek Tandon",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd7d",
                    "name": "Eli Shechtman",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd7e",
                    "name": "Krishna Kumar Singh",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd7f",
                    "user": {
                        "_id": "649f41ee70a478f8b36b2984",
                        "avatarUrl": "/avatars/1c9a76717a450ac4aeb62a1e823d2e4a.svg",
                        "isPro": false,
                        "fullname": "Yong Jae Lee",
                        "user": "yjlee0222",
                        "type": "user"
                    },
                    "name": "Yong Jae Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:23:52.478Z",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd80",
                    "user": {
                        "_id": "64aedf11c693a19547999a59",
                        "avatarUrl": "/avatars/83a54b7c217168c91cea15dd000f4af2.svg",
                        "isPro": false,
                        "fullname": "Bolei Zhou",
                        "user": "zhoubolei",
                        "type": "user"
                    },
                    "name": "Bolei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-04-30T12:23:58.680Z",
                    "hidden": false
                },
                {
                    "_id": "6811c55384adfa26b82abd81",
                    "name": "Yuheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T17:59:45.000Z",
            "submittedOnDailyAt": "2025-04-30T05:08:54.031Z",
            "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
            "submittedOnDailyBy": {
                "_id": "637c94d3f219c71f93eda9ad",
                "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
                "isPro": false,
                "fullname": "Sicheng Mo",
                "user": "Sichengmo",
                "type": "user"
            },
            "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
            "upvotes": 7,
            "discussionId": "6811c55584adfa26b82abdfe",
            "projectPage": "https://sichengmo.github.io/XFusion/",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "multimodal tasks",
                "dual-tower design",
                "modality-specific weights",
                "vision-specific information",
                "image-to-text",
                "text-to-image",
                "understanding-focused data",
                "feature alignment",
                "unified multimodal models"
            ]
        },
        "publishedAt": "2025-04-29T13:59:45.000Z",
        "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
        "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20996.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637c94d3f219c71f93eda9ad",
            "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
            "fullname": "Sicheng Mo",
            "name": "Sichengmo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20073",
            "authors": [
                {
                    "_id": "68124aad34af9c89c46a800b",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a800c",
                    "name": "Kangrui Wang",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a800d",
                    "name": "Qineng Wang",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a800e",
                    "name": "Pingyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a800f",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8010",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8011",
                    "name": "Kefan Yu",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8012",
                    "name": "Minh Nhat Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8013",
                    "name": "Licheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8014",
                    "name": "Eli Gottlieb",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8015",
                    "name": "Monica Lam",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8016",
                    "name": "Yiping Lu",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8017",
                    "name": "Kyunghyun Cho",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8018",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a8019",
                    "name": "Li Fei-Fei",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a801a",
                    "name": "Lijuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a801b",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68124aad34af9c89c46a801c",
                    "name": "Manling Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T17:57:08.000Z",
            "submittedOnDailyAt": "2025-04-30T15:51:18.888Z",
            "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6483af58571c2dcfa98cae82",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483af58571c2dcfa98cae82/d8Vnh_EG1KxeEoNAAQXHZ.jpeg",
                "isPro": false,
                "fullname": "Canyu Chen",
                "user": "canyuchen",
                "type": "user"
            },
            "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.",
            "upvotes": 7,
            "discussionId": "68124aae34af9c89c46a8055",
            "projectPage": "https://ragen-ai.github.io",
            "githubRepo": "https://github.com/RAGEN-AI/RAGEN",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "multi-turn agent RL",
                "StarPO (State-Thinking-Actions-Reward Policy Optimization)",
                "RAGEN",
                "trajectory-level agent RL",
                "Echo Trap",
                "reward variance cliffs",
                "gradient spikes",
                "StarPO-S",
                "trajectory filtering",
                "critic incorporation",
                "decoupled clipping",
                "RL rollouts",
                "diverse initial states",
                "medium interaction granularity",
                "frequent sampling",
                "fine-grained",
                "reasoning-aware reward signals",
                "shallow strategies",
                "hallucinated thoughts"
            ]
        },
        "publishedAt": "2025-04-24T13:57:08.000Z",
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn\n  Reinforcement Learning",
        "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20073.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6483af58571c2dcfa98cae82",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483af58571c2dcfa98cae82/d8Vnh_EG1KxeEoNAAQXHZ.jpeg",
            "fullname": "Canyu Chen",
            "name": "canyuchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.20690",
            "authors": [
                {
                    "_id": "6811c39584adfa26b82a4370",
                    "name": "Zechuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6811c39584adfa26b82a4371",
                    "name": "Ji Xie",
                    "hidden": false
                },
                {
                    "_id": "6811c39584adfa26b82a4372",
                    "name": "Yu Lu",
                    "hidden": false
                },
                {
                    "_id": "6811c39584adfa26b82a4373",
                    "name": "Zongxin Yang",
                    "hidden": false
                },
                {
                    "_id": "6811c39584adfa26b82a4374",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T12:14:47.000Z",
            "submittedOnDailyAt": "2025-04-30T12:51:25.833Z",
            "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/.",
            "upvotes": 5,
            "discussionId": "6811c39884adfa26b82a4426",
            "ai_keywords": [
                "instruction-based image editing",
                "natural language prompts",
                "precision-efficiency tradeoff",
                "fine-tuning methods",
                "computational resources",
                "large datasets",
                "training-free techniques",
                "instruction comprehension",
                "edit quality",
                "Diffusion Transformer (DiT)",
                "generation capacity",
                "contextual awareness",
                "in-context editing framework",
                "zero-shot instruction compliance",
                "in-context prompting",
                "structural changes",
                "LoRA-MoE hybrid tuning strategy",
                "flexible adaptation",
                "dynamic expert routing",
                "extensive retraining",
                "early filter inference-time scaling method",
                "vision-language models (VLMs)",
                "initial noise",
                "high-precision",
                "efficient instruction-guided editing"
            ]
        },
        "publishedAt": "2025-04-29T08:14:47.000Z",
        "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer",
        "summary": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20690.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6751
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.18087",
            "authors": [
                {
                    "_id": "6810746e4be021d4dcd8d4de",
                    "name": "Weipeng Tan",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4df",
                    "name": "Chuming Lin",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e0",
                    "user": {
                        "_id": "652fab9d04a34a9282bf29d6",
                        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                        "isPro": false,
                        "fullname": "Chengming Xu",
                        "user": "ChengmingX",
                        "type": "user"
                    },
                    "name": "Chengming Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T09:56:46.843Z",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e1",
                    "name": "FeiFan Xu",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e2",
                    "name": "Xiaobin Hu",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e3",
                    "name": "Xiaozhong Ji",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e4",
                    "name": "Junwei Zhu",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e5",
                    "name": "Chengjie Wang",
                    "hidden": false
                },
                {
                    "_id": "6810746e4be021d4dcd8d4e6",
                    "name": "Yanwei Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-25T05:28:21.000Z",
            "submittedOnDailyAt": "2025-04-30T07:13:30.231Z",
            "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation",
            "submittedOnDailyBy": {
                "_id": "652fab9d04a34a9282bf29d6",
                "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
                "isPro": false,
                "fullname": "Chengming Xu",
                "user": "ChengmingX",
                "type": "user"
            },
            "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.",
            "upvotes": 4,
            "discussionId": "681074704be021d4dcd8d57c",
            "projectPage": "https://toto222.github.io/DICE-Talk/",
            "githubRepo": "https://github.com/toto222/DICE-Talk"
        },
        "publishedAt": "2025-04-25T01:28:21.000Z",
        "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional\n  Talking Portrait Generation",
        "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18087.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652fab9d04a34a9282bf29d6",
            "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
            "fullname": "Chengming Xu",
            "name": "ChengmingX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.16272",
            "authors": [
                {
                    "_id": "68127237228929e037039e84",
                    "name": "Ryan Koo",
                    "hidden": false
                },
                {
                    "_id": "68127237228929e037039e85",
                    "user": {
                        "_id": "649339184005b1c3c021529a",
                        "avatarUrl": "/avatars/67391550642d2a476d388b4898134b8c.svg",
                        "isPro": false,
                        "fullname": "Ian Yang",
                        "user": "iyang30",
                        "type": "user"
                    },
                    "name": "Ian Yang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-30T18:55:52.538Z",
                    "hidden": false
                },
                {
                    "_id": "68127237228929e037039e86",
                    "name": "Vipul Raheja",
                    "hidden": false
                },
                {
                    "_id": "68127237228929e037039e87",
                    "name": "Mingyi Hong",
                    "hidden": false
                },
                {
                    "_id": "68127237228929e037039e88",
                    "name": "Kwang-Sung Jun",
                    "hidden": false
                },
                {
                    "_id": "68127237228929e037039e89",
                    "name": "Dongyeop Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-22T21:09:33.000Z",
            "submittedOnDailyAt": "2025-04-30T17:26:32.842Z",
            "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization",
            "submittedOnDailyBy": {
                "_id": "63ed75f3679c2cc40ab9e40f",
                "avatarUrl": "/avatars/e70bc88e72082e5ad5a106bc92a27ab1.svg",
                "isPro": false,
                "fullname": "Ryan Koo",
                "user": "rngusry",
                "type": "user"
            },
            "summary": "Current reinforcement learning from human feedback (RLHF) pipelines for large\nlanguage model (LLM) alignment typically assign scalar rewards to sequences,\nusing the final token as a surrogate indicator for the quality of the entire\nsequence. However, this leads to sparse feedback and suboptimal token-level\ncredit assignment. In this work, we frame reward shaping as an optimization\nproblem focused on token-level credit assignment. We propose a reward-shaping\nfunction leveraging explainability methods such as SHAP and LIME to estimate\nper-token rewards from the reward model. To learn parameters of this shaping\nfunction, we employ a bilevel optimization framework that integrates Bayesian\nOptimization and policy training to handle noise from the token reward\nestimates. Our experiments show that achieving a better balance of token-level\nreward attribution leads to performance improvements over baselines on\ndownstream tasks and finds an optimal policy faster during training.\nFurthermore, we show theoretically that explainability methods that are feature\nadditive attribution functions maintain the optimal policy as the original\nreward.",
            "upvotes": 4,
            "discussionId": "68127238228929e037039ee9",
            "ai_keywords": [
                "reinforcement learning from human feedback (RLHF)",
                "large language model (LLM) alignment",
                "scalar rewards",
                "final token",
                "token-level credit assignment",
                "reward shaping",
                "optimization problem",
                "reward-shaping function",
                "SHAP",
                "LIME",
                "per-token rewards",
                "reward model",
                "bilevel optimization framework",
                "Bayesian Optimization",
                "policy training",
                "noise from the token reward estimates",
                "token-level reward attribution",
                "downstream tasks",
                "optimal policy",
                "feature additive attribution functions"
            ]
        },
        "publishedAt": "2025-04-22T17:09:33.000Z",
        "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization",
        "summary": "Current reinforcement learning from human feedback (RLHF) pipelines for large\nlanguage model (LLM) alignment typically assign scalar rewards to sequences,\nusing the final token as a surrogate indicator for the quality of the entire\nsequence. However, this leads to sparse feedback and suboptimal token-level\ncredit assignment. In this work, we frame reward shaping as an optimization\nproblem focused on token-level credit assignment. We propose a reward-shaping\nfunction leveraging explainability methods such as SHAP and LIME to estimate\nper-token rewards from the reward model. To learn parameters of this shaping\nfunction, we employ a bilevel optimization framework that integrates Bayesian\nOptimization and policy training to handle noise from the token reward\nestimates. Our experiments show that achieving a better balance of token-level\nreward attribution leads to performance improvements over baselines on\ndownstream tasks and finds an optimal policy faster during training.\nFurthermore, we show theoretically that explainability methods that are feature\nadditive attribution functions maintain the optimal policy as the original\nreward.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16272.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ed75f3679c2cc40ab9e40f",
            "avatarUrl": "/avatars/e70bc88e72082e5ad5a106bc92a27ab1.svg",
            "fullname": "Ryan Koo",
            "name": "rngusry",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.20114",
            "authors": [
                {
                    "_id": "6811ee5994d2a4531ca2b697",
                    "user": {
                        "_id": "66a378a7a24a4e3865e6aadf",
                        "avatarUrl": "/avatars/ff1560941fad2a6224418f399d38b1cc.svg",
                        "isPro": false,
                        "fullname": "Allen Li",
                        "user": "allen-li1231",
                        "type": "user"
                    },
                    "name": "Zhonghao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T12:24:10.370Z",
                    "hidden": false
                },
                {
                    "_id": "6811ee5994d2a4531ca2b698",
                    "user": {
                        "_id": "65cb93826427380bc218c408",
                        "avatarUrl": "/avatars/73d834cf4ce7d09d6adb68b1e974cbf8.svg",
                        "isPro": false,
                        "fullname": "KZ Zhang",
                        "user": "kpzhang1028",
                        "type": "user"
                    },
                    "name": "Kunpeng Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-04-30T09:33:14.657Z",
                    "hidden": false
                },
                {
                    "_id": "6811ee5994d2a4531ca2b699",
                    "name": "Jinghuai Ou",
                    "hidden": false
                },
                {
                    "_id": "6811ee5994d2a4531ca2b69a",
                    "name": "Shuliang Liu",
                    "hidden": false
                },
                {
                    "_id": "6811ee5994d2a4531ca2b69b",
                    "name": "Xuming Hu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66a378a7a24a4e3865e6aadf/R4pY7UEmL3C1Y_U1DQfkd.png"
            ],
            "publishedAt": "2025-04-28T01:56:31.000Z",
            "submittedOnDailyAt": "2025-04-30T08:56:44.588Z",
            "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering",
            "submittedOnDailyBy": {
                "_id": "66a378a7a24a4e3865e6aadf",
                "avatarUrl": "/avatars/ff1560941fad2a6224418f399d38b1cc.svg",
                "isPro": false,
                "fullname": "Allen Li",
                "user": "allen-li1231",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop.",
            "upvotes": 3,
            "discussionId": "6811ee5a94d2a4531ca2b6e4",
            "githubRepo": "https://github.com/allen-li1231/treehop-rag",
            "ai_keywords": [
                "retrieval-augmented generation (RAG)",
                "multi-hop question answering (MHQA)",
                "iterative LLM-based query rewriting and routing",
                "TreeHop",
                "embedding-level framework",
                "query embeddings",
                "semantic information",
                "embedding-space operations",
                "\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle",
                "\"Retrieve-Embed-Retrieve\" loop",
                "rule-based stop criterion",
                "open-domain MHQA datasets",
                "model parameter size",
                "query latency"
            ]
        },
        "publishedAt": "2025-04-27T21:56:31.000Z",
        "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for\n  Multi-hop Question Answering",
        "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66a378a7a24a4e3865e6aadf/R4pY7UEmL3C1Y_U1DQfkd.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20114.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a378a7a24a4e3865e6aadf",
            "avatarUrl": "/avatars/ff1560941fad2a6224418f399d38b1cc.svg",
            "fullname": "Allen Li",
            "name": "allen-li1231",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.18942",
            "authors": [
                {
                    "_id": "681140d7f8ca0d9acb1690a9",
                    "name": "Debarati Das",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690aa",
                    "name": "Khanh Chi Le",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690ab",
                    "name": "Ritik Sachin Parkar",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690ac",
                    "name": "Karin De Langis",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690ad",
                    "name": "Brendan Madson",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690ae",
                    "name": "Chad M. Berryman",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690af",
                    "name": "Robin M. Willis",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690b0",
                    "name": "Daniel H. Moses",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690b1",
                    "name": "Brett McDonnell",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690b2",
                    "name": "Daniel Schwarcz",
                    "hidden": false
                },
                {
                    "_id": "681140d7f8ca0d9acb1690b3",
                    "name": "Dongyeop Kang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-26T15:01:55.000Z",
            "submittedOnDailyAt": "2025-04-30T17:48:52.727Z",
            "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes",
            "submittedOnDailyBy": {
                "_id": "63ed75f3679c2cc40ab9e40f",
                "avatarUrl": "/avatars/e70bc88e72082e5ad5a106bc92a27ab1.svg",
                "isPro": false,
                "fullname": "Ryan Koo",
                "user": "rngusry",
                "type": "user"
            },
            "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/).",
            "upvotes": 3,
            "discussionId": "681140d9f8ca0d9acb16916a"
        },
        "publishedAt": "2025-04-26T11:01:55.000Z",
        "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes",
        "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18942.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ed75f3679c2cc40ab9e40f",
            "avatarUrl": "/avatars/e70bc88e72082e5ad5a106bc92a27ab1.svg",
            "fullname": "Ryan Koo",
            "name": "rngusry",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.18738",
            "authors": [
                {
                    "_id": "681179a26a014c6e057963d8",
                    "user": {
                        "_id": "67ddd80896ac367438d400a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                        "isPro": false,
                        "fullname": "Ranjan Sapkota",
                        "user": "RanjanSapkota",
                        "type": "user"
                    },
                    "name": "Ranjan Sapkota",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-04-30T07:56:21.388Z",
                    "hidden": false
                },
                {
                    "_id": "681179a26a014c6e057963d9",
                    "name": "Konstantinos I Roumeliotis",
                    "hidden": false
                },
                {
                    "_id": "681179a26a014c6e057963da",
                    "name": "Rahul Harsha Cheppally",
                    "hidden": false
                },
                {
                    "_id": "681179a26a014c6e057963db",
                    "name": "Marco Flores Calero",
                    "hidden": false
                },
                {
                    "_id": "681179a26a014c6e057963dc",
                    "name": "Manoj Karkee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/lI5GaBWbn8adHbGwWiFvT.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ytkmLrQr1_9QIremyU0S5.jpeg"
            ],
            "publishedAt": "2025-04-25T23:27:26.000Z",
            "submittedOnDailyAt": "2025-04-30T13:01:45.052Z",
            "title": "A Review of 3D Object Detection with Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "67ddd80896ac367438d400a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                "isPro": false,
                "fullname": "Ranjan Sapkota",
                "user": "RanjanSapkota",
                "type": "user"
            },
            "summary": "This review provides a systematic analysis of comprehensive survey of 3D\nobject detection with vision-language models(VLMs) , a rapidly advancing area\nat the intersection of 3D vision and multimodal AI. By examining over 100\nresearch papers, we provide the first systematic analysis dedicated to 3D\nobject detection with vision-language models. We begin by outlining the unique\nchallenges of 3D object detection with vision-language models, emphasizing\ndifferences from 2D detection in spatial reasoning and data complexity.\nTraditional approaches using point clouds and voxel grids are compared to\nmodern vision-language frameworks like CLIP and 3D LLMs, which enable\nopen-vocabulary detection and zero-shot generalization. We review key\narchitectures, pretraining strategies, and prompt engineering methods that\nalign textual and 3D features for effective 3D object detection with\nvision-language models. Visualization examples and evaluation benchmarks are\ndiscussed to illustrate performance and behavior. Finally, we highlight current\nchallenges, such as limited 3D-language datasets and computational demands, and\npropose future research directions to advance 3D object detection with\nvision-language models. >Object Detection, Vision-Language Models, Agents,\nVLMs, LLMs, AI",
            "upvotes": 2,
            "discussionId": "681179a36a014c6e05796455",
            "ai_keywords": [
                "point clouds",
                "voxel grids",
                "CLIP",
                "3D LLMs",
                "open-vocabulary detection",
                "zero-shot generalization",
                "key architectures",
                "pretraining strategies",
                "prompt engineering",
                "3D-language datasets"
            ]
        },
        "publishedAt": "2025-04-25T19:27:26.000Z",
        "title": "A Review of 3D Object Detection with Vision-Language Models",
        "summary": "This review provides a systematic analysis of comprehensive survey of 3D\nobject detection with vision-language models(VLMs) , a rapidly advancing area\nat the intersection of 3D vision and multimodal AI. By examining over 100\nresearch papers, we provide the first systematic analysis dedicated to 3D\nobject detection with vision-language models. We begin by outlining the unique\nchallenges of 3D object detection with vision-language models, emphasizing\ndifferences from 2D detection in spatial reasoning and data complexity.\nTraditional approaches using point clouds and voxel grids are compared to\nmodern vision-language frameworks like CLIP and 3D LLMs, which enable\nopen-vocabulary detection and zero-shot generalization. We review key\narchitectures, pretraining strategies, and prompt engineering methods that\nalign textual and 3D features for effective 3D object detection with\nvision-language models. Visualization examples and evaluation benchmarks are\ndiscussed to illustrate performance and behavior. Finally, we highlight current\nchallenges, such as limited 3D-language datasets and computational demands, and\npropose future research directions to advance 3D object detection with\nvision-language models. >Object Detection, Vision-Language Models, Agents,\nVLMs, LLMs, AI",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/lI5GaBWbn8adHbGwWiFvT.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ytkmLrQr1_9QIremyU0S5.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18738.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ddd80896ac367438d400a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
            "fullname": "Ranjan Sapkota",
            "name": "RanjanSapkota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20769",
            "authors": [
                {
                    "_id": "68129287421d23bcbdbc535f",
                    "name": "Wenxiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68129287421d23bcbdbc5360",
                    "name": "Parsa Hosseini",
                    "hidden": false
                },
                {
                    "_id": "68129287421d23bcbdbc5361",
                    "name": "Soheil Feizi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T13:50:05.000Z",
            "submittedOnDailyAt": "2025-04-30T21:11:54.609Z",
            "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in\n  Large Language Models against Reference Corruption",
            "submittedOnDailyBy": {
                "_id": "659dc02d72238596c24d49f5",
                "avatarUrl": "/avatars/d4600d23ccc72f296fab7f626d5895e7.svg",
                "isPro": false,
                "fullname": "Wenxiao Wang",
                "user": "wangwenxiao",
                "type": "user"
            },
            "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%.",
            "upvotes": 1,
            "discussionId": "68129287421d23bcbdbc539f",
            "ai_keywords": [
                "large language models",
                "chain-of-thought prompting",
                "reasoning abilities",
                "robustness",
                "reference corruption",
                "chain-of-defensive-thought",
                "exemplars",
                "structured reasoning",
                "defensive reasoning",
                "prompt injection attacks",
                "Natural Questions task",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-04-29T09:50:05.000Z",
        "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in\n  Large Language Models against Reference Corruption",
        "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20769.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "659dc02d72238596c24d49f5",
            "avatarUrl": "/avatars/d4600d23ccc72f296fab7f626d5895e7.svg",
            "fullname": "Wenxiao Wang",
            "name": "wangwenxiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.17838",
            "authors": [
                {
                    "_id": "68126aa5ec67f8f30bf43f20",
                    "name": "Bernhard Jaeger",
                    "hidden": false
                },
                {
                    "_id": "68126aa5ec67f8f30bf43f21",
                    "name": "Daniel Dauner",
                    "hidden": false
                },
                {
                    "_id": "68126aa5ec67f8f30bf43f22",
                    "name": "Jens Beißwenger",
                    "hidden": false
                },
                {
                    "_id": "68126aa5ec67f8f30bf43f23",
                    "name": "Simon Gerstenecker",
                    "hidden": false
                },
                {
                    "_id": "68126aa5ec67f8f30bf43f24",
                    "name": "Kashyap Chitta",
                    "hidden": false
                },
                {
                    "_id": "68126aa5ec67f8f30bf43f25",
                    "name": "Andreas Geiger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-24T17:56:01.000Z",
            "submittedOnDailyAt": "2025-04-30T16:54:58.037Z",
            "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
            "submittedOnDailyBy": {
                "_id": "6508883c61c4bb4636d25f9f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508883c61c4bb4636d25f9f/Zb6WwrH-0bhjBxxeMZq65.jpeg",
                "isPro": false,
                "fullname": "Bernhard Jaeger",
                "user": "0Kaito",
                "type": "user"
            },
            "summary": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.",
            "upvotes": 1,
            "discussionId": "68126aa6ec67f8f30bf43f5c",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "privileged planning",
                "autonomous driving",
                "rule-based approaches",
                "compounding errors",
                "imitation learning",
                "complex shaped rewards",
                "progress",
                "position",
                "orientation rewards",
                "PPO",
                "mini-batch size",
                "route completion",
                "infractions",
                "distributed data parallelism",
                "CARLA",
                "nuPlan",
                "DS",
                "Val14 benchmark",
                "non-reactive",
                "reactive traffic"
            ]
        },
        "publishedAt": "2025-04-24T13:56:01.000Z",
        "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
        "summary": "We investigate reinforcement learning (RL) for privileged planning in\nautonomous driving. State-of-the-art approaches for this task are rule-based,\nbut these methods do not scale to the long tail. RL, on the other hand, is\nscalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum\nmultiple individual rewards, \\eg~progress, position, or orientation rewards. We\nshow that PPO fails to optimize a popular version of these rewards when the\nmini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single\nintuitive reward term: route completion. Infractions are penalized by\nterminating the episode or multiplicatively reducing route completion. We find\nthat PPO scales well with higher mini-batch sizes when trained with our simple\nreward, even improving performance. Training with large mini-batch sizes\nenables efficient scaling via distributed data parallelism. We scale PPO to\n300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The\nresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,\noutperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is\nthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and\n90.6 in reactive traffic on the Val14 benchmark while being an order of\nmagnitude faster than prior work.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17838.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6508883c61c4bb4636d25f9f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508883c61c4bb4636d25f9f/Zb6WwrH-0bhjBxxeMZq65.jpeg",
            "fullname": "Bernhard Jaeger",
            "name": "0Kaito",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]