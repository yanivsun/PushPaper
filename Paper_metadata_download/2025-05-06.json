[
    {
        "paper": {
            "id": "2505.02707",
            "authors": [
                {
                    "_id": "6819982f17007d963b9d4166",
                    "name": "Yemin Shi",
                    "hidden": false
                },
                {
                    "_id": "6819982f17007d963b9d4167",
                    "name": "Yu Shu",
                    "hidden": false
                },
                {
                    "_id": "6819982f17007d963b9d4168",
                    "name": "Siwei Dong",
                    "hidden": false
                },
                {
                    "_id": "6819982f17007d963b9d4169",
                    "user": {
                        "_id": "6108ae87823007eaf0c7bd1e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6108ae87823007eaf0c7bd1e/dKjdx9I5waJs6oUQ0_mmT.png",
                        "isPro": false,
                        "fullname": "Guangyi Liu",
                        "user": "guangyil",
                        "type": "user"
                    },
                    "name": "Guangyi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:46:52.667Z",
                    "hidden": false
                },
                {
                    "_id": "6819982f17007d963b9d416a",
                    "user": {
                        "_id": "6438a9027de34e8ea7e4b257",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438a9027de34e8ea7e4b257/vib8QSd1AWMr_bR9ig_xJ.jpeg",
                        "isPro": false,
                        "fullname": "Jaward Sesay",
                        "user": "Jaward",
                        "type": "user"
                    },
                    "name": "Jaward Sesay",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:32:48.746Z",
                    "hidden": false
                },
                {
                    "_id": "6819982f17007d963b9d416b",
                    "name": "Jingwen Li",
                    "hidden": false
                },
                {
                    "_id": "6819982f17007d963b9d416c",
                    "user": {
                        "_id": "665bfa1b0d71762b8613282d",
                        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
                        "isPro": false,
                        "fullname": "Zhiting Hu",
                        "user": "zhitinghu",
                        "type": "user"
                    },
                    "name": "Zhiting Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:46:15.191Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
            ],
            "publishedAt": "2025-05-05T15:05:01.000Z",
            "submittedOnDailyAt": "2025-05-06T03:36:16.945Z",
            "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
            "submittedOnDailyBy": {
                "_id": "665bfa1b0d71762b8613282d",
                "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
                "isPro": false,
                "fullname": "Zhiting Hu",
                "user": "zhitinghu",
                "type": "user"
            },
            "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
            "upvotes": 56,
            "discussionId": "6819983117007d963b9d4247",
            "projectPage": "https://voila.maitrix.org",
            "githubRepo": "https://github.com/maitrix-org/Voila",
            "ai_keywords": [
                "full-duplex",
                "low-latency conversations",
                "hierarchical multi-scale Transformer",
                "reasoning capabilities",
                "large language models (LLMs)",
                "acoustic modeling",
                "persona-aware voice generation",
                "automatic speech recognition (ASR)",
                "Text-to-Speech (TTS)",
                "multilingual speech translation",
                "pre-built voices",
                "efficient customization"
            ]
        },
        "publishedAt": "2025-05-05T11:05:01.000Z",
        "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
        "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02707.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "665bfa1b0d71762b8613282d",
            "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
            "fullname": "Zhiting Hu",
            "name": "zhitinghu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02387",
            "authors": [
                {
                    "_id": "681988d6d6a5fee26b52ac28",
                    "user": {
                        "_id": "6270ff726417aed8a7340c8b",
                        "avatarUrl": "/avatars/3f14913c55cc4fc78678ac43fb603e80.svg",
                        "isPro": false,
                        "fullname": "Xiusi Chen",
                        "user": "XtremSup",
                        "type": "user"
                    },
                    "name": "Xiusi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:47:11.654Z",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac29",
                    "user": {
                        "_id": "654d784d71a30c4bca09a319",
                        "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
                        "isPro": false,
                        "fullname": "Gaotang Li",
                        "user": "gaotang",
                        "type": "user"
                    },
                    "name": "Gaotang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:13.258Z",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac2a",
                    "name": "Ziqi Wang",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac2b",
                    "name": "Bowen Jin",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac2c",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac2d",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac2e",
                    "user": {
                        "_id": "65f906e5c3dbdcae83ff7aac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                        "isPro": false,
                        "fullname": "Hongru Wang",
                        "user": "Merlin-Hongru",
                        "type": "user"
                    },
                    "name": "Hongru Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:11.136Z",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac2f",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac30",
                    "user": {
                        "_id": "66285acb73af5913c6bbf1ec",
                        "avatarUrl": "/avatars/8969e3a6ae2dcc0b1c49768fd044b9e0.svg",
                        "isPro": false,
                        "fullname": "Denghui Zhang",
                        "user": "zhangdenghui123",
                        "type": "user"
                    },
                    "name": "Denghui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:48:00.793Z",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac31",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac32",
                    "name": "Hanghang Tong",
                    "hidden": false
                },
                {
                    "_id": "681988d6d6a5fee26b52ac33",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T06:11:12.000Z",
            "submittedOnDailyAt": "2025-05-06T02:32:05.558Z",
            "title": "RM-R1: Reward Modeling as Reasoning",
            "submittedOnDailyBy": {
                "_id": "654d784d71a30c4bca09a319",
                "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
                "isPro": false,
                "fullname": "Gaotang Li",
                "user": "gaotang",
                "type": "user"
            },
            "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
            "upvotes": 46,
            "discussionId": "681988d7d6a5fee26b52ac7e",
            "githubRepo": "https://github.com/RM-R1-UIUC/RM-R1",
            "ai_keywords": [
                "reward modeling",
                "reinforcement learning from human feedback (RLHF)",
                "reward model (RM)",
                "scalar scores",
                "preferred answer",
                "natural language critiques",
                "long chain-of-thought (CoT)",
                "reasoning capabilities",
                "Reasoning Reward Models (ReasRMs)",
                "reasoning-oriented training pipeline",
                "distillation",
                "high-quality reasoning chains",
                "reinforcement learning",
                "verifiable rewards",
                "LLM rollouts",
                "self-generating reasoning traces",
                "chat-specific rubrics",
                "candidate responses",
                "generative reward models",
                "state-of-the-art",
                "near state-of-the-art",
                "reward model benchmarks",
                "open-weight models",
                "proprietary models",
                "empirical analysis",
                "ReasRM models"
            ]
        },
        "publishedAt": "2025-05-05T02:11:12.000Z",
        "title": "RM-R1: Reward Modeling as Reasoning",
        "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02387.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654d784d71a30c4bca09a319",
            "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
            "fullname": "Gaotang Li",
            "name": "gaotang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.20752",
            "authors": [
                {
                    "_id": "6818c145daa8955b2085667d",
                    "user": {
                        "_id": "63e7b92c4577a86987a53cd6",
                        "avatarUrl": "/avatars/6cb0b5a1eaf4a84db663eeda96a3967d.svg",
                        "isPro": false,
                        "fullname": "Roman Abramov",
                        "user": "monsetrum",
                        "type": "user"
                    },
                    "name": "Roman Abramov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T16:53:05.323Z",
                    "hidden": false
                },
                {
                    "_id": "6818c145daa8955b2085667e",
                    "user": {
                        "_id": "6679882913c63ebaa8ff62fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
                        "isPro": false,
                        "fullname": "Felix Steinbauer",
                        "user": "fsteinbauer",
                        "type": "user"
                    },
                    "name": "Felix Steinbauer",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-06T13:01:28.304Z",
                    "hidden": false
                },
                {
                    "_id": "6818c145daa8955b2085667f",
                    "name": "Gjergji Kasneci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-29T13:33:29.000Z",
            "submittedOnDailyAt": "2025-05-06T03:38:21.809Z",
            "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
            "submittedOnDailyBy": {
                "_id": "6679882913c63ebaa8ff62fe",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
                "isPro": false,
                "fullname": "Felix Steinbauer",
                "user": "fsteinbauer",
                "type": "user"
            },
            "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
            "upvotes": 44,
            "discussionId": "6818c146daa8955b208566f1",
            "ai_keywords": [
                "Transformers",
                "multi-step factual reasoning",
                "grokking",
                "neural networks",
                "perfect generalization",
                "logical patterns",
                "real-world factual data",
                "dataset sparsity",
                "knowledge graphs",
                "synthetic data",
                "inferred facts",
                "atomic facts",
                "factually incorrect synthetic data",
                "relational structure",
                "memorization",
                "multi-hop reasoning",
                "benchmarks",
                "2WikiMultiHopQA",
                "baselines",
                "state-of-the-art results",
                "generalizing circuits",
                "grokking-based data augmentation",
                "implicit multi-hop reasoning capabilities",
                "robust",
                "interpretable factual reasoning"
            ]
        },
        "publishedAt": "2025-04-29T09:33:29.000Z",
        "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
        "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20752.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6679882913c63ebaa8ff62fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6679882913c63ebaa8ff62fe/zufYEHw7QNp50pfZx9SmF.jpeg",
            "fullname": "Felix Steinbauer",
            "name": "fsteinbauer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02222",
            "authors": [
                {
                    "_id": "6819780dc3d212ad5b48cc07",
                    "user": {
                        "_id": "67f40efb64c87965b9063655",
                        "avatarUrl": "/avatars/a25abd6f3318036c8d589633221c2010.svg",
                        "isPro": false,
                        "fullname": "Research at Essential AI",
                        "user": "Research-EAI",
                        "type": "user"
                    },
                    "name": "Essential AI",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T16:53:02.976Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc09",
                    "user": {
                        "_id": "65ef97d0e5fc4abe66c05ed0",
                        "avatarUrl": "/avatars/1d601a22639b3136bfb3519826451ddb.svg",
                        "isPro": false,
                        "fullname": "Ishaan Shah",
                        "user": "ishaan-essential",
                        "type": "user"
                    },
                    "name": "Ishaan Shah",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:24.884Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc0a",
                    "user": {
                        "_id": "6675e3ed66c4fa6d0c10e229",
                        "avatarUrl": "/avatars/f73c347d824a56079729c82d60d3edc3.svg",
                        "isPro": false,
                        "fullname": "Anthony Polloreno",
                        "user": "ampolloreno",
                        "type": "user"
                    },
                    "name": "Anthony M. Polloreno",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:01:47.445Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc0b",
                    "user": {
                        "_id": "64d9ac38badf1110f7fcf030",
                        "avatarUrl": "/avatars/c55c61af8dd52e6b4856684638b850a6.svg",
                        "isPro": false,
                        "fullname": "Karl Stratos",
                        "user": "karlstratos",
                        "type": "user"
                    },
                    "name": "Karl Stratos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:01:53.826Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc0c",
                    "user": {
                        "_id": "66622dacec18341b268f97a6",
                        "avatarUrl": "/avatars/8bc7d6a7c28c83aacdbeeb770716b1c0.svg",
                        "isPro": false,
                        "fullname": "Philip Monk",
                        "user": "monk-essential",
                        "type": "user"
                    },
                    "name": "Philip Monk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:01:59.992Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc0d",
                    "user": {
                        "_id": "67bfd6daca6e3c22b6de31ee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/TTt6_o9tYEgeWK26DgI_7.png",
                        "isPro": false,
                        "fullname": "Adarsh Chaluvaraju",
                        "user": "cadarsh-essential",
                        "type": "user"
                    },
                    "name": "Adarsh Chaluvaraju",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:02:16.867Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc0e",
                    "user": {
                        "_id": "6408e4f93461c51cf7345060",
                        "avatarUrl": "/avatars/328b508e2de9e50dca2412adeb3542f5.svg",
                        "isPro": false,
                        "fullname": "Andrew Hojel",
                        "user": "andrewhojel",
                        "type": "user"
                    },
                    "name": "Andrew Hojel",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:02:24.655Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc0f",
                    "user": {
                        "_id": "62e24efc3a616d16e2f426ea",
                        "avatarUrl": "/avatars/a2433c971f80e6cf738c03e843666cff.svg",
                        "isPro": false,
                        "fullname": "Andrew Ma",
                        "user": "AndrewMa",
                        "type": "user"
                    },
                    "name": "Andrew Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:02:30.884Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc10",
                    "name": "Anil Thomas",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc11",
                    "name": "Ashish Tanwer",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc12",
                    "name": "Darsh J Shah",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc13",
                    "user": {
                        "_id": "67ed7aa9290a7f9d33113fb5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jMMTObGZktuXWJ3wVVSxj.png",
                        "isPro": false,
                        "fullname": "Khoi Nguyen",
                        "user": "KTLK",
                        "type": "user"
                    },
                    "name": "Khoi Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:20.971Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc14",
                    "name": "Kurt Smith",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc15",
                    "name": "Michael Callahan",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc16",
                    "user": {
                        "_id": "66cd078ea796074d428fde0f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qThw3H6ukqIAuRy7aJTN1.jpeg",
                        "isPro": false,
                        "fullname": "Michael Pust",
                        "user": "essentialpust",
                        "type": "user"
                    },
                    "name": "Michael Pust",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:03:14.427Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc17",
                    "user": {
                        "_id": "671aeee626d87414f59ee7d6",
                        "avatarUrl": "/avatars/131d0ff4c12bc70e74945910ce0e1d56.svg",
                        "isPro": false,
                        "fullname": "Mohit Parmar",
                        "user": "mohit-essential",
                        "type": "user"
                    },
                    "name": "Mohit Parmar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T16:52:57.811Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc18",
                    "name": "Peter Rushton",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc19",
                    "user": {
                        "_id": "67f4ced58c4cbc2f5d95cd17",
                        "avatarUrl": "/avatars/3f440a59c38f5c0c7a77746ef54ed0a5.svg",
                        "isPro": false,
                        "fullname": "Platon Mazarakis",
                        "user": "Platona",
                        "type": "user"
                    },
                    "name": "Platon Mazarakis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:03:32.983Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc1a",
                    "user": {
                        "_id": "654bdcf2e06d25def57cc54b",
                        "avatarUrl": "/avatars/2d2612bd7072edd60876b504345fbf25.svg",
                        "isPro": false,
                        "fullname": "Ritvik Kapila",
                        "user": "rkapila",
                        "type": "user"
                    },
                    "name": "Ritvik Kapila",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:03:51.154Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc1b",
                    "name": "Saurabh Srivastava",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc1c",
                    "user": {
                        "_id": "679bc0b23e12a166672e5275",
                        "avatarUrl": "/avatars/fe9d0e79c21c9d3594420e69e3809f0f.svg",
                        "isPro": false,
                        "fullname": "Somanshu Singla",
                        "user": "somanshu-essential",
                        "type": "user"
                    },
                    "name": "Somanshu Singla",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:27.349Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc1d",
                    "user": {
                        "_id": "67101a7165442ddc48cb4b07",
                        "avatarUrl": "/avatars/551777fcd1638998ad9fd16804b313ec.svg",
                        "isPro": false,
                        "fullname": "Tim Romanski",
                        "user": "tim-essential",
                        "type": "user"
                    },
                    "name": "Tim Romanski",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:04:09.387Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc1e",
                    "user": {
                        "_id": "66f5f3f99ef08fe3c1f4c35a",
                        "avatarUrl": "/avatars/41b8b6f90eb87e685b74587317296a1b.svg",
                        "isPro": false,
                        "fullname": "Yash Vanjani",
                        "user": "yash-essential",
                        "type": "user"
                    },
                    "name": "Yash Vanjani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:04:16.859Z",
                    "hidden": false
                },
                {
                    "_id": "6819780dc3d212ad5b48cc1f",
                    "name": "Ashish Vaswani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-04T19:14:43.000Z",
            "submittedOnDailyAt": "2025-05-06T07:30:25.714Z",
            "title": "Practical Efficiency of Muon for Pretraining",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.",
            "upvotes": 30,
            "discussionId": "6819780fc3d212ad5b48cc89",
            "ai_keywords": [
                "second-order optimizer",
                "Pareto frontier",
                "AdamW",
                "data efficiency",
                "critical batch size",
                "computationally efficient",
                "maximal update parameterization",
                "telescoping algorithm",
                "hyperparameter transfer",
                "error sources",
                "model sizes",
                "data distribution",
                "architecture"
            ]
        },
        "publishedAt": "2025-05-04T15:14:43.000Z",
        "title": "Practical Efficiency of Muon for Pretraining",
        "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02222.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6789
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.02819",
            "authors": [
                {
                    "_id": "6819b5da3d9c61444380f4c5",
                    "user": {
                        "_id": "66465dfa508db0bde50d95f2",
                        "avatarUrl": "/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg",
                        "isPro": false,
                        "fullname": "Dmitry Shopkhoev",
                        "user": "dimitriish",
                        "type": "user"
                    },
                    "name": "Dmitriy Shopkhoev",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-06T13:37:06.786Z",
                    "hidden": false
                },
                {
                    "_id": "6819b5da3d9c61444380f4c6",
                    "user": {
                        "_id": "6166db59f78a267701a78c2a",
                        "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg",
                        "isPro": false,
                        "fullname": "Ammar Ali",
                        "user": "ammarali32",
                        "type": "user"
                    },
                    "name": "Ammar Ali",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:32:17.870Z",
                    "hidden": false
                },
                {
                    "_id": "6819b5da3d9c61444380f4c7",
                    "name": "Magauiya Zhussip",
                    "hidden": false
                },
                {
                    "_id": "6819b5da3d9c61444380f4c8",
                    "user": {
                        "_id": "66b1ce4ca14db5aac3e5e755",
                        "avatarUrl": "/avatars/ab55ef112fba091813e1cc1f43857cf9.svg",
                        "isPro": false,
                        "fullname": "Valentin Malykh",
                        "user": "madrugado",
                        "type": "user"
                    },
                    "name": "Valentin Malykh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:04:42.358Z",
                    "hidden": false
                },
                {
                    "_id": "6819b5da3d9c61444380f4c9",
                    "user": {
                        "_id": "6683cc62b466c0d8e60e1bbc",
                        "avatarUrl": "/avatars/d781cfb113263f88eaa3250bef521c53.svg",
                        "isPro": false,
                        "fullname": "Stamatis Lefkimmiatis",
                        "user": "stamatisl",
                        "type": "user"
                    },
                    "name": "Stamatios Lefkimmiatis",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:32:13.923Z",
                    "hidden": false
                },
                {
                    "_id": "6819b5da3d9c61444380f4ca",
                    "name": "Nikos Komodakis",
                    "hidden": false
                },
                {
                    "_id": "6819b5da3d9c61444380f4cb",
                    "user": {
                        "_id": "667e7f968c6d7aede7ecb94b",
                        "avatarUrl": "/avatars/d6dabd9b909b1f20f661dc4bc07af23f.svg",
                        "isPro": false,
                        "fullname": "Sergey Zagoruyko",
                        "user": "szagoruyko121",
                        "type": "user"
                    },
                    "name": "Sergey Zagoruyko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:04:52.244Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T17:47:42.000Z",
            "submittedOnDailyAt": "2025-05-06T07:03:26.032Z",
            "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
            "submittedOnDailyBy": {
                "_id": "610e8c12119bebecb4d807b6",
                "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
                "isPro": false,
                "fullname": "Ivan Sedykh",
                "user": "idsedykh",
                "type": "user"
            },
            "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
            "upvotes": 20,
            "discussionId": "6819b5db3d9c61444380f518",
            "githubRepo": "https://github.com/mts-ai/ReplaceMe",
            "ai_keywords": [
                "training-free depth pruning",
                "transformer blocks",
                "linear operation",
                "calibration dataset",
                "linear transformation",
                "computational overhead",
                "large language models (LLMs)",
                "open benchmarks",
                "open-source library"
            ]
        },
        "publishedAt": "2025-05-05T13:47:42.000Z",
        "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
        "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02819.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "610e8c12119bebecb4d807b6",
            "avatarUrl": "/avatars/7230b1584ec45585c12eb5703fd80ff3.svg",
            "fullname": "Ivan Sedykh",
            "name": "idsedykh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02735",
            "authors": [
                {
                    "_id": "6819742e0d1c56fe9124fe3a",
                    "user": {
                        "_id": "62a80fe3ac97233f1625235a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
                        "isPro": false,
                        "fullname": "Zhouliang Yu",
                        "user": "zhouliang",
                        "type": "user"
                    },
                    "name": "Zhouliang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:34:10.190Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe3b",
                    "user": {
                        "_id": "662f2c8435ab6df959b005de",
                        "avatarUrl": "/avatars/3e30053ecbe9cc14b5e1eb2b014755de.svg",
                        "isPro": false,
                        "fullname": "ruotian peng",
                        "user": "prt66",
                        "type": "user"
                    },
                    "name": "Ruotian Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:48:20.491Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe3c",
                    "name": "Keyi Ding",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe3d",
                    "name": "Yizhe Li",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe3e",
                    "name": "Zhongyuan Peng",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe3f",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:31.975Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe40",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T14:10:20.040Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe41",
                    "user": {
                        "_id": "649da6b4599302cdb9bc232b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DxQT6LCDTZvyGUUe2t19c.jpeg",
                        "isPro": false,
                        "fullname": "Zheng Yuan",
                        "user": "ZhengYuan",
                        "type": "user"
                    },
                    "name": "Zheng Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:49:20.735Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe42",
                    "user": {
                        "_id": "6532a060a78e70d19c669103",
                        "avatarUrl": "/avatars/3cc9309b0e31da0fb83f1c3ef87dbe9f.svg",
                        "isPro": false,
                        "fullname": "HuajianXin",
                        "user": "HuajianXin",
                        "type": "user"
                    },
                    "name": "Huajian Xin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:49:28.104Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe43",
                    "user": {
                        "_id": "641e5bf65f274a0a92c2f6a2",
                        "avatarUrl": "/avatars/c15a54c51998c0e6367685e8e1737ec9.svg",
                        "isPro": false,
                        "fullname": "Wenhao Huang",
                        "user": "EZ-hwh",
                        "type": "user"
                    },
                    "name": "Wenhao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:49:44.482Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe44",
                    "user": {
                        "_id": "643c21735fcffe09fb68a46f",
                        "avatarUrl": "/avatars/76aabacd318aa954d4c53094ad456056.svg",
                        "isPro": false,
                        "fullname": "Yandong Wen",
                        "user": "ydwen",
                        "type": "user"
                    },
                    "name": "Yandong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:49:51.642Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe45",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:49:59.764Z",
                    "hidden": false
                },
                {
                    "_id": "6819742e0d1c56fe9124fe46",
                    "user": {
                        "_id": "648905d1a15c43c791d4381f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
                        "isPro": false,
                        "fullname": "Weiyang Liu",
                        "user": "wy1iu",
                        "type": "user"
                    },
                    "name": "Weiyang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:50:07.063Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T15:37:00.000Z",
            "submittedOnDailyAt": "2025-05-06T01:00:48.636Z",
            "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "62a80fe3ac97233f1625235a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
                "isPro": false,
                "fullname": "Zhouliang Yu",
                "user": "zhouliang",
                "type": "user"
            },
            "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
            "upvotes": 20,
            "discussionId": "6819742f0d1c56fe9124fe8a",
            "projectPage": "https://spherelab.ai/FormalMATH/",
            "githubRepo": "https://github.com/Sphere-AI-Lab/FormalMATH-Bench"
        },
        "publishedAt": "2025-05-05T11:37:00.000Z",
        "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
        "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02735.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62a80fe3ac97233f1625235a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
            "fullname": "Zhouliang Yu",
            "name": "zhouliang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02391",
            "authors": [
                {
                    "_id": "6819a63c64ae18f1b60a5c43",
                    "user": {
                        "_id": "66f8689725464a7989b75845",
                        "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
                        "isPro": false,
                        "fullname": "Jiarui Yao",
                        "user": "FlippyDora",
                        "type": "user"
                    },
                    "name": "Jiarui Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:32:24.344Z",
                    "hidden": false
                },
                {
                    "_id": "6819a63c64ae18f1b60a5c44",
                    "name": "Yifan Hao",
                    "hidden": false
                },
                {
                    "_id": "6819a63c64ae18f1b60a5c45",
                    "user": {
                        "_id": "6470e0f1cfd57849519033a5",
                        "avatarUrl": "/avatars/7ffefee3e36a4e37b9f4510bc6b689d1.svg",
                        "isPro": false,
                        "fullname": "Hanning Zhang",
                        "user": "HanningZhang",
                        "type": "user"
                    },
                    "name": "Hanning Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:59:20.459Z",
                    "hidden": false
                },
                {
                    "_id": "6819a63c64ae18f1b60a5c46",
                    "user": {
                        "_id": "63a3ff69f91ad3ea5703841d",
                        "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
                        "isPro": false,
                        "fullname": "Hanze Dong",
                        "user": "hendrydong",
                        "type": "user"
                    },
                    "name": "Hanze Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:59:27.200Z",
                    "hidden": false
                },
                {
                    "_id": "6819a63c64ae18f1b60a5c47",
                    "user": {
                        "_id": "6319b29809baf858241f05de",
                        "avatarUrl": "/avatars/29eef2c52814abea82e2aa9bf37a7f9c.svg",
                        "isPro": false,
                        "fullname": "Xiong",
                        "user": "WeiXiong",
                        "type": "user"
                    },
                    "name": "Wei Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:59:34.381Z",
                    "hidden": false
                },
                {
                    "_id": "6819a63c64ae18f1b60a5c48",
                    "user": {
                        "_id": "64b8922ca1827cc8d04ae919",
                        "avatarUrl": "/avatars/0aaa83e3d09a82434e1d6af724aaa485.svg",
                        "isPro": false,
                        "fullname": "Nan Jiang",
                        "user": "nanjiang",
                        "type": "user"
                    },
                    "name": "Nan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:59:47.400Z",
                    "hidden": false
                },
                {
                    "_id": "6819a63c64ae18f1b60a5c49",
                    "name": "Tong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T06:26:00.000Z",
            "submittedOnDailyAt": "2025-05-06T04:34:14.120Z",
            "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
            "submittedOnDailyBy": {
                "_id": "64d45451c34a346181b130dd",
                "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                "isPro": false,
                "fullname": "Rui Yang",
                "user": "Ray2333",
                "type": "user"
            },
            "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
            "upvotes": 20,
            "discussionId": "6819a63d64ae18f1b60a5c75",
            "ai_keywords": [
                "Chain-of-thought (CoT)",
                "latent variable problem",
                "iterative reward-ranked fine-tuning (RAFT)",
                "inference budget",
                "static sampling strategies",
                "GVM-RAFT",
                "Dynamic Sample Allocation Strategy",
                "prompt-specific",
                "computational budget constraint",
                "prompt acceptance rates",
                "stochastic gradient norms",
                "stochastic gradient variance",
                "accelerated convergence guarantees",
                "GRPO",
                "convergence",
                "test accuracy"
            ]
        },
        "publishedAt": "2025-05-05T02:26:00.000Z",
        "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
        "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02391.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "fullname": "Rui Yang",
            "name": "Ray2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.01658",
            "authors": [
                {
                    "_id": "6819950bd55db085708dd2e5",
                    "user": {
                        "_id": "670cb786e73576f33a339144",
                        "avatarUrl": "/avatars/c172887c32878aebafd786061680ea1e.svg",
                        "isPro": false,
                        "fullname": "Sihyeong Park",
                        "user": "inputsh",
                        "type": "user"
                    },
                    "name": "Sihyeong Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:09:00.902Z",
                    "hidden": false
                },
                {
                    "_id": "6819950bd55db085708dd2e6",
                    "name": "Sungryeol Jeon",
                    "hidden": false
                },
                {
                    "_id": "6819950bd55db085708dd2e7",
                    "user": {
                        "_id": "64aaa12a04e7b379fed24327",
                        "avatarUrl": "/avatars/327482e569c24ee4c97064f07ddd6de7.svg",
                        "isPro": false,
                        "fullname": "Chaelyn Lee",
                        "user": "oos2",
                        "type": "user"
                    },
                    "name": "Chaelyn Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:09:13.239Z",
                    "hidden": false
                },
                {
                    "_id": "6819950bd55db085708dd2e8",
                    "user": {
                        "_id": "6719f17ac5837d514cfff13b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GnL4RCj7xncVhCIFN5y35.png",
                        "isPro": false,
                        "fullname": "Seokhun Jeon",
                        "user": "Devcow",
                        "type": "user"
                    },
                    "name": "Seokhun Jeon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:09:18.870Z",
                    "hidden": false
                },
                {
                    "_id": "6819950bd55db085708dd2e9",
                    "name": "Byung-Soo Kim",
                    "hidden": false
                },
                {
                    "_id": "6819950bd55db085708dd2ea",
                    "user": {
                        "_id": "65b9dee19c4955ae7aee4954",
                        "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
                        "isPro": false,
                        "fullname": "Jemin Lee",
                        "user": "leejaymin",
                        "type": "user"
                    },
                    "name": "Jemin Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:32:52.950Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-03T02:47:43.000Z",
            "submittedOnDailyAt": "2025-05-06T03:21:53.083Z",
            "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
            "submittedOnDailyBy": {
                "_id": "65b9dee19c4955ae7aee4954",
                "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
                "isPro": false,
                "fullname": "Jemin Lee",
                "user": "leejaymin",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
            "upvotes": 19,
            "discussionId": "6819950cd55db085708dd32a",
            "ai_keywords": [
                "chain-of-thought",
                "complex reasoning",
                "agent services",
                "inference cost",
                "parallelism",
                "compression",
                "caching",
                "LLM inference engines",
                "ease-of-use",
                "ease-of-deployment",
                "general-purpose support",
                "scalability",
                "throughput-aware computation",
                "latency-aware computation",
                "optimization techniques",
                "ecosystem maturity",
                "performance",
                "cost policy",
                "LLM-based services",
                "enhanced security"
            ]
        },
        "publishedAt": "2025-05-02T22:47:43.000Z",
        "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
        "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01658.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b9dee19c4955ae7aee4954",
            "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
            "fullname": "Jemin Lee",
            "name": "leejaymin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02835",
            "authors": [
                {
                    "_id": "6819762e64ae18f1b6fde347",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "Yi-Fan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:57:15.220Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde348",
                    "user": {
                        "_id": "664ba004bfd9b93ba4bfb353",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/UHaEcXmMSKvvFDsY3hCnb.jpeg",
                        "isPro": false,
                        "fullname": "LuXingyu",
                        "user": "XingyuLu",
                        "type": "user"
                    },
                    "name": "Xingyu Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:57:24.963Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde349",
                    "name": "Xiao Hu",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde34a",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde34b",
                    "name": "Bin Wen",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde34c",
                    "name": "Tianke Zhang",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde34d",
                    "user": {
                        "_id": "673421bf18caf8e877861cc6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8UfIZTUTFaCnWmJ_Bztr.png",
                        "isPro": false,
                        "fullname": "Changyi Liu",
                        "user": "bhsc24",
                        "type": "user"
                    },
                    "name": "Changyi Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:58:28.151Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde34e",
                    "user": {
                        "_id": "63774c47455f6ad89ac41be1",
                        "avatarUrl": "/avatars/e7d6048155cdf4497d58aa18523e745e.svg",
                        "isPro": false,
                        "fullname": "Kaiyu Jiang",
                        "user": "KaiyuValley",
                        "type": "user"
                    },
                    "name": "Kaiyu Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:58:21.743Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde34f",
                    "name": "Kaibing Chen",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde350",
                    "user": {
                        "_id": "66c605e808fee728d0dd94f5",
                        "avatarUrl": "/avatars/d2ff37fedc5ac1b5b817543b80bf5256.svg",
                        "isPro": false,
                        "fullname": "Kaiyu Tang",
                        "user": "KevinTowne",
                        "type": "user"
                    },
                    "name": "Kaiyu Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:58:06.587Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde351",
                    "user": {
                        "_id": "6610f64ee94d9046b71e19c8",
                        "avatarUrl": "/avatars/11cc11199669129a740956d12c7214e8.svg",
                        "isPro": false,
                        "fullname": "Haojie Ding",
                        "user": "haojieding",
                        "type": "user"
                    },
                    "name": "Haojie Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:57:59.222Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde352",
                    "user": {
                        "_id": "6433abff546e16f17a0f1cd8",
                        "avatarUrl": "/avatars/7c9bbcba69b823834eb0232da12cc7a9.svg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "jiankang",
                        "type": "user"
                    },
                    "name": "Jiankang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:57:51.408Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde353",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde354",
                    "name": "Zhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde355",
                    "user": {
                        "_id": "656453832bdaccfcd5379431",
                        "avatarUrl": "/avatars/a0d764ce6b3fd05532c7a9cb2f263e33.svg",
                        "isPro": false,
                        "fullname": "Gao Ting",
                        "user": "TingTingGao",
                        "type": "user"
                    },
                    "name": "Tingting Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T08:57:35.059Z",
                    "hidden": false
                },
                {
                    "_id": "6819762e64ae18f1b6fde356",
                    "name": "Liang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T17:59:50.000Z",
            "submittedOnDailyAt": "2025-05-06T01:09:45.446Z",
            "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "623d8ca4c29adf5ef6175615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                "isPro": false,
                "fullname": "Yi-Fan Zhang",
                "user": "yifanzhang114",
                "type": "user"
            },
            "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
            "upvotes": 18,
            "discussionId": "6819762f64ae18f1b6fde387",
            "projectPage": "https://github.com/yfzhang114/r1_reward",
            "githubRepo": "https://github.com/yfzhang114/r1_reward",
            "ai_keywords": [
                "Multimodal Reward Models (MRMs)",
                "Multimodal Large Language Models (MLLMs)",
                "Reinforcement Learning (RL)",
                "rule-based RL task",
                "Reinforce++",
                "StableReinforce",
                "training loss",
                "advantage estimation strategy",
                "reward design",
                "preference data",
                "VL Reward-Bench",
                "Multimodal Reward Bench"
            ]
        },
        "publishedAt": "2025-05-05T13:59:50.000Z",
        "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
        "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02835.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.01441",
            "authors": [
                {
                    "_id": "68198aea57d4de18fb3e69d6",
                    "user": {
                        "_id": "61ffaa2943eb0913fa2df74a",
                        "avatarUrl": "/avatars/a19971f830abb8a8ae95e5800beb9fcd.svg",
                        "isPro": false,
                        "fullname": "Singh",
                        "user": "joykirat",
                        "type": "user"
                    },
                    "name": "Joykirat Singh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:08.255Z",
                    "hidden": false
                },
                {
                    "_id": "68198aea57d4de18fb3e69d7",
                    "user": {
                        "_id": "622ca32345261ac5cc0bdade",
                        "avatarUrl": "/avatars/7e1d633be69cf86a3affb9168b1cc27b.svg",
                        "isPro": false,
                        "fullname": "Raghav Magazine",
                        "user": "Raghav2002",
                        "type": "user"
                    },
                    "name": "Raghav Magazine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:06:32.498Z",
                    "hidden": false
                },
                {
                    "_id": "68198aea57d4de18fb3e69d8",
                    "user": {
                        "_id": "64aba383fddf117e6e5ba818",
                        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
                        "isPro": false,
                        "fullname": "Akshay  Nambi",
                        "user": "akshaynambi",
                        "type": "user"
                    },
                    "name": "Yash Pandya",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-06T04:08:10.843Z",
                    "hidden": false
                },
                {
                    "_id": "68198aea57d4de18fb3e69d9",
                    "user": {
                        "_id": "64aba383fddf117e6e5ba818",
                        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
                        "isPro": false,
                        "fullname": "Akshay  Nambi",
                        "user": "akshaynambi",
                        "type": "user"
                    },
                    "name": "Akshay Nambi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:06:46.024Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-28T10:42:49.000Z",
            "submittedOnDailyAt": "2025-05-06T02:43:42.049Z",
            "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "64aba383fddf117e6e5ba818",
                "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
                "isPro": false,
                "fullname": "Akshay  Nambi",
                "user": "akshaynambi",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
            "upvotes": 18,
            "discussionId": "68198aec57d4de18fb3e6a30",
            "projectPage": "https://www.microsoft.com/en-us/research/people/akshayn/unlocking-agentic-reasoning-in-llms/",
            "ai_keywords": [
                "agentic reasoning",
                "reinforcement learning",
                "tool integration",
                "ARTIST",
                "multi-turn reasoning chains",
                "outcome-based RL",
                "mathematical reasoning",
                "function calling",
                "agentic RL",
                "tool use",
                "environment interaction"
            ]
        },
        "publishedAt": "2025-04-28T06:42:49.000Z",
        "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
        "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01441.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "fullname": "Akshay  Nambi",
            "name": "akshaynambi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02156",
            "authors": [
                {
                    "_id": "681975a9fdcf582e6d0effdb",
                    "user": {
                        "_id": "64bcc373ef8c0e42bf16acc5",
                        "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
                        "isPro": false,
                        "fullname": "mz.w",
                        "user": "iiiiwis",
                        "type": "user"
                    },
                    "name": "Minzheng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:29.678Z",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effdc",
                    "user": {
                        "_id": "66641b2fd8e1e34bc621e688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
                        "isPro": false,
                        "fullname": "Yongbin Li",
                        "user": "Yongbin-Li",
                        "type": "user"
                    },
                    "name": "Yongbin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:05:06.051Z",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effdd",
                    "name": "Haobo Wang",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effde",
                    "name": "Xinghua Zhang",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effdf",
                    "name": "Nan Xu",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effe0",
                    "user": {
                        "_id": "668bd45044ab5de7e4c5b1e7",
                        "avatarUrl": "/avatars/9b087cfcac65a649a12568b601d5ca53.svg",
                        "isPro": false,
                        "fullname": "bingli wu",
                        "user": "bingliwu",
                        "type": "user"
                    },
                    "name": "Bingli Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:05:37.861Z",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effe1",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effe2",
                    "name": "Haiyang Yu",
                    "hidden": false
                },
                {
                    "_id": "681975a9fdcf582e6d0effe3",
                    "name": "Wenji Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-04T15:39:58.000Z",
            "submittedOnDailyAt": "2025-05-06T01:07:27.275Z",
            "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
            "submittedOnDailyBy": {
                "_id": "64bcc373ef8c0e42bf16acc5",
                "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
                "isPro": false,
                "fullname": "mz.w",
                "user": "iiiiwis",
                "type": "user"
            },
            "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
            "upvotes": 16,
            "discussionId": "681975a9fdcf582e6d0f0014",
            "githubRepo": "https://github.com/MozerWang/AMPO",
            "ai_keywords": [
                "Adaptive Mode Learning (AML)",
                "Adaptive Mode Policy Optimization (AMPO)",
                "multi-granular thinking mode design",
                "context-aware mode switching",
                "token-efficient reasoning",
                "depth-adaptive processing",
                "intuitive reaction",
                "deep contemplation",
                "social interaction",
                "reasoning chains",
                "fixed-depth approach"
            ]
        },
        "publishedAt": "2025-05-04T11:39:58.000Z",
        "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
        "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02156.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64bcc373ef8c0e42bf16acc5",
            "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
            "fullname": "mz.w",
            "name": "iiiiwis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02094",
            "authors": [
                {
                    "_id": "681992911e0fae3880173d43",
                    "user": {
                        "_id": "66d59dc9b005ad82ca6fc61d",
                        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
                        "isPro": false,
                        "fullname": "Runyi YU",
                        "user": "IngridYU",
                        "type": "user"
                    },
                    "name": "Runyi Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:00:09.333Z",
                    "hidden": false
                },
                {
                    "_id": "681992911e0fae3880173d44",
                    "name": "Yinhuai Wang",
                    "hidden": false
                },
                {
                    "_id": "681992911e0fae3880173d45",
                    "user": {
                        "_id": "64341911546e16f17a129733",
                        "avatarUrl": "/avatars/ae12aafc8932a7537838e6d3964858cb.svg",
                        "isPro": false,
                        "fullname": "QiHan Zhao",
                        "user": "Crimnos",
                        "type": "user"
                    },
                    "name": "Qihan Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:00:32.182Z",
                    "hidden": false
                },
                {
                    "_id": "681992911e0fae3880173d46",
                    "name": "Hok Wai Tsui",
                    "hidden": false
                },
                {
                    "_id": "681992911e0fae3880173d47",
                    "name": "Jingbo Wang",
                    "hidden": false
                },
                {
                    "_id": "681992911e0fae3880173d48",
                    "name": "Ping Tan",
                    "hidden": false
                },
                {
                    "_id": "681992911e0fae3880173d49",
                    "user": {
                        "_id": "6467b121e7a6a374fd19b44b",
                        "avatarUrl": "/avatars/3f2874d58986d651aef55e3408b05700.svg",
                        "isPro": false,
                        "fullname": "Qifeng Chen",
                        "user": "cqf",
                        "type": "user"
                    },
                    "name": "Qifeng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:01:21.751Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-04T13:00:29.000Z",
            "submittedOnDailyAt": "2025-05-06T03:11:28.738Z",
            "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
            "submittedOnDailyBy": {
                "_id": "66d59dc9b005ad82ca6fc61d",
                "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
                "isPro": false,
                "fullname": "Runyi YU",
                "user": "IngridYU",
                "type": "user"
            },
            "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
            "upvotes": 14,
            "discussionId": "681992931e0fae3880173dcf",
            "ai_keywords": [
                "Reinforcement Learning from Interaction Demonstration (RLID)",
                "demonstration noise",
                "coverage limitations",
                "interaction demonstrations",
                "sparse trajectories",
                "disconnected trajectories",
                "noise",
                "skill variations",
                "transitions",
                "physically feasible trajectories",
                "Stitched Trajectory Graph (STG)",
                "State Transition Field (STF)",
                "Adaptive Trajectory Sampling (ATS)",
                "dynamic curriculum generation",
                "historical encoding mechanism",
                "skill acquisition",
                "convergence stability",
                "generalization capability",
                "recovery robustness"
            ]
        },
        "publishedAt": "2025-05-04T09:00:29.000Z",
        "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
        "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02094.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66d59dc9b005ad82ca6fc61d",
            "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
            "fullname": "Runyi YU",
            "name": "IngridYU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02625",
            "authors": [
                {
                    "_id": "681975abba26bf20601bb7ca",
                    "user": {
                        "_id": "65b7573482d384513443875e",
                        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
                        "isPro": false,
                        "fullname": "Qingkai Fang",
                        "user": "poeroz",
                        "type": "user"
                    },
                    "name": "Qingkai Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:15:43.683Z",
                    "hidden": false
                },
                {
                    "_id": "681975abba26bf20601bb7cb",
                    "name": "Yan Zhou",
                    "hidden": false
                },
                {
                    "_id": "681975abba26bf20601bb7cc",
                    "user": {
                        "_id": "66680c0505c407bfea87667c",
                        "avatarUrl": "/avatars/e3c26d2eb13fe8ad2b3fd16897e61e6d.svg",
                        "isPro": false,
                        "fullname": "Shoutao Guo",
                        "user": "guoshoutao",
                        "type": "user"
                    },
                    "name": "Shoutao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:15:53.154Z",
                    "hidden": false
                },
                {
                    "_id": "681975abba26bf20601bb7cd",
                    "user": {
                        "_id": "64803e5dc57f629056c601f1",
                        "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
                        "isPro": false,
                        "fullname": "Shaolei Zhang",
                        "user": "zhangshaolei",
                        "type": "user"
                    },
                    "name": "Shaolei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:16:00.139Z",
                    "hidden": false
                },
                {
                    "_id": "681975abba26bf20601bb7ce",
                    "name": "Yang Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T12:53:09.000Z",
            "submittedOnDailyAt": "2025-05-06T01:07:20.259Z",
            "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
            "submittedOnDailyBy": {
                "_id": "65b7573482d384513443875e",
                "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
                "isPro": false,
                "fullname": "Qingkai Fang",
                "user": "poeroz",
                "type": "user"
            },
            "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
            "upvotes": 10,
            "discussionId": "681975abba26bf20601bb7f2",
            "ai_keywords": [
                "speech language models (SpeechLMs)",
                "Qwen2.5",
                "speech encoder",
                "autoregressive streaming speech decoder",
                "spoken question answering",
                "speech instruction following",
                "GLM-4-Voice"
            ]
        },
        "publishedAt": "2025-05-05T08:53:09.000Z",
        "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
        "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02625.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b7573482d384513443875e",
            "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
            "fullname": "Qingkai Fang",
            "name": "poeroz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02471",
            "authors": [
                {
                    "_id": "681973cfa70a4728958323aa",
                    "user": {
                        "_id": "644fcbea4f7316588267dc80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
                        "isPro": false,
                        "fullname": "Biao Gong",
                        "user": "BiaoGong",
                        "type": "user"
                    },
                    "name": "Biao Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:10:35.774Z",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323ab",
                    "name": "Cheng Zou",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323ac",
                    "user": {
                        "_id": "65dd699a89a2a760d15f7d35",
                        "avatarUrl": "/avatars/e098b56c413d147d1f38cf33a4b0ecde.svg",
                        "isPro": false,
                        "fullname": "Dandan Zheng",
                        "user": "zhengdd0422",
                        "type": "user"
                    },
                    "name": "Dandan Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:10:11.829Z",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323ad",
                    "name": "Hu Yu",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323ae",
                    "user": {
                        "_id": "64575ac8cd935d48a47774ec",
                        "avatarUrl": "/avatars/5d211e2c13d6c4e011e5e58b738413f7.svg",
                        "isPro": false,
                        "fullname": "chenjingdong ",
                        "user": "chenjingdong",
                        "type": "user"
                    },
                    "name": "Jingdong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:10:51.440Z",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323af",
                    "user": {
                        "_id": "6417cd278f689506e71439ac",
                        "avatarUrl": "/avatars/0993d834c6c3bbc53081aa139ee14a12.svg",
                        "isPro": false,
                        "fullname": "jianxinsun",
                        "user": "jianxinsun",
                        "type": "user"
                    },
                    "name": "Jianxin Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:10:02.728Z",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b0",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b1",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b2",
                    "name": "Kaixiang Ji",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b3",
                    "name": "Lixiang Ru",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b4",
                    "name": "Libin Wang",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b5",
                    "name": "Qingpei Guo",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b6",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b7",
                    "name": "Weilong Chai",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b8",
                    "user": {
                        "_id": "67cc852d2cfa481bce2dd07e",
                        "avatarUrl": "/avatars/0c1c32ec066a8de9148b083b39d1fab8.svg",
                        "isPro": false,
                        "fullname": "xinyu xiao",
                        "user": "bear-xxy",
                        "type": "user"
                    },
                    "name": "Xinyu Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:11:49.316Z",
                    "hidden": false
                },
                {
                    "_id": "681973cfa70a4728958323b9",
                    "name": "Ziyuan Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T08:56:12.000Z",
            "submittedOnDailyAt": "2025-05-06T01:00:49.692Z",
            "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
            "submittedOnDailyBy": {
                "_id": "644fcbea4f7316588267dc80",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
                "isPro": false,
                "fullname": "Biao Gong",
                "user": "BiaoGong",
                "type": "user"
            },
            "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
            "upvotes": 9,
            "discussionId": "681973d2a70a47289583249d",
            "projectPage": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
            "githubRepo": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
            "ai_keywords": [
                "unified visual generator",
                "multimodal autoregressive model",
                "MetaQueries",
                "M2-omni framework",
                "multi-scale learnable tokens",
                "multi-scale representation alignment strategy",
                "MLLM",
                "learnable diffusion model",
                "text-to-image generation",
                "instruction based image editing"
            ]
        },
        "publishedAt": "2025-05-05T04:56:12.000Z",
        "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
        "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02471.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "644fcbea4f7316588267dc80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "fullname": "Biao Gong",
            "name": "BiaoGong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02370",
            "authors": [
                {
                    "_id": "68197c200e4203d6bc84cdfb",
                    "user": {
                        "_id": "637f0eb22438d7485b8ef5d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
                        "isPro": false,
                        "fullname": "Ming Li",
                        "user": "limingcv",
                        "type": "user"
                    },
                    "name": "Ming Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:18.243Z",
                    "hidden": false
                },
                {
                    "_id": "68197c200e4203d6bc84cdfc",
                    "name": "Xin Gu",
                    "hidden": false
                },
                {
                    "_id": "68197c200e4203d6bc84cdfd",
                    "name": "Fan Chen",
                    "hidden": false
                },
                {
                    "_id": "68197c200e4203d6bc84cdfe",
                    "user": {
                        "_id": "64ca92f738837b12d5f63729",
                        "avatarUrl": "/avatars/a361be3a5ccf9368717980d1faf69df0.svg",
                        "isPro": false,
                        "fullname": "Xiaoying Xing",
                        "user": "xiaoying0505",
                        "type": "user"
                    },
                    "name": "Xiaoying Xing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:06:58.770Z",
                    "hidden": false
                },
                {
                    "_id": "68197c200e4203d6bc84cdff",
                    "user": {
                        "_id": "644df7eacfb40c94eae71186",
                        "avatarUrl": "/avatars/1daa4967efd34d54c59aa95970093dbd.svg",
                        "isPro": false,
                        "fullname": "Longyin Wen",
                        "user": "lionwen",
                        "type": "user"
                    },
                    "name": "Longyin Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:07:05.196Z",
                    "hidden": false
                },
                {
                    "_id": "68197c200e4203d6bc84ce00",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "68197c200e4203d6bc84ce01",
                    "user": {
                        "_id": "65cbdea6d6c974694f09249a",
                        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
                        "isPro": false,
                        "fullname": "Sijie Zhu",
                        "user": "Zilence006",
                        "type": "user"
                    },
                    "name": "Sijie Zhu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-06T03:04:04.536Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T05:19:40.000Z",
            "submittedOnDailyAt": "2025-05-06T01:34:41.608Z",
            "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
            "submittedOnDailyBy": {
                "_id": "637f0eb22438d7485b8ef5d7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
                "isPro": false,
                "fullname": "Ming Li",
                "user": "limingcv",
                "type": "user"
            },
            "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
            "upvotes": 9,
            "discussionId": "68197c240e4203d6bc84cee9",
            "projectPage": "https://liming-ai.github.io/SuperEdit/",
            "githubRepo": "https://github.com/bytedance/SuperEdit",
            "ai_keywords": [
                "contrastive editing instructions",
                "triplet loss",
                "instruction-based image editing",
                "contrastive supervision signals",
                "generation attributes",
                "unified guide",
                "vision-language models (VLMs)",
                "real-edit benchmark",
                "smartedit"
            ]
        },
        "publishedAt": "2025-05-05T01:19:40.000Z",
        "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
        "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02370.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637f0eb22438d7485b8ef5d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
            "fullname": "Ming Li",
            "name": "limingcv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.01043",
            "authors": [
                {
                    "_id": "68196e23d9cad0bb5c90dd9b",
                    "user": {
                        "_id": "64a62e3302e46deb19a7937e",
                        "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
                        "isPro": false,
                        "fullname": "Zhiwei Hao",
                        "user": "Zhiwei840",
                        "type": "user"
                    },
                    "name": "Zhiwei Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:34:14.150Z",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dd9c",
                    "user": {
                        "_id": "65c4a574d2db41f74ab2a808",
                        "avatarUrl": "/avatars/997a8a51996e909eeb318dc592b6c67a.svg",
                        "isPro": false,
                        "fullname": "Jianyuan Guo",
                        "user": "GGJY",
                        "type": "user"
                    },
                    "name": "Jianyuan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:08:12.386Z",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dd9d",
                    "name": "Li Shen",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dd9e",
                    "user": {
                        "_id": "6306dc1fd37ce67e0e53c202",
                        "avatarUrl": "/avatars/d53a29925511a516495b1597fd5dc764.svg",
                        "isPro": false,
                        "fullname": "Yong Luo",
                        "user": "csdvT",
                        "type": "user"
                    },
                    "name": "Yong Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:08:19.043Z",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dd9f",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dda0",
                    "user": {
                        "_id": "662520a75480987954af60b5",
                        "avatarUrl": "/avatars/75d2509d21901c4bb187e93b23540e19.svg",
                        "isPro": false,
                        "fullname": "Guoxia Wang",
                        "user": "Guoxia",
                        "type": "user"
                    },
                    "name": "Guoxia Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:08:27.139Z",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dda1",
                    "name": "Dianhai Yu",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dda2",
                    "name": "Yonggang Wen",
                    "hidden": false
                },
                {
                    "_id": "68196e23d9cad0bb5c90dda3",
                    "name": "Dacheng Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-02T06:33:25.000Z",
            "submittedOnDailyAt": "2025-05-06T00:36:54.063Z",
            "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
            "submittedOnDailyBy": {
                "_id": "64a62e3302e46deb19a7937e",
                "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
                "isPro": false,
                "fullname": "Zhiwei Hao",
                "user": "Zhiwei840",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
            "upvotes": 9,
            "discussionId": "68196e24d9cad0bb5c90de08",
            "githubRepo": "https://github.com/Hao840/Awesome-Low-Precision-Training",
            "ai_keywords": [
                "low-precision training",
                "weights",
                "activations",
                "gradients",
                "fixed-point",
                "integer-based methods",
                "floating-point-based methods",
                "customized format-based methods",
                "quantization-aware training"
            ]
        },
        "publishedAt": "2025-05-02T02:33:25.000Z",
        "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
        "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01043.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64a62e3302e46deb19a7937e",
            "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
            "fullname": "Zhiwei Hao",
            "name": "Zhiwei840",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.01583",
            "authors": [
                {
                    "_id": "6819814653612b577df718e7",
                    "user": {
                        "_id": "65cd4d6256671dee8ee46392",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65cd4d6256671dee8ee46392/SH30XVQnGiYqYQIeDk3na.jpeg",
                        "isPro": false,
                        "fullname": "Jen-Hao (Andy) Cheng",
                        "user": "andaba",
                        "type": "user"
                    },
                    "name": "Jen-Hao Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T08:33:15.728Z",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718e8",
                    "name": "Vivian Wang",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718e9",
                    "name": "Huayu Wang",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718ea",
                    "name": "Huapeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718eb",
                    "name": "Yi-Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718ec",
                    "name": "Hou-I Liu",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718ed",
                    "user": {
                        "_id": "647e4e8da49bffab5d72fbe0",
                        "avatarUrl": "/avatars/c5fb00019c7cea23fe3351ecb1e43195.svg",
                        "isPro": false,
                        "fullname": "Hsiang-Wei Huang",
                        "user": "hsiangwei0903",
                        "type": "user"
                    },
                    "name": "Hsiang-Wei Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:13:19.727Z",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718ee",
                    "name": "Kuang-Ming Chen",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718ef",
                    "name": "Cheng-Yen Yang",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718f0",
                    "user": {
                        "_id": "637c7503fe115289cfecbe6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
                        "isPro": false,
                        "fullname": "Wenhao Chai",
                        "user": "wchai",
                        "type": "user"
                    },
                    "name": "Wenhao Chai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:13:37.240Z",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718f1",
                    "user": {
                        "_id": "65f8cb651e0c65c13a2b906a",
                        "avatarUrl": "/avatars/ffc8ac8f29ab1a3142fe5fab1b2302ca.svg",
                        "isPro": false,
                        "fullname": "Yi-Ling Chen",
                        "user": "yilche",
                        "type": "user"
                    },
                    "name": "Yi-Ling Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:13:43.313Z",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718f2",
                    "user": {
                        "_id": "63c8527becdb7c9fdd9cacc6",
                        "avatarUrl": "/avatars/c8a3f5e1e5159ae5ead41bd9fc2b9b34.svg",
                        "isPro": false,
                        "fullname": "Vibhav Vineet",
                        "user": "vibhav-vineet",
                        "type": "user"
                    },
                    "name": "Vibhav Vineet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:13:49.482Z",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718f3",
                    "name": "Qin Cai",
                    "hidden": false
                },
                {
                    "_id": "6819814653612b577df718f4",
                    "name": "Jenq-Neng Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-02T21:00:17.000Z",
            "submittedOnDailyAt": "2025-05-06T01:56:09.960Z",
            "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
            "submittedOnDailyBy": {
                "_id": "637c7503fe115289cfecbe6b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
                "isPro": false,
                "fullname": "Wenhao Chai",
                "user": "wchai",
                "type": "user"
            },
            "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
            "upvotes": 7,
            "discussionId": "6819814853612b577df71943",
            "ai_keywords": [
                "TEMPURA",
                "masked event prediction",
                "causal explanations",
                "dense event annotations",
                "infilling techniques",
                "video segmentation",
                "dense captioning",
                "non-overlapping events",
                "timestamp-aligned descriptions",
                "VER",
                "temporal grounding",
                "highlight detection",
                "baseline models",
                "causal reasoning",
                "fine-grained temporal segmentation"
            ]
        },
        "publishedAt": "2025-05-02T17:00:17.000Z",
        "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
        "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01583.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "fullname": "Wenhao Chai",
            "name": "wchai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02823",
            "authors": [
                {
                    "_id": "6819893117007d963b997a0b",
                    "user": {
                        "_id": "66b2e5f5523bf90aa7057467",
                        "avatarUrl": "/avatars/ccdb58c2e56cf861e9dcec50c85d7778.svg",
                        "isPro": false,
                        "fullname": "Guo",
                        "user": "Zinan123212",
                        "type": "user"
                    },
                    "name": "Zinan Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:14:34.748Z",
                    "hidden": false
                },
                {
                    "_id": "6819893117007d963b997a0c",
                    "name": "Pengze Zhang",
                    "hidden": false
                },
                {
                    "_id": "6819893117007d963b997a0d",
                    "user": {
                        "_id": "639709c2be8a14bb9eeea8f6",
                        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
                        "isPro": false,
                        "fullname": "Yanze Wu",
                        "user": "yanze",
                        "type": "user"
                    },
                    "name": "Yanze Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:15:02.034Z",
                    "hidden": false
                },
                {
                    "_id": "6819893117007d963b997a0e",
                    "name": "Chong Mou",
                    "hidden": false
                },
                {
                    "_id": "6819893117007d963b997a0f",
                    "name": "Songtao Zhao",
                    "hidden": false
                },
                {
                    "_id": "6819893117007d963b997a10",
                    "user": {
                        "_id": "645dcad7a19f3e64bbf35e6c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rV1uHDSnZv7jAvFq4ftj4.jpeg",
                        "isPro": false,
                        "fullname": "Qian He",
                        "user": "heqian",
                        "type": "user"
                    },
                    "name": "Qian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:15:28.996Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T17:50:24.000Z",
            "submittedOnDailyAt": "2025-05-06T02:30:32.888Z",
            "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
            "submittedOnDailyBy": {
                "_id": "639709c2be8a14bb9eeea8f6",
                "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
                "isPro": false,
                "fullname": "Yanze Wu",
                "user": "yanze",
                "type": "user"
            },
            "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
            "upvotes": 3,
            "discussionId": "6819893317007d963b997ab1",
            "githubRepo": "https://github.com/guozinan126/MUSAR",
            "ai_keywords": [
                "debiased diptych learning",
                "diptych training pairs",
                "static attention routing",
                "dual-branch LoRA",
                "dynamic attention routing mechanism",
                "bijective mappings",
                "multi-subject representations",
                "scalable generalization performance"
            ]
        },
        "publishedAt": "2025-05-05T13:50:24.000Z",
        "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
        "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02823.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "fullname": "Yanze Wu",
            "name": "yanze",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 140
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02005",
            "authors": [
                {
                    "_id": "6819f4557ef45d006f8b6755",
                    "user": {
                        "_id": "6354bda206d707b33249c4c2",
                        "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
                        "isPro": false,
                        "fullname": "Zhenxing Mi",
                        "user": "Mifucius",
                        "type": "user"
                    },
                    "name": "Zhenxing Mi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T16:51:34.076Z",
                    "hidden": false
                },
                {
                    "_id": "6819f4557ef45d006f8b6756",
                    "name": "Ping Yin",
                    "hidden": false
                },
                {
                    "_id": "6819f4557ef45d006f8b6757",
                    "name": "Xue Xiao",
                    "hidden": false
                },
                {
                    "_id": "6819f4557ef45d006f8b6758",
                    "name": "Dan Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-04T06:25:14.000Z",
            "submittedOnDailyAt": "2025-05-06T10:07:34.406Z",
            "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields",
            "submittedOnDailyBy": {
                "_id": "6354bda206d707b33249c4c2",
                "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
                "isPro": false,
                "fullname": "Zhenxing Mi",
                "user": "Mifucius",
                "type": "user"
            },
            "summary": "Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decomposes scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts, by our proposed Sparsely Gated Mixture of\nExperts (MoE) NeRF framework. We incorporate a hash-based gating network and\ndistinct heterogeneous hash experts. The hash-based gating efficiently learns\nthe decomposition of the large-scale scene. The distinct heterogeneous hash\nexperts consist of hash grids of different resolution ranges, enabling\neffective learning of the heterogeneous representation of different scene\nparts. These design choices make our framework an end-to-end and highly\nscalable NeRF solution for real-world large-scale scene modeling to achieve\nboth quality and efficiency. We evaluate our accuracy and scalability on\nexisting large-scale NeRF datasets and a new dataset with very large-scale\nscenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our\napproach can be easily scaled to various large-scale scenes and achieve\nstate-of-the-art scene rendering accuracy. Furthermore, our method exhibits\nsignificant efficiency, with an 8x acceleration in training and a 16x\nacceleration in rendering compared to Switch-NeRF. Codes will be released in\nhttps://github.com/MiZhenxing/Switch-NeRF.",
            "upvotes": 3,
            "discussionId": "6819f4577ef45d006f8b67ca",
            "ai_keywords": [
                "NeRF",
                "scene decomposition",
                "scalable NeRFs",
                "learnable decomposition",
                "scene heterogeneity",
                "modeling efficiency",
                "Switch-NeRF++",
                "Heterogeneous Mixture of Hash Experts (HMoHE)",
                "gating network",
                "3D points",
                "NeRF experts",
                "Sparsely Gated Mixture of Experts (MoE)",
                "hash-based gating network",
                "distinct heterogeneous hash experts",
                "hash grids",
                "resolution ranges",
                "heterogeneous representation",
                "real-world large-scale scene modeling",
                "scene rendering accuracy",
                "UrbanBIS"
            ]
        },
        "publishedAt": "2025-05-04T02:25:14.000Z",
        "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields",
        "summary": "Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decomposes scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts, by our proposed Sparsely Gated Mixture of\nExperts (MoE) NeRF framework. We incorporate a hash-based gating network and\ndistinct heterogeneous hash experts. The hash-based gating efficiently learns\nthe decomposition of the large-scale scene. The distinct heterogeneous hash\nexperts consist of hash grids of different resolution ranges, enabling\neffective learning of the heterogeneous representation of different scene\nparts. These design choices make our framework an end-to-end and highly\nscalable NeRF solution for real-world large-scale scene modeling to achieve\nboth quality and efficiency. We evaluate our accuracy and scalability on\nexisting large-scale NeRF datasets and a new dataset with very large-scale\nscenes (>6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our\napproach can be easily scaled to various large-scale scenes and achieve\nstate-of-the-art scene rendering accuracy. Furthermore, our method exhibits\nsignificant efficiency, with an 8x acceleration in training and a 16x\nacceleration in rendering compared to Switch-NeRF. Codes will be released in\nhttps://github.com/MiZhenxing/Switch-NeRF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02005.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6354bda206d707b33249c4c2",
            "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
            "fullname": "Zhenxing Mi",
            "name": "Mifucius",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.01456",
            "authors": [
                {
                    "_id": "6819c7577c36c576e9cb6bfa",
                    "user": {
                        "_id": "64f64da90efa33bfe0a3d9ba",
                        "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
                        "isPro": false,
                        "fullname": "Vaidehi Patil",
                        "user": "vaidehi99",
                        "type": "user"
                    },
                    "name": "Vaidehi Patil",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:16:35.372Z",
                    "hidden": false
                },
                {
                    "_id": "6819c7577c36c576e9cb6bfb",
                    "user": {
                        "_id": "654ffe334d9e71e17becc660",
                        "avatarUrl": "/avatars/022b7a77051d26c4e5cbf254b7352eb9.svg",
                        "isPro": false,
                        "fullname": "Yi-Lin Sung",
                        "user": "a2889184",
                        "type": "user"
                    },
                    "name": "Yi-Lin Sung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:16:41.799Z",
                    "hidden": false
                },
                {
                    "_id": "6819c7577c36c576e9cb6bfc",
                    "name": "Peter Hase",
                    "hidden": false
                },
                {
                    "_id": "6819c7577c36c576e9cb6bfd",
                    "name": "Jie Peng",
                    "hidden": false
                },
                {
                    "_id": "6819c7577c36c576e9cb6bfe",
                    "name": "Tianlong Chen",
                    "hidden": false
                },
                {
                    "_id": "6819c7577c36c576e9cb6bff",
                    "user": {
                        "_id": "665d9d3a057f7c508f98c625",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d9d3a057f7c508f98c625/u1R9P9sJoAl4zEIcetbPy.jpeg",
                        "isPro": false,
                        "fullname": "Mohit Bansal",
                        "user": "mohitbansal",
                        "type": "user"
                    },
                    "name": "Mohit Bansal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-06T09:17:14.823Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-01T01:54:00.000Z",
            "submittedOnDailyAt": "2025-05-06T06:55:27.299Z",
            "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
            "submittedOnDailyBy": {
                "_id": "64f64da90efa33bfe0a3d9ba",
                "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
                "isPro": false,
                "fullname": "Vaidehi Patil",
                "user": "vaidehi99",
                "type": "user"
            },
            "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.",
            "upvotes": 2,
            "discussionId": "6819c7597c36c576e9cb6c6b",
            "githubRepo": "https://github.com/Vaidehi99/UnLOK-VQA",
            "ai_keywords": [
                "multimodal LLMs",
                "multimodal prompts",
                "targeted unlearning",
                "high-quality, well-annotated image-text pairs",
                "multimodal unlearning",
                "UnLOK-VQA (Unlearning Outside Knowledge VQA)",
                "visual question-answering dataset",
                "varying-proximity samples",
                "whitebox attacks",
                "blackbox attacks",
                "interpretability of hidden states",
                "multimodal attacks",
                "post-editing robustness"
            ]
        },
        "publishedAt": "2025-04-30T21:54:00.000Z",
        "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and\n  Attack-Defense Evaluation",
        "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01456.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "fullname": "Vaidehi Patil",
            "name": "vaidehi99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02130",
            "authors": [
                {
                    "_id": "6819f9c7b4ee941e883b2fd2",
                    "user": {
                        "_id": "660162a9eee53450ba93c34b",
                        "avatarUrl": "/avatars/d6128f630e041e29d1cdc178e112f23f.svg",
                        "isPro": false,
                        "fullname": "guanzhong",
                        "user": "guanzhong2",
                        "type": "user"
                    },
                    "name": "Zhong Guan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-06T16:51:31.791Z",
                    "hidden": false
                },
                {
                    "_id": "6819f9c7b4ee941e883b2fd3",
                    "name": "Likang Wu",
                    "hidden": false
                },
                {
                    "_id": "6819f9c7b4ee941e883b2fd4",
                    "name": "Hongke Zhao",
                    "hidden": false
                },
                {
                    "_id": "6819f9c7b4ee941e883b2fd5",
                    "name": "Ming He",
                    "hidden": false
                },
                {
                    "_id": "6819f9c7b4ee941e883b2fd6",
                    "name": "Jianpin Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-04T14:40:31.000Z",
            "submittedOnDailyAt": "2025-05-06T15:30:56.062Z",
            "title": "Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data",
            "submittedOnDailyBy": {
                "_id": "660162a9eee53450ba93c34b",
                "avatarUrl": "/avatars/d6128f630e041e29d1cdc178e112f23f.svg",
                "isPro": false,
                "fullname": "guanzhong",
                "user": "guanzhong2",
                "type": "user"
            },
            "summary": "Attention mechanisms are critical to the success of large language models\n(LLMs), driving significant advancements in multiple fields. However, for\ngraph-structured data, which requires emphasis on topological connections, they\nfall short compared to message-passing mechanisms on fixed links, such as those\nemployed by Graph Neural Networks (GNNs). This raises a question: ``Does\nattention fail for graphs in natural language settings?'' Motivated by these\nobservations, we embarked on an empirical study from the perspective of\nattention mechanisms to explore how LLMs process graph-structured data. The\ngoal is to gain deeper insights into the attention behavior of LLMs over graph\nstructures. We uncovered unique phenomena regarding how LLMs apply attention to\ngraph-structured data and analyzed these findings to improve the modeling of\nsuch data by LLMs. The primary findings of our research are: 1) While LLMs can\nrecognize graph data and capture text-node interactions, they struggle to model\ninter-node relationships within graph structures due to inherent architectural\nconstraints. 2) The attention distribution of LLMs across graph nodes does not\nalign with ideal structural patterns, indicating a failure to adapt to graph\ntopology nuances. 3) Neither fully connected attention nor fixed connectivity\nis optimal; each has specific limitations in its application scenarios.\nInstead, intermediate-state attention windows improve LLM training performance\nand seamlessly transition to fully connected windows during inference. Source\ncode: https://github.com/millioniron/LLM_exploration{LLM4Exploration}",
            "upvotes": 1,
            "discussionId": "6819f9c8b4ee941e883b304a",
            "ai_keywords": [
                "attention mechanisms",
                "large language models (LLMs)",
                "graph-structured data",
                "topological connections",
                "message-passing mechanisms",
                "Graph Neural Networks (GNNs)",
                "empirical study",
                "attention behavior",
                "text-node interactions",
                "inter-node relationships",
                "architectural constraints",
                "attention distribution",
                "ideal structural patterns",
                "fully connected attention",
                "fixed connectivity",
                "intermediate-state attention windows"
            ]
        },
        "publishedAt": "2025-05-04T10:40:31.000Z",
        "title": "Attention Mechanisms Perspective: Exploring LLM Processing of\n  Graph-Structured Data",
        "summary": "Attention mechanisms are critical to the success of large language models\n(LLMs), driving significant advancements in multiple fields. However, for\ngraph-structured data, which requires emphasis on topological connections, they\nfall short compared to message-passing mechanisms on fixed links, such as those\nemployed by Graph Neural Networks (GNNs). This raises a question: ``Does\nattention fail for graphs in natural language settings?'' Motivated by these\nobservations, we embarked on an empirical study from the perspective of\nattention mechanisms to explore how LLMs process graph-structured data. The\ngoal is to gain deeper insights into the attention behavior of LLMs over graph\nstructures. We uncovered unique phenomena regarding how LLMs apply attention to\ngraph-structured data and analyzed these findings to improve the modeling of\nsuch data by LLMs. The primary findings of our research are: 1) While LLMs can\nrecognize graph data and capture text-node interactions, they struggle to model\ninter-node relationships within graph structures due to inherent architectural\nconstraints. 2) The attention distribution of LLMs across graph nodes does not\nalign with ideal structural patterns, indicating a failure to adapt to graph\ntopology nuances. 3) Neither fully connected attention nor fixed connectivity\nis optimal; each has specific limitations in its application scenarios.\nInstead, intermediate-state attention windows improve LLM training performance\nand seamlessly transition to fully connected windows during inference. Source\ncode: https://github.com/millioniron/LLM_exploration{LLM4Exploration}",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02130.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "660162a9eee53450ba93c34b",
            "avatarUrl": "/avatars/d6128f630e041e29d1cdc178e112f23f.svg",
            "fullname": "guanzhong",
            "name": "guanzhong2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.01548",
            "authors": [
                {
                    "_id": "681a662d26c8bff22d483fac",
                    "user": {
                        "_id": "654939b935fa2ade2defa15b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654939b935fa2ade2defa15b/r-Ku79eVKxeCa1f8Ogf9C.png",
                        "isPro": false,
                        "fullname": "Zhen Yao",
                        "user": "Chrisathy",
                        "type": "user"
                    },
                    "name": "Zhen Yao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-06T19:44:04.570Z",
                    "hidden": false
                },
                {
                    "_id": "681a662d26c8bff22d483fad",
                    "name": "Xiaowen Ying",
                    "hidden": false
                },
                {
                    "_id": "681a662d26c8bff22d483fae",
                    "name": "Mooi Choo Chuah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-02T19:19:58.000Z",
            "submittedOnDailyAt": "2025-05-06T18:19:01.849Z",
            "title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional\n  Motion-enhanced Event Representation",
            "submittedOnDailyBy": {
                "_id": "654939b935fa2ade2defa15b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654939b935fa2ade2defa15b/r-Ku79eVKxeCa1f8Ogf9C.png",
                "isPro": false,
                "fullname": "Zhen Yao",
                "user": "Chrisathy",
                "type": "user"
            },
            "summary": "Event cameras capture motion dynamics, offering a unique modality with great\npotential in various computer vision tasks. However, RGB-Event fusion faces\nthree intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal\nmisalignment. Existing voxel grid representations neglect temporal correlations\nbetween consecutive event windows, and their formulation with simple\naccumulation of asynchronous and sparse events is incompatible with the\nsynchronous and dense nature of RGB modality. To tackle these challenges, we\npropose a novel event representation, Motion-enhanced Event Tensor (MET), which\ntransforms sparse event voxels into a dense and temporally coherent form by\nleveraging dense optical flows and event temporal features. In addition, we\nintroduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a\nTemporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to\nmitigate modal misalignment, while bidirectional flow aggregation and temporal\nfusion mechanisms resolve spatiotemporal misalignment. Experimental results on\ntwo large-scale datasets demonstrate that our framework significantly\noutperforms state-of-the-art RGB-Event semantic segmentation approaches. Our\ncode is available at: https://github.com/zyaocoder/BRENet.",
            "upvotes": 1,
            "discussionId": "681a662e26c8bff22d483ff0",
            "githubRepo": "https://github.com/zyaocoder/BRENet",
            "ai_keywords": [
                "Motion-enhanced Event Tensor (MET)",
                "dense optical flows",
                "Frequency-aware Bidirectional Flow Aggregation Module (BFAM)",
                "Temporal Fusion Module (TFM)",
                "spatiotemporal misalignment"
            ]
        },
        "publishedAt": "2025-05-02T15:19:58.000Z",
        "title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional\n  Motion-enhanced Event Representation",
        "summary": "Event cameras capture motion dynamics, offering a unique modality with great\npotential in various computer vision tasks. However, RGB-Event fusion faces\nthree intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal\nmisalignment. Existing voxel grid representations neglect temporal correlations\nbetween consecutive event windows, and their formulation with simple\naccumulation of asynchronous and sparse events is incompatible with the\nsynchronous and dense nature of RGB modality. To tackle these challenges, we\npropose a novel event representation, Motion-enhanced Event Tensor (MET), which\ntransforms sparse event voxels into a dense and temporally coherent form by\nleveraging dense optical flows and event temporal features. In addition, we\nintroduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a\nTemporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to\nmitigate modal misalignment, while bidirectional flow aggregation and temporal\nfusion mechanisms resolve spatiotemporal misalignment. Experimental results on\ntwo large-scale datasets demonstrate that our framework significantly\noutperforms state-of-the-art RGB-Event semantic segmentation approaches. Our\ncode is available at: https://github.com/zyaocoder/BRENet.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01548.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654939b935fa2ade2defa15b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654939b935fa2ade2defa15b/r-Ku79eVKxeCa1f8Ogf9C.png",
            "fullname": "Zhen Yao",
            "name": "Chrisathy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]