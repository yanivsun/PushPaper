[
    {
        "paper": {
            "id": "2505.02567",
            "authors": [
                {
                    "_id": "681c7895c7211b7efbc49f17",
                    "name": "Xinjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f18",
                    "name": "Jintao Guo",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f19",
                    "user": {
                        "_id": "66ab4c8a1703f12f49583c6d",
                        "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
                        "isPro": false,
                        "fullname": "zss",
                        "user": "Suikong",
                        "type": "user"
                    },
                    "name": "Shanshan Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T10:07:03.107Z",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f1a",
                    "name": "Minghao Fu",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f1b",
                    "name": "Lunhao Duan",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f1c",
                    "user": {
                        "_id": "636f4c6b5d2050767e4a1491",
                        "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
                        "isPro": false,
                        "fullname": "Guo-Hua Wang",
                        "user": "Flourish",
                        "type": "user"
                    },
                    "name": "Guo-Hua Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T09:58:04.757Z",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f1d",
                    "name": "Qing-Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f1e",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f1f",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "681c7895c7211b7efbc49f20",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
            ],
            "publishedAt": "2025-05-05T11:18:03.000Z",
            "submittedOnDailyAt": "2025-05-08T07:57:47.854Z",
            "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
            "submittedOnDailyBy": {
                "_id": "658a8a837959448ef5500ce5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
                "isPro": false,
                "fullname": "Shiyin Lu",
                "user": "runninglsy",
                "type": "user"
            },
            "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
            "upvotes": 57,
            "discussionId": "681c7896c7211b7efbc49f76",
            "githubRepo": "https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
            "ai_keywords": [
                "autoregressive-based architectures",
                "diffusion-based models",
                "unified frameworks",
                "GPT-4o",
                "multimodal understanding",
                "text-to-image generation models",
                "diffusion-based",
                "autoregressive-based",
                "hybrid approaches",
                "cross-modal attention"
            ]
        },
        "publishedAt": "2025-05-05T07:18:03.000Z",
        "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
        "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "658a8a837959448ef5500ce5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
            "fullname": "Shiyin Lu",
            "name": "runninglsy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04588",
            "authors": [
                {
                    "_id": "681c15ab84d0a008fcdb1ee8",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1ee9",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1eea",
                    "user": {
                        "_id": "66224557c61c7fbd98099079",
                        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
                        "isPro": false,
                        "fullname": "GJ",
                        "user": "SpaceProduct",
                        "type": "user"
                    },
                    "name": "Jiayan Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:59:01.834Z",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1eeb",
                    "name": "Xuanbo Fan",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1eec",
                    "name": "Yingyan Hou",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1eed",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1eee",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1eef",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "681c15ab84d0a008fcdb1ef0",
                    "name": "Yan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T17:30:22.000Z",
            "submittedOnDailyAt": "2025-05-08T00:54:07.103Z",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "submittedOnDailyBy": {
                "_id": "66224557c61c7fbd98099079",
                "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
                "isPro": false,
                "fullname": "GJ",
                "user": "SpaceProduct",
                "type": "user"
            },
            "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
            "upvotes": 33,
            "discussionId": "681c15ac84d0a008fcdb1f21",
            "projectPage": "https://alibaba-nlp.github.io/ZeroSearch/",
            "githubRepo": "https://github.com/Alibaba-nlp/ZeroSearch",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "large language models (LLMs)",
                "search capabilities",
                "live search engines",
                "real-world environments",
                "document quality",
                "noise",
                "instability",
                "training process",
                "API costs",
                "rollouts",
                "search requests",
                "ZeroSearch",
                "lightweight supervised fine-tuning",
                "retrieval module",
                "relevant documents",
                "noisy documents",
                "query",
                "curriculum-based rollout strategy",
                "reasoning ability",
                "retrieval scenarios",
                "base models",
                "instruction-tuned models",
                "parameter sizes",
                "RL algorithms"
            ]
        },
        "publishedAt": "2025-05-07T13:30:22.000Z",
        "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
        "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66224557c61c7fbd98099079",
            "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
            "fullname": "GJ",
            "name": "SpaceProduct",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.03821",
            "authors": [
                {
                    "_id": "681c7a3829ba66a745217db5",
                    "user": {
                        "_id": "63caf7ce9f78909f9f81eb72",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
                        "isPro": true,
                        "fullname": "Gracjan Goral",
                        "user": "Gracjan",
                        "type": "user"
                    },
                    "name": "Gracjan Góral",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T09:58:02.558Z",
                    "hidden": false
                },
                {
                    "_id": "681c7a3829ba66a745217db6",
                    "name": "Alicja Ziarko",
                    "hidden": false
                },
                {
                    "_id": "681c7a3829ba66a745217db7",
                    "name": "Piotr Miłoś",
                    "hidden": false
                },
                {
                    "_id": "681c7a3829ba66a745217db8",
                    "name": "Michał Nauman",
                    "hidden": false
                },
                {
                    "_id": "681c7a3829ba66a745217db9",
                    "name": "Maciej Wołczyk",
                    "hidden": false
                },
                {
                    "_id": "681c7a3829ba66a745217dba",
                    "name": "Michał Kosiński",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
            ],
            "publishedAt": "2025-05-03T00:10:41.000Z",
            "submittedOnDailyAt": "2025-05-08T08:19:59.040Z",
            "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "63caf7ce9f78909f9f81eb72",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
                "isPro": true,
                "fullname": "Gracjan Goral",
                "user": "Gracjan",
                "type": "user"
            },
            "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
            "upvotes": 19,
            "discussionId": "681c7a3e29ba66a745217f0c",
            "ai_keywords": [
                "Vision Language Models (VLMs)",
                "visual perspective taking",
                "visual tasks",
                "humanoid minifigure",
                "spatial configurations",
                "bird's-eye view",
                "surface-level view",
                "diagnostic questions",
                "scene understanding",
                "spatial reasoning",
                "visual perspective taking",
                "GPT-4-Turbo",
                "GPT-4o",
                "Llama-3.2-11B-Vision-Instruct",
                "Claude Sonnet",
                "geometric representations",
                "tailored training protocols"
            ]
        },
        "publishedAt": "2025-05-02T20:10:41.000Z",
        "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
        "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63caf7ce9f78909f9f81eb72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
            "fullname": "Gracjan Goral",
            "name": "Gracjan",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.00358",
            "authors": [
                {
                    "_id": "68154d77c8ab88a66b8d81a7",
                    "name": "Albert Ge",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81a8",
                    "name": "Tzu-Heng Huang",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81a9",
                    "name": "John Cooper",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81aa",
                    "name": "Avi Trost",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81ab",
                    "name": "Ziyi Chu",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81ac",
                    "name": "Satya Sai Srinath Namburi GNVV",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81ad",
                    "name": "Ziyang Cai",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81ae",
                    "name": "Kendall Park",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81af",
                    "name": "Nicholas Roberts",
                    "hidden": false
                },
                {
                    "_id": "68154d77c8ab88a66b8d81b0",
                    "name": "Frederic Sala",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-01T07:08:19.000Z",
            "submittedOnDailyAt": "2025-05-08T05:38:42.650Z",
            "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
            "submittedOnDailyBy": {
                "_id": "650263c89a612aa33a018383",
                "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
                "isPro": false,
                "fullname": "Albert Ge",
                "user": "albertge",
                "type": "user"
            },
            "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
            "upvotes": 17,
            "discussionId": "68154d78c8ab88a66b8d820c",
            "ai_keywords": [
                "semantic similarity",
                "Gram matrix",
                "domain gradients"
            ]
        },
        "publishedAt": "2025-05-01T03:08:19.000Z",
        "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
        "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "650263c89a612aa33a018383",
            "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
            "fullname": "Albert Ge",
            "name": "albertge",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04512",
            "authors": [
                {
                    "_id": "681c546817fc8222efed5318",
                    "name": "Teng Hu",
                    "hidden": false
                },
                {
                    "_id": "681c546817fc8222efed5319",
                    "name": "Zhentao Yu",
                    "hidden": false
                },
                {
                    "_id": "681c546817fc8222efed531a",
                    "name": "Zhengguang Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c546817fc8222efed531b",
                    "name": "Sen Liang",
                    "hidden": false
                },
                {
                    "_id": "681c546817fc8222efed531c",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c546817fc8222efed531d",
                    "name": "Qin Lin",
                    "hidden": false
                },
                {
                    "_id": "681c546817fc8222efed531e",
                    "name": "Qinglin Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T15:33:18.000Z",
            "submittedOnDailyAt": "2025-05-08T05:21:39.978Z",
            "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
            "upvotes": 15,
            "discussionId": "681c546e17fc8222efed54ce",
            "ai_keywords": [
                "LLaVA",
                "text-image fusion module",
                "image ID enhancement module",
                "temporal concatenation",
                "modality-specific condition injection mechanisms",
                "AudioNet module",
                "spatial cross-attention",
                "video-driven injection module",
                "latent-compressed conditional video",
                "patchify-based feature-alignment network",
                "ID consistency",
                "text-video alignment",
                "controllable video generation"
            ]
        },
        "publishedAt": "2025-05-07T11:33:18.000Z",
        "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
        "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 50
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04622",
            "authors": [
                {
                    "_id": "681c03418ff29a163ef5f370",
                    "name": "Jingwen Ye",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f371",
                    "user": {
                        "_id": "64c903957b4d0d947ce86bc6",
                        "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
                        "isPro": false,
                        "fullname": "Yuze He",
                        "user": "hyz317",
                        "type": "user"
                    },
                    "name": "Yuze He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:59:10.350Z",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f372",
                    "name": "Yanning Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f373",
                    "name": "Yiqin Zhu",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f374",
                    "user": {
                        "_id": "6441491c5d600fb0951cd872",
                        "avatarUrl": "/avatars/d98892f3b52d87c2328201efa9897110.svg",
                        "isPro": false,
                        "fullname": "Kaiwen Xiao",
                        "user": "loktarxiao",
                        "type": "user"
                    },
                    "name": "Kaiwen Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:59:12.445Z",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f375",
                    "name": "Yong-Jin Liu",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f376",
                    "name": "Wei Yang",
                    "hidden": false
                },
                {
                    "_id": "681c03418ff29a163ef5f377",
                    "name": "Xiao Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T17:59:46.000Z",
            "submittedOnDailyAt": "2025-05-08T05:41:14.360Z",
            "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
            "submittedOnDailyBy": {
                "_id": "64c903957b4d0d947ce86bc6",
                "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
                "isPro": false,
                "fullname": "Yuze He",
                "user": "hyz317",
                "type": "user"
            },
            "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
            "upvotes": 13,
            "discussionId": "681c03468ff29a163ef5f4d7",
            "projectPage": "https://primitiveanything.github.io/",
            "githubRepo": "https://github.com/PrimitiveAnything/PrimitiveAnything",
            "ai_keywords": [
                "shape primitive abstraction",
                "geometric elements",
                "human visual cognition",
                "computer vision",
                "graphics",
                "3D content generation",
                "geometric optimization",
                "semantic understanding",
                "category-specific datasets",
                "primitive assembly generation task",
                "shape-conditioned primitive transformer",
                "auto-regressive generation",
                "ambiguity-free parameterization scheme",
                "human-crafted abstractions",
                "high-quality primitive assemblies",
                "human perception",
                "geometric fidelity",
                "3D applications",
                "user-generated content (UGC)"
            ]
        },
        "publishedAt": "2025-05-07T13:59:46.000Z",
        "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
        "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c903957b4d0d947ce86bc6",
            "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
            "fullname": "Yuze He",
            "name": "hyz317",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.04364",
            "authors": [
                {
                    "_id": "681c189c791c72783efe5a94",
                    "user": {
                        "_id": "6205fefd3f1dc8a642d70b10",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
                        "isPro": false,
                        "fullname": "Kai Ruan",
                        "user": "6cf",
                        "type": "user"
                    },
                    "name": "Kai Ruan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:58:58.134Z",
                    "hidden": false
                },
                {
                    "_id": "681c189c791c72783efe5a95",
                    "name": "Mowen Huang",
                    "hidden": false
                },
                {
                    "_id": "681c189c791c72783efe5a96",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "681c189c791c72783efe5a97",
                    "name": "Hao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T12:32:01.000Z",
            "submittedOnDailyAt": "2025-05-08T01:06:26.256Z",
            "title": "Benchmarking LLMs' Swarm intelligence",
            "submittedOnDailyBy": {
                "_id": "6205fefd3f1dc8a642d70b10",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
                "isPro": false,
                "fullname": "Kai Ruan",
                "user": "6cf",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
            "upvotes": 13,
            "discussionId": "681c189e791c72783efe5b2d",
            "githubRepo": "https://github.com/x66ccff/swarmbench",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Multi-Agent Systems (MAS)",
                "swarm intelligence",
                "decentralized coordination",
                "spatio-temporal information",
                "SwarmBench",
                "foundational MAS coordination tasks",
                "2D grid environment",
                "local sensory input",
                "local communication",
                "coordination effectiveness",
                "emergent group dynamics",
                "zero-shot setting",
                "robust planning",
                "strategy formation",
                "uncertainty",
                "decentralized scenarios",
                "Embodied MAS"
            ]
        },
        "publishedAt": "2025-05-07T08:32:01.000Z",
        "title": "Benchmarking LLMs' Swarm intelligence",
        "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png",
        "numComments": 0,
        "submittedBy": {
            "_id": "6205fefd3f1dc8a642d70b10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
            "fullname": "Kai Ruan",
            "name": "6cf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04528",
            "authors": [
                {
                    "_id": "681c5152c7211b7efbba4b73",
                    "user": {
                        "_id": "641aef7b1911d3be67425338",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641aef7b1911d3be67425338/CmCbWWB6NxkAaus59q31w.jpeg",
                        "isPro": false,
                        "fullname": "Qi Liu",
                        "user": "purewhite42",
                        "type": "user"
                    },
                    "name": "Qi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:58:55.624Z",
                    "hidden": false
                },
                {
                    "_id": "681c5152c7211b7efbba4b74",
                    "name": "Xinhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "681c5152c7211b7efbba4b75",
                    "name": "Renqiu Xia",
                    "hidden": false
                },
                {
                    "_id": "681c5152c7211b7efbba4b76",
                    "name": "Xingzhi Qi",
                    "hidden": false
                },
                {
                    "_id": "681c5152c7211b7efbba4b77",
                    "name": "Qinxiang Cao",
                    "hidden": false
                },
                {
                    "_id": "681c5152c7211b7efbba4b78",
                    "name": "Junchi Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T16:02:14.000Z",
            "submittedOnDailyAt": "2025-05-08T05:14:50.449Z",
            "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
            "submittedOnDailyBy": {
                "_id": "65b7ae76768464877cdb2e39",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
                "isPro": false,
                "fullname": "Renqiu Xia",
                "user": "renqiux0302",
                "type": "user"
            },
            "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
            "upvotes": 8,
            "discussionId": "681c5153c7211b7efbba4bb4",
            "githubRepo": "https://github.com/Purewhite2019/formal_problem_solving_main",
            "ai_keywords": [
                "Markov decision process",
                "FPS (Formal Problem-Solving)",
                "FTP (formal theorem proving)",
                "D-FPS (Deductive FPS)",
                "FormalMath500",
                "MiniF2F-Solving",
                "PutnamBench-Solving",
                "RPE (Restricted Propositional Equivalence)"
            ]
        },
        "publishedAt": "2025-05-07T12:02:14.000Z",
        "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
        "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65b7ae76768464877cdb2e39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
            "fullname": "Renqiu Xia",
            "name": "renqiux0302",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04606",
            "authors": [
                {
                    "_id": "681c7a87215f1831099fdfd2",
                    "name": "Lianghong Guo",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd3",
                    "user": {
                        "_id": "6355473d525beaee688b7ba1",
                        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                        "isPro": false,
                        "fullname": "Wei Tao",
                        "user": "itaowe",
                        "type": "user"
                    },
                    "name": "Wei Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T09:58:00.562Z",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd4",
                    "name": "Runhan Jiang",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd5",
                    "name": "Yanlin Wang",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd6",
                    "name": "Jiachi Chen",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd7",
                    "name": "Xilin Liu",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd8",
                    "name": "Yuchi Ma",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfd9",
                    "name": "Mingzhi Mao",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfda",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "681c7a87215f1831099fdfdb",
                    "name": "Zibin Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T17:51:10.000Z",
            "submittedOnDailyAt": "2025-05-08T10:02:33.623Z",
            "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution",
            "submittedOnDailyBy": {
                "_id": "6355473d525beaee688b7ba1",
                "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                "isPro": false,
                "fullname": "Wei Tao",
                "user": "itaowe",
                "type": "user"
            },
            "summary": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements.",
            "upvotes": 6,
            "discussionId": "681c7a87215f1831099fe010",
            "projectPage": "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html",
            "githubRepo": "https://github.com/DeepSoftwareAnalytics/OmniGIRL",
            "ai_keywords": [
                "OmniGIRL",
                "GitHub Issue ResoLution benchmark",
                "multilingual",
                "multimodal",
                "multi-domain",
                "programming languages",
                "Python",
                "JavaScript",
                "TypeScript",
                "Java",
                "domains",
                "LLMs",
                "GPT-4o",
                "Claude-3.5-Sonnet"
            ]
        },
        "publishedAt": "2025-05-07T13:51:10.000Z",
        "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution",
        "summary": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04606.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "fullname": "Wei Tao",
            "name": "itaowe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.03912",
            "authors": [
                {
                    "_id": "681c549cb322a2fe864c8b0d",
                    "name": "Can Cui",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b0e",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b0f",
                    "name": "Wenxuan Song",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b10",
                    "name": "Shuanghao Bai",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b11",
                    "name": "Xinyang Tong",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b12",
                    "name": "Zirui Ge",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b13",
                    "name": "Runze Suo",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b14",
                    "name": "Wanqi Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b15",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b16",
                    "name": "Bofang Jia",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b17",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b18",
                    "name": "Siteng Huang",
                    "hidden": false
                },
                {
                    "_id": "681c549cb322a2fe864c8b19",
                    "name": "Donglin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T18:35:07.000Z",
            "submittedOnDailyAt": "2025-05-08T05:23:33.004Z",
            "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
            "submittedOnDailyBy": {
                "_id": "65fd82762bf2cd20ddaa193f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                "isPro": false,
                "fullname": "Siteng Huang",
                "user": "huangsiteng",
                "type": "user"
            },
            "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
            "upvotes": 6,
            "discussionId": "681c549eb322a2fe864c8b6e",
            "projectPage": "https://openhelix-robot.github.io/",
            "githubRepo": "https://github.com/OpenHelix-robot/OpenHelix/"
        },
        "publishedAt": "2025-05-06T14:35:07.000Z",
        "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
        "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04601",
            "authors": [
                {
                    "_id": "681d1be954749869547df71e",
                    "name": "Xianhang Li",
                    "hidden": false
                },
                {
                    "_id": "681d1be954749869547df71f",
                    "name": "Yanqing Liu",
                    "hidden": false
                },
                {
                    "_id": "681d1be954749869547df720",
                    "name": "Haoqin Tu",
                    "hidden": false
                },
                {
                    "_id": "681d1be954749869547df721",
                    "name": "Hongru Zhu",
                    "hidden": false
                },
                {
                    "_id": "681d1be954749869547df722",
                    "name": "Cihang Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T17:48:35.000Z",
            "submittedOnDailyAt": "2025-05-08T19:32:57.571Z",
            "title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning",
            "submittedOnDailyBy": {
                "_id": "604ae011caabafacfa48e3de",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
                "isPro": false,
                "fullname": "Haoqin Tu",
                "user": "PahaII",
                "type": "user"
            },
            "summary": "OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments.",
            "upvotes": 5,
            "discussionId": "681d1bea54749869547df769"
        },
        "publishedAt": "2025-05-07T13:48:35.000Z",
        "title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning",
        "summary": "OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04601.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "604ae011caabafacfa48e3de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
            "fullname": "Haoqin Tu",
            "name": "PahaII",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.03570",
            "authors": [
                {
                    "_id": "681b518bf497fd5e45b55eeb",
                    "user": {
                        "_id": "667ed2bf12e48bee0e972ccc",
                        "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
                        "isPro": false,
                        "fullname": "Mariya Davydova",
                        "user": "mariya-davydova",
                        "type": "user"
                    },
                    "name": "Mariya Davydova",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:59:24.254Z",
                    "hidden": false
                },
                {
                    "_id": "681b518bf497fd5e45b55eec",
                    "name": "Daniel Jeffries",
                    "hidden": false
                },
                {
                    "_id": "681b518bf497fd5e45b55eed",
                    "name": "Patrick Barker",
                    "hidden": false
                },
                {
                    "_id": "681b518bf497fd5e45b55eee",
                    "name": "Arturo Márquez Flores",
                    "hidden": false
                },
                {
                    "_id": "681b518bf497fd5e45b55eef",
                    "name": "Sinéad Ryan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
                "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
            ],
            "publishedAt": "2025-05-06T14:29:47.000Z",
            "submittedOnDailyAt": "2025-05-08T07:36:52.978Z",
            "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
            "submittedOnDailyBy": {
                "_id": "667ed2bf12e48bee0e972ccc",
                "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
                "isPro": false,
                "fullname": "Mariya Davydova",
                "user": "mariya-davydova",
                "type": "user"
            },
            "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
            "upvotes": 4,
            "discussionId": "681b518cf497fd5e45b55f0f",
            "projectPage": "https://agentsea.github.io/osuniverse/",
            "githubRepo": "https://github.com/agentsea/osuniverse"
        },
        "publishedAt": "2025-05-06T10:29:47.000Z",
        "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
        "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
            "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "667ed2bf12e48bee0e972ccc",
            "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
            "fullname": "Mariya Davydova",
            "name": "mariya-davydova",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.03418",
            "authors": [
                {
                    "_id": "681c4d5b5971460af345032a",
                    "name": "Da Zheng",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af345032b",
                    "name": "Lun Du",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af345032c",
                    "name": "Junwei Su",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af345032d",
                    "name": "Yuchen Tian",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af345032e",
                    "name": "Yuqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af345032f",
                    "name": "Jintian Zhang",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af3450330",
                    "name": "Lanning Wei",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af3450331",
                    "name": "Ningyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "681c4d5b5971460af3450332",
                    "name": "Huajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T10:53:58.000Z",
            "submittedOnDailyAt": "2025-05-08T04:51:36.213Z",
            "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
            "upvotes": 4,
            "discussionId": "681c4d5f5971460af3450465",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Chain-of-Thought (CoT) reasoning",
                "knowledge augmentation",
                "verification techniques",
                "software engineering",
                "mathematical reasoning and proving",
                "data analysis and modeling",
                "scientific research",
                "multi-step reasoning",
                "domain knowledge integration",
                "result verification"
            ]
        },
        "publishedAt": "2025-05-06T06:53:58.000Z",
        "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
        "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04253",
            "authors": [
                {
                    "_id": "681c8f10822d05bba75de266",
                    "name": "Maria Marina",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de267",
                    "name": "Nikolay Ivanov",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de268",
                    "name": "Sergey Pletenev",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de269",
                    "user": {
                        "_id": "62bd6c6baaf1480f1aa2222e",
                        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
                        "isPro": false,
                        "fullname": "Mikhail Salnikov",
                        "user": "msalnikov",
                        "type": "user"
                    },
                    "name": "Mikhail Salnikov",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-08T11:01:37.065Z",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de26a",
                    "name": "Daria Galimzianova",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de26b",
                    "name": "Nikita Krayko",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de26c",
                    "name": "Vasily Konovalov",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de26d",
                    "name": "Alexander Panchenko",
                    "hidden": false
                },
                {
                    "_id": "681c8f10822d05bba75de26e",
                    "name": "Viktor Moskvoretskii",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T08:58:52.000Z",
            "submittedOnDailyAt": "2025-05-08T13:27:56.797Z",
            "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself",
            "submittedOnDailyBy": {
                "_id": "63bbfd74141c7d395c471768",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
                "isPro": false,
                "fullname": "Viktor Moskvoretskii",
                "user": "VityaVitalich",
                "type": "user"
            },
            "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval.",
            "upvotes": 3,
            "discussionId": "681c8f11822d05bba75de29c",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "hallucinations",
                "Retrieval-Augmented Generation (RAG)",
                "adaptive retrieval",
                "uncertainty estimation",
                "lightweight LLM-independent adaptive retrieval methods",
                "external information",
                "QA datasets",
                "QA performance"
            ]
        },
        "publishedAt": "2025-05-07T04:58:52.000Z",
        "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself",
        "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04253.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "fullname": "Viktor Moskvoretskii",
            "name": "VityaVitalich",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.03538",
            "authors": [
                {
                    "_id": "681c0409ecb9979e658951f3",
                    "user": {
                        "_id": "681bfd5a7a587a523b07c587",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4W3HskyYW_z-LBkX3upDM.jpeg",
                        "isPro": false,
                        "fullname": "Chuyu Zhao",
                        "user": "Tournesol-Saturday",
                        "type": "user"
                    },
                    "name": "Chuyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:59:08.240Z",
                    "hidden": false
                },
                {
                    "_id": "681c0409ecb9979e658951f4",
                    "name": "Hao Huang",
                    "hidden": false
                },
                {
                    "_id": "681c0409ecb9979e658951f5",
                    "name": "Jiashuo Guo",
                    "hidden": false
                },
                {
                    "_id": "681c0409ecb9979e658951f6",
                    "name": "Ziyu Shen",
                    "hidden": false
                },
                {
                    "_id": "681c0409ecb9979e658951f7",
                    "name": "Zhongwei Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c0409ecb9979e658951f8",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "681c0409ecb9979e658951f9",
                    "name": "Zekuan Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T13:50:57.000Z",
            "submittedOnDailyAt": "2025-05-08T13:09:18.653Z",
            "title": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT",
            "submittedOnDailyBy": {
                "_id": "681bfd5a7a587a523b07c587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4W3HskyYW_z-LBkX3upDM.jpeg",
                "isPro": false,
                "fullname": "Chuyu Zhao",
                "user": "Tournesol-Saturday",
                "type": "user"
            },
            "summary": "Semi-supervised learning has become a compelling approach for 3D tooth\nsegmentation from CBCT scans, where labeled data is minimal. However, existing\nmethods still face two persistent challenges: limited corrective supervision in\nstructurally ambiguous or mislabeled regions during supervised training and\nperformance degradation caused by unreliable pseudo-labels on unlabeled data.\nTo address these problems, we propose Region-Aware Instructive Learning (RAIL),\na dual-group dual-student, semi-supervised framework. Each group contains two\nstudent models guided by a shared teacher network. By alternating training\nbetween the two groups, RAIL promotes intergroup knowledge transfer and\ncollaborative region-aware instruction while reducing overfitting to the\ncharacteristics of any single model. Specifically, RAIL introduces two\ninstructive mechanisms. Disagreement-Focused Supervision (DFS) Controller\nimproves supervised learning by instructing predictions only within areas where\nstudent outputs diverge from both ground truth and the best student, thereby\nconcentrating supervision on structurally ambiguous or mislabeled areas. In the\nunsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces\nagreement in regions with high model certainty while reducing the effect of\nlow-confidence predictions during training. This helps prevent our model from\nlearning unstable patterns and improves the overall reliability of\npseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets\nshow that RAIL surpasses state-of-the-art methods under limited annotation. Our\ncode will be available at https://github.com/Tournesol-Saturday/RAIL.",
            "upvotes": 2,
            "discussionId": "681c040aecb9979e65895238",
            "githubRepo": "https://github.com/Tournesol-Saturday/RAIL",
            "ai_keywords": [
                "Region-Aware Instructive Learning (RAIL)",
                "dual-group dual-student",
                "teacher network",
                "intergroup knowledge transfer",
                "collaborative region-aware instruction",
                "Disagreement-Focused Supervision (DFS) Controller",
                "Confidence-Aware Learning (CAL) Modulator",
                "structurally ambiguous",
                "mislabeled areas",
                "high model certainty",
                "low-confidence predictions",
                "pseudo-labels"
            ]
        },
        "publishedAt": "2025-05-06T09:50:57.000Z",
        "title": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT",
        "summary": "Semi-supervised learning has become a compelling approach for 3D tooth\nsegmentation from CBCT scans, where labeled data is minimal. However, existing\nmethods still face two persistent challenges: limited corrective supervision in\nstructurally ambiguous or mislabeled regions during supervised training and\nperformance degradation caused by unreliable pseudo-labels on unlabeled data.\nTo address these problems, we propose Region-Aware Instructive Learning (RAIL),\na dual-group dual-student, semi-supervised framework. Each group contains two\nstudent models guided by a shared teacher network. By alternating training\nbetween the two groups, RAIL promotes intergroup knowledge transfer and\ncollaborative region-aware instruction while reducing overfitting to the\ncharacteristics of any single model. Specifically, RAIL introduces two\ninstructive mechanisms. Disagreement-Focused Supervision (DFS) Controller\nimproves supervised learning by instructing predictions only within areas where\nstudent outputs diverge from both ground truth and the best student, thereby\nconcentrating supervision on structurally ambiguous or mislabeled areas. In the\nunsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces\nagreement in regions with high model certainty while reducing the effect of\nlow-confidence predictions during training. This helps prevent our model from\nlearning unstable patterns and improves the overall reliability of\npseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets\nshow that RAIL surpasses state-of-the-art methods under limited annotation. Our\ncode will be available at https://github.com/Tournesol-Saturday/RAIL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03538.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "681bfd5a7a587a523b07c587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4W3HskyYW_z-LBkX3upDM.jpeg",
            "fullname": "Chuyu Zhao",
            "name": "Tournesol-Saturday",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.03105",
            "authors": [
                {
                    "_id": "681c08ca822d05bba73a8e0b",
                    "user": {
                        "_id": "64f733a9ffdc2d790432e317",
                        "avatarUrl": "/avatars/35792659acd945c978594dc50e5322f0.svg",
                        "isPro": false,
                        "fullname": "Xule Lin",
                        "user": "linxule",
                        "type": "user"
                    },
                    "name": "Xule Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T08:59:05.089Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T01:49:44.000Z",
            "submittedOnDailyAt": "2025-05-08T10:22:20.998Z",
            "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation",
            "submittedOnDailyBy": {
                "_id": "64f733a9ffdc2d790432e317",
                "avatarUrl": "/avatars/35792659acd945c978594dc50e5322f0.svg",
                "isPro": false,
                "fullname": "Xule Lin",
                "user": "linxule",
                "type": "user"
            },
            "summary": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.",
            "upvotes": 1,
            "discussionId": "681c08cb822d05bba73a8e42"
        },
        "publishedAt": "2025-05-05T21:49:44.000Z",
        "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation",
        "summary": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03105.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64f733a9ffdc2d790432e317",
            "avatarUrl": "/avatars/35792659acd945c978594dc50e5322f0.svg",
            "fullname": "Xule Lin",
            "name": "linxule",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02820",
            "authors": [
                {
                    "_id": "681cd2499722c5341cabd43f",
                    "name": "Hao Zhu",
                    "hidden": false
                },
                {
                    "_id": "681cd2499722c5341cabd440",
                    "name": "Phil Cuvin",
                    "hidden": false
                },
                {
                    "_id": "681cd2499722c5341cabd441",
                    "name": "Xinkai Yu",
                    "hidden": false
                },
                {
                    "_id": "681cd2499722c5341cabd442",
                    "name": "Charlotte Ka Yee Yan",
                    "hidden": false
                },
                {
                    "_id": "681cd2499722c5341cabd443",
                    "name": "Jason Zhang",
                    "hidden": false
                },
                {
                    "_id": "681cd2499722c5341cabd444",
                    "name": "Diyi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T17:47:49.000Z",
            "submittedOnDailyAt": "2025-05-08T14:18:56.490Z",
            "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
            "submittedOnDailyBy": {
                "_id": "61aa376688c20eebf1e8deb3",
                "avatarUrl": "/avatars/7c11dcb232c73547d7d87834be287822.svg",
                "isPro": false,
                "fullname": "Hao Zhu",
                "user": "ProKil",
                "type": "user"
            },
            "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.",
            "upvotes": 1,
            "discussionId": "681cd24c9722c5341cabd505",
            "githubRepo": "https://github.com/Open-Social-World/autolibra",
            "ai_keywords": [
                "LLM-as-a-Judge",
                "agent evaluation",
                "open-ended human feedback",
                "fine-grained behaviors",
                "agent trajectories",
                "clustering",
                "concrete metrics",
                "meta-metrics",
                "coverage",
                "redundancy",
                "prompt-engineering",
                "text game tasks",
                "fine-tuning data",
                "web navigation agents"
            ]
        },
        "publishedAt": "2025-05-05T13:47:49.000Z",
        "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
        "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02820.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61aa376688c20eebf1e8deb3",
            "avatarUrl": "/avatars/7c11dcb232c73547d7d87834be287822.svg",
            "fullname": "Hao Zhu",
            "name": "ProKil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.02393",
            "authors": [
                {
                    "_id": "681c423f198e1dea5c26f2f4",
                    "user": {
                        "_id": "6445e9bd1cfc9ae6bb40985c",
                        "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
                        "isPro": false,
                        "fullname": "Evan Jeong",
                        "user": "Eavn",
                        "type": "user"
                    },
                    "name": "Sungheon Jeong",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-08T09:38:37.142Z",
                    "hidden": false
                },
                {
                    "_id": "681c423f198e1dea5c26f2f5",
                    "user": {
                        "_id": "646b57c6e5abcbf6709fabf6",
                        "avatarUrl": "/avatars/e9749acf7866eeaf017f0a43351794fc.svg",
                        "isPro": false,
                        "fullname": "Jihong Park",
                        "user": "Paper9795",
                        "type": "user"
                    },
                    "name": "Jihong Park",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-08T05:36:09.797Z",
                    "hidden": false
                },
                {
                    "_id": "681c423f198e1dea5c26f2f6",
                    "name": "Mohsen Imani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T06:33:20.000Z",
            "submittedOnDailyAt": "2025-05-08T04:12:55.976Z",
            "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
            "submittedOnDailyBy": {
                "_id": "6445e9bd1cfc9ae6bb40985c",
                "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
                "isPro": false,
                "fullname": "Evan Jeong",
                "user": "Eavn",
                "type": "user"
            },
            "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
            "upvotes": 1,
            "discussionId": "681c4243198e1dea5c26f3cd",
            "githubRepo": "https://github.com/EavnJeong/IEF-VAD",
            "ai_keywords": [
                "Image-Event Fusion",
                "Video Anomaly Detection",
                "event representations",
                "Student`s-t likelihood",
                "Laplace approximation",
                "Kalman-style frame-wise updates",
                "fused latent state",
                "cross-modal noise"
            ]
        },
        "publishedAt": "2025-05-05T02:33:20.000Z",
        "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
        "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "fullname": "Evan Jeong",
            "name": "Eavn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.01449",
            "authors": [
                {
                    "_id": "681d01a340485f29676c6da2",
                    "name": "Jiayu Wang",
                    "hidden": false
                },
                {
                    "_id": "681d01a340485f29676c6da3",
                    "name": "Aws Albarghouthi",
                    "hidden": false
                },
                {
                    "_id": "681d01a340485f29676c6da4",
                    "name": "Frederic Sala",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-30T02:06:26.000Z",
            "submittedOnDailyAt": "2025-05-08T17:41:52.022Z",
            "title": "COSMOS: Predictable and Cost-Effective Adaptation of LLMs",
            "submittedOnDailyBy": {
                "_id": "651651f5d93a51ceda3021c3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/AjkXG6Z4yveKTZ8NXc5A3.jpeg",
                "isPro": false,
                "fullname": "Jiayu (Mila) Wang",
                "user": "MilaWang",
                "type": "user"
            },
            "summary": "Large language models (LLMs) achieve remarkable performance across numerous\ntasks by using a diverse array of adaptation strategies. However, optimally\nselecting a model and adaptation strategy under resource constraints is\nchallenging and often requires extensive experimentation. We investigate\nwhether it is possible to accurately predict both performance and cost without\nexpensive trials. We formalize the strategy selection problem for LLMs and\nintroduce COSMOS, a unified prediction framework that efficiently estimates\nadaptation outcomes at minimal cost. We instantiate and study the capability of\nour framework via a pair of powerful predictors: embedding-augmented\nlightweight proxy models to predict fine-tuning performance, and low-sample\nscaling laws to forecast retrieval-augmented in-context learning. Extensive\nevaluation across eight representative benchmarks demonstrates that COSMOS\nachieves high prediction accuracy while reducing computational costs by 92.72%\non average, and up to 98.71% in resource-intensive scenarios. Our results show\nthat efficient prediction of adaptation outcomes is not only feasible but can\nsubstantially reduce the computational overhead of LLM deployment while\nmaintaining performance standards.",
            "upvotes": 1,
            "discussionId": "681d01a640485f29676c6e42"
        },
        "publishedAt": "2025-04-29T22:06:26.000Z",
        "title": "COSMOS: Predictable and Cost-Effective Adaptation of LLMs",
        "summary": "Large language models (LLMs) achieve remarkable performance across numerous\ntasks by using a diverse array of adaptation strategies. However, optimally\nselecting a model and adaptation strategy under resource constraints is\nchallenging and often requires extensive experimentation. We investigate\nwhether it is possible to accurately predict both performance and cost without\nexpensive trials. We formalize the strategy selection problem for LLMs and\nintroduce COSMOS, a unified prediction framework that efficiently estimates\nadaptation outcomes at minimal cost. We instantiate and study the capability of\nour framework via a pair of powerful predictors: embedding-augmented\nlightweight proxy models to predict fine-tuning performance, and low-sample\nscaling laws to forecast retrieval-augmented in-context learning. Extensive\nevaluation across eight representative benchmarks demonstrates that COSMOS\nachieves high prediction accuracy while reducing computational costs by 92.72%\non average, and up to 98.71% in resource-intensive scenarios. Our results show\nthat efficient prediction of adaptation outcomes is not only feasible but can\nsubstantially reduce the computational overhead of LLM deployment while\nmaintaining performance standards.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01449.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "651651f5d93a51ceda3021c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/AjkXG6Z4yveKTZ8NXc5A3.jpeg",
            "fullname": "Jiayu (Mila) Wang",
            "name": "MilaWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]