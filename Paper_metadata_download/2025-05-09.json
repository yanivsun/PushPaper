[
    {
        "paper": {
            "id": "2505.04921",
            "authors": [
                {
                    "_id": "681dbb9988ca86d430f1d0d2",
                    "user": {
                        "_id": "62fdb01bc1588e1d4c6c1a7c",
                        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                        "isPro": false,
                        "fullname": "Yunxin Li",
                        "user": "YunxinLi",
                        "type": "user"
                    },
                    "name": "Yunxin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:05:42.761Z",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d3",
                    "user": {
                        "_id": "64380ae1819f3ab20d17431b",
                        "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
                        "isPro": false,
                        "fullname": "ZhenyuLiu",
                        "user": "foggyforest",
                        "type": "user"
                    },
                    "name": "Zhenyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T10:08:42.493Z",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d4",
                    "user": {
                        "_id": "67ecc6a08647cfa1775a9fda",
                        "avatarUrl": "/avatars/bb15abd7a3d2c51380b0b1f819ef76e2.svg",
                        "isPro": false,
                        "fullname": "Zitao Li",
                        "user": "TerenceL-TL",
                        "type": "user"
                    },
                    "name": "Zitao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T08:33:26.702Z",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d5",
                    "user": {
                        "_id": "676fcc8b98612a870ddb848a",
                        "avatarUrl": "/avatars/7da1b77da1d18758b08d49436c5971c1.svg",
                        "isPro": false,
                        "fullname": "xuanyu",
                        "user": "xyidealist",
                        "type": "user"
                    },
                    "name": "Xuanyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T10:36:03.450Z",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d6",
                    "user": {
                        "_id": "639c379cdb7c5f35004066cb",
                        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                        "isPro": false,
                        "fullname": "Zhenran Xu",
                        "user": "imryanxu",
                        "type": "user"
                    },
                    "name": "Zhenran Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T10:08:40.339Z",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d7",
                    "user": {
                        "_id": "64d9da538767727dff1e8f19",
                        "avatarUrl": "/avatars/aaad38795007c6cbcb94c7eed1706e51.svg",
                        "isPro": false,
                        "fullname": "Yu",
                        "user": "Ghaser",
                        "type": "user"
                    },
                    "name": "Xinyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T13:54:43.127Z",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d8",
                    "name": "Haoyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0d9",
                    "name": "Shenyuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0da",
                    "name": "Xintong Wang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0db",
                    "name": "Jifang Wang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0dc",
                    "name": "Shouzheng Huang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0dd",
                    "name": "Xinping Zhao",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0de",
                    "name": "Borui Jiang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0df",
                    "name": "Lanqing Hong",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e0",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e1",
                    "name": "Zhuotao Tian",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e2",
                    "name": "Baoxing Huai",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e3",
                    "name": "Wenhan Luo",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e4",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e5",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e6",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "681dbb9988ca86d430f1d0e7",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T03:35:23.000Z",
            "submittedOnDailyAt": "2025-05-09T06:54:36.013Z",
            "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "64380ae1819f3ab20d17431b",
                "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
                "isPro": false,
                "fullname": "ZhenyuLiu",
                "user": "foggyforest",
                "type": "user"
            },
            "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
            "upvotes": 77,
            "discussionId": "681dbb9b88ca86d430f1d183",
            "projectPage": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
            "githubRepo": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
            "ai_keywords": [
                "Large Multimodal Reasoning Models (LMRMs)",
                "multimodal reasoning",
                "Cross-modal understanding",
                "task-specific modules",
                "representation",
                "alignment",
                "fusion",
                "Multimodal Chain-of-Thought (MCoT)",
                "multimodal reinforcement learning",
                "native large multimodal reasoning models (N-LMRMs)",
                "scalable",
                "agentic",
                "adaptive reasoning",
                "planning"
            ]
        },
        "publishedAt": "2025-05-07T23:35:23.000Z",
        "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
        "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04921.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64380ae1819f3ab20d17431b",
            "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
            "fullname": "ZhenyuLiu",
            "name": "foggyforest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.04620",
            "authors": [
                {
                    "_id": "681c6c1817fc8222eff39a1a",
                    "user": {
                        "_id": "647773a1168cb428e00e9a8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                        "isPro": false,
                        "fullname": "Hao Fei",
                        "user": "scofield7419",
                        "type": "user"
                    },
                    "name": "Hao Fei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-08T09:58:07.591Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a1b",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a1c",
                    "user": {
                        "_id": "67bc247b593452cc18965cb1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
                        "isPro": false,
                        "fullname": "JUNCHENG LI",
                        "user": "JunchengLi",
                        "type": "user"
                    },
                    "name": "Juncheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:36:52.461Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a1d",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:36:59.117Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a1e",
                    "name": "Qingshan Xu",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a1f",
                    "name": "Bobo Li",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a20",
                    "user": {
                        "_id": "64c139d867eff857ea51caa8",
                        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
                        "isPro": false,
                        "fullname": "Shengqiong Wu",
                        "user": "ChocoWu",
                        "type": "user"
                    },
                    "name": "Shengqiong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:22:39.333Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a21",
                    "user": {
                        "_id": "64ff369d9abcc85a5519b33e",
                        "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
                        "isPro": false,
                        "fullname": "Yaoting Wang",
                        "user": "Gh0stAR",
                        "type": "user"
                    },
                    "name": "Yaoting Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:37:27.790Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a22",
                    "user": {
                        "_id": "67e906836c7216f5bf91f70c",
                        "avatarUrl": "/avatars/9c7f34d5b1d41ad7231d2733a399abb3.svg",
                        "isPro": false,
                        "fullname": "junbao.zhou",
                        "user": "junbaozhou",
                        "type": "user"
                    },
                    "name": "Junbao Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:37:34.046Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a23",
                    "user": {
                        "_id": "65a28e129acab19980226731",
                        "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
                        "isPro": false,
                        "fullname": "Jiahao Meng",
                        "user": "marinero4972",
                        "type": "user"
                    },
                    "name": "Jiahao Meng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:37:40.200Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a24",
                    "user": {
                        "_id": "656724074f6ec72017754d33",
                        "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
                        "isPro": false,
                        "fullname": "QingyuShi",
                        "user": "QingyuShi",
                        "type": "user"
                    },
                    "name": "Qingyu Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:22:34.088Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a25",
                    "name": "Zhiyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a26",
                    "name": "Liangtao Shi",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a27",
                    "user": {
                        "_id": "648ef24dc92367eecac0f4bd",
                        "avatarUrl": "/avatars/38f1afd6b52efeee3aa41cc80225d788.svg",
                        "isPro": false,
                        "fullname": "Minghe Gao",
                        "user": "gmh5811",
                        "type": "user"
                    },
                    "name": "Minghe Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:38:16.554Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a28",
                    "user": {
                        "_id": "6241b95cfee9374a2598ecfe",
                        "avatarUrl": "/avatars/196669df1689a5872fc18b271e80fdc1.svg",
                        "isPro": false,
                        "fullname": "Zhang Daoan",
                        "user": "hazard",
                        "type": "user"
                    },
                    "name": "Daoan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:38:28.567Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a29",
                    "name": "Zhiqi Ge",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a2a",
                    "user": {
                        "_id": "67ebcf69f5870fc1da715eda",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/TbfMOG2cKwPEh2xQdIqbr.png",
                        "isPro": false,
                        "fullname": "Weiming Wu",
                        "user": "wwm1415",
                        "type": "user"
                    },
                    "name": "Weiming Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T10:36:05.177Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a2b",
                    "name": "Siliang Tang",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a2c",
                    "name": "Kaihang Pan",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a2d",
                    "user": {
                        "_id": "662917afda1cae6cbb50cd00",
                        "avatarUrl": "/avatars/aa66de6cef6665c5d67071d82bac35c4.svg",
                        "isPro": false,
                        "fullname": "Yaobo Ye",
                        "user": "superyyb",
                        "type": "user"
                    },
                    "name": "Yaobo Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:55:06.463Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a2e",
                    "user": {
                        "_id": "6391e41f2e73987364e6bcb2",
                        "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
                        "isPro": false,
                        "fullname": "Haobo Yuan",
                        "user": "HarborYuan",
                        "type": "user"
                    },
                    "name": "Haobo Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:39:11.094Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a2f",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a30",
                    "user": {
                        "_id": "6816d98fc075e49c1b15928e",
                        "avatarUrl": "/avatars/6b24d047fc25075bedb3e74f78981bc0.svg",
                        "isPro": false,
                        "fullname": "Tianjie Ju",
                        "user": "jometeorieNUS",
                        "type": "user"
                    },
                    "name": "Tianjie Ju",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:53:06.930Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a31",
                    "name": "Zixiang Meng",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a32",
                    "name": "Shilin Xu",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a33",
                    "name": "Liyu Jia",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a34",
                    "name": "Wentao Hu",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a35",
                    "user": {
                        "_id": "64ad1c0bad6218d51a07b54e",
                        "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
                        "isPro": false,
                        "fullname": "LUO MENG",
                        "user": "Eureka-Leo",
                        "type": "user"
                    },
                    "name": "Meng Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:22:37.235Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a36",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a37",
                    "name": "Tat-Seng Chua",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a38",
                    "user": {
                        "_id": "67eaa070b9fa8908e151fd7d",
                        "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
                        "isPro": false,
                        "fullname": "shuicheng yan",
                        "user": "shuicheng",
                        "type": "user"
                    },
                    "name": "Shuicheng Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:52:30.205Z",
                    "hidden": false
                },
                {
                    "_id": "681c6c1817fc8222eff39a39",
                    "name": "Hanwang Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
            ],
            "publishedAt": "2025-05-07T17:59:32.000Z",
            "submittedOnDailyAt": "2025-05-09T01:19:17.510Z",
            "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
            "submittedOnDailyBy": {
                "_id": "647773a1168cb428e00e9a8f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
                "isPro": false,
                "fullname": "Hao Fei",
                "user": "scofield7419",
                "type": "user"
            },
            "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
            "upvotes": 54,
            "discussionId": "681c6c1d17fc8222eff39b45",
            "projectPage": "https://generalist.top/",
            "githubRepo": "https://github.com/path2generalist/General-Level",
            "ai_keywords": [
                "Multimodal Large Language Model (MLLM)",
                "Multimodal Generalist",
                "multimodal understanding",
                "comprehension",
                "generation",
                "General-Level",
                "Synergy",
                "General-Bench",
                "AGI (Artificial General Intelligence)"
            ]
        },
        "publishedAt": "2025-05-07T13:59:32.000Z",
        "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
        "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04620.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "fullname": "Hao Fei",
            "name": "scofield7419",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05470",
            "authors": [
                {
                    "_id": "681d9829edf34a77aab565eb",
                    "user": {
                        "_id": "639be86b59473c6ae02ef9c4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/639be86b59473c6ae02ef9c4/gw34RBCVZCOkcAA79xUr3.png",
                        "isPro": false,
                        "fullname": "Jie Liu",
                        "user": "jieliu",
                        "type": "user"
                    },
                    "name": "Jie Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T13:54:46.048Z",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565ec",
                    "user": {
                        "_id": "6553316bf151de82f6a23e1d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6553316bf151de82f6a23e1d/GTBkSj4Fa3OoyM6Muz_Sc.jpeg",
                        "isPro": false,
                        "fullname": "Gongye Liu",
                        "user": "liuhuohuo",
                        "type": "user"
                    },
                    "name": "Gongye Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:41:47.403Z",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565ed",
                    "name": "Jiajun Liang",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565ee",
                    "user": {
                        "_id": "64d71083a787c9bc7b9f1238",
                        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                        "isPro": false,
                        "fullname": "Yangguang Li",
                        "user": "Lp256",
                        "type": "user"
                    },
                    "name": "Yangguang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:43:44.697Z",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565ef",
                    "user": {
                        "_id": "65377c30e48353201e6fdda0",
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:45:07.297Z",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565f0",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:42:27.977Z",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565f1",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565f2",
                    "user": {
                        "_id": "644c8324f02250233d0d67d9",
                        "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "dizhang",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:43:01.366Z",
                    "hidden": false
                },
                {
                    "_id": "681d9829edf34a77aab565f3",
                    "name": "Wanli Ouyang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T17:58:45.000Z",
            "submittedOnDailyAt": "2025-05-09T05:45:53.355Z",
            "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
            "submittedOnDailyBy": {
                "_id": "64d71083a787c9bc7b9f1238",
                "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
                "isPro": false,
                "fullname": "Yangguang Li",
                "user": "Lp256",
                "type": "user"
            },
            "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
            "upvotes": 35,
            "discussionId": "681d982aedf34a77aab56635",
            "githubRepo": "https://github.com/yifan123/flow_grpo",
            "ai_keywords": [
                "Flow-GRPO",
                "reinforcement learning (RL)",
                "flow matching models",
                "ODE-to-SDE conversion",
                "Ordinary Differential Equation (ODE)",
                "Stochastic Differential Equation (SDE)",
                "Denoising Reduction strategy",
                "GenEval accuracy",
                "text-to-image tasks",
                "SD3.5",
                "visual text rendering",
                "human preference alignment",
                "reward hacking"
            ]
        },
        "publishedAt": "2025-05-08T13:58:45.000Z",
        "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
        "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05470.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "fullname": "Yangguang Li",
            "name": "Lp256",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05315",
            "authors": [
                {
                    "_id": "681d7ccb572e742b3f42d1f3",
                    "user": {
                        "_id": "6602869253a0518b2a98cafd",
                        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
                        "isPro": false,
                        "fullname": "Yuhui Xu",
                        "user": "yuhuixu",
                        "type": "user"
                    },
                    "name": "Yuhui Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:56:04.644Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ccb572e742b3f42d1f4",
                    "user": {
                        "_id": "63a3ff69f91ad3ea5703841d",
                        "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
                        "isPro": false,
                        "fullname": "Hanze Dong",
                        "user": "hendrydong",
                        "type": "user"
                    },
                    "name": "Hanze Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:56:10.829Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ccb572e742b3f42d1f5",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "681d7ccb572e742b3f42d1f6",
                    "user": {
                        "_id": "65f84fd980481173afd91233",
                        "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
                        "isPro": false,
                        "fullname": "Doyen",
                        "user": "doyensahoo",
                        "type": "user"
                    },
                    "name": "Doyen Sahoo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:56:18.676Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ccb572e742b3f42d1f7",
                    "user": {
                        "_id": "61f9d3b54ac99e8a1bae85f4",
                        "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
                        "isPro": false,
                        "fullname": "JunnanLi",
                        "user": "JunnanLi",
                        "type": "user"
                    },
                    "name": "Junnan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:56:32.272Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ccb572e742b3f42d1f8",
                    "user": {
                        "_id": "649dbcc4e0fff1ed099dc80a",
                        "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
                        "isPro": false,
                        "fullname": "Caiming Xiong",
                        "user": "cxiong",
                        "type": "user"
                    },
                    "name": "Caiming Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:56:38.430Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T15:01:06.000Z",
            "submittedOnDailyAt": "2025-05-09T02:31:21.542Z",
            "title": "Scalable Chain of Thoughts via Elastic Reasoning",
            "submittedOnDailyBy": {
                "_id": "6602869253a0518b2a98cafd",
                "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
                "isPro": false,
                "fullname": "Yuhui Xu",
                "user": "yuhuixu",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
            "upvotes": 16,
            "discussionId": "681d7ccc572e742b3f42d21a",
            "ai_keywords": [
                "Large reasoning models (LRMs)",
                "chain of thought (CoT)",
                "inference-time budgets",
                "tokens",
                "latency",
                "compute",
                "Elastic Reasoning",
                "scalable chain of thoughts",
                "thinking phase",
                "solution phase",
                "independently allocated budgets",
                "completeness of solution segments",
                "reliability",
                "resource constraints",
                "lightweight budget-constrained rollout strategy",
                "GRPO",
                "adaptive reasoning",
                "unseen budget constraints",
                "mathematical benchmarks (AIME, MATH500)",
                "programming benchmarks (LiveCodeBench, Codeforces)",
                "unconstrained settings",
                "principled solution"
            ]
        },
        "publishedAt": "2025-05-08T11:01:06.000Z",
        "title": "Scalable Chain of Thoughts via Elastic Reasoning",
        "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05315.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "fullname": "Yuhui Xu",
            "name": "yuhuixu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02847",
            "authors": [
                {
                    "_id": "681d7031e9969eecfcb4eb81",
                    "name": "Bang Zhang",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb82",
                    "user": {
                        "_id": "648294b2eb4befee378951c1",
                        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
                        "isPro": false,
                        "fullname": "Ruotian Ma",
                        "user": "vvibt",
                        "type": "user"
                    },
                    "name": "Ruotian Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:30.886Z",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb83",
                    "name": "Qingxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb84",
                    "name": "Peisong Wang",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb85",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb86",
                    "name": "Zheng Xie",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb87",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb88",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb89",
                    "name": "Fanghua Ye",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb8a",
                    "name": "Jian Li",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb8b",
                    "name": "Yifan Yang",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb8c",
                    "user": {
                        "_id": "67485743561b1e6f9579389f",
                        "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
                        "isPro": false,
                        "fullname": "Zhaopeng Tu",
                        "user": "zptu",
                        "type": "user"
                    },
                    "name": "Zhaopeng Tu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:28.468Z",
                    "hidden": false
                },
                {
                    "_id": "681d7031e9969eecfcb4eb8d",
                    "name": "Xiaolong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-01T19:06:10.000Z",
            "submittedOnDailyAt": "2025-05-09T01:37:10.548Z",
            "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "648294b2eb4befee378951c1",
                "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
                "isPro": false,
                "fullname": "Ruotian Ma",
                "user": "vvibt",
                "type": "user"
            },
            "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
            "upvotes": 16,
            "discussionId": "681d7033e9969eecfcb4ec2d",
            "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/SAGE",
            "ai_keywords": [
                "Sentient Agent as a Judge (SAGE)",
                "higher-order social cognition",
                "emotional changes",
                "inner thoughts",
                "multi-turn conversations",
                "numerical emotion trajectory",
                "Barrett-Lennard Relationship Inventory (BLRI)",
                "utterance-level empathy metrics",
                "psychological fidelity",
                "Sentient Leaderboard",
                "empathetic",
                "socially adept language agents"
            ]
        },
        "publishedAt": "2025-05-01T15:06:10.000Z",
        "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
        "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02847.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "fullname": "Ruotian Ma",
            "name": "vvibt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05474",
            "authors": [
                {
                    "_id": "681d7b5ae27a030c96a28bde",
                    "user": {
                        "_id": "672392c4a4c4381cefc06416",
                        "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg",
                        "isPro": false,
                        "fullname": "Wen Beichen",
                        "user": "wenbc21",
                        "type": "user"
                    },
                    "name": "Beichen Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:56:55.900Z",
                    "hidden": false
                },
                {
                    "_id": "681d7b5ae27a030c96a28bdf",
                    "user": {
                        "_id": "63f47b5321eb234ab739e91a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
                        "isPro": false,
                        "fullname": "Haozhe Xie",
                        "user": "hzxie",
                        "type": "user"
                    },
                    "name": "Haozhe Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:57:02.341Z",
                    "hidden": false
                },
                {
                    "_id": "681d7b5ae27a030c96a28be0",
                    "user": {
                        "_id": "62fc8cf7ee999004b5a8b982",
                        "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
                        "isPro": false,
                        "fullname": "Zhaoxi Chen",
                        "user": "FrozenBurning",
                        "type": "user"
                    },
                    "name": "Zhaoxi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:57:08.543Z",
                    "hidden": false
                },
                {
                    "_id": "681d7b5ae27a030c96a28be1",
                    "name": "Fangzhou Hong",
                    "hidden": false
                },
                {
                    "_id": "681d7b5ae27a030c96a28be2",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:57:27.473Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
            ],
            "publishedAt": "2025-05-08T17:59:54.000Z",
            "submittedOnDailyAt": "2025-05-09T02:21:54.023Z",
            "title": "3D Scene Generation: A Survey",
            "submittedOnDailyBy": {
                "_id": "63f47b5321eb234ab739e91a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
                "isPro": false,
                "fullname": "Haozhe Xie",
                "user": "hzxie",
                "type": "user"
            },
            "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
            "upvotes": 10,
            "discussionId": "681d7b5be27a030c96a28c29",
            "githubRepo": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
            "ai_keywords": [
                "deep generative models",
                "GANs",
                "diffusion models",
                "NeRF",
                "3D Gaussians",
                "procedural generation",
                "neural 3D-based generation",
                "image-based generation",
                "video-based generation"
            ]
        },
        "publishedAt": "2025-05-08T13:59:54.000Z",
        "title": "3D Scene Generation: A Survey",
        "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05474.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63f47b5321eb234ab739e91a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
            "fullname": "Haozhe Xie",
            "name": "hzxie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05071",
            "authors": [
                {
                    "_id": "681da6375f701833274a0d21",
                    "user": {
                        "_id": "6621e591c50869c1e91a1639",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621e591c50869c1e91a1639/L_PoEn2BRAJcnWZX-JebR.jpeg",
                        "isPro": false,
                        "fullname": "Chunyu Xie",
                        "user": "xiechunyu",
                        "type": "user"
                    },
                    "name": "Chunyu Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:03.296Z",
                    "hidden": false
                },
                {
                    "_id": "681da6375f701833274a0d22",
                    "name": "Bin Wang",
                    "hidden": true
                },
                {
                    "_id": "681da6375f701833274a0d23",
                    "user": {
                        "_id": "632c098b456c31252774e7c5",
                        "avatarUrl": "/avatars/e3720d2fcb69d93c8d5aa5f50aab5f0e.svg",
                        "isPro": false,
                        "fullname": "kong",
                        "user": "fanjing",
                        "type": "user"
                    },
                    "name": "Fanjing Kong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:58:42.881Z",
                    "hidden": false
                },
                {
                    "_id": "681da6375f701833274a0d24",
                    "user": {
                        "_id": "65b793b374698ba5a815bf4f",
                        "avatarUrl": "/avatars/44a7e694a5089dbc773018111270ac26.svg",
                        "isPro": false,
                        "fullname": "Jincheng Li",
                        "user": "jinchenglijc",
                        "type": "user"
                    },
                    "name": "Jincheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:58:50.538Z",
                    "hidden": false
                },
                {
                    "_id": "681da6375f701833274a0d25",
                    "user": {
                        "_id": "659b8576999b82db2ad8a398",
                        "avatarUrl": "/avatars/2ec7663e25e4a0238819818e69d9a5bd.svg",
                        "isPro": false,
                        "fullname": "Liang",
                        "user": "DaweiLiang",
                        "type": "user"
                    },
                    "name": "Dawei Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:58:56.445Z",
                    "hidden": false
                },
                {
                    "_id": "681da6375f701833274a0d26",
                    "name": "Gengshen Zhang",
                    "hidden": false
                },
                {
                    "_id": "681da6375f701833274a0d27",
                    "user": {
                        "_id": "649935abbe8fd92c27ab1ed8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
                        "isPro": false,
                        "fullname": "David Leon",
                        "user": "DavidLeon",
                        "type": "user"
                    },
                    "name": "Dawei Leng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-09T07:00:10.249Z",
                    "hidden": false
                },
                {
                    "_id": "681da6375f701833274a0d28",
                    "name": "Yuhui Yin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T09:06:53.000Z",
            "submittedOnDailyAt": "2025-05-09T05:27:38.509Z",
            "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
            "submittedOnDailyBy": {
                "_id": "649935abbe8fd92c27ab1ed8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
                "isPro": false,
                "fullname": "David Leon",
                "user": "DavidLeon",
                "type": "user"
            },
            "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
            "upvotes": 10,
            "discussionId": "681da6385f701833274a0d8a",
            "githubRepo": "https://github.com/360CVGroup/FG-CLIP",
            "ai_keywords": [
                "Contrastive Language-Image Pre-training (CLIP)",
                "image-text retrieval",
                "zero-shot classification",
                "fine-grained understanding",
                "coarse-grained short captions",
                "multimodal models",
                "1.6 billion long caption-image pairs",
                "high-quality dataset",
                "12 million images",
                "40 million region-specific bounding boxes",
                "detailed captions",
                "10 million hard fine-grained negative samples",
                "fine-grained understanding",
                "open-vocabulary object detection",
                "general multimodal benchmarks",
                "FG-CLIP"
            ]
        },
        "publishedAt": "2025-05-08T05:06:53.000Z",
        "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
        "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05071.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "fullname": "David Leon",
            "name": "DavidLeon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05469",
            "authors": [
                {
                    "_id": "681db3a8a9286b53a51dc77b",
                    "user": {
                        "_id": "672403d5f328a3e6638331ee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TXr0SKWI-z-6FvUXTNWXT.jpeg",
                        "isPro": true,
                        "fullname": "Ava Pun",
                        "user": "AvaLovelace",
                        "type": "user"
                    },
                    "name": "Ava Pun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:06:33.422Z",
                    "hidden": false
                },
                {
                    "_id": "681db3a8a9286b53a51dc77c",
                    "user": {
                        "_id": "645d34ecce72244df7b29317",
                        "avatarUrl": "/avatars/1248933d9f89a15e67086325a8322d5e.svg",
                        "isPro": false,
                        "fullname": "Kangle Deng",
                        "user": "kangled",
                        "type": "user"
                    },
                    "name": "Kangle Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:06:39.736Z",
                    "hidden": false
                },
                {
                    "_id": "681db3a8a9286b53a51dc77d",
                    "user": {
                        "_id": "658b307b75ddc76f9dc747ca",
                        "avatarUrl": "/avatars/fc5393dc0bb33a8c0fea3a6f79640386.svg",
                        "isPro": false,
                        "fullname": "Ruixuan Liu",
                        "user": "RLCMU",
                        "type": "user"
                    },
                    "name": "Ruixuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:06:45.442Z",
                    "hidden": false
                },
                {
                    "_id": "681db3a8a9286b53a51dc77e",
                    "user": {
                        "_id": "6337151b0267ebcf02640eb6",
                        "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
                        "isPro": false,
                        "fullname": "Deva Ramanan",
                        "user": "devakramanan",
                        "type": "user"
                    },
                    "name": "Deva Ramanan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:06:52.637Z",
                    "hidden": false
                },
                {
                    "_id": "681db3a8a9286b53a51dc77f",
                    "name": "Changliu Liu",
                    "hidden": false
                },
                {
                    "_id": "681db3a8a9286b53a51dc780",
                    "user": {
                        "_id": "63a0acc32fabbbb899952a2b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671474335794-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Jun-Yan Zhu",
                        "user": "junyanz",
                        "type": "user"
                    },
                    "name": "Jun-Yan Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:07:07.095Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T17:58:18.000Z",
            "submittedOnDailyAt": "2025-05-09T06:30:13.597Z",
            "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
            "upvotes": 9,
            "discussionId": "681db3aca9286b53a51dc875",
            "ai_keywords": [
                "autoregressive large language model",
                "next-token prediction",
                "validity check",
                "physics-aware rollback",
                "autoregressive inference",
                "physics laws",
                "assembly constraints",
                "text-based LEGO texturing method",
                "automatic assembly",
                "robotic arms"
            ]
        },
        "publishedAt": "2025-05-08T13:58:18.000Z",
        "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
        "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05469.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6800
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.05327",
            "authors": [
                {
                    "_id": "681d95bc11abe59dc97e4c5a",
                    "user": {
                        "_id": "647e99d9becb41a272970ca4",
                        "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
                        "isPro": false,
                        "fullname": "Ann",
                        "user": "yyxsghx",
                        "type": "user"
                    },
                    "name": "Yixin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:18.753Z",
                    "hidden": false
                },
                {
                    "_id": "681d95bc11abe59dc97e4c5b",
                    "user": {
                        "_id": "670740744341dcee459fb990",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
                        "isPro": false,
                        "fullname": "Qingxiu Dong",
                        "user": "Rsy24",
                        "type": "user"
                    },
                    "name": "Qingxiu Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:57:38.844Z",
                    "hidden": false
                },
                {
                    "_id": "681d95bc11abe59dc97e4c5c",
                    "user": {
                        "_id": "655ca347f426a304c6b393a1",
                        "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
                        "isPro": false,
                        "fullname": "Linli Yao",
                        "user": "yaolily",
                        "type": "user"
                    },
                    "name": "Linli Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:57:47.561Z",
                    "hidden": false
                },
                {
                    "_id": "681d95bc11abe59dc97e4c5d",
                    "user": {
                        "_id": "654cca3fe1b4cd6d40d5a7ae",
                        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
                        "isPro": false,
                        "fullname": "Fangwei Zhu",
                        "user": "soliz1998",
                        "type": "user"
                    },
                    "name": "Fangwei Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:57:53.796Z",
                    "hidden": false
                },
                {
                    "_id": "681d95bc11abe59dc97e4c5e",
                    "name": "Zhifang Sui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T15:17:37.000Z",
            "submittedOnDailyAt": "2025-05-09T07:13:58.961Z",
            "title": "ICon: In-Context Contribution for Automatic Data Selection",
            "submittedOnDailyBy": {
                "_id": "647e99d9becb41a272970ca4",
                "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
                "isPro": false,
                "fullname": "Ann",
                "user": "yyxsghx",
                "type": "user"
            },
            "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
            "upvotes": 9,
            "discussionId": "681d95bd11abe59dc97e4c87",
            "ai_keywords": [
                "in-context learning (ICL)",
                "implicit fine-tuning",
                "In-context Learning for Contribution Measurement (ICon)"
            ]
        },
        "publishedAt": "2025-05-08T11:17:37.000Z",
        "title": "ICon: In-Context Contribution for Automatic Data Selection",
        "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05327.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "647e99d9becb41a272970ca4",
            "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
            "fullname": "Ann",
            "name": "yyxsghx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05467",
            "authors": [
                {
                    "_id": "681d95b5c7ae5f65b0e55ff9",
                    "user": {
                        "_id": "63fee47352441fe3e87b5088",
                        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
                        "isPro": false,
                        "fullname": "WANG HAIBO",
                        "user": "WHB139426",
                        "type": "user"
                    },
                    "name": "Haibo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:20.829Z",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e55ffa",
                    "name": "Bo Feng",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e55ffb",
                    "user": {
                        "_id": "66b5295f83425904fa7a1a6a",
                        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
                        "isPro": false,
                        "fullname": "Zhengfeng Lai",
                        "user": "jefflai",
                        "type": "user"
                    },
                    "name": "Zhengfeng Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:00:45.264Z",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e55ffc",
                    "name": "Mingze Xu",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e55ffd",
                    "user": {
                        "_id": "67fa856547b40f55b7ff3ce5",
                        "avatarUrl": "/avatars/745937497772e9b533ba7940d758d30d.svg",
                        "isPro": false,
                        "fullname": "Shiyu Li",
                        "user": "ShiyuLi",
                        "type": "user"
                    },
                    "name": "Shiyu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:01:08.366Z",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e55ffe",
                    "name": "Weifeng Ge",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e55fff",
                    "user": {
                        "_id": "66fc2377516eaf950d4b8209",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mcUxUxXy18Gv9KvCW23s0.png",
                        "isPro": false,
                        "fullname": "Afshin Dehghan",
                        "user": "afshindn",
                        "type": "user"
                    },
                    "name": "Afshin Dehghan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:01:21.716Z",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e56000",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "681d95b5c7ae5f65b0e56001",
                    "name": "Ping Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T17:57:40.000Z",
            "submittedOnDailyAt": "2025-05-09T04:13:07.003Z",
            "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
            "submittedOnDailyBy": {
                "_id": "63fee47352441fe3e87b5088",
                "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
                "isPro": false,
                "fullname": "WANG HAIBO",
                "user": "WHB139426",
                "type": "user"
            },
            "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
            "upvotes": 8,
            "discussionId": "681d95b6c7ae5f65b0e5606c",
            "ai_keywords": [
                "Video-LLMs",
                "streaming-capable models",
                "multi-turn real-time understanding",
                "proactive response mechanisms",
                "memory buffer",
                "round-decayed compression strategy",
                "long-context multi-turn interactions",
                "decoupled activation model",
                "Stream-IT",
                "interleaved video-text sequences",
                "standard video understanding benchmarks",
                "GPT-4o",
                "Gemini 1.5 Pro"
            ]
        },
        "publishedAt": "2025-05-08T13:57:40.000Z",
        "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
        "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05467.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63fee47352441fe3e87b5088",
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "fullname": "WANG HAIBO",
            "name": "WHB139426",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.03981",
            "authors": [
                {
                    "_id": "681d7ee755699177c7fb636a",
                    "user": {
                        "_id": "617e7729129c9e67703ffe61",
                        "avatarUrl": "/avatars/f47ee9f2f0e2b1075bebf3682ee2f817.svg",
                        "isPro": false,
                        "fullname": "qianchu liu",
                        "user": "qianchu",
                        "type": "user"
                    },
                    "name": "Qianchu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:59:18.501Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb636b",
                    "user": {
                        "_id": "6234c11b7d5de9839bc44163",
                        "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
                        "isPro": false,
                        "fullname": "Sheng Zhang",
                        "user": "shengz",
                        "type": "user"
                    },
                    "name": "Sheng Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:22.974Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb636c",
                    "user": {
                        "_id": "64b8e41d52b7353d8c6dd38f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IAItP4FvD6JX9s1jwnQwF.png",
                        "isPro": false,
                        "fullname": "Guanghui Qin",
                        "user": "hiaoxui",
                        "type": "user"
                    },
                    "name": "Guanghui Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:59:24.320Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb636d",
                    "name": "Timothy Ossowski",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb636e",
                    "name": "Yu Gu",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb636f",
                    "name": "Ying Jin",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb6370",
                    "user": {
                        "_id": "627bd86f7e62b4bf5c367108",
                        "avatarUrl": "/avatars/4e87eea02d51680ebac7992dfe527e07.svg",
                        "isPro": false,
                        "fullname": "Sid Kiblawi",
                        "user": "sidkiblawi",
                        "type": "user"
                    },
                    "name": "Sid Kiblawi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:59:37.798Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb6371",
                    "user": {
                        "_id": "65a13da85dce70a3025b7534",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zSPDBEGIULEYN4P7JCdyC.png",
                        "isPro": false,
                        "fullname": "Sam Preston",
                        "user": "RustyArchimedes",
                        "type": "user"
                    },
                    "name": "Sam Preston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T08:59:43.654Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb6372",
                    "name": "Mu Wei",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb6373",
                    "user": {
                        "_id": "6797f24ded1557b14d708541",
                        "avatarUrl": "/avatars/d69ac80a9a500764766ce9ac7d549cc2.svg",
                        "isPro": false,
                        "fullname": "Paul Vozila",
                        "user": "Paulvozila",
                        "type": "user"
                    },
                    "name": "Paul Vozila",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:00:01.673Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb6374",
                    "user": {
                        "_id": "5e5870466bc35159a08ca572",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e5870466bc35159a08ca572/pT6gEEs8RLRJGeM-faNWj.jpeg",
                        "isPro": false,
                        "fullname": "Tristan Naumann",
                        "user": "tnaumann",
                        "type": "user"
                    },
                    "name": "Tristan Naumann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:00:15.476Z",
                    "hidden": false
                },
                {
                    "_id": "681d7ee755699177c7fb6375",
                    "user": {
                        "_id": "664d07456083f276c4feb1a4",
                        "avatarUrl": "/avatars/1bfa6d8f82e9223b47630cefd79d7d0e.svg",
                        "isPro": false,
                        "fullname": "Hoifung Poon",
                        "user": "hoifung",
                        "type": "user"
                    },
                    "name": "Hoifung Poon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:00:22.468Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T21:08:27.000Z",
            "submittedOnDailyAt": "2025-05-09T02:41:23.534Z",
            "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
            "submittedOnDailyBy": {
                "_id": "6234c11b7d5de9839bc44163",
                "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
                "isPro": false,
                "fullname": "Sheng Zhang",
                "user": "shengz",
                "type": "user"
            },
            "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
            "upvotes": 8,
            "discussionId": "681d7ee855699177c7fb63b7",
            "projectPage": "https://github.com/microsoft/x-reasoner",
            "ai_keywords": [
                "multimodal reasoning",
                "vision-language model",
                "post-training",
                "long chain-of-thoughts",
                "reinforcement learning",
                "verifiable rewards",
                "X-Reasoner",
                "out-of-domain settings",
                "X-Reasoner-Med"
            ]
        },
        "publishedAt": "2025-05-06T17:08:27.000Z",
        "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
        "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03981.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6234c11b7d5de9839bc44163",
            "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
            "fullname": "Sheng Zhang",
            "name": "shengz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.03422",
            "authors": [
                {
                    "_id": "681db58b1f1c39ba8fbe0162",
                    "user": {
                        "_id": "681db120007a2d4056d25c70",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
                        "isPro": false,
                        "fullname": "yepeng liu",
                        "user": "pengliu123",
                        "type": "user"
                    },
                    "name": "Yepeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T08:33:21.362Z",
                    "hidden": false
                },
                {
                    "_id": "681db58b1f1c39ba8fbe0163",
                    "name": "Wenpeng Lai",
                    "hidden": false
                },
                {
                    "_id": "681db58b1f1c39ba8fbe0164",
                    "name": "Zhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "681db58b1f1c39ba8fbe0165",
                    "name": "Yuxuan Xiong",
                    "hidden": false
                },
                {
                    "_id": "681db58b1f1c39ba8fbe0166",
                    "name": "Jinchi Zhu",
                    "hidden": false
                },
                {
                    "_id": "681db58b1f1c39ba8fbe0167",
                    "name": "Jun Cheng",
                    "hidden": false
                },
                {
                    "_id": "681db58b1f1c39ba8fbe0168",
                    "name": "Yongchao Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T10:59:23.000Z",
            "submittedOnDailyAt": "2025-05-09T08:38:34.519Z",
            "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
            "submittedOnDailyBy": {
                "_id": "681db120007a2d4056d25c70",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
                "isPro": false,
                "fullname": "yepeng liu",
                "user": "pengliu123",
                "type": "user"
            },
            "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled LiftFeat, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.",
            "upvotes": 6,
            "discussionId": "681db58d1f1c39ba8fbe01fb",
            "githubRepo": "https://github.com/lyp-deeplearning/LiftFeat",
            "ai_keywords": [
                "monocular depth estimation model",
                "pseudo surface normal label",
                "3D geometric feature",
                "surface normal feature",
                "2D descriptor feature",
                "3D geometry-aware feature lifting module",
                "relative pose estimation",
                "homography estimation"
            ]
        },
        "publishedAt": "2025-05-06T06:59:23.000Z",
        "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
        "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled LiftFeat, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03422.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "681db120007a2d4056d25c70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
            "fullname": "yepeng liu",
            "name": "pengliu123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05408",
            "authors": [
                {
                    "_id": "681daba2e3775056736651ce",
                    "user": {
                        "_id": "61424bf4f0d914a5f606a823",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
                        "isPro": false,
                        "fullname": "Yong Zheng-Xin",
                        "user": "yongzx",
                        "type": "user"
                    },
                    "name": "Zheng-Xin Yong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:20:45.630Z",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651cf",
                    "name": "M. Farid Adilazuarda",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d0",
                    "user": {
                        "_id": "6509feb92257a3afbaeecfea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509feb92257a3afbaeecfea/a_UbA-2WtZeLTf0ugVzSh.jpeg",
                        "isPro": false,
                        "fullname": "Jonibek Mansurov",
                        "user": "MJonibek",
                        "type": "user"
                    },
                    "name": "Jonibek Mansurov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:03:39.843Z",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d1",
                    "name": "Ruochen Zhang",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d2",
                    "user": {
                        "_id": "5f1eb362eec0ad2a071ad6e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
                        "isPro": false,
                        "fullname": "Niklas Muennighoff",
                        "user": "Muennighoff",
                        "type": "user"
                    },
                    "name": "Niklas Muennighoff",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:04:00.900Z",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d3",
                    "name": "Carsten Eickhoff",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d4",
                    "user": {
                        "_id": "5f5c4b20e56d546cd6233098",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
                        "isPro": false,
                        "fullname": "Genta Indra Winata",
                        "user": "gentaiscool",
                        "type": "user"
                    },
                    "name": "Genta Indra Winata",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:04:13.878Z",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d5",
                    "user": {
                        "_id": "6544e43b12da508864c38f96",
                        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
                        "isPro": false,
                        "fullname": "Julia Kreutzer",
                        "user": "JuliaKreutzerCohere",
                        "type": "user"
                    },
                    "name": "Julia Kreutzer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:04:29.257Z",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d6",
                    "name": "Stephen H. Bach",
                    "hidden": false
                },
                {
                    "_id": "681daba2e3775056736651d7",
                    "name": "Alham Fikri Aji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T16:50:06.000Z",
            "submittedOnDailyAt": "2025-05-09T05:46:57.523Z",
            "title": "Crosslingual Reasoning through Test-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "61424bf4f0d914a5f606a823",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
                "isPro": false,
                "fullname": "Yong Zheng-Xin",
                "user": "yongzx",
                "type": "user"
            },
            "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
            "upvotes": 5,
            "discussionId": "681daba2e3775056736651f9",
            "ai_keywords": [
                "reasoning language models (RLMs)",
                "long chain-of-thoughts (CoTs)",
                "multilingual mathematical reasoning",
                "low-resource languages",
                "quote-and-think pattern",
                "scaling up inference compute",
                "high-resource languages",
                "out-of-domain reasoning generalization",
                "STEM",
                "cultural commonsense knowledge",
                "crosslingual generalization",
                "test-time scaling"
            ]
        },
        "publishedAt": "2025-05-08T12:50:06.000Z",
        "title": "Crosslingual Reasoning through Test-Time Scaling",
        "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05408.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "fullname": "Yong Zheng-Xin",
            "name": "yongzx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05288",
            "authors": [
                {
                    "_id": "681d9dd229119d666079b275",
                    "user": {
                        "_id": "63a3170f8c0c89dcae316858",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
                        "isPro": false,
                        "fullname": "Ahmed Abdelreheem",
                        "user": "Samir55",
                        "type": "user"
                    },
                    "name": "Ahmed Abdelreheem",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T07:21:06.667Z",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b276",
                    "user": {
                        "_id": "66eae6491cd3794eb4cd1992",
                        "avatarUrl": "/avatars/5802de373ccc815f68b98b320aa787bf.svg",
                        "isPro": false,
                        "fullname": "Filippo Aleotti",
                        "user": "Filippo8",
                        "type": "user"
                    },
                    "name": "Filippo Aleotti",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:04:46.129Z",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b277",
                    "user": {
                        "_id": "63166685f5e32157c51fe616",
                        "avatarUrl": "/avatars/5b7d8b0e54339d2dc982676af9e4f4fe.svg",
                        "isPro": false,
                        "fullname": "Jamie Watson",
                        "user": "Aileron",
                        "type": "user"
                    },
                    "name": "Jamie Watson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:04:52.606Z",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b278",
                    "user": {
                        "_id": "6703aece547acbd64c531b72",
                        "avatarUrl": "/avatars/8042f99c6eb2c9b61be8c9b950818b2f.svg",
                        "isPro": false,
                        "fullname": "Zawar Qureshi",
                        "user": "zuluquebec",
                        "type": "user"
                    },
                    "name": "Zawar Qureshi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:04:58.663Z",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b279",
                    "user": {
                        "_id": "6577f3bacfb2207f11e847bb",
                        "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
                        "isPro": false,
                        "fullname": "Abdelrahman Eldesokey",
                        "user": "abdo-eldesokey",
                        "type": "user"
                    },
                    "name": "Abdelrahman Eldesokey",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:05:05.828Z",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b27a",
                    "name": "Peter Wonka",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b27b",
                    "name": "Gabriel Brostow",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b27c",
                    "user": {
                        "_id": "67f517dcbd75b1099bba2857",
                        "avatarUrl": "/avatars/a1a25d7972b1857f8bb49bc9efc02ded.svg",
                        "isPro": false,
                        "fullname": "Sara Vicente",
                        "user": "svicente",
                        "type": "user"
                    },
                    "name": "Sara Vicente",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:05:21.990Z",
                    "hidden": false
                },
                {
                    "_id": "681d9dd229119d666079b27d",
                    "name": "Guillermo Garcia-Hernando",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T14:29:11.000Z",
            "submittedOnDailyAt": "2025-05-09T04:50:18.526Z",
            "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
            "submittedOnDailyBy": {
                "_id": "63a3170f8c0c89dcae316858",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
                "isPro": false,
                "fullname": "Ahmed Abdelreheem",
                "user": "Samir55",
                "type": "user"
            },
            "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
            "upvotes": 5,
            "discussionId": "681d9dd529119d666079b348",
            "projectPage": "https://nianticlabs.github.io/placeit3d/",
            "githubRepo": "https://github.com/nianticlabs/placeit3d",
            "ai_keywords": [
                "point cloud",
                "3D asset",
                "textual prompt",
                "3D LLMs",
                "bounding",
                "grounding",
                "3D geometric relationships",
                "free space",
                "evaluation protocol",
                "benchmark"
            ]
        },
        "publishedAt": "2025-05-08T10:29:11.000Z",
        "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
        "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05288.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a3170f8c0c89dcae316858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
            "fullname": "Ahmed Abdelreheem",
            "name": "Samir55",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05064",
            "authors": [
                {
                    "_id": "681d93e929119d6660775937",
                    "user": {
                        "_id": "64424c46afba527ed365bf7a",
                        "avatarUrl": "/avatars/29ed5c6cbc397932ba5434dfd52f705c.svg",
                        "isPro": false,
                        "fullname": "Xinyang Lu",
                        "user": "xylu",
                        "type": "user"
                    },
                    "name": "Xinyang Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T13:54:48.001Z",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d6660775938",
                    "name": "Xinyuan Niu",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d6660775939",
                    "name": "Gregory Kang Ruey Lau",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d666077593a",
                    "name": "Bui Thi Cam Nhung",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d666077593b",
                    "name": "Rachael Hwee Ling Sim",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d666077593c",
                    "name": "Fanyu Wen",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d666077593d",
                    "name": "Chuan-Sheng Foo",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d666077593e",
                    "name": "See-Kiong Ng",
                    "hidden": false
                },
                {
                    "_id": "681d93e929119d666077593f",
                    "name": "Bryan Kian Hsiang Low",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T08:56:46.000Z",
            "submittedOnDailyAt": "2025-05-09T14:30:08.768Z",
            "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric",
            "submittedOnDailyBy": {
                "_id": "64424c46afba527ed365bf7a",
                "avatarUrl": "/avatars/29ed5c6cbc397932ba5434dfd52f705c.svg",
                "isPro": false,
                "fullname": "Xinyang Lu",
                "user": "xylu",
                "type": "user"
            },
            "summary": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.",
            "upvotes": 5,
            "discussionId": "681d93eb29119d666077599a",
            "githubRepo": "https://github.com/lululu008/WaterDrum",
            "ai_keywords": [
                "large language model (LLM) unlearning",
                "robust text watermarking",
                "WaterDrum",
                "benchmarks",
                "WaterDrum-Ax"
            ]
        },
        "publishedAt": "2025-05-08T04:56:46.000Z",
        "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric",
        "summary": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05064.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64424c46afba527ed365bf7a",
            "avatarUrl": "/avatars/29ed5c6cbc397932ba5434dfd52f705c.svg",
            "fullname": "Xinyang Lu",
            "name": "xylu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.04842",
            "authors": [
                {
                    "_id": "681e4de2f2cba17ad23d9f08",
                    "name": "Kusha Sareen",
                    "hidden": false
                },
                {
                    "_id": "681e4de2f2cba17ad23d9f09",
                    "name": "Morgane M Moss",
                    "hidden": false
                },
                {
                    "_id": "681e4de2f2cba17ad23d9f0a",
                    "name": "Alessandro Sordoni",
                    "hidden": false
                },
                {
                    "_id": "681e4de2f2cba17ad23d9f0b",
                    "name": "Rishabh Agarwal",
                    "hidden": false
                },
                {
                    "_id": "681e4de2f2cba17ad23d9f0c",
                    "name": "Arian Hosseini",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62e447e9640206c5d21e5ff6/JOR9S-SVAuxfpTkVcCYYu.png"
            ],
            "publishedAt": "2025-05-07T22:41:26.000Z",
            "submittedOnDailyAt": "2025-05-09T17:57:56.605Z",
            "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers",
            "submittedOnDailyBy": {
                "_id": "62e447e9640206c5d21e5ff6",
                "avatarUrl": "/avatars/42d8247c2ffa98a4c6255ce84051f1b9.svg",
                "isPro": false,
                "fullname": "Arian Hosseini",
                "user": "arianhosseini",
                "type": "user"
            },
            "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL^V that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL^V\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n8-32times efficient test-time compute scaling compared to the base RL\nmethod. RL^V also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL^V achieves\n1.2-1.6times higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model.",
            "upvotes": 4,
            "discussionId": "681e4de2f2cba17ad23d9f72",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "GRPO",
                "Leave-one-out PPO",
                "value function",
                "verification",
                "RL$^V$",
                "generative verifier",
                "RL-generated data",
                "MATH accuracy",
                "parallel sampling",
                "test-time compute scaling",
                "generalization capabilities",
                "out-of-domain tasks",
                "reasoning R1 model"
            ]
        },
        "publishedAt": "2025-05-07T18:41:26.000Z",
        "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers",
        "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL^V that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL^V\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n8-32times efficient test-time compute scaling compared to the base RL\nmethod. RL^V also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL^V achieves\n1.2-1.6times higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62e447e9640206c5d21e5ff6/JOR9S-SVAuxfpTkVcCYYu.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04842.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62e447e9640206c5d21e5ff6",
            "avatarUrl": "/avatars/42d8247c2ffa98a4c6255ce84051f1b9.svg",
            "fullname": "Arian Hosseini",
            "name": "arianhosseini",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2504.19314",
            "authors": [
                {
                    "_id": "681d89e6d025518b321f67ce",
                    "user": {
                        "_id": "6673cf668d570d59b83511cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
                        "isPro": false,
                        "fullname": "Peilin Zhou",
                        "user": "PALIN2018",
                        "type": "user"
                    },
                    "name": "Peilin Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:01:42.666Z",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67cf",
                    "name": "Bruce Leon",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d0",
                    "user": {
                        "_id": "64489ca21d52a633c8f55aba",
                        "avatarUrl": "/avatars/5199a5e93161c61d14ec13f79dd8c2c5.svg",
                        "isPro": false,
                        "fullname": "Xiang Ying",
                        "user": "MindYing",
                        "type": "user"
                    },
                    "name": "Xiang Ying",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-09T04:51:51.507Z",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d1",
                    "name": "Can Zhang",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d2",
                    "name": "Yifan Shao",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d3",
                    "user": {
                        "_id": "636dfa6193d9a0c987d41b73",
                        "avatarUrl": "/avatars/14396c8beb376b0d3c27a23fadaeb15e.svg",
                        "isPro": false,
                        "fullname": "Qichen YE",
                        "user": "yeeeqichen99",
                        "type": "user"
                    },
                    "name": "Qichen Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:02:33.873Z",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d4",
                    "name": "Dading Chong",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d5",
                    "user": {
                        "_id": "66bc683f432c73a183ef787c",
                        "avatarUrl": "/avatars/9505a1e6131093a91d0454e50bcbba00.svg",
                        "isPro": false,
                        "fullname": "Zhiling Jin",
                        "user": "HawkFaust",
                        "type": "user"
                    },
                    "name": "Zhiling Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:02:48.180Z",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d6",
                    "name": "Chenxuan Xie",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d7",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d8",
                    "name": "Yuxin Gu",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67d9",
                    "name": "Sixin Hong",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67da",
                    "name": "Jing Ren",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67db",
                    "name": "Jian Chen",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67dc",
                    "name": "Chao Liu",
                    "hidden": false
                },
                {
                    "_id": "681d89e6d025518b321f67dd",
                    "name": "Yining Hua",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
            ],
            "publishedAt": "2025-04-27T17:32:43.000Z",
            "submittedOnDailyAt": "2025-05-09T03:24:22.739Z",
            "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
            "submittedOnDailyBy": {
                "_id": "6673cf668d570d59b83511cc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
                "isPro": false,
                "fullname": "Peilin Zhou",
                "user": "PALIN2018",
                "type": "user"
            },
            "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
            "upvotes": 4,
            "discussionId": "681d89e7d025518b321f6807",
            "githubRepo": "https://github.com/PALIN2018/BrowseComp-ZH",
            "ai_keywords": [
                "tool-using agents",
                "multihop questions",
                "information ecosystems",
                "quality control protocol",
                "reasoning and information reconciliation"
            ]
        },
        "publishedAt": "2025-04-27T13:32:43.000Z",
        "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
        "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19314.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6673cf668d570d59b83511cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
            "fullname": "Peilin Zhou",
            "name": "PALIN2018",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.04769",
            "authors": [
                {
                    "_id": "681e0020bb3bac4b88e07a34",
                    "user": {
                        "_id": "67ddd80896ac367438d400a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                        "isPro": false,
                        "fullname": "Ranjan Sapkota",
                        "user": "RanjanSapkota",
                        "type": "user"
                    },
                    "name": "Ranjan Sapkota",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-09T13:54:15.281Z",
                    "hidden": false
                },
                {
                    "_id": "681e0020bb3bac4b88e07a35",
                    "name": "Yang Cao",
                    "hidden": false
                },
                {
                    "_id": "681e0020bb3bac4b88e07a36",
                    "name": "Konstantinos I. Roumeliotis",
                    "hidden": false
                },
                {
                    "_id": "681e0020bb3bac4b88e07a37",
                    "name": "Manoj Karkee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/dGLriRAHBv2MRep8YUG0L.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/MGf5uf8WWYxkfuWL6b3GP.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/tf5QmWeJjq8DUKh85j41R.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/xpmRKY_GI-pI23OfejhAa.jpeg"
            ],
            "publishedAt": "2025-05-07T19:46:43.000Z",
            "submittedOnDailyAt": "2025-05-09T12:29:46.820Z",
            "title": "Vision-Language-Action Models: Concepts, Progress, Applications and\n  Challenges",
            "submittedOnDailyBy": {
                "_id": "67ddd80896ac367438d400a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                "isPro": false,
                "fullname": "Ranjan Sapkota",
                "user": "RanjanSapkota",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models mark a transformative advancement in\nartificial intelligence, aiming to unify perception, natural language\nunderstanding, and embodied action within a single computational framework.\nThis foundational review presents a comprehensive synthesis of recent\nadvancements in Vision-Language-Action models, systematically organized across\nfive thematic pillars that structure the landscape of this rapidly evolving\nfield. We begin by establishing the conceptual foundations of VLA systems,\ntracing their evolution from cross-modal learning architectures to generalist\nagents that tightly integrate vision-language models (VLMs), action planners,\nand hierarchical controllers. Our methodology adopts a rigorous literature\nreview framework, covering over 80 VLA models published in the past three\nyears. Key progress areas include architectural innovations,\nparameter-efficient training strategies, and real-time inference accelerations.\nWe explore diverse application domains such as humanoid robotics, autonomous\nvehicles, medical and industrial robotics, precision agriculture, and augmented\nreality navigation. The review further addresses major challenges across\nreal-time control, multimodal action representation, system scalability,\ngeneralization to unseen tasks, and ethical deployment risks. Drawing from the\nstate-of-the-art, we propose targeted solutions including agentic AI\nadaptation, cross-embodiment generalization, and unified neuro-symbolic\nplanning. In our forward-looking discussion, we outline a future roadmap where\nVLA models, VLMs, and agentic AI converge to power socially aligned, adaptive,\nand general-purpose embodied agents. This work serves as a foundational\nreference for advancing intelligent, real-world robotics and artificial general\nintelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language\nModels",
            "upvotes": 3,
            "discussionId": "681e0021bb3bac4b88e07a73",
            "ai_keywords": [
                "cross-modal learning architectures",
                "generalist agents",
                "vision-language models (VLMs)",
                "action planners",
                "hierarchical controllers",
                "parameter-efficient training strategies",
                "real-time inference accelerations",
                "humanoid robotics",
                "autonomous vehicles",
                "medical and industrial robotics",
                "precision agriculture",
                "augmented reality navigation",
                "real-time control",
                "multimodal action representation",
                "system scalability",
                "ethical deployment risks",
                "agentic AI",
                "cross-embodiment generalization",
                "unified neuro-symbolic planning",
                "intelligent, real-world robotics",
                "artificial general intelligence"
            ]
        },
        "publishedAt": "2025-05-07T15:46:43.000Z",
        "title": "Vision-Language-Action Models: Concepts, Progress, Applications and\n  Challenges",
        "summary": "Vision-Language-Action (VLA) models mark a transformative advancement in\nartificial intelligence, aiming to unify perception, natural language\nunderstanding, and embodied action within a single computational framework.\nThis foundational review presents a comprehensive synthesis of recent\nadvancements in Vision-Language-Action models, systematically organized across\nfive thematic pillars that structure the landscape of this rapidly evolving\nfield. We begin by establishing the conceptual foundations of VLA systems,\ntracing their evolution from cross-modal learning architectures to generalist\nagents that tightly integrate vision-language models (VLMs), action planners,\nand hierarchical controllers. Our methodology adopts a rigorous literature\nreview framework, covering over 80 VLA models published in the past three\nyears. Key progress areas include architectural innovations,\nparameter-efficient training strategies, and real-time inference accelerations.\nWe explore diverse application domains such as humanoid robotics, autonomous\nvehicles, medical and industrial robotics, precision agriculture, and augmented\nreality navigation. The review further addresses major challenges across\nreal-time control, multimodal action representation, system scalability,\ngeneralization to unseen tasks, and ethical deployment risks. Drawing from the\nstate-of-the-art, we propose targeted solutions including agentic AI\nadaptation, cross-embodiment generalization, and unified neuro-symbolic\nplanning. In our forward-looking discussion, we outline a future roadmap where\nVLA models, VLMs, and agentic AI converge to power socially aligned, adaptive,\nand general-purpose embodied agents. This work serves as a foundational\nreference for advancing intelligent, real-world robotics and artificial general\nintelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language\nModels",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/dGLriRAHBv2MRep8YUG0L.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/MGf5uf8WWYxkfuWL6b3GP.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/tf5QmWeJjq8DUKh85j41R.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/xpmRKY_GI-pI23OfejhAa.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04769.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67ddd80896ac367438d400a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
            "fullname": "Ranjan Sapkota",
            "name": "RanjanSapkota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02363",
            "authors": [
                {
                    "_id": "681a1efda513c2226dff667c",
                    "name": "Tianjian Li",
                    "hidden": false
                },
                {
                    "_id": "681a1efda513c2226dff667d",
                    "name": "Daniel Khashabi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T04:54:44.000Z",
            "submittedOnDailyAt": "2025-05-09T17:14:46.290Z",
            "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in\n  Language Model Preference Learning",
            "submittedOnDailyBy": {
                "_id": "62bc72d464b7a623f3a242e9",
                "avatarUrl": "/avatars/db79848dc78a5c4927e27de4426c3b47.svg",
                "isPro": false,
                "fullname": "Tianjian Li",
                "user": "dogtooth",
                "type": "user"
            },
            "summary": "Aligning language models with human preferences relies on pairwise preference\ndatasets. While some studies suggest that on-policy data consistently\noutperforms off -policy data for preference learning, others indicate that the\nadvantages of on-policy data may be task-dependent, highlighting the need for a\nsystematic exploration of their interplay.\n  In this work, we show that on-policy and off-policy data offer complementary\nstrengths in preference optimization: on-policy data is particularly effective\nfor reasoning tasks like math and coding, while off-policy data performs better\non open-ended tasks such as creative writing and making personal\nrecommendations. Guided by these findings, we introduce SIMPLEMIX, an approach\nto combine the complementary strengths of on-policy and off-policy preference\nlearning by simply mixing these two data sources. Our empirical results across\ndiverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves\nlanguage model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO\nand off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it\noutperforms prior approaches that are much more complex in combining on- and\noff-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.",
            "upvotes": 3,
            "discussionId": "681a1effa513c2226dff6726"
        },
        "publishedAt": "2025-05-05T00:54:44.000Z",
        "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in\n  Language Model Preference Learning",
        "summary": "Aligning language models with human preferences relies on pairwise preference\ndatasets. While some studies suggest that on-policy data consistently\noutperforms off -policy data for preference learning, others indicate that the\nadvantages of on-policy data may be task-dependent, highlighting the need for a\nsystematic exploration of their interplay.\n  In this work, we show that on-policy and off-policy data offer complementary\nstrengths in preference optimization: on-policy data is particularly effective\nfor reasoning tasks like math and coding, while off-policy data performs better\non open-ended tasks such as creative writing and making personal\nrecommendations. Guided by these findings, we introduce SIMPLEMIX, an approach\nto combine the complementary strengths of on-policy and off-policy preference\nlearning by simply mixing these two data sources. Our empirical results across\ndiverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves\nlanguage model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO\nand off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it\noutperforms prior approaches that are much more complex in combining on- and\noff-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02363.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62bc72d464b7a623f3a242e9",
            "avatarUrl": "/avatars/db79848dc78a5c4927e27de4426c3b47.svg",
            "fullname": "Tianjian Li",
            "name": "dogtooth",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04955",
            "authors": [
                {
                    "_id": "681dc1b7965798cccfeab83c",
                    "user": {
                        "_id": "654cca3fe1b4cd6d40d5a7ae",
                        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
                        "isPro": false,
                        "fullname": "Fangwei Zhu",
                        "user": "soliz1998",
                        "type": "user"
                    },
                    "name": "Fangwei Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:07:17.372Z",
                    "hidden": false
                },
                {
                    "_id": "681dc1b7965798cccfeab83d",
                    "user": {
                        "_id": "656873f33fd0bf1f82558695",
                        "avatarUrl": "/avatars/7a085da2e2a91d7f41988501a573ebf9.svg",
                        "isPro": false,
                        "fullname": "PEIYI, WANG",
                        "user": "peiyiwang89",
                        "type": "user"
                    },
                    "name": "Peiyi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-09T09:07:23.247Z",
                    "hidden": false
                },
                {
                    "_id": "681dc1b7965798cccfeab83e",
                    "name": "Zhifang Sui",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
            ],
            "publishedAt": "2025-05-08T05:32:36.000Z",
            "submittedOnDailyAt": "2025-05-09T07:21:04.391Z",
            "title": "Chain-of-Thought Tokens are Computer Program Variables",
            "submittedOnDailyBy": {
                "_id": "654cca3fe1b4cd6d40d5a7ae",
                "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
                "isPro": false,
                "fullname": "Fangwei Zhu",
                "user": "soliz1998",
                "type": "user"
            },
            "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
            "upvotes": 1,
            "discussionId": "681dc1b8965798cccfeab86d",
            "githubRepo": "https://github.com/solitaryzero/CoTs_are_Variables",
            "ai_keywords": [
                "chain-of-thought (CoT)",
                "large language models (LLMs)",
                "intermediate steps",
                "reasoning tasks",
                "inner mechanism",
                "CoT tokens",
                "compositional tasks",
                "multi-digit multiplication",
                "dynamic programming",
                "intermediate results",
                "latent form",
                "variables",
                "unintended shortcuts",
                "computational complexity limits"
            ]
        },
        "publishedAt": "2025-05-08T01:32:36.000Z",
        "title": "Chain-of-Thought Tokens are Computer Program Variables",
        "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04955.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "fullname": "Fangwei Zhu",
            "name": "soliz1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]