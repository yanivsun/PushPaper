[
    {
        "paper": {
            "id": "2505.02550",
            "authors": [
                {
                    "_id": "6819ef0b2ff435c58da4d860",
                    "user": {
                        "_id": "63ecbccac8827dd0f0f59579",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
                        "isPro": false,
                        "fullname": "Chris Ociepa",
                        "user": "chrisociepa",
                        "type": "user"
                    },
                    "name": "Krzysztof Ociepa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T09:03:56.213Z",
                    "hidden": false
                },
                {
                    "_id": "6819ef0b2ff435c58da4d861",
                    "name": "Łukasz Flis",
                    "hidden": false
                },
                {
                    "_id": "6819ef0b2ff435c58da4d862",
                    "user": {
                        "_id": "61786d0b038518aa2827c6b7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
                        "isPro": false,
                        "fullname": "Remek Kinas",
                        "user": "Remek",
                        "type": "user"
                    },
                    "name": "Remigiusz Kinas",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T06:51:15.217Z",
                    "hidden": false
                },
                {
                    "_id": "6819ef0b2ff435c58da4d863",
                    "user": {
                        "_id": "5e47d3eb178ca95365287400",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
                        "isPro": true,
                        "fullname": "Krzysztof Wróbel",
                        "user": "djstrong",
                        "type": "user"
                    },
                    "name": "Krzysztof Wróbel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T09:03:54.135Z",
                    "hidden": false
                },
                {
                    "_id": "6819ef0b2ff435c58da4d864",
                    "user": {
                        "_id": "636b9b006890dd5a450081a5",
                        "avatarUrl": "/avatars/2b4ba89894d952f95ecd1e9926580608.svg",
                        "isPro": false,
                        "fullname": "AG",
                        "user": "adgw",
                        "type": "user"
                    },
                    "name": "Adrian Gwoździej",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T12:56:24.966Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T10:39:51.000Z",
            "submittedOnDailyAt": "2025-05-12T07:26:20.895Z",
            "title": "Bielik v3 Small: Technical Report",
            "submittedOnDailyBy": {
                "_id": "5e47d3eb178ca95365287400",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
                "isPro": true,
                "fullname": "Krzysztof Wróbel",
                "user": "djstrong",
                "type": "user"
            },
            "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
            "upvotes": 53,
            "discussionId": "6819ef0c2ff435c58da4d892",
            "projectPage": "https://bielik.ai/",
            "githubRepo": "https://github.com/speakleash",
            "ai_keywords": [
                "parameter-efficient",
                "generative text models",
                "token efficiency",
                "custom Polish tokenizer",
                "Weighted Instruction Cross-Entropy Loss",
                "Adaptive Learning Rate"
            ]
        },
        "publishedAt": "2025-05-05T06:39:51.000Z",
        "title": "Bielik v3 Small: Technical Report",
        "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02550.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "fullname": "Krzysztof Wróbel",
            "name": "djstrong",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02410",
            "authors": [
                {
                    "_id": "6819f19e5c7ea9f74284d3a3",
                    "user": {
                        "_id": "63ecbccac8827dd0f0f59579",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ecbccac8827dd0f0f59579/kz-2F9Z0QKllifgZmr8tH.jpeg",
                        "isPro": false,
                        "fullname": "Chris Ociepa",
                        "user": "chrisociepa",
                        "type": "user"
                    },
                    "name": "Krzysztof Ociepa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T09:03:52.265Z",
                    "hidden": false
                },
                {
                    "_id": "6819f19e5c7ea9f74284d3a4",
                    "name": "Łukasz Flis",
                    "hidden": false
                },
                {
                    "_id": "6819f19e5c7ea9f74284d3a5",
                    "user": {
                        "_id": "5e47d3eb178ca95365287400",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
                        "isPro": true,
                        "fullname": "Krzysztof Wróbel",
                        "user": "djstrong",
                        "type": "user"
                    },
                    "name": "Krzysztof Wróbel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T09:03:50.340Z",
                    "hidden": false
                },
                {
                    "_id": "6819f19e5c7ea9f74284d3a6",
                    "user": {
                        "_id": "636b9b006890dd5a450081a5",
                        "avatarUrl": "/avatars/2b4ba89894d952f95ecd1e9926580608.svg",
                        "isPro": false,
                        "fullname": "AG",
                        "user": "adgw",
                        "type": "user"
                    },
                    "name": "Adrian Gwoździej",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T12:56:22.907Z",
                    "hidden": false
                },
                {
                    "_id": "6819f19e5c7ea9f74284d3a7",
                    "user": {
                        "_id": "61786d0b038518aa2827c6b7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61786d0b038518aa2827c6b7/d1UnfivoVreYebS5JM3P9.jpeg",
                        "isPro": false,
                        "fullname": "Remek Kinas",
                        "user": "Remek",
                        "type": "user"
                    },
                    "name": "Remigiusz Kinas",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T06:51:13.426Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T07:03:41.000Z",
            "submittedOnDailyAt": "2025-05-12T07:25:02.402Z",
            "title": "Bielik 11B v2 Technical Report",
            "submittedOnDailyBy": {
                "_id": "5e47d3eb178ca95365287400",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
                "isPro": true,
                "fullname": "Krzysztof Wróbel",
                "user": "djstrong",
                "type": "user"
            },
            "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
            "upvotes": 44,
            "discussionId": "6819f19e5c7ea9f74284d3cc",
            "projectPage": "https://bielik.ai/",
            "githubRepo": "https://github.com/speakleash",
            "ai_keywords": [
                "Weighted Instruction Cross-Entropy Loss",
                "Adaptive Learning Rate",
                "depth up-scaling",
                "parameter efficiency",
                "quantization"
            ]
        },
        "publishedAt": "2025-05-05T03:03:41.000Z",
        "title": "Bielik 11B v2 Technical Report",
        "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02410.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5e47d3eb178ca95365287400",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png",
            "fullname": "Krzysztof Wróbel",
            "name": "djstrong",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06111",
            "authors": [
                {
                    "_id": "68218b847202d193249511b6",
                    "user": {
                        "_id": "64ac1f169dcc5787461468a4",
                        "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
                        "isPro": false,
                        "fullname": "Qingwen Bu",
                        "user": "qwbu",
                        "type": "user"
                    },
                    "name": "Qingwen Bu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:14:55.391Z",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511b7",
                    "name": "Yanting Yang",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511b8",
                    "user": {
                        "_id": "66a3402e4c2093e582bdf511",
                        "avatarUrl": "/avatars/6f2e1f37b6a6cf9dc6df228482c0777a.svg",
                        "isPro": false,
                        "fullname": "Jisong Cai",
                        "user": "SereneC",
                        "type": "user"
                    },
                    "name": "Jisong Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:15:26.413Z",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511b9",
                    "user": {
                        "_id": "654a31c073416a223f3b5fca",
                        "avatarUrl": "/avatars/bab382c46787eaf7889ed241e12775ee.svg",
                        "isPro": false,
                        "fullname": "Shenyuan Gao",
                        "user": "Little-Podi",
                        "type": "user"
                    },
                    "name": "Shenyuan Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:15:33.171Z",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511ba",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T06:50:15.305Z",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511bb",
                    "user": {
                        "_id": "67739bfa64e8b7438ae68eb4",
                        "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
                        "isPro": false,
                        "fullname": "Maoqing Yao",
                        "user": "AutobotZero",
                        "type": "user"
                    },
                    "name": "Maoqing Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:15:39.850Z",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511bc",
                    "user": {
                        "_id": "67cb7d55560c3dcbb1adeaa3",
                        "avatarUrl": "/avatars/0b616d3655b0b54a621c2608b2f14379.svg",
                        "isPro": false,
                        "fullname": "Ping Luo",
                        "user": "appleluo",
                        "type": "user"
                    },
                    "name": "Ping Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:15:47.622Z",
                    "hidden": false
                },
                {
                    "_id": "68218b847202d193249511bd",
                    "user": {
                        "_id": "6499b0184936457997180c90",
                        "avatarUrl": "/avatars/b8be7bfabf746639e30330f5f623f560.svg",
                        "isPro": false,
                        "fullname": "Hongyang Li",
                        "user": "compileme",
                        "type": "user"
                    },
                    "name": "Hongyang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:16:21.369Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-09T15:11:13.000Z",
            "submittedOnDailyAt": "2025-05-12T04:30:20.087Z",
            "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
            "submittedOnDailyBy": {
                "_id": "64ac1f169dcc5787461468a4",
                "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
                "isPro": false,
                "fullname": "Qingwen Bu",
                "user": "qwbu",
                "type": "user"
            },
            "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
            "upvotes": 17,
            "discussionId": "68218b857202d19324951214",
            "githubRepo": "https://github.com/OpenDriveLab/UniVLA",
            "ai_keywords": [
                "UniVLA",
                "vision-language-action (VLA) policies",
                "latent action model",
                "DINO feature space",
                "latent action decoding",
                "manipulation benchmarks",
                "navigation benchmarks",
                "real-robot deployments",
                "OpenVLA"
            ]
        },
        "publishedAt": "2025-05-09T11:11:13.000Z",
        "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
        "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06111.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ac1f169dcc5787461468a4",
            "avatarUrl": "/avatars/c031a75989147009b7850df4eddfcb27.svg",
            "fullname": "Qingwen Bu",
            "name": "qwbu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05026",
            "authors": [
                {
                    "_id": "6821771ddf190eabf5f666d8",
                    "user": {
                        "_id": "655c44752205aab35222aca3",
                        "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
                        "isPro": false,
                        "fullname": "Jaehyun Jeon",
                        "user": "jeochris",
                        "type": "user"
                    },
                    "name": "Jaehyun Jeon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T06:50:17.832Z",
                    "hidden": false
                },
                {
                    "_id": "6821771ddf190eabf5f666d9",
                    "user": {
                        "_id": "65c071e569429d85dc5e7e9c",
                        "avatarUrl": "/avatars/62a33b17db44e725da4df47ae3d8d554.svg",
                        "isPro": false,
                        "fullname": "Jang Han Yoon",
                        "user": "jeffrobot",
                        "type": "user"
                    },
                    "name": "Jang Han Yoon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:16:34.355Z",
                    "hidden": false
                },
                {
                    "_id": "6821771ddf190eabf5f666da",
                    "name": "Min Soo Kim",
                    "hidden": false
                },
                {
                    "_id": "6821771ddf190eabf5f666db",
                    "user": {
                        "_id": "6548fceac8f267c7c40c85d9",
                        "avatarUrl": "/avatars/0c44a17f92519c8533c6de994aedb954.svg",
                        "isPro": false,
                        "fullname": "SUMIN SHIM",
                        "user": "use08174",
                        "type": "user"
                    },
                    "name": "Sumin Shim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:16:50.404Z",
                    "hidden": false
                },
                {
                    "_id": "6821771ddf190eabf5f666dc",
                    "user": {
                        "_id": "64d42729f63b01b7f676b176",
                        "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
                        "isPro": false,
                        "fullname": "Yejin Choi",
                        "user": "yejinchoinka",
                        "type": "user"
                    },
                    "name": "Yejin Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:16:57.521Z",
                    "hidden": false
                },
                {
                    "_id": "6821771ddf190eabf5f666dd",
                    "name": "Hanbin Kim",
                    "hidden": false
                },
                {
                    "_id": "6821771ddf190eabf5f666de",
                    "name": "Youngjae Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T08:00:32.000Z",
            "submittedOnDailyAt": "2025-05-12T05:33:20.932Z",
            "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
            "submittedOnDailyBy": {
                "_id": "655c44752205aab35222aca3",
                "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
                "isPro": false,
                "fullname": "Jaehyun Jeon",
                "user": "jeochris",
                "type": "user"
            },
            "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.",
            "upvotes": 12,
            "discussionId": "68217722df190eabf5f66814",
            "ai_keywords": [
                "Vision-Language Models",
                "WiserUI-Bench",
                "Pairwise UI Design Persuasiveness Assessment",
                "G-FOCUS",
                "inference-time reasoning strategy",
                "position bias",
                "VLM-driven evaluation"
            ]
        },
        "publishedAt": "2025-05-08T04:00:32.000Z",
        "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
        "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05026.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655c44752205aab35222aca3",
            "avatarUrl": "/avatars/57900539952382de0ce6892faf50b401.svg",
            "fullname": "Jaehyun Jeon",
            "name": "jeochris",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.02686",
            "authors": [
                {
                    "_id": "6821acfb2808328b91c0e365",
                    "user": {
                        "_id": "64cb02869e30a46f7b80b355",
                        "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
                        "isPro": false,
                        "fullname": "Xiaobao Wu",
                        "user": "bobxwu",
                        "type": "user"
                    },
                    "name": "Xiaobao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:17:15.826Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-05T14:33:49.000Z",
            "submittedOnDailyAt": "2025-05-12T06:41:36.276Z",
            "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64cb02869e30a46f7b80b355",
                "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
                "isPro": false,
                "fullname": "Xiaobao Wu",
                "user": "bobxwu",
                "type": "user"
            },
            "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
            "upvotes": 11,
            "discussionId": "6821acfd2808328b91c0e3e3",
            "githubRepo": "https://github.com/bobxwu/learning-from-rewards-llm-papers",
            "ai_keywords": [
                "reinforcement learning",
                "RLHF",
                "DPO",
                "GRPO",
                "reward-guided decoding",
                "post-hoc correction",
                "active learning",
                "reward models"
            ]
        },
        "publishedAt": "2025-05-05T10:33:49.000Z",
        "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
        "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02686.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64cb02869e30a46f7b80b355",
            "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
            "fullname": "Xiaobao Wu",
            "name": "bobxwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06046",
            "authors": [
                {
                    "_id": "6821af48696b63e207ae8474",
                    "user": {
                        "_id": "64cb98c6f103036e23c69b1d",
                        "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
                        "isPro": false,
                        "fullname": "Harris",
                        "user": "Joshua-Harris",
                        "type": "user"
                    },
                    "name": "Joshua Harris",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T09:03:48.631Z",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae8475",
                    "user": {
                        "_id": "6422f276320bd601bff05691",
                        "avatarUrl": "/avatars/7f43ca6cb485a73e3de8a0653f109735.svg",
                        "isPro": false,
                        "fullname": "Fan Grayson",
                        "user": "fangrayson",
                        "type": "user"
                    },
                    "name": "Fan Grayson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:17:30.555Z",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae8476",
                    "user": {
                        "_id": "681205b04119e4ecc947632d",
                        "avatarUrl": "/avatars/94f18aeed6fef7764252e06244e1e7c9.svg",
                        "isPro": false,
                        "fullname": "Felix Feldman",
                        "user": "felixFeldman",
                        "type": "user"
                    },
                    "name": "Felix Feldman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:17:36.880Z",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae8477",
                    "user": {
                        "_id": "64b9323691876a8d16d2dae5",
                        "avatarUrl": "/avatars/ea583af4ec8e5ad18fefbec2281ae50e.svg",
                        "isPro": false,
                        "fullname": "Timothy Laurence",
                        "user": "TRLaurence",
                        "type": "user"
                    },
                    "name": "Timothy Laurence",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T13:17:44.371Z",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae8478",
                    "name": "Toby Nonnenmacher",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae8479",
                    "name": "Oliver Higgins",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae847a",
                    "name": "Leo Loman",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae847b",
                    "name": "Selina Patel",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae847c",
                    "name": "Thomas Finnie",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae847d",
                    "name": "Samuel Collins",
                    "hidden": false
                },
                {
                    "_id": "6821af48696b63e207ae847e",
                    "name": "Michael Borowitz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-09T13:42:59.000Z",
            "submittedOnDailyAt": "2025-05-12T07:35:55.202Z",
            "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
            "submittedOnDailyBy": {
                "_id": "64cb98c6f103036e23c69b1d",
                "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
                "isPro": false,
                "fullname": "Harris",
                "user": "Joshua-Harris",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
            "upvotes": 10,
            "discussionId": "6821af49696b63e207ae84c6",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Multiple Choice Question Answering (MCQA)",
                "PubHealthBench",
                "UK Government public health information",
                "automated pipeline",
                "extracted UK Government public health guidance documents",
                "SOTA (state of the art) LLMs",
                "GPT-4.5",
                "GPT-4.1",
                "o1"
            ]
        },
        "publishedAt": "2025-05-09T09:42:59.000Z",
        "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
        "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cb98c6f103036e23c69b1d",
            "avatarUrl": "/avatars/7ee33880ad39f5335b618dc53554124a.svg",
            "fullname": "Harris",
            "name": "Joshua-Harris",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05621",
            "authors": [
                {
                    "_id": "6821d8d0ef32025e55dac5e6",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "6821d8d0ef32025e55dac5e7",
                    "user": {
                        "_id": "655b813476e4fad5529f3256",
                        "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
                        "isPro": false,
                        "fullname": "Yan Yang",
                        "user": "HelloKKMe",
                        "type": "user"
                    },
                    "name": "Yan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-12T15:41:27.008Z",
                    "hidden": false
                },
                {
                    "_id": "6821d8d0ef32025e55dac5e8",
                    "name": "Ruikun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6821d8d0ef32025e55dac5e9",
                    "name": "Liyuan Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T20:00:11.000Z",
            "submittedOnDailyAt": "2025-05-12T09:53:55.820Z",
            "title": "A Preliminary Study for GPT-4o on Image Restoration",
            "submittedOnDailyBy": {
                "_id": "655b813476e4fad5529f3256",
                "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
                "isPro": false,
                "fullname": "Yan Yang",
                "user": "HelloKKMe",
                "type": "user"
            },
            "summary": "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an\nautoregressive architecture, has demonstrated unprecedented performance in\nimage generation. In this work, we investigate its potential impact on the\nimage restoration community. We present the first systematic evaluation of\nGPT-4o across diverse restoration tasks. Our experiments reveal that, although\nrestoration outputs from GPT-4o are visually appealing, they often suffer from\npixel-level structural fidelity when compared to ground-truth images. Common\nissues are variations in image proportions, shifts in object positions and\nquantities, and changes in viewpoint.To address it, taking image dehazing,\nderainning, and low-light enhancement as representative case studies, we show\nthat GPT-4o's outputs can serve as powerful visual priors, substantially\nenhancing the performance of existing dehazing networks. It offers practical\nguidelines and a baseline framework to facilitate the integration of GPT-4o\ninto future image restoration pipelines. We hope the study on GPT-4o image\nrestoration will accelerate innovation in the broader field of image generation\nareas. To support further research, we will release GPT-4o-restored images from\nover 10 widely used image restoration datasets.",
            "upvotes": 4,
            "discussionId": "6821d8d2ef32025e55dac6c3",
            "ai_keywords": [
                "multi-modal",
                "autoregressive architecture",
                "image restoration",
                "image generation",
                "pixel-level structural fidelity",
                "image proportions",
                "object positions",
                "viewpoint",
                "image dehazing",
                "derainning",
                "low-light enhancement",
                "visual priors",
                "dehazing networks",
                "image restoration pipelines"
            ]
        },
        "publishedAt": "2025-05-08T16:00:11.000Z",
        "title": "A Preliminary Study for GPT-4o on Image Restoration",
        "summary": "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an\nautoregressive architecture, has demonstrated unprecedented performance in\nimage generation. In this work, we investigate its potential impact on the\nimage restoration community. We present the first systematic evaluation of\nGPT-4o across diverse restoration tasks. Our experiments reveal that, although\nrestoration outputs from GPT-4o are visually appealing, they often suffer from\npixel-level structural fidelity when compared to ground-truth images. Common\nissues are variations in image proportions, shifts in object positions and\nquantities, and changes in viewpoint.To address it, taking image dehazing,\nderainning, and low-light enhancement as representative case studies, we show\nthat GPT-4o's outputs can serve as powerful visual priors, substantially\nenhancing the performance of existing dehazing networks. It offers practical\nguidelines and a baseline framework to facilitate the integration of GPT-4o\ninto future image restoration pipelines. We hope the study on GPT-4o image\nrestoration will accelerate innovation in the broader field of image generation\nareas. To support further research, we will release GPT-4o-restored images from\nover 10 widely used image restoration datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05621.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655b813476e4fad5529f3256",
            "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
            "fullname": "Yan Yang",
            "name": "HelloKKMe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.21467",
            "authors": [
                {
                    "_id": "6822175ee436561326740a3a",
                    "user": {
                        "_id": "6300a2bc5ea4c617b216ef52",
                        "avatarUrl": "/avatars/ca26855881f8fba44fe9d8a2a353b8b0.svg",
                        "isPro": false,
                        "fullname": "Luc Vedrenne",
                        "user": "almotasim",
                        "type": "user"
                    },
                    "name": "Luc Vedrenne",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T18:34:49.671Z",
                    "hidden": false
                },
                {
                    "_id": "6822175ee436561326740a3b",
                    "name": "Sylvain Faisan",
                    "hidden": false
                },
                {
                    "_id": "6822175ee436561326740a3c",
                    "name": "Denis Fortun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-30T09:42:38.000Z",
            "submittedOnDailyAt": "2025-05-12T18:17:58.308Z",
            "title": "Multiview Point Cloud Registration via Optimization in an Autoencoder\n  Latent Space",
            "submittedOnDailyBy": {
                "_id": "6300a2bc5ea4c617b216ef52",
                "avatarUrl": "/avatars/ca26855881f8fba44fe9d8a2a353b8b0.svg",
                "isPro": false,
                "fullname": "Luc Vedrenne",
                "user": "almotasim",
                "type": "user"
            },
            "summary": "Point cloud rigid registration is a fundamental problem in 3D computer\nvision. In the multiview case, we aim to find a set of 6D poses to align a set\nof objects. Methods based on pairwise registration rely on a subsequent\nsynchronization algorithm, which makes them poorly scalable with the number of\nviews. Generative approaches overcome this limitation, but are based on\nGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,\nthey are not well suited to handle large transformations. Moreover, most\nexisting methods cannot handle high levels of degradations. In this paper, we\nintroduce POLAR (POint cloud LAtent Registration), a multiview registration\nmethod able to efficiently deal with a large number of views, while being\nrobust to a high level of degradations and large initial angles. To achieve\nthis, we transpose the registration problem into the latent space of a\npretrained autoencoder, design a loss taking degradations into account, and\ndevelop an efficient multistart optimization strategy. Our proposed method\nsignificantly outperforms state-of-the-art approaches on synthetic and real\ndata. POLAR is available at github.com/pypolar/polar or as a standalone package\nwhich can be installed with pip install polaregistration.",
            "upvotes": 0,
            "discussionId": "68221760e436561326740ae7",
            "projectPage": "https://pypolar.github.io/polar/",
            "githubRepo": "https://github.com/pypolar/polar",
            "ai_keywords": [
                "point cloud rigid registration",
                "3D computer vision",
                "multiview case",
                "6D poses",
                "pairwise registration",
                "synchronization algorithm",
                "generative approaches",
                "Gaussian Mixture Models",
                "Expectation-Maximization algorithm",
                "latent space",
                "pretrained autoencoder",
                "multistart optimization strategy"
            ]
        },
        "publishedAt": "2025-04-30T05:42:38.000Z",
        "title": "Multiview Point Cloud Registration via Optimization in an Autoencoder\n  Latent Space",
        "summary": "Point cloud rigid registration is a fundamental problem in 3D computer\nvision. In the multiview case, we aim to find a set of 6D poses to align a set\nof objects. Methods based on pairwise registration rely on a subsequent\nsynchronization algorithm, which makes them poorly scalable with the number of\nviews. Generative approaches overcome this limitation, but are based on\nGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,\nthey are not well suited to handle large transformations. Moreover, most\nexisting methods cannot handle high levels of degradations. In this paper, we\nintroduce POLAR (POint cloud LAtent Registration), a multiview registration\nmethod able to efficiently deal with a large number of views, while being\nrobust to a high level of degradations and large initial angles. To achieve\nthis, we transpose the registration problem into the latent space of a\npretrained autoencoder, design a loss taking degradations into account, and\ndevelop an efficient multistart optimization strategy. Our proposed method\nsignificantly outperforms state-of-the-art approaches on synthetic and real\ndata. POLAR is available at github.com/pypolar/polar or as a standalone package\nwhich can be installed with pip install polaregistration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21467.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6300a2bc5ea4c617b216ef52",
            "avatarUrl": "/avatars/ca26855881f8fba44fe9d8a2a353b8b0.svg",
            "fullname": "Luc Vedrenne",
            "name": "almotasim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]