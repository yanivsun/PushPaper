[
    {
        "paper": {
            "id": "2505.07062",
            "authors": [
                {
                    "_id": "6822ab54a79983c00c8164eb",
                    "name": "Dong Guo",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164ec",
                    "name": "Faming Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164ed",
                    "name": "Feida Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164ee",
                    "name": "Fuxing Leng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164ef",
                    "user": {
                        "_id": "63561bfdbcf42eac0b8f13cf",
                        "avatarUrl": "/avatars/6c126e2681b930e0bad3255358fc6e48.svg",
                        "isPro": false,
                        "fullname": "Guang Shi",
                        "user": "anyuzx",
                        "type": "user"
                    },
                    "name": "Guang Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:17:52.619Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f0",
                    "user": {
                        "_id": "642a8cbfa096201096ea8fca",
                        "avatarUrl": "/avatars/232b5c38034d777d63be1157e37866b6.svg",
                        "isPro": false,
                        "fullname": "haobin chen",
                        "user": "chb1997",
                        "type": "user"
                    },
                    "name": "Haobin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:17:44.245Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f1",
                    "name": "Haoqi Fan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f2",
                    "name": "Jian Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f3",
                    "name": "Jianyu Jiang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f4",
                    "user": {
                        "_id": "6788122301d5ba1d3eff23ba",
                        "avatarUrl": "/avatars/203dc8e1d542be55ea16eafcd7f396ff.svg",
                        "isPro": false,
                        "fullname": "Jiawei Wang",
                        "user": "0nejiawei",
                        "type": "user"
                    },
                    "name": "Jiawei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:18:32.411Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f5",
                    "name": "Jingji Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f6",
                    "name": "Jingjia Huang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f7",
                    "name": "Kang Lei",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f8",
                    "user": {
                        "_id": "6638cd255b81e56d337a4a98",
                        "avatarUrl": "/avatars/b00805982755e4838b5ce0b23e3357c2.svg",
                        "isPro": false,
                        "fullname": "Liping Yuan",
                        "user": "yuanlp",
                        "type": "user"
                    },
                    "name": "Liping Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:18:54.175Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164f9",
                    "name": "Lishu Luo",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164fa",
                    "name": "Pengfei Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164fb",
                    "name": "Qinghao Ye",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164fc",
                    "name": "Rui Qian",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164fd",
                    "name": "Shen Yan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164fe",
                    "user": {
                        "_id": "61e5fb1377496de0a6d95c74",
                        "avatarUrl": "/avatars/d9864dc9033e2a6f6563a645cf9f455f.svg",
                        "isPro": false,
                        "fullname": "Shixiong Zhao",
                        "user": "kuma-zhao",
                        "type": "user"
                    },
                    "name": "Shixiong Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:19:22.238Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8164ff",
                    "name": "Shuai Peng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816500",
                    "name": "Shuangye Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816501",
                    "name": "Sihang Yuan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816502",
                    "name": "Sijin Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816503",
                    "user": {
                        "_id": "646b3db131968a60a01e4cf5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
                        "isPro": false,
                        "fullname": "Tianheng Cheng",
                        "user": "wondervictor",
                        "type": "user"
                    },
                    "name": "Tianheng Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T09:54:28.393Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816504",
                    "name": "Weiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816505",
                    "name": "Wenqian Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816506",
                    "user": {
                        "_id": "666d10506ec1340a7421a92d",
                        "avatarUrl": "/avatars/5bff16fca817b35760e0c26d8d7c9424.svg",
                        "isPro": false,
                        "fullname": "Xianhan Zeng",
                        "user": "RitzzZz23",
                        "type": "user"
                    },
                    "name": "Xianhan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:07.125Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816507",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816508",
                    "name": "Xiaobo Qin",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816509",
                    "name": "Xiaohan Ding",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81650a",
                    "name": "Xiaojun Xiao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81650b",
                    "name": "Xiaoying Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81650c",
                    "name": "Xuanwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81650d",
                    "name": "Xuehan Xiong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81650e",
                    "name": "Yanghua Peng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81650f",
                    "name": "Yangrui Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816510",
                    "name": "Yanwei Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816511",
                    "name": "Yanxu Hu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816512",
                    "name": "Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816513",
                    "name": "Yiyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816514",
                    "user": {
                        "_id": "63176933b58b0184630d2c74",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
                        "isPro": false,
                        "fullname": "Yiyuan Zhang",
                        "user": "Yiyuan",
                        "type": "user"
                    },
                    "name": "Yiyuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:09.217Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816515",
                    "name": "Youbin Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816516",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816517",
                    "name": "Yudong Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816518",
                    "name": "Yue Ling",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816519",
                    "name": "Yujia Qin",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81651a",
                    "name": "Zanbo Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81651b",
                    "user": {
                        "_id": "65bf84e772615eb159a604cc",
                        "avatarUrl": "/avatars/c0ff550281e6894a762ac57d362ffdde.svg",
                        "isPro": false,
                        "fullname": "hezhiwu",
                        "user": "hezhiwu",
                        "type": "user"
                    },
                    "name": "Zhiwu He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:22:32.103Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81651c",
                    "name": "Aoxue Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81651d",
                    "name": "Bairen Yi",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81651e",
                    "name": "Bencheng Liao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81651f",
                    "name": "Can Huang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816520",
                    "name": "Can Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816521",
                    "name": "Chaorui Deng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816522",
                    "name": "Chaoyi Deng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816523",
                    "name": "Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816524",
                    "name": "Cheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816525",
                    "name": "Chenggang Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816526",
                    "name": "Chenhui Gou",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816527",
                    "name": "Chenwei Lou",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816528",
                    "name": "Chengzhi Wei",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816529",
                    "name": "Chundian Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81652a",
                    "name": "Chunyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81652b",
                    "name": "Deyao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81652c",
                    "name": "Donghong Zhong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81652d",
                    "name": "Feng Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81652e",
                    "name": "Feng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81652f",
                    "name": "Gang Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816530",
                    "name": "Guodong Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816531",
                    "name": "Guohong Xiao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816532",
                    "name": "Haibin Lin",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816533",
                    "name": "Haihua Yang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816534",
                    "name": "Haoming Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816535",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816536",
                    "name": "Hongxiang Hao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816537",
                    "name": "Hui Shen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816538",
                    "name": "Huixia Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816539",
                    "name": "Jiahao Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81653a",
                    "name": "Jialong Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81653b",
                    "name": "Jianhua Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81653c",
                    "name": "Jianpeng Jiao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81653d",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81653e",
                    "name": "Jiaze Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81653f",
                    "name": "Jianhui Duan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816540",
                    "name": "Jihao Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816541",
                    "name": "Jin Zeng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816542",
                    "name": "Jingqun Tang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816543",
                    "name": "Jingyu Sun",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816544",
                    "user": {
                        "_id": "642435a1a3adbc7142c3b0a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/wgLT_w9jNWRU3O0jU0646.jpeg",
                        "isPro": true,
                        "fullname": "Joya Chen",
                        "user": "chenjoya",
                        "type": "user"
                    },
                    "name": "Joya Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:04.988Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816545",
                    "name": "Jun Long",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816546",
                    "name": "Junda Feng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816547",
                    "name": "Junfeng Zhan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816548",
                    "name": "Junjie Fang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816549",
                    "name": "Junting Lu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81654a",
                    "name": "Kai Hua",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81654b",
                    "name": "Kai Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81654c",
                    "name": "Kai Shen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81654d",
                    "name": "Kaiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81654e",
                    "name": "Ke Shen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81654f",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816550",
                    "name": "Keyu Pan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816551",
                    "name": "Kun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816552",
                    "user": {
                        "_id": "61fb81006374891646732f37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
                        "isPro": false,
                        "fullname": "Kunchang Li",
                        "user": "Andy1621",
                        "type": "user"
                    },
                    "name": "Kunchang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T10:27:23.151Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816553",
                    "name": "Lanxin Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816554",
                    "name": "Lei Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816555",
                    "name": "Lei Shi",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816556",
                    "name": "Li Han",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816557",
                    "name": "Liang Xiang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816558",
                    "name": "Liangqiang Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816559",
                    "user": {
                        "_id": "64b02ec0e5000ae8a572ced5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                        "isPro": false,
                        "fullname": "Lin Chen",
                        "user": "Lin-Chen",
                        "type": "user"
                    },
                    "name": "Lin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:12.502Z",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81655a",
                    "name": "Lin Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81655b",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81655c",
                    "name": "Liying Chi",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81655d",
                    "name": "Longxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81655e",
                    "name": "Mengfei Du",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81655f",
                    "name": "Mingxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816560",
                    "name": "Ningxin Pan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816561",
                    "name": "Peibin Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816562",
                    "name": "Pengfei Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816563",
                    "name": "Pengfei Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816564",
                    "name": "Qingqing Yuan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816565",
                    "name": "Qingyao Shuai",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816566",
                    "name": "Qiuyan Tao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816567",
                    "name": "Renjie Zheng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816568",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816569",
                    "name": "Ru Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81656a",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81656b",
                    "name": "Rui Yang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81656c",
                    "name": "Rui Zhao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81656d",
                    "name": "Shaoqiang Xu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81656e",
                    "name": "Shihao Liang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81656f",
                    "name": "Shipeng Yan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816570",
                    "name": "Shu Zhong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816571",
                    "name": "Shuaishuai Cao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816572",
                    "name": "Shuangzhi Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816573",
                    "name": "Shufan Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816574",
                    "name": "Shuhan Chang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816575",
                    "name": "Songhua Cai",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816576",
                    "name": "Tenglong Ao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816577",
                    "name": "Tianhao Yang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816578",
                    "name": "Tingting Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816579",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81657a",
                    "name": "Wei Jia",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81657b",
                    "name": "Wei Weng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81657c",
                    "name": "Weihao Yu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81657d",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81657e",
                    "name": "Wenjia Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81657f",
                    "name": "Wenli Yang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816580",
                    "name": "Wenzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816581",
                    "name": "Xiang Long",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816582",
                    "name": "XiangRui Yin",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816583",
                    "name": "Xiao Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816584",
                    "name": "Xiaolei Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816585",
                    "name": "Xiaoying Jia",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816586",
                    "name": "Xijin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816587",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816588",
                    "name": "Xinchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816589",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81658a",
                    "name": "Xiongcai Luo",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81658b",
                    "name": "Xiuli Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81658c",
                    "name": "Xuantong Zhong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81658d",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81658e",
                    "name": "Xujing Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81658f",
                    "name": "Yan Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816590",
                    "name": "Yawei Wen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816591",
                    "name": "Yifan Du",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816592",
                    "name": "Yihao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816593",
                    "name": "Yining Ye",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816594",
                    "name": "Yonghui Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816595",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816596",
                    "name": "Yu Yue",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816597",
                    "name": "Yufeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816598",
                    "name": "Yufeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c816599",
                    "name": "Yuhang Xu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81659a",
                    "name": "Yuhong Yang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81659b",
                    "name": "Yun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81659c",
                    "name": "Yunhao Fang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81659d",
                    "name": "Yuntao Li",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81659e",
                    "name": "Yurui Ren",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c81659f",
                    "name": "Yuwen Xiong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a0",
                    "name": "Zehua Hong",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a1",
                    "name": "Zehua Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a2",
                    "name": "Zewei Sun",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a3",
                    "name": "Zeyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a4",
                    "name": "Zhao Cai",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a5",
                    "name": "Zhaoyue Zha",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a6",
                    "name": "Zhecheng An",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a7",
                    "name": "Zhehui Zhao",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a8",
                    "name": "Zhengzhuo Xu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165a9",
                    "name": "Zhipeng Chen",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165aa",
                    "name": "Zhiyong Wu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165ab",
                    "name": "Zhuofan Zheng",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165ac",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165ad",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165ae",
                    "name": "Ziyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822ab54a79983c00c8165af",
                    "name": "Zuquan Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-11T17:28:30.000Z",
            "submittedOnDailyAt": "2025-05-13T00:47:43.019Z",
            "title": "Seed1.5-VL Technical Report",
            "submittedOnDailyBy": {
                "_id": "646b3db131968a60a01e4cf5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
                "isPro": false,
                "fullname": "Tianheng Cheng",
                "user": "wondervictor",
                "type": "user"
            },
            "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)",
            "upvotes": 84,
            "discussionId": "6822ab59a79983c00c8166ff",
            "projectPage": "https://seed.bytedance.com/en/tech/seed1_5_vl",
            "githubRepo": "https://github.com/ByteDance-Seed/Seed1.5-VL",
            "ai_keywords": [
                "vision-language foundation model",
                "multimodal understanding",
                "reasoning",
                "vision encoder",
                "Mixture-of-Experts (MoE)",
                "LLM",
                "active parameters",
                "public VLM benchmarks",
                "internal evaluation suites",
                "state-of-the-art performance",
                "GUI control",
                "gameplay",
                "multimodal systems",
                "visual puzzles",
                "multimodal reasoning"
            ]
        },
        "publishedAt": "2025-05-11T13:28:30.000Z",
        "title": "Seed1.5-VL Technical Report",
        "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07062.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "646b3db131968a60a01e4cf5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "fullname": "Tianheng Cheng",
            "name": "wondervictor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 31
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07608",
            "authors": [
                {
                    "_id": "6822b5441f775dd12f753469",
                    "name": "Xiaomi LLM-Core Team",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75346b",
                    "user": {
                        "_id": "6348c3cda6aa28fa6313e906",
                        "avatarUrl": "/avatars/2c0f0b08b1371689ebb4df18ddf45e54.svg",
                        "isPro": false,
                        "fullname": "Bingquan Xia",
                        "user": "xiabingquan",
                        "type": "user"
                    },
                    "name": "Bingquan Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:22:57.166Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75346c",
                    "name": "Bowen Shen",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75346d",
                    "name": "Cici",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75346e",
                    "user": {
                        "_id": "64d2fce8129a210e569e0c76",
                        "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
                        "isPro": false,
                        "fullname": "Dawei Zhu",
                        "user": "dwzhu",
                        "type": "user"
                    },
                    "name": "Dawei Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:55.974Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75346f",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753470",
                    "name": "Gang Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753471",
                    "user": {
                        "_id": "673dae4e4903a361fd571d1a",
                        "avatarUrl": "/avatars/df3ec2dfcacf0bd9b80b1a6955cd2a23.svg",
                        "isPro": false,
                        "fullname": "Hailin Zhang",
                        "user": "HugoZHL",
                        "type": "user"
                    },
                    "name": "Hailin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:23:37.452Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753472",
                    "user": {
                        "_id": "680e9a219e529f779991be0c",
                        "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
                        "isPro": false,
                        "fullname": "Huaqiu Liu",
                        "user": "Prestonprom",
                        "type": "user"
                    },
                    "name": "Huaqiu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T09:54:22.144Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753473",
                    "name": "Jiebao Xiao",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753474",
                    "user": {
                        "_id": "6470390f850a938d6c571cd8",
                        "avatarUrl": "/avatars/8d6eab491315e1938b2c2a52b44889f8.svg",
                        "isPro": false,
                        "fullname": "Jinhao Dong",
                        "user": "whatseeker",
                        "type": "user"
                    },
                    "name": "Jinhao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:24:09.783Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753475",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753476",
                    "name": "Peidian Li",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753477",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753478",
                    "name": "Shihua Yu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753479",
                    "user": {
                        "_id": "64f5216e3c9b5e07f85582e3",
                        "avatarUrl": "/avatars/7b5303f3fb4bc223ee6217403ed5c949.svg",
                        "isPro": false,
                        "fullname": "Ezio Chen",
                        "user": "Ezioii",
                        "type": "user"
                    },
                    "name": "Shimao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:48.569Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75347a",
                    "user": {
                        "_id": "6822b86ea371047fe15248ab",
                        "avatarUrl": "/avatars/94a4ab0ceff03df2c996d4cf2ba7f435.svg",
                        "isPro": false,
                        "fullname": "Weikun Wang",
                        "user": "kunwww",
                        "type": "user"
                    },
                    "name": "Weikun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:24:32.489Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75347b",
                    "user": {
                        "_id": "6686f695840ee769597de318",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6686f695840ee769597de318/50XZQ80Y2PElI35O0qVFc.jpeg",
                        "isPro": false,
                        "fullname": "Wenhan Ma",
                        "user": "CuteNPC",
                        "type": "user"
                    },
                    "name": "Wenhan Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:24:41.645Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75347c",
                    "user": {
                        "_id": "65add7b1abf6d1ccb77b797c",
                        "avatarUrl": "/avatars/d386eb16053f65c0425e4878b0956fb2.svg",
                        "isPro": false,
                        "fullname": "Xiangwei Deng",
                        "user": "crybymyself",
                        "type": "user"
                    },
                    "name": "Xiangwei Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:24:49.400Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75347d",
                    "name": "Yi Huang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75347e",
                    "user": {
                        "_id": "61d595f0bfc2372aaf42b766",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675437310060-61d595f0bfc2372aaf42b766.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Song",
                        "user": "Solaris99",
                        "type": "user"
                    },
                    "name": "Yifan Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T09:54:25.964Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75347f",
                    "user": {
                        "_id": "665ee916b9e194a20c4769da",
                        "avatarUrl": "/avatars/cfb3ff26979bed1a0c65a6fa318af3ad.svg",
                        "isPro": false,
                        "fullname": "Zihan Jiang",
                        "user": "Jumbo0715",
                        "type": "user"
                    },
                    "name": "Zihan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T09:54:24.059Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753480",
                    "user": {
                        "_id": "663f2e2d9efa4d370061f6f6",
                        "avatarUrl": "/avatars/6e1391b66870f03b65c3fe346b6c37c0.svg",
                        "isPro": false,
                        "fullname": "BoWen Ye",
                        "user": "Ymiracle",
                        "type": "user"
                    },
                    "name": "Bowen Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:25:12.162Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753481",
                    "name": "Can Cai",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753482",
                    "name": "Chenhong He",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753483",
                    "name": "Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753484",
                    "name": "Duo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753485",
                    "user": {
                        "_id": "669e0733de95466572d98e74",
                        "avatarUrl": "/avatars/f50ed26f62f94c95dea6d78833564295.svg",
                        "isPro": false,
                        "fullname": "Dylan Wang",
                        "user": "Dylan0318",
                        "type": "user"
                    },
                    "name": "Guoan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T09:54:19.600Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753486",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753487",
                    "name": "Haochen Zhao",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753488",
                    "name": "Heng Qu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753489",
                    "name": "Hongshen Xu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75348a",
                    "name": "Jun Shi",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75348b",
                    "name": "Kainan Bao",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75348c",
                    "user": {
                        "_id": "65b7573482d384513443875e",
                        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
                        "isPro": false,
                        "fullname": "Qingkai Fang",
                        "user": "poeroz",
                        "type": "user"
                    },
                    "name": "QingKai Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:25:28.094Z",
                    "hidden": true
                },
                {
                    "_id": "6822b5441f775dd12f75348d",
                    "name": "Kang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75348e",
                    "name": "Kangyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75348f",
                    "user": {
                        "_id": "6038d6d0612f5eef3cc05ea9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
                        "isPro": false,
                        "fullname": "Lei Li",
                        "user": "tobiaslee",
                        "type": "user"
                    },
                    "name": "Lei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:58.039Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753490",
                    "name": "Menghang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753491",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753492",
                    "user": {
                        "_id": "668e4a80f4ab25acfe353946",
                        "avatarUrl": "/avatars/a94e5b16c2f058eb03fe322fe7d0b355.svg",
                        "isPro": false,
                        "fullname": "qiantong wang",
                        "user": "MightyAtom520",
                        "type": "user"
                    },
                    "name": "Qiantong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:26:40.387Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753493",
                    "name": "Shaohui Liu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753494",
                    "name": "Shicheng Li",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753495",
                    "user": {
                        "_id": "642e72cec1b0f8e4e76af16d",
                        "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
                        "isPro": false,
                        "fullname": "shuhao gu",
                        "user": "gsh33",
                        "type": "user"
                    },
                    "name": "Shuhao Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:26:56.436Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753496",
                    "user": {
                        "_id": "60d2e681b8448e1785bbda06",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Shuhuai Ren",
                        "user": "ShuhuaiRen",
                        "type": "user"
                    },
                    "name": "Shuhuai Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:51.419Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753497",
                    "name": "Shuo Liu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753498",
                    "user": {
                        "_id": "671560869c2303f00087f648",
                        "avatarUrl": "/avatars/2e48cf091346bbe48a4ffe0eb211870c.svg",
                        "isPro": false,
                        "fullname": "Sirui Deng",
                        "user": "sirui1228",
                        "type": "user"
                    },
                    "name": "Sirui Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:26:12.959Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f753499",
                    "name": "Weiji Zhuang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75349a",
                    "name": "Weiwei Lv",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75349b",
                    "name": "Wenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75349c",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75349d",
                    "name": "Xing Yong",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75349e",
                    "name": "Xing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f75349f",
                    "name": "Xingchen Song",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a0",
                    "user": {
                        "_id": "67e3cd03df8f424f1159a134",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4F0LvQOERakQ_2Kvmed9u.png",
                        "isPro": false,
                        "fullname": "Xinzhe Xu",
                        "user": "muyun727",
                        "type": "user"
                    },
                    "name": "Xinzhe Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:26:28.273Z",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a1",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a2",
                    "name": "Yihan Yan",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a3",
                    "name": "Yu Tu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a4",
                    "name": "Yuanyuan Tian",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a5",
                    "name": "Yudong Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a6",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a7",
                    "name": "Zhenru Lin",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a8",
                    "name": "Zhichao Song",
                    "hidden": false
                },
                {
                    "_id": "6822b5441f775dd12f7534a9",
                    "user": {
                        "_id": "6517f0df593b3af3120d242e",
                        "avatarUrl": "/avatars/bc28ceb48b206502d8cf9e1c2e130066.svg",
                        "isPro": false,
                        "fullname": "Zihao Yue",
                        "user": "yuezih",
                        "type": "user"
                    },
                    "name": "Zihao Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:53.954Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T14:30:11.000Z",
            "submittedOnDailyAt": "2025-05-13T01:36:30.279Z",
            "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining",
            "submittedOnDailyBy": {
                "_id": "61d595f0bfc2372aaf42b766",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675437310060-61d595f0bfc2372aaf42b766.jpeg",
                "isPro": false,
                "fullname": "Yifan Song",
                "user": "Solaris99",
                "type": "user"
            },
            "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.",
            "upvotes": 53,
            "discussionId": "6822b5451f775dd12f7534e6",
            "githubRepo": "https://github.com/XiaomiMiMo/MiMo",
            "ai_keywords": [
                "large language model",
                "reasoning tasks",
                "data preprocessing pipeline",
                "three-stage data mixing strategy",
                "pre-training",
                "post-training",
                "Multi-Token Prediction objective",
                "verifiable mathematics and programming problems",
                "reinforcement learning",
                "test-difficulty-driven code-reward scheme",
                "strategic data resampling",
                "RL-tuned model"
            ]
        },
        "publishedAt": "2025-05-12T10:30:11.000Z",
        "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining",
        "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07608.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "61d595f0bfc2372aaf42b766",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675437310060-61d595f0bfc2372aaf42b766.jpeg",
            "fullname": "Yifan Song",
            "name": "Solaris99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07747",
            "authors": [
                {
                    "_id": "6822b668a307d0f1bf9bb4e0",
                    "name": "Weiyu Li",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e1",
                    "user": {
                        "_id": "6675854966c4fa6d0cee4d50",
                        "avatarUrl": "/avatars/aa6041a97985078e82cc89bfbade9828.svg",
                        "isPro": false,
                        "fullname": "xuanyang zhang",
                        "user": "xuanyangz",
                        "type": "user"
                    },
                    "name": "Xuanyang Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:28:16.581Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e2",
                    "name": "Zheng Sun",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e3",
                    "name": "Di Qi",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e4",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e5",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:44.705Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e6",
                    "name": "Weiwei Cai",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e7",
                    "user": {
                        "_id": "67dd1a0968dc64637481ec72",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hQvTErvt9-rIsMPxZFwq1.png",
                        "isPro": false,
                        "fullname": "shihao wu",
                        "user": "shian7",
                        "type": "user"
                    },
                    "name": "Shihao Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:28:49.833Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e8",
                    "name": "Jiarui Liu",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4e9",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4ea",
                    "name": "Xiao Chen",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4eb",
                    "user": {
                        "_id": "65fbe6c7ae01a24b40f6d0a5",
                        "avatarUrl": "/avatars/3f396e5623dcdbe11721744138245ee7.svg",
                        "isPro": false,
                        "fullname": "Fei-Peng Tian",
                        "user": "flybirdtian",
                        "type": "user"
                    },
                    "name": "Feipeng Tian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:29:33.047Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4ec",
                    "user": {
                        "_id": "64f7f70f8a4f9aef21879553",
                        "avatarUrl": "/avatars/3357142ca73e03457ff2a7d04143351c.svg",
                        "isPro": false,
                        "fullname": "PanJianxiong",
                        "user": "PanJianxiong",
                        "type": "user"
                    },
                    "name": "Jianxiong Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:29:44.675Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4ed",
                    "name": "Zeming Li",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4ee",
                    "user": {
                        "_id": "63417332c5565a4b8d43a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
                        "isPro": false,
                        "fullname": "Gang Yu",
                        "user": "skicy",
                        "type": "user"
                    },
                    "name": "Gang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T09:52:39.039Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4ef",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4f0",
                    "user": {
                        "_id": "60d4440fe648443279aaffd8",
                        "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
                        "isPro": false,
                        "fullname": "Daxin Jiang",
                        "user": "djiang",
                        "type": "user"
                    },
                    "name": "Daxin Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:30:25.055Z",
                    "hidden": false
                },
                {
                    "_id": "6822b668a307d0f1bf9bb4f1",
                    "name": "Ping Tan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6675854966c4fa6d0cee4d50/hd94r_Cvaejm26QVh2iaM.png"
            ],
            "publishedAt": "2025-05-12T16:56:30.000Z",
            "submittedOnDailyAt": "2025-05-13T02:07:41.773Z",
            "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
            "submittedOnDailyBy": {
                "_id": "6675854966c4fa6d0cee4d50",
                "avatarUrl": "/avatars/aa6041a97985078e82cc89bfbade9828.svg",
                "isPro": false,
                "fullname": "xuanyang zhang",
                "user": "xuanyangz",
                "type": "user"
            },
            "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.",
            "upvotes": 47,
            "discussionId": "6822b66ea307d0f1bf9bb688",
            "githubRepo": "https://github.com/stepfun-ai/Step1X-3D",
            "ai_keywords": [
                "VAE",
                "DiT",
                "diffusion-based",
                "TSDF",
                "perceiver-based",
                "latent encoding",
                "edge sampling",
                "cross-view consistency",
                "geometric conditioning",
                "latent-space synchronization",
                "LoRA"
            ]
        },
        "publishedAt": "2025-05-12T12:56:30.000Z",
        "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
        "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6675854966c4fa6d0cee4d50/hd94r_Cvaejm26QVh2iaM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07747.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6675854966c4fa6d0cee4d50",
            "avatarUrl": "/avatars/aa6041a97985078e82cc89bfbade9828.svg",
            "fullname": "xuanyang zhang",
            "name": "xuanyangz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07787",
            "authors": [
                {
                    "_id": "6822a9b0e6328d29a9c721d6",
                    "user": {
                        "_id": "6421b07e918f0fd889f0a682",
                        "avatarUrl": "/avatars/314b55c2428426c846d9449f98db4355.svg",
                        "isPro": false,
                        "fullname": "Tongxu Luo",
                        "user": "Zeno-Luo",
                        "type": "user"
                    },
                    "name": "Tongxu Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:19.861Z",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721d7",
                    "user": {
                        "_id": "624c3d2ca19f20b197761ba9",
                        "avatarUrl": "/avatars/7a64b81c29f4f6700fa18effc5616865.svg",
                        "isPro": false,
                        "fullname": "Wenyu Du",
                        "user": "wydu",
                        "type": "user"
                    },
                    "name": "Wenyu Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:30:44.940Z",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721d8",
                    "user": {
                        "_id": "6819c4b3ba781b9ebb08cf48",
                        "avatarUrl": "/avatars/6a51be63df7469d34a2f5c5229dac88d.svg",
                        "isPro": false,
                        "fullname": "Jiaxi Bi",
                        "user": "Jiaxi0775",
                        "type": "user"
                    },
                    "name": "Jiaxi Bi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:14.622Z",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721d9",
                    "name": "Stephen Chung",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721da",
                    "user": {
                        "_id": "64912976b95c3f0a1e6233cb",
                        "avatarUrl": "/avatars/c0615f8c6606073faffb419757d4e667.svg",
                        "isPro": false,
                        "fullname": "Zhengyang Tang",
                        "user": "tangzhy",
                        "type": "user"
                    },
                    "name": "Zhengyang Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:31:01.937Z",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721db",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721dc",
                    "name": "Min Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822a9b0e6328d29a9c721dd",
                    "user": {
                        "_id": "637c6703ca8542a0ba900ccb",
                        "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Benyou",
                        "type": "user"
                    },
                    "name": "Benyou Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:31:21.723Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:39:56.000Z",
            "submittedOnDailyAt": "2025-05-13T00:46:25.483Z",
            "title": "Learning from Peers in Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "6421b07e918f0fd889f0a682",
                "avatarUrl": "/avatars/314b55c2428426c846d9449f98db4355.svg",
                "isPro": false,
                "fullname": "Tongxu Luo",
                "user": "Zeno-Luo",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .",
            "upvotes": 34,
            "discussionId": "6822a9b1e6328d29a9c7222c",
            "projectPage": "https://learning-from-peers.github.io/",
            "githubRepo": "https://github.com/tongxuluo/LeaP",
            "ai_keywords": [
                "Learning from Peers (LeaP)",
                "LeaP-T",
                "Prefix Dominance Trap",
                "routing mechanism",
                "summarization",
                "reflection",
                "DeepSeek-R1-671B",
                "DeepSeek-R1-Distill-Qwen-14B",
                "error correction",
                "error tolerance"
            ]
        },
        "publishedAt": "2025-05-12T13:39:56.000Z",
        "title": "Learning from Peers in Reasoning Models",
        "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07787.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6421b07e918f0fd889f0a682",
            "avatarUrl": "/avatars/314b55c2428426c846d9449f98db4355.svg",
            "fullname": "Tongxu Luo",
            "name": "Zeno-Luo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07447",
            "authors": [
                {
                    "_id": "6822c7d78fc623ca8c6b2578",
                    "user": {
                        "_id": "6305a4cf5054a1c5b82ae8f7",
                        "avatarUrl": "/avatars/7138103d5b86d8b0b6cdd71e1dd4f1eb.svg",
                        "isPro": false,
                        "fullname": "Black Box",
                        "user": "sp12138sp",
                        "type": "user"
                    },
                    "name": "Peng Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:37.679Z",
                    "hidden": false
                },
                {
                    "_id": "6822c7d78fc623ca8c6b2579",
                    "name": "Yi Jiang",
                    "hidden": false
                },
                {
                    "_id": "6822c7d78fc623ca8c6b257a",
                    "user": {
                        "_id": "65296b8a6cdea40585bd81e2",
                        "avatarUrl": "/avatars/824452e8b8fd056eaf7549c46393c47b.svg",
                        "isPro": false,
                        "fullname": "Tao LIN",
                        "user": "tlin-taolin",
                        "type": "user"
                    },
                    "name": "Tao Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:57:54.259Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T11:15:39.000Z",
            "submittedOnDailyAt": "2025-05-13T03:12:37.227Z",
            "title": "Unified Continuous Generative Models",
            "submittedOnDailyBy": {
                "_id": "65028e8389707f182386588c",
                "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg",
                "isPro": false,
                "fullname": "Zhenglin Cheng",
                "user": "kenshinn",
                "type": "user"
            },
            "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.",
            "upvotes": 32,
            "discussionId": "6822c7d98fc623ca8c6b260c",
            "githubRepo": "https://github.com/LINs-lab/UCGM",
            "ai_keywords": [
                "diffusion",
                "flow-matching",
                "consistency models",
                "generative performance",
                "unified framework",
                "training",
                "sampling",
                "analysis",
                "Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S})",
                "state-of-the-art (SOTA)",
                "ImageNet 256x256",
                "diffusion transformer",
                "FID (Frchet Inception Distance)"
            ]
        },
        "publishedAt": "2025-05-12T07:15:39.000Z",
        "title": "Unified Continuous Generative Models",
        "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07447.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65028e8389707f182386588c",
            "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg",
            "fullname": "Zhenglin Cheng",
            "name": "kenshinn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06548",
            "authors": [
                {
                    "_id": "6822a0f7f90521b5d8cb4573",
                    "name": "Aniruddha Roy",
                    "hidden": false
                },
                {
                    "_id": "6822a0f7f90521b5d8cb4574",
                    "user": {
                        "_id": "62d3ac975a869f4338852b6f",
                        "avatarUrl": "/avatars/4094eb1ea1244b0a9b60344528efa25a.svg",
                        "isPro": false,
                        "fullname": "Pretam Ray",
                        "user": "Pretam",
                        "type": "user"
                    },
                    "name": "Pretam Ray",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:32:17.365Z",
                    "hidden": false
                },
                {
                    "_id": "6822a0f7f90521b5d8cb4575",
                    "user": {
                        "_id": "5f89da6c5d083370c711f37c",
                        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                        "isPro": false,
                        "fullname": "Abhilash Nandy",
                        "user": "abhi1nandy2",
                        "type": "user"
                    },
                    "name": "Abhilash Nandy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:32:23.396Z",
                    "hidden": false
                },
                {
                    "_id": "6822a0f7f90521b5d8cb4576",
                    "name": "Somak Aditya",
                    "hidden": false
                },
                {
                    "_id": "6822a0f7f90521b5d8cb4577",
                    "name": "Pawan Goyal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-10T07:23:19.000Z",
            "submittedOnDailyAt": "2025-05-13T00:03:01.117Z",
            "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback",
            "submittedOnDailyBy": {
                "_id": "5f89da6c5d083370c711f37c",
                "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                "isPro": false,
                "fullname": "Abhilash Nandy",
                "user": "abhi1nandy2",
                "type": "user"
            },
            "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.",
            "upvotes": 26,
            "discussionId": "6822a0f8f90521b5d8cb45c1",
            "ai_keywords": [
                "small LLMs",
                "LLaMA 2-7B",
                "LLaMA 2-13B",
                "Mistral 7B",
                "semi-automated framework",
                "Reinforcement Learning (RL) based training algorithm"
            ]
        },
        "publishedAt": "2025-05-10T03:23:19.000Z",
        "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback",
        "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06548.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "fullname": "Abhilash Nandy",
            "name": "abhi1nandy2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07818",
            "authors": [
                {
                    "_id": "6822b53e731ca42bb4ba7d2f",
                    "user": {
                        "_id": "63721f5ada3183d9d53cfe1f",
                        "avatarUrl": "/avatars/593c14c907848da7dbc9e5418751bd94.svg",
                        "isPro": false,
                        "fullname": "Xue Zeyue",
                        "user": "xzyhku",
                        "type": "user"
                    },
                    "name": "Zeyue Xue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:33:44.632Z",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d30",
                    "user": {
                        "_id": "6381c5d63680a7cf34e08ca9",
                        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                        "isPro": false,
                        "fullname": "wujie10558@gmail.com",
                        "user": "wujie10",
                        "type": "user"
                    },
                    "name": "Jie Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:12:02.685Z",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d31",
                    "name": "Yu Gao",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d32",
                    "name": "Fangyuan Kong",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d33",
                    "user": {
                        "_id": "6423f5e6774cc34079730f31",
                        "avatarUrl": "/avatars/7ebeb1f623c86b1a676c95bb67572f8b.svg",
                        "isPro": false,
                        "fullname": "Lingting Zhu",
                        "user": "ltzhu",
                        "type": "user"
                    },
                    "name": "Lingting Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:34:00.874Z",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d34",
                    "user": {
                        "_id": "64aea082704210bf815e7551",
                        "avatarUrl": "/avatars/5c8dc0df57596c526b2bccea21835f53.svg",
                        "isPro": false,
                        "fullname": "Mengzhao Chen",
                        "user": "ChenMnZ",
                        "type": "user"
                    },
                    "name": "Mengzhao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:34:06.563Z",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d35",
                    "name": "Zhiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d36",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d37",
                    "name": "Qiushan Guo",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d38",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "6822b53e731ca42bb4ba7d39",
                    "user": {
                        "_id": "67cb7d55560c3dcbb1adeaa3",
                        "avatarUrl": "/avatars/0b616d3655b0b54a621c2608b2f14379.svg",
                        "isPro": false,
                        "fullname": "Ping Luo",
                        "user": "appleluo",
                        "type": "user"
                    },
                    "name": "Ping Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:35:40.333Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:59:34.000Z",
            "submittedOnDailyAt": "2025-05-13T01:28:55.906Z",
            "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
            "submittedOnDailyBy": {
                "_id": "6381c5d63680a7cf34e08ca9",
                "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                "isPro": false,
                "fullname": "wujie10558@gmail.com",
                "user": "wujie10",
                "type": "user"
            },
            "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.",
            "upvotes": 17,
            "discussionId": "6822b541731ca42bb4ba7e19",
            "projectPage": "https://dancegrpo.github.io/",
            "githubRepo": "https://github.com/XueZeyue/DanceGRPO",
            "ai_keywords": [
                "diffusion models",
                "rectified flows",
                "reinforcement learning (RL)",
                "Ordinary Differential Equations (ODEs)",
                "Group Relative Policy Optimization (GRPO)",
                "text-to-image",
                "text-to-video",
                "image-to-video",
                "Stable Diffusion",
                "HunyuanVideo",
                "FLUX",
                "SkyReel-I2V",
                "image/video aesthetics",
                "text-image alignment",
                "video motion quality",
                "binary reward",
                "HPS-v2.1",
                "CLIP Score",
                "VideoAlign",
                "GenEval",
                "Best-of-N",
                "policy optimization",
                "denoising trajectories",
                "sparse binary feedback",
                "Reinforcement Learning from Human Feedback (RLHF)"
            ]
        },
        "publishedAt": "2025-05-12T13:59:34.000Z",
        "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
        "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07818.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "fullname": "wujie10558@gmail.com",
            "name": "wujie10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07293",
            "authors": [
                {
                    "_id": "6822d1f9c9cb5b62cd606495",
                    "name": "Kai Hua",
                    "hidden": false
                },
                {
                    "_id": "6822d1f9c9cb5b62cd606496",
                    "name": "Steven Wu",
                    "hidden": false
                },
                {
                    "_id": "6822d1f9c9cb5b62cd606497",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:29.702Z",
                    "hidden": false
                },
                {
                    "_id": "6822d1f9c9cb5b62cd606498",
                    "user": {
                        "_id": "645604eebabbbbd3486dc615",
                        "avatarUrl": "/avatars/17a5ca8274e2bfc8f183a4af9878a930.svg",
                        "isPro": false,
                        "fullname": "shenke",
                        "user": "shenke18",
                        "type": "user"
                    },
                    "name": "Ke Shen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-13T05:00:43.145Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T07:25:51.000Z",
            "submittedOnDailyAt": "2025-05-13T03:31:43.177Z",
            "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection.",
            "upvotes": 17,
            "discussionId": "6822d1fbc9cb5b62cd6064f4",
            "ai_keywords": [
                "AttentionInfluence",
                "attention heads",
                "in-context reasoning",
                "attention head masking",
                "retrieval heads",
                "SmolLM corpus",
                "WSD learning rate scheduling",
                "MMLU",
                "MMLU-Pro",
                "AGIEval-en",
                "GSM8K",
                "HumanEval",
                "weak-to-strong scaling"
            ]
        },
        "publishedAt": "2025-05-12T03:25:51.000Z",
        "title": "AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection",
        "summary": "Recently, there has been growing interest in collecting reasoning-intensive\npretraining data to improve LLMs' complex reasoning ability. Prior approaches\ntypically rely on supervised classifiers to identify such data, which requires\nlabeling by humans or LLMs, often introducing domain-specific biases. Due to\nthe attention heads being crucial to in-context reasoning, we propose\nAttentionInfluence, a simple yet effective, training-free method without\nsupervision signal. Our approach enables a small pretrained language model to\nact as a strong data selector through a simple attention head masking\noperation. Specifically, we identify retrieval heads and compute the loss\ndifference when masking these heads. We apply AttentionInfluence to a\n1.3B-parameter dense model to conduct data selection on the SmolLM corpus of\n241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B\ntokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD\nlearning rate scheduling. Our experimental results demonstrate substantial\nimprovements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive\nand reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and\nHumanEval). This demonstrates an effective weak-to-strong scaling property,\nwith small models improving the final performance of larger models-offering a\npromising and scalable path for reasoning-centric data selection.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07293.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 47
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07263",
            "authors": [
                {
                    "_id": "6822cf4c3b7392637edd756a",
                    "user": {
                        "_id": "62be9b5aae56e75e4d689e7c",
                        "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
                        "isPro": false,
                        "fullname": "wangxiaokun",
                        "user": "shawn0wang",
                        "type": "user"
                    },
                    "name": "Xiaokun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T10:27:21.212Z",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd756b",
                    "user": {
                        "_id": "620f5a1c3f76c50e6458a9b6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
                        "isPro": false,
                        "fullname": "chris",
                        "user": "OrlandoHugBot",
                        "type": "user"
                    },
                    "name": "Chris",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T10:27:19.259Z",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd756c",
                    "name": "Jiangbo Pei",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd756d",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd756e",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd756f",
                    "name": "Yunzhuo Hao",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd7570",
                    "name": "Weijie Qiu",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd7571",
                    "name": "Ai Jian",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd7572",
                    "name": "Tianyidan Xie",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd7573",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd7574",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6822cf4c3b7392637edd7575",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T06:23:08.000Z",
            "submittedOnDailyAt": "2025-05-13T08:34:42.870Z",
            "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning",
            "submittedOnDailyBy": {
                "_id": "62be9b5aae56e75e4d689e7c",
                "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
                "isPro": false,
                "fullname": "wangxiaokun",
                "user": "shawn0wang",
                "type": "user"
            },
            "summary": "We propose Skywork-VL Reward, a multimodal reward model that provides reward\nsignals for both multimodal understanding and reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based on\nQwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage\nfine-tuning using pairwise ranking loss on pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults on multimodal VL-RewardBench and exhibits competitive performance on\nthe text-only RewardBench benchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements in\nmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models for\nmultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility.",
            "upvotes": 15,
            "discussionId": "6822cf4d3b7392637edd75a5",
            "ai_keywords": [
                "multimodal reward model",
                "multimodal preference dataset",
                "vision-language models (VLMs)",
                "VLM reasoners",
                "reward model architecture",
                "Qwen2.5-VL-7B-Instruct",
                "reward head",
                "multi-stage fine-tuning",
                "pairwise ranking loss",
                "pairwise preference data",
                "multimodal VL-RewardBench",
                "text-only RewardBench benchmark",
                "Mixed Preference Optimization (MPO)",
                "multimodal reasoning capabilities",
                "general-purpose, reliable reward models",
                "multimodal alignment"
            ]
        },
        "publishedAt": "2025-05-12T02:23:08.000Z",
        "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning",
        "summary": "We propose Skywork-VL Reward, a multimodal reward model that provides reward\nsignals for both multimodal understanding and reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based on\nQwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage\nfine-tuning using pairwise ranking loss on pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults on multimodal VL-RewardBench and exhibits competitive performance on\nthe text-only RewardBench benchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements in\nmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models for\nmultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07263.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62be9b5aae56e75e4d689e7c",
            "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
            "fullname": "wangxiaokun",
            "name": "shawn0wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.03733",
            "authors": [
                {
                    "_id": "681cb3dc815cbdd72910e310",
                    "user": {
                        "_id": "64b0bfef2f2f9c345b87e673",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
                        "isPro": false,
                        "fullname": "Zimu Lu",
                        "user": "luzimu",
                        "type": "user"
                    },
                    "name": "Zimu Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:13:03.384Z",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e311",
                    "user": {
                        "_id": "66965c927e75dfb108c269b4",
                        "avatarUrl": "/avatars/3263fb0742a3d35fa19d6856d59701ee.svg",
                        "isPro": false,
                        "fullname": "Yunqiao Yang",
                        "user": "Yqy6",
                        "type": "user"
                    },
                    "name": "Yunqiao Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:36:05.709Z",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e312",
                    "user": {
                        "_id": "63e4b63dd6278c161be47f4b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e4b63dd6278c161be47f4b/u0y1_o9WBqPJh8C8dimJ8.jpeg",
                        "isPro": false,
                        "fullname": "Houxing Ren",
                        "user": "Houxing",
                        "type": "user"
                    },
                    "name": "Houxing Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:36:11.710Z",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e313",
                    "user": {
                        "_id": "66cd8abad5026ff32afa0b95",
                        "avatarUrl": "/avatars/f0d0ad73d0f64a27f2aefe0de2608c44.svg",
                        "isPro": false,
                        "fullname": "Hou Haotian",
                        "user": "hht1113",
                        "type": "user"
                    },
                    "name": "Haotian Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:36:22.510Z",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e314",
                    "name": "Han Xiao",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e315",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e316",
                    "user": {
                        "_id": "6337e1d1107c4835a04c8607",
                        "avatarUrl": "/avatars/e85353b34c3da6efef84951934811fee.svg",
                        "isPro": false,
                        "fullname": "Weikang Shi",
                        "user": "shiwk20",
                        "type": "user"
                    },
                    "name": "Weikang Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:36:43.939Z",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e317",
                    "name": "Aojun Zhou",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e318",
                    "user": {
                        "_id": "6301e46858f2b86bdfd49933",
                        "avatarUrl": "/avatars/d1c3fd94581f22a41506b24375a22bd3.svg",
                        "isPro": false,
                        "fullname": "ZhanMingjie",
                        "user": "Mingjie",
                        "type": "user"
                    },
                    "name": "Mingjie Zhan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:37:12.420Z",
                    "hidden": false
                },
                {
                    "_id": "681cb3dc815cbdd72910e319",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:37:20.031Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-06T17:59:15.000Z",
            "submittedOnDailyAt": "2025-05-13T00:00:09.592Z",
            "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
            "submittedOnDailyBy": {
                "_id": "64b0bfef2f2f9c345b87e673",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
                "isPro": false,
                "fullname": "Zimu Lu",
                "user": "luzimu",
                "type": "user"
            },
            "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
            "upvotes": 15,
            "discussionId": "681cb3de815cbdd72910e38b",
            "githubRepo": "https://github.com/mnluzimu/WebGen-Bench",
            "ai_keywords": [
                "LLM-based agents",
                "WebGen-Bench",
                "multi-file website codebases",
                "GPT-4o",
                "web applications",
                "test cases",
                "web-navigation agent",
                "Bolt.diy",
                "OpenHands",
                "Aider",
                "DeepSeek-R1",
                "WebGen-Instruct",
                "Qwen2.5-Coder-32B-Instruct"
            ]
        },
        "publishedAt": "2025-05-06T13:59:15.000Z",
        "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
        "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03733.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b0bfef2f2f9c345b87e673",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
            "fullname": "Zimu Lu",
            "name": "luzimu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07796",
            "authors": [
                {
                    "_id": "6822b079c10ac9c466c546e3",
                    "name": "Xingjin Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b079c10ac9c466c546e4",
                    "user": {
                        "_id": "6718fc605e14ff6b94a7109f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
                        "isPro": false,
                        "fullname": "Howe Tissue",
                        "user": "Howe77",
                        "type": "user"
                    },
                    "name": "Howe Tissue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:37:40.091Z",
                    "hidden": false
                },
                {
                    "_id": "6822b079c10ac9c466c546e5",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "6822b079c10ac9c466c546e6",
                    "user": {
                        "_id": "668268ace365c0f6660b67d3",
                        "avatarUrl": "/avatars/3b179b65a09dc8896b993d2013baeafb.svg",
                        "isPro": false,
                        "fullname": "ll",
                        "user": "linjinglian",
                        "type": "user"
                    },
                    "name": "Linjing Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:37:54.756Z",
                    "hidden": false
                },
                {
                    "_id": "6822b079c10ac9c466c546e7",
                    "name": "Daniel Dajun Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:47:32.000Z",
            "submittedOnDailyAt": "2025-05-13T01:10:50.906Z",
            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6718fc605e14ff6b94a7109f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
                "isPro": false,
                "fullname": "Howe Tissue",
                "user": "Howe77",
                "type": "user"
            },
            "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters.",
            "upvotes": 13,
            "discussionId": "6822b07ac10ac9c466c5470f",
            "ai_keywords": [
                "Continual Pre-Training (CPT)",
                "large language models",
                "distribution shift",
                "learning rate annealing",
                "CPT scaling law",
                "loss potential",
                "peak learning rate",
                "training steps",
                "replay ratio"
            ]
        },
        "publishedAt": "2025-05-12T13:47:32.000Z",
        "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
        "summary": "Continual Pre-Training (CPT) has become a popular and effective method to\napply strong foundation models to specific downstream tasks. In this work, we\nexplore the learning dynamics throughout the CPT process for large language\nmodels. We specifically focus on how general and downstream domain performance\nevolves at each training step, with domain performance measured via validation\nlosses. We have observed that the CPT loss curve fundamentally characterizes\nthe transition from one curve to another hidden curve, and could be described\nby decoupling the effects of distribution shift and learning rate annealing. We\nderive a CPT scaling law that combines the two factors, enabling the prediction\nof loss at any (continual) training steps and across learning rate schedules\n(LRS) in CPT. Our formulation presents a comprehensive understanding of several\ncritical factors in CPT, including loss potential, peak learning rate, training\nsteps, replay ratio, etc. Moreover, our approach can be adapted to customize\ntraining hyper-parameters to different CPT goals such as balancing general and\ndomain-specific performance. Extensive experiments demonstrate that our scaling\nlaw holds across various CPT datasets and training hyper-parameters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07796.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "fullname": "Howe Tissue",
            "name": "Howe77",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07596",
            "authors": [
                {
                    "_id": "6822ba9e4867f1e9dc14c294",
                    "user": {
                        "_id": "616648c84c0937d31946f21b",
                        "avatarUrl": "/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg",
                        "isPro": false,
                        "fullname": "Ziyang",
                        "user": "hzy",
                        "type": "user"
                    },
                    "name": "Ziyang Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:42.410Z",
                    "hidden": false
                },
                {
                    "_id": "6822ba9e4867f1e9dc14c295",
                    "name": "Xiaowei Yuan",
                    "hidden": false
                },
                {
                    "_id": "6822ba9e4867f1e9dc14c296",
                    "name": "Yiming Ju",
                    "hidden": false
                },
                {
                    "_id": "6822ba9e4867f1e9dc14c297",
                    "name": "Jun Zhao",
                    "hidden": false
                },
                {
                    "_id": "6822ba9e4867f1e9dc14c298",
                    "name": "Kang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T14:21:57.000Z",
            "submittedOnDailyAt": "2025-05-13T01:51:27.880Z",
            "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent",
            "submittedOnDailyBy": {
                "_id": "616648c84c0937d31946f21b",
                "avatarUrl": "/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg",
                "isPro": false,
                "fullname": "Ziyang",
                "user": "hzy",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.",
            "upvotes": 10,
            "discussionId": "6822ba9f4867f1e9dc14c2b9",
            "githubRepo": "https://github.com/hzy312/knowledge-r1",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "Large Language Models (LLMs)",
                "reinforcement learning (RL)",
                "search agent",
                "retrieval capabilities",
                "internal knowledge",
                "external knowledge",
                "redundant retrievals",
                "knowledge conflicts",
                "inference latency",
                "Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA)",
                "knowledge boundary",
                "knowledge-boundary aware reward function",
                "knowledge-boundary aware training dataset",
                "internal-external knowledge synergy",
                "knowledge reasoning tasks",
                "generalization capabilities"
            ]
        },
        "publishedAt": "2025-05-12T10:21:57.000Z",
        "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent",
        "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07596.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "616648c84c0937d31946f21b",
            "avatarUrl": "/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg",
            "fullname": "Ziyang",
            "name": "hzy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06176",
            "authors": [
                {
                    "_id": "68233e2a952abfa2afb65c6d",
                    "name": "Niladri Shekhar Dutt",
                    "hidden": false
                },
                {
                    "_id": "68233e2a952abfa2afb65c6e",
                    "user": {
                        "_id": "632322ca2c9469d56c27e1a3",
                        "avatarUrl": "/avatars/55b12a72bf67a401bcc4fd4b4fafef80.svg",
                        "isPro": false,
                        "fullname": "Duygu Ceylan",
                        "user": "duyguceylan",
                        "type": "user"
                    },
                    "name": "Duygu Ceylan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-13T12:42:24.105Z",
                    "hidden": false
                },
                {
                    "_id": "68233e2a952abfa2afb65c6f",
                    "name": "Niloy J. Mitra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-09T16:38:27.000Z",
            "submittedOnDailyAt": "2025-05-13T11:12:43.380Z",
            "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills",
            "submittedOnDailyBy": {
                "_id": "65b3e258d5bf0d622a89701b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b3e258d5bf0d622a89701b/Bv2Fv8Hyc6w5tvNCPTg2_.jpeg",
                "isPro": false,
                "fullname": "Niladri Shekhar Dutt",
                "user": "niladridutt",
                "type": "user"
            },
            "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.",
            "upvotes": 7,
            "discussionId": "68233e30952abfa2afb65e16",
            "projectPage": "https://monetgpt.github.io/",
            "githubRepo": "https://github.com/niladridutt/monetGPT",
            "ai_keywords": [
                "multimodal large language model (MLLM)",
                "generative editing",
                "procedural edits",
                "image processing operations",
                "visual puzzles",
                "reasoning dataset",
                "expert-edited photos",
                "explainability",
                "identity preservation"
            ]
        },
        "publishedAt": "2025-05-09T12:38:27.000Z",
        "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills",
        "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06176.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b3e258d5bf0d622a89701b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b3e258d5bf0d622a89701b/Bv2Fv8Hyc6w5tvNCPTg2_.jpeg",
            "fullname": "Niladri Shekhar Dutt",
            "name": "niladridutt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07819",
            "authors": [
                {
                    "_id": "6822e15ef544b9a0196b855d",
                    "user": {
                        "_id": "661678b244425d16e37f2341",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661678b244425d16e37f2341/VUSQ3S0IXZLoIxO4oX4cv.png",
                        "isPro": false,
                        "fullname": "Yiyang Lu",
                        "user": "Lyy0725",
                        "type": "user"
                    },
                    "name": "Yiyang Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:25.750Z",
                    "hidden": false
                },
                {
                    "_id": "6822e15ef544b9a0196b855e",
                    "name": "Yufeng Tian",
                    "hidden": false
                },
                {
                    "_id": "6822e15ef544b9a0196b855f",
                    "name": "Zhecheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "6822e15ef544b9a0196b8560",
                    "user": {
                        "_id": "6738670f5deec72e829e99e0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/LM9pPOgHXg2I7nn2A632t.png",
                        "isPro": false,
                        "fullname": "Xianbang Wang",
                        "user": "binaryXwizard",
                        "type": "user"
                    },
                    "name": "Xianbang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:40:08.476Z",
                    "hidden": false
                },
                {
                    "_id": "6822e15ef544b9a0196b8561",
                    "name": "Pu Hua",
                    "hidden": false
                },
                {
                    "_id": "6822e15ef544b9a0196b8562",
                    "name": "Zhengrong Xue",
                    "hidden": false
                },
                {
                    "_id": "6822e15ef544b9a0196b8563",
                    "name": "Huazhe Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:59:43.000Z",
            "submittedOnDailyAt": "2025-05-13T07:15:33.900Z",
            "title": "H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning",
            "submittedOnDailyBy": {
                "_id": "661678b244425d16e37f2341",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661678b244425d16e37f2341/VUSQ3S0IXZLoIxO4oX4cv.png",
                "isPro": false,
                "fullname": "Yiyang Lu",
                "user": "Lyy0725",
                "type": "user"
            },
            "summary": "Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce Triply-Hierarchical Diffusion\nPolicy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H^{3}DP contains\n3 levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H^{3}DP yields a +27.5%\naverage relative improvement over baselines across 44 simulation\ntasks and achieves superior performance in 4 challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.",
            "upvotes": 5,
            "discussionId": "6822e15ff544b9a0196b85ab",
            "projectPage": "https://lyy-iiis.github.io/h3dp/",
            "ai_keywords": [
                "Triply-Hierarchical Diffusion Policy",
                "H$^{3}$DP",
                "visuomotor policy learning",
                "generative models",
                "action distribution",
                "visual perception",
                "action prediction",
                "depth-aware input layering",
                "RGB-D observations",
                "multi-scale visual representations",
                "semantic features",
                "hierarchically conditioned diffusion process",
                "coarse-to-fine actions",
                "visual features"
            ]
        },
        "publishedAt": "2025-05-12T13:59:43.000Z",
        "title": "H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning",
        "summary": "Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce Triply-Hierarchical Diffusion\nPolicy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H^{3}DP contains\n3 levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H^{3}DP yields a +27.5%\naverage relative improvement over baselines across 44 simulation\ntasks and achieves superior performance in 4 challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07819.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "661678b244425d16e37f2341",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661678b244425d16e37f2341/VUSQ3S0IXZLoIxO4oX4cv.png",
            "fullname": "Yiyang Lu",
            "name": "Lyy0725",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07260",
            "authors": [
                {
                    "_id": "682307ad692919b8cc2a7912",
                    "user": {
                        "_id": "621731ee29500f41901123d5",
                        "avatarUrl": "/avatars/ec413c33eb77b67dba35e03c4857f222.svg",
                        "isPro": false,
                        "fullname": "YUANHANG YANG",
                        "user": "ysngkil",
                        "type": "user"
                    },
                    "name": "Yuanhang Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:57:51.750Z",
                    "hidden": false
                },
                {
                    "_id": "682307ad692919b8cc2a7913",
                    "name": "Chaozheng Wang",
                    "hidden": false
                },
                {
                    "_id": "682307ad692919b8cc2a7914",
                    "name": "Jing Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T06:21:44.000Z",
            "submittedOnDailyAt": "2025-05-13T07:21:49.753Z",
            "title": "UMoE: Unifying Attention and FFN with Shared Experts",
            "submittedOnDailyBy": {
                "_id": "621731ee29500f41901123d5",
                "avatarUrl": "/avatars/ec413c33eb77b67dba35e03c4857f222.svg",
                "isPro": false,
                "fullname": "YUANHANG YANG",
                "user": "ysngkil",
                "type": "user"
            },
            "summary": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising\napproach for scaling Transformer models. While initial works primarily\nincorporated MoE into feed-forward network (FFN) layers, recent studies have\nexplored extending the MoE paradigm to attention layers to enhance model\nperformance. However, existing attention-based MoE layers require specialized\nimplementations and demonstrate suboptimal performance compared to their\nFFN-based counterparts. In this paper, we aim to unify the MoE designs in\nattention and FFN layers by introducing a novel reformulation of the attention\nmechanism, revealing an underlying FFN-like structure within attention modules.\nOur proposed architecture, UMoE, achieves superior performance through\nattention-based MoE layers while enabling efficient parameter sharing between\nFFN and attention components.",
            "upvotes": 5,
            "discussionId": "682307ad692919b8cc2a7977",
            "ai_keywords": [
                "Sparse Mixture of Experts (MoE) architectures",
                "Transformer models",
                "feed-forward network (FFN) layers",
                "attention layers",
                "attention-based MoE layers",
                "specialized implementations",
                "underlying FFN-like structure",
                "attention modules",
                "UMoE architecture",
                "efficient parameter sharing"
            ]
        },
        "publishedAt": "2025-05-12T02:21:44.000Z",
        "title": "UMoE: Unifying Attention and FFN with Shared Experts",
        "summary": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising\napproach for scaling Transformer models. While initial works primarily\nincorporated MoE into feed-forward network (FFN) layers, recent studies have\nexplored extending the MoE paradigm to attention layers to enhance model\nperformance. However, existing attention-based MoE layers require specialized\nimplementations and demonstrate suboptimal performance compared to their\nFFN-based counterparts. In this paper, we aim to unify the MoE designs in\nattention and FFN layers by introducing a novel reformulation of the attention\nmechanism, revealing an underlying FFN-like structure within attention modules.\nOur proposed architecture, UMoE, achieves superior performance through\nattention-based MoE layers while enabling efficient parameter sharing between\nFFN and attention components.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07260.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "621731ee29500f41901123d5",
            "avatarUrl": "/avatars/ec413c33eb77b67dba35e03c4857f222.svg",
            "fullname": "YUANHANG YANG",
            "name": "ysngkil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.00612",
            "authors": [
                {
                    "_id": "68233e50c28353126f22bf74",
                    "name": "D. Sculley",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf75",
                    "name": "Will Cukierski",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf76",
                    "name": "Phil Culliton",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf77",
                    "name": "Sohier Dane",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf78",
                    "name": "Maggie Demkin",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf79",
                    "name": "Ryan Holbrook",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf7a",
                    "name": "Addison Howard",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf7b",
                    "name": "Paul Mooney",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf7c",
                    "name": "Walter Reade",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf7d",
                    "user": {
                        "_id": "626b23fc822e3d85324babed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b23fc822e3d85324babed/CbRuisnyy5JsCeA5uxYL7.jpeg",
                        "isPro": false,
                        "fullname": "Meg Risdal",
                        "user": "megrisdal",
                        "type": "user"
                    },
                    "name": "Megan Risdal",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-13T13:17:20.366Z",
                    "hidden": false
                },
                {
                    "_id": "68233e50c28353126f22bf7e",
                    "name": "Nate Keating",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-01T15:43:51.000Z",
            "submittedOnDailyAt": "2025-05-13T11:17:53.772Z",
            "title": "Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation",
            "submittedOnDailyBy": {
                "_id": "626b23fc822e3d85324babed",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b23fc822e3d85324babed/CbRuisnyy5JsCeA5uxYL7.jpeg",
                "isPro": false,
                "fullname": "Meg Risdal",
                "user": "megrisdal",
                "type": "user"
            },
            "summary": "In this position paper, we observe that empirical evaluation in Generative AI\nis at a crisis point since traditional ML evaluation and benchmarking\nstrategies are insufficient to meet the needs of evaluating modern GenAI models\nand systems. There are many reasons for this, including the fact that these\nmodels typically have nearly unbounded input and output spaces, typically do\nnot have a well defined ground truth target, and typically exhibit strong\nfeedback loops and prediction dependence based on context of previous model\noutputs. On top of these critical issues, we argue that the problems of {\\em\nleakage} and {\\em contamination} are in fact the most important and difficult\nissues to address for GenAI evaluations. Interestingly, the field of AI\nCompetitions has developed effective measures and practices to combat leakage\nfor the purpose of counteracting cheating by bad actors within a competition\nsetting. This makes AI Competitions an especially valuable (but underutilized)\nresource. Now is time for the field to view AI Competitions as the gold\nstandard for empirical rigor in GenAI evaluation, and to harness and harvest\ntheir results with according value.",
            "upvotes": 5,
            "discussionId": "68233e50c28353126f22bfed"
        },
        "publishedAt": "2025-05-01T11:43:51.000Z",
        "title": "Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation",
        "summary": "In this position paper, we observe that empirical evaluation in Generative AI\nis at a crisis point since traditional ML evaluation and benchmarking\nstrategies are insufficient to meet the needs of evaluating modern GenAI models\nand systems. There are many reasons for this, including the fact that these\nmodels typically have nearly unbounded input and output spaces, typically do\nnot have a well defined ground truth target, and typically exhibit strong\nfeedback loops and prediction dependence based on context of previous model\noutputs. On top of these critical issues, we argue that the problems of {\\em\nleakage} and {\\em contamination} are in fact the most important and difficult\nissues to address for GenAI evaluations. Interestingly, the field of AI\nCompetitions has developed effective measures and practices to combat leakage\nfor the purpose of counteracting cheating by bad actors within a competition\nsetting. This makes AI Competitions an especially valuable (but underutilized)\nresource. Now is time for the field to view AI Competitions as the gold\nstandard for empirical rigor in GenAI evaluation, and to harness and harvest\ntheir results with according value.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00612.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626b23fc822e3d85324babed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b23fc822e3d85324babed/CbRuisnyy5JsCeA5uxYL7.jpeg",
            "fullname": "Meg Risdal",
            "name": "megrisdal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07812",
            "authors": [
                {
                    "_id": "68230f352dcd4033dcf9d1a5",
                    "user": {
                        "_id": "67a42bf8dba32bb665e351ad",
                        "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
                        "isPro": false,
                        "fullname": "Chenze Shao",
                        "user": "cccczshao",
                        "type": "user"
                    },
                    "name": "Chenze Shao",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-13T09:21:58.755Z",
                    "hidden": false
                },
                {
                    "_id": "68230f352dcd4033dcf9d1a6",
                    "name": "Fandong Meng",
                    "hidden": false
                },
                {
                    "_id": "68230f352dcd4033dcf9d1a7",
                    "name": "Jie Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:58:14.000Z",
            "submittedOnDailyAt": "2025-05-13T07:53:41.897Z",
            "title": "Continuous Visual Autoregressive Generation via Score Maximization",
            "submittedOnDailyBy": {
                "_id": "67a42bf8dba32bb665e351ad",
                "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
                "isPro": false,
                "fullname": "Chenze Shao",
                "user": "cccczshao",
                "type": "user"
            },
            "summary": "Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR.",
            "upvotes": 3,
            "discussionId": "68230f362dcd4033dcf9d1f7",
            "ai_keywords": [
                "Visual AutoRegressive modeling (VAR)",
                "Continuous VAR framework",
                "strictly proper scoring rules",
                "energy score",
                "GIVT",
                "diffusion loss"
            ]
        },
        "publishedAt": "2025-05-12T13:58:14.000Z",
        "title": "Continuous Visual Autoregressive Generation via Score Maximization",
        "summary": "Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07812.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67a42bf8dba32bb665e351ad",
            "avatarUrl": "/avatars/9c13810fe789ddcd9cefd4f2c924e4aa.svg",
            "fullname": "Chenze Shao",
            "name": "cccczshao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07793",
            "authors": [
                {
                    "_id": "6822fa434c32644c4cad2fd7",
                    "user": {
                        "_id": "65810ac5bc91d2ab5d8f4e03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65810ac5bc91d2ab5d8f4e03/mp-oYeE8vJ97YeqbKsp8N.png",
                        "isPro": false,
                        "fullname": "Assaf Ben-Kish",
                        "user": "assafbk",
                        "type": "user"
                    },
                    "name": "Assaf Ben-Kish",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:39:13.815Z",
                    "hidden": false
                },
                {
                    "_id": "6822fa434c32644c4cad2fd8",
                    "user": {
                        "_id": "65376feed325b3f02fb92c69",
                        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
                        "isPro": false,
                        "fullname": "Itamar Zimerman",
                        "user": "ItamarZ",
                        "type": "user"
                    },
                    "name": "Itamar Zimerman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:39:19.726Z",
                    "hidden": false
                },
                {
                    "_id": "6822fa434c32644c4cad2fd9",
                    "name": "M. Jehanzeb Mirza",
                    "hidden": false
                },
                {
                    "_id": "6822fa434c32644c4cad2fda",
                    "user": {
                        "_id": "62a793d33e101ec156cc58c4",
                        "avatarUrl": "/avatars/d3607cdb2cac1b2a3011d7441e6ea321.svg",
                        "isPro": false,
                        "fullname": "James Glass",
                        "user": "OmegaLittleBob",
                        "type": "user"
                    },
                    "name": "James Glass",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:39:38.415Z",
                    "hidden": false
                },
                {
                    "_id": "6822fa434c32644c4cad2fdb",
                    "user": {
                        "_id": "62558f7cebfc059817b06690",
                        "avatarUrl": "/avatars/40fbfdbeb0b33e21a48478deb2a742d1.svg",
                        "isPro": false,
                        "fullname": "Leonid Karlinsky",
                        "user": "leokarlin",
                        "type": "user"
                    },
                    "name": "Leonid Karlinsky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:39:44.021Z",
                    "hidden": false
                },
                {
                    "_id": "6822fa434c32644c4cad2fdc",
                    "user": {
                        "_id": "630e63148df86f1e5bf17fb6",
                        "avatarUrl": "/avatars/750dfc7f6dec2c0aa9eeb005210d3a27.svg",
                        "isPro": false,
                        "fullname": "Raja Giryes",
                        "user": "rgiryes",
                        "type": "user"
                    },
                    "name": "Raja Giryes",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:39:49.959Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:45:05.000Z",
            "submittedOnDailyAt": "2025-05-13T06:26:32.551Z",
            "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs",
            "submittedOnDailyBy": {
                "_id": "65810ac5bc91d2ab5d8f4e03",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65810ac5bc91d2ab5d8f4e03/mp-oYeE8vJ97YeqbKsp8N.png",
                "isPro": false,
                "fullname": "Assaf Ben-Kish",
                "user": "assafbk",
                "type": "user"
            },
            "summary": "A recent trend in LLMs is developing recurrent sub-quadratic models that\nimprove long-context processing efficiency. We investigate leading large\nlong-context models, focusing on how their fixed-size recurrent memory affects\ntheir performance. Our experiments reveal that, even when these models are\ntrained for extended contexts, their use of long contexts remains\nunderutilized. Specifically, we demonstrate that a chunk-based inference\nprocedure, which identifies and processes only the most relevant portion of the\ninput can mitigate recurrent memory failures and be effective for many\nlong-context tasks: On LongBench, our method improves the overall performance\nof Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,\nRecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this\nsimple approach also leads to state-of-the-art results in the challenging\nLongBench v2 benchmark, showing competitive performance with equivalent size\nTransformers. Furthermore, our findings raise questions about whether recurrent\nmodels genuinely exploit long-range dependencies, as our single-chunk strategy\ndelivers stronger performance - even in tasks that presumably require\ncross-context relations.",
            "upvotes": 3,
            "discussionId": "6822fa444c32644c4cad3040",
            "githubRepo": "https://github.com/assafbk/OPRM",
            "ai_keywords": [
                "recurrent sub-quadratic models",
                "long-context processing",
                "recurrent memory",
                "chunk-based inference",
                "LongBench",
                "Falcon3-Mamba-Inst-7B",
                "Falcon-Mamba-Inst-7B",
                "RecurrentGemma-IT-9B",
                "RWKV6-Finch-7B",
                "long-range dependencies",
                "cross-context relations"
            ]
        },
        "publishedAt": "2025-05-12T13:45:05.000Z",
        "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs",
        "summary": "A recent trend in LLMs is developing recurrent sub-quadratic models that\nimprove long-context processing efficiency. We investigate leading large\nlong-context models, focusing on how their fixed-size recurrent memory affects\ntheir performance. Our experiments reveal that, even when these models are\ntrained for extended contexts, their use of long contexts remains\nunderutilized. Specifically, we demonstrate that a chunk-based inference\nprocedure, which identifies and processes only the most relevant portion of the\ninput can mitigate recurrent memory failures and be effective for many\nlong-context tasks: On LongBench, our method improves the overall performance\nof Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,\nRecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this\nsimple approach also leads to state-of-the-art results in the challenging\nLongBench v2 benchmark, showing competitive performance with equivalent size\nTransformers. Furthermore, our findings raise questions about whether recurrent\nmodels genuinely exploit long-range dependencies, as our single-chunk strategy\ndelivers stronger performance - even in tasks that presumably require\ncross-context relations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07793.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65810ac5bc91d2ab5d8f4e03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65810ac5bc91d2ab5d8f4e03/mp-oYeE8vJ97YeqbKsp8N.png",
            "fullname": "Assaf Ben-Kish",
            "name": "assafbk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06324",
            "authors": [
                {
                    "_id": "6822bfeaa371047fe1545a13",
                    "name": "Vipula Rawte",
                    "hidden": false
                },
                {
                    "_id": "6822bfeaa371047fe1545a14",
                    "user": {
                        "_id": "62a3ab83e4dd6252344d27cd",
                        "avatarUrl": "/avatars/7ca8510f70a58dc207b104240e30c35c.svg",
                        "isPro": false,
                        "fullname": "Ryan A. Rossi",
                        "user": "ryanrossi",
                        "type": "user"
                    },
                    "name": "Ryan A. Rossi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:40:15.552Z",
                    "hidden": false
                },
                {
                    "_id": "6822bfeaa371047fe1545a15",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-13T08:11:40.296Z",
                    "hidden": false
                },
                {
                    "_id": "6822bfeaa371047fe1545a16",
                    "name": "Nedim Lipka",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-09T04:40:11.000Z",
            "submittedOnDailyAt": "2025-05-13T02:13:48.132Z",
            "title": "Document Attribution: Examining Citation Relationships using Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11.",
            "upvotes": 3,
            "discussionId": "6822bfeba371047fe1545a39",
            "ai_keywords": [
                "Large Language Models",
                "document summarization",
                "question answering",
                "information extraction",
                "attribution",
                "textual entailment",
                "flan-ul2",
                "AttributionBench",
                "F1 scores",
                "attention mechanism"
            ]
        },
        "publishedAt": "2025-05-09T00:40:11.000Z",
        "title": "Document Attribution: Examining Citation Relationships using Large\n  Language Models",
        "summary": "As Large Language Models (LLMs) are increasingly applied to document-based\ntasks - such as document summarization, question answering, and information\nextraction - where user requirements focus on retrieving information from\nprovided documents rather than relying on the model's parametric knowledge,\nensuring the trustworthiness and interpretability of these systems has become a\ncritical concern. A central approach to addressing this challenge is\nattribution, which involves tracing the generated outputs back to their source\ndocuments. However, since LLMs can produce inaccurate or imprecise responses,\nit is crucial to assess the reliability of these citations.\n  To tackle this, our work proposes two techniques. (1) A zero-shot approach\nthat frames attribution as a straightforward textual entailment task. Our\nmethod using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the\nbest baseline of ID and OOD sets of AttributionBench, respectively. (2) We also\nexplore the role of the attention mechanism in enhancing the attribution\nprocess. Using a smaller LLM, flan-t5-small, the F1 scores outperform the\nbaseline across almost all layers except layer 4 and layers 8 through 11.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06324.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07291",
            "authors": [
                {
                    "_id": "6822b9035f801d383a4b8742",
                    "name": "Prime Intellect Team",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8743",
                    "name": "Sami Jaghouar",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8744",
                    "name": "Justus Mattern",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8745",
                    "name": "Jack Min Ong",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8746",
                    "name": "Jannik Straube",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8747",
                    "name": "Manveer Basra",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8748",
                    "name": "Aaron Pazdera",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b8749",
                    "name": "Kushal Thaman",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b874a",
                    "name": "Matthew Di Ferrante",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b874b",
                    "name": "Felix Gabriel",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b874c",
                    "name": "Fares Obeid",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b874d",
                    "name": "Kemal Erdem",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b874e",
                    "name": "Michael Keiblinger",
                    "hidden": false
                },
                {
                    "_id": "6822b9035f801d383a4b874f",
                    "name": "Johannes Hagemann",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T07:24:33.000Z",
            "submittedOnDailyAt": "2025-05-13T20:11:05.996Z",
            "title": "INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "5e67bdd61009063689407479",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
                "isPro": true,
                "fullname": "Clem ",
                "user": "clem",
                "type": "user"
            },
            "summary": "We introduce INTELLECT-2, the first globally distributed reinforcement\nlearning (RL) training run of a 32 billion parameter language model. Unlike\ntraditional centralized training efforts, INTELLECT-2 trains a reasoning model\nusing fully asynchronous RL across a dynamic, heterogeneous swarm of\npermissionless compute contributors.\n  To enable a training run with this unique infrastructure, we built various\ncomponents from scratch: we introduce PRIME-RL, our training framework\npurpose-built for distributed asynchronous reinforcement learning, based on top\nof novel components such as TOPLOC, which verifies rollouts from untrusted\ninference workers, and SHARDCAST, which efficiently broadcasts policy weights\nfrom training nodes to inference workers.\n  Beyond infrastructure components, we propose modifications to the standard\nGRPO training recipe and data filtering techniques that were crucial to achieve\ntraining stability and ensure that our model successfully learned its training\nobjective, thus improving upon QwQ-32B, the state of the art reasoning model in\nthe 32B parameter range.\n  We open-source INTELLECT-2 along with all of our code and data, hoping to\nencourage and enable more open research in the field of decentralized training.",
            "upvotes": 2,
            "discussionId": "6822b9045f801d383a4b8792",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "training framework",
                "distributed asynchronous reinforcement learning",
                "TOPLOC",
                "SHARDCAST",
                "GRPO training recipe",
                "decentralized training"
            ]
        },
        "publishedAt": "2025-05-12T03:24:33.000Z",
        "title": "INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning",
        "summary": "We introduce INTELLECT-2, the first globally distributed reinforcement\nlearning (RL) training run of a 32 billion parameter language model. Unlike\ntraditional centralized training efforts, INTELLECT-2 trains a reasoning model\nusing fully asynchronous RL across a dynamic, heterogeneous swarm of\npermissionless compute contributors.\n  To enable a training run with this unique infrastructure, we built various\ncomponents from scratch: we introduce PRIME-RL, our training framework\npurpose-built for distributed asynchronous reinforcement learning, based on top\nof novel components such as TOPLOC, which verifies rollouts from untrusted\ninference workers, and SHARDCAST, which efficiently broadcasts policy weights\nfrom training nodes to inference workers.\n  Beyond infrastructure components, we propose modifications to the standard\nGRPO training recipe and data filtering techniques that were crucial to achieve\ntraining stability and ensure that our model successfully learned its training\nobjective, thus improving upon QwQ-32B, the state of the art reasoning model in\nthe 32B parameter range.\n  We open-source INTELLECT-2 along with all of our code and data, hoping to\nencourage and enable more open research in the field of decentralized training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07291.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e67bdd61009063689407479",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
            "fullname": "Clem ",
            "name": "clem",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2344
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07233",
            "authors": [
                {
                    "_id": "6823496167281062097f3ac6",
                    "name": "Jiashuo Sun",
                    "hidden": false
                },
                {
                    "_id": "6823496167281062097f3ac7",
                    "name": "Xianrui Zhong",
                    "hidden": false
                },
                {
                    "_id": "6823496167281062097f3ac8",
                    "name": "Sizhe Zhou",
                    "hidden": false
                },
                {
                    "_id": "6823496167281062097f3ac9",
                    "name": "Jiawei Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T05:19:01.000Z",
            "submittedOnDailyAt": "2025-05-13T12:00:42.729Z",
            "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation",
            "submittedOnDailyBy": {
                "_id": "64527f701a57e1179c1c3693",
                "avatarUrl": "/avatars/25b2632d7aa9ce26d5d4924ecb00c4f4.svg",
                "isPro": false,
                "fullname": "Jiashuo Sun",
                "user": "gasolsun",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker, which refines retrieved documents to enhance\ngeneration quality and explainability. The challenge of selecting the optimal\nnumber of documents (k) remains unsolved: too few may omit critical\ninformation, while too many introduce noise and inefficiencies. Although recent\nstudies have explored LLM-based rerankers, they primarily leverage internal\nmodel knowledge and overlook the rich supervisory signals that LLMs can\nprovide, such as using response quality as feedback for optimizing reranking\ndecisions. In this paper, we propose DynamicRAG, a novel RAG framework where\nthe reranker dynamically adjusts both the order and number of retrieved\ndocuments based on the query. We model the reranker as an agent optimized\nthrough reinforcement learning (RL), using rewards derived from LLM output\nquality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates\nsuperior performance, achieving state-of-the-art results. The model, data and\ncode are available at https://github.com/GasolSun36/DynamicRAG",
            "upvotes": 2,
            "discussionId": "6823496267281062097f3b16",
            "ai_keywords": [
                "DynamicRAG",
                "reranker",
                "reinforcement learning (RL)",
                "response quality",
                "retrieval",
                "optimization",
                "query",
                "knowledge-intensive tasks",
                "large language models (LLMs)",
                "LLM-based rerankers"
            ]
        },
        "publishedAt": "2025-05-12T01:19:01.000Z",
        "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation",
        "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker, which refines retrieved documents to enhance\ngeneration quality and explainability. The challenge of selecting the optimal\nnumber of documents (k) remains unsolved: too few may omit critical\ninformation, while too many introduce noise and inefficiencies. Although recent\nstudies have explored LLM-based rerankers, they primarily leverage internal\nmodel knowledge and overlook the rich supervisory signals that LLMs can\nprovide, such as using response quality as feedback for optimizing reranking\ndecisions. In this paper, we propose DynamicRAG, a novel RAG framework where\nthe reranker dynamically adjusts both the order and number of retrieved\ndocuments based on the query. We model the reranker as an agent optimized\nthrough reinforcement learning (RL), using rewards derived from LLM output\nquality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates\nsuperior performance, achieving state-of-the-art results. The model, data and\ncode are available at https://github.com/GasolSun36/DynamicRAG",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07233.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64527f701a57e1179c1c3693",
            "avatarUrl": "/avatars/25b2632d7aa9ce26d5d4924ecb00c4f4.svg",
            "fullname": "Jiashuo Sun",
            "name": "gasolsun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.04918",
            "authors": [
                {
                    "_id": "681e0786377fcb995a1a3cde",
                    "user": {
                        "_id": "681e0624479a016a7b4fb88a",
                        "avatarUrl": "/avatars/98d2c88cad56e4453dead6e50a64957b.svg",
                        "isPro": false,
                        "fullname": "Jiaqi Zheng",
                        "user": "Yumenomae",
                        "type": "user"
                    },
                    "name": "Jiaqi Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-12T06:50:54.481Z",
                    "hidden": false
                },
                {
                    "_id": "681e0786377fcb995a1a3cdf",
                    "name": "Qing Ling",
                    "hidden": false
                },
                {
                    "_id": "681e0786377fcb995a1a3ce0",
                    "name": "Yerong Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T03:25:55.000Z",
            "submittedOnDailyAt": "2025-05-13T06:38:33.439Z",
            "title": "Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction",
            "submittedOnDailyBy": {
                "_id": "681e0624479a016a7b4fb88a",
                "avatarUrl": "/avatars/98d2c88cad56e4453dead6e50a64957b.svg",
                "isPro": false,
                "fullname": "Jiaqi Zheng",
                "user": "Yumenomae",
                "type": "user"
            },
            "summary": "Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the physics of the\nunderlying weather evolution or the topology of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the 5.625^circ-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625.",
            "upvotes": 2,
            "discussionId": "681e0788377fcb995a1a3d6f",
            "ai_keywords": [
                "advection equation",
                "Navier-Stokes equation",
                "Earth-atmosphere interaction",
                "spherical manifold",
                "spherical graph neural network",
                "PASSAT",
                "weather evolution",
                "initial velocity fields",
                "state-of-the-art deep learning-based weather prediction models",
                "operational numerical weather prediction model IFS T42"
            ]
        },
        "publishedAt": "2025-05-07T23:25:55.000Z",
        "title": "Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction",
        "summary": "Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the physics of the\nunderlying weather evolution or the topology of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the 5.625^circ-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04918.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "681e0624479a016a7b4fb88a",
            "avatarUrl": "/avatars/98d2c88cad56e4453dead6e50a64957b.svg",
            "fullname": "Jiaqi Zheng",
            "name": "Yumenomae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.04066",
            "authors": [
                {
                    "_id": "68239705889f2a9d374c1774",
                    "name": "Tuochao Chen",
                    "hidden": false
                },
                {
                    "_id": "68239705889f2a9d374c1775",
                    "name": "Nicholas Batchelder",
                    "hidden": false
                },
                {
                    "_id": "68239705889f2a9d374c1776",
                    "name": "Alisa Liu",
                    "hidden": false
                },
                {
                    "_id": "68239705889f2a9d374c1777",
                    "name": "Noah Smith",
                    "hidden": false
                },
                {
                    "_id": "68239705889f2a9d374c1778",
                    "name": "Shyamnath Gollakota",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-07T02:08:56.000Z",
            "submittedOnDailyAt": "2025-05-13T17:32:23.741Z",
            "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants",
            "submittedOnDailyBy": {
                "_id": "65a9d16f3b9e1f0f307c96c3",
                "avatarUrl": "/avatars/882a0746a0adb2d7befe563f7fbc6321.svg",
                "isPro": false,
                "fullname": "Tuochao Chen",
                "user": "tuochao",
                "type": "user"
            },
            "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations.",
            "upvotes": 1,
            "discussionId": "68239706889f2a9d374c17d8",
            "githubRepo": "https://github.com/chentuochao/LlamaPIE",
            "ai_keywords": [
                "semi-synthetic dialogue dataset",
                "two-model pipeline"
            ]
        },
        "publishedAt": "2025-05-06T22:08:56.000Z",
        "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants",
        "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04066.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a9d16f3b9e1f0f307c96c3",
            "avatarUrl": "/avatars/882a0746a0adb2d7befe563f7fbc6321.svg",
            "fullname": "Tuochao Chen",
            "name": "tuochao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07086",
            "authors": [
                {
                    "_id": "6822c81dbda1314b4c967f53",
                    "name": "Tong Chen",
                    "hidden": false
                },
                {
                    "_id": "6822c81dbda1314b4c967f54",
                    "user": {
                        "_id": "64ffdd2ae3201fff884c257f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ffdd2ae3201fff884c257f/EZXWdkW33wfoGmYGZLdkh.png",
                        "isPro": false,
                        "fullname": "z",
                        "user": "yinuozhang",
                        "type": "user"
                    },
                    "name": "Yinuo Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:41:33.531Z",
                    "hidden": false
                },
                {
                    "_id": "6822c81dbda1314b4c967f55",
                    "user": {
                        "_id": "66d89392c81167fc5e1bdada",
                        "avatarUrl": "/avatars/4d06f2abec11e471a918353d687f77fe.svg",
                        "isPro": false,
                        "fullname": "Sophia Tang",
                        "user": "sophtang",
                        "type": "user"
                    },
                    "name": "Sophia Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-13T08:41:19.741Z",
                    "hidden": false
                },
                {
                    "_id": "6822c81dbda1314b4c967f56",
                    "user": {
                        "_id": "64cd5b3f0494187a9e8b7c69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
                        "isPro": false,
                        "fullname": "Pranam Chatterjee",
                        "user": "pranamanam",
                        "type": "user"
                    },
                    "name": "Pranam Chatterjee",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-13T04:18:39.835Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-11T18:17:44.000Z",
            "submittedOnDailyAt": "2025-05-13T03:01:46.396Z",
            "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design",
            "submittedOnDailyBy": {
                "_id": "64cd5b3f0494187a9e8b7c69",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
                "isPro": false,
                "fullname": "Pranam Chatterjee",
                "user": "pranamanam",
                "type": "user"
            },
            "summary": "Designing biological sequences that satisfy multiple, often conflicting,\nfunctional and biophysical criteria remains a central challenge in biomolecule\nengineering. While discrete flow matching models have recently shown promise\nfor efficient sampling in high-dimensional sequence spaces, existing approaches\naddress only single objectives or require continuous embeddings that can\ndistort discrete distributions. We present Multi-Objective-Guided Discrete Flow\nMatching (MOG-DFM), a general framework to steer any pretrained discrete-time\nflow matching generator toward Pareto-efficient trade-offs across multiple\nscalar objectives. At each sampling step, MOG-DFM computes a hybrid\nrank-directional score for candidate transitions and applies an adaptive\nhypercone filter to enforce consistent multi-objective progression. We also\ntrained two unconditional discrete flow matching models, PepDFM for diverse\npeptide generation and EnhancerDFM for functional enhancer DNA generation, as\nbase generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in\ngenerating peptide binders optimized across five properties (hemolysis,\nnon-fouling, solubility, half-life, and binding affinity), and in designing DNA\nsequences with specific enhancer classes and DNA shapes. In total, MOG-DFM\nproves to be a powerful tool for multi-property-guided biomolecule sequence\ndesign.",
            "upvotes": 0,
            "discussionId": "6822c81fbda1314b4c967fda",
            "ai_keywords": [
                "discrete flow matching models",
                "flow matching generators",
                "Pareto-efficient trade-offs",
                "hybrid rank-directional score",
                "adaptive hypercone filter",
                "unconditional discrete flow matching models",
                "PepDFM",
                "EnhancerDFM",
                "peptide binders",
                "hemolysis",
                "non-fouling",
                "solubility",
                "half-life",
                "binding affinity",
                "DNA sequences",
                "specific enhancer classes",
                "DNA shapes"
            ]
        },
        "publishedAt": "2025-05-11T14:17:44.000Z",
        "title": "Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design",
        "summary": "Designing biological sequences that satisfy multiple, often conflicting,\nfunctional and biophysical criteria remains a central challenge in biomolecule\nengineering. While discrete flow matching models have recently shown promise\nfor efficient sampling in high-dimensional sequence spaces, existing approaches\naddress only single objectives or require continuous embeddings that can\ndistort discrete distributions. We present Multi-Objective-Guided Discrete Flow\nMatching (MOG-DFM), a general framework to steer any pretrained discrete-time\nflow matching generator toward Pareto-efficient trade-offs across multiple\nscalar objectives. At each sampling step, MOG-DFM computes a hybrid\nrank-directional score for candidate transitions and applies an adaptive\nhypercone filter to enforce consistent multi-objective progression. We also\ntrained two unconditional discrete flow matching models, PepDFM for diverse\npeptide generation and EnhancerDFM for functional enhancer DNA generation, as\nbase generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in\ngenerating peptide binders optimized across five properties (hemolysis,\nnon-fouling, solubility, half-life, and binding affinity), and in designing DNA\nsequences with specific enhancer classes and DNA shapes. In total, MOG-DFM\nproves to be a powerful tool for multi-property-guided biomolecule sequence\ndesign.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07086.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "fullname": "Pranam Chatterjee",
            "name": "pranamanam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    }
]