[
    {
        "paper": {
            "id": "2505.07916",
            "authors": [
                {
                    "_id": "68244ea3bfb1b25f60400efd",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400efe",
                    "name": "Congchao Guo",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400eff",
                    "name": "Geng Yang",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f00",
                    "name": "Hang Yu",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f01",
                    "name": "Haozhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f02",
                    "name": "Heidi Lei",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f03",
                    "name": "Jialong Mai",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f04",
                    "user": {
                        "_id": "63390ce41718795719635b1e",
                        "avatarUrl": "/avatars/ad03a2b349f01c1ac1fedfb95d02d43e.svg",
                        "isPro": false,
                        "fullname": "JunjieYan",
                        "user": "JunjieYan",
                        "type": "user"
                    },
                    "name": "Junjie Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:05:37.903Z",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f05",
                    "name": "Kaiyue Yang",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f06",
                    "user": {
                        "_id": "65e29a93e142ecfc09bddf3a",
                        "avatarUrl": "/avatars/70168cae7aef1bb2c00392b926eabb18.svg",
                        "isPro": false,
                        "fullname": "Mingqi Yang",
                        "user": "mqyang1s",
                        "type": "user"
                    },
                    "name": "Mingqi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:05:51.310Z",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f07",
                    "name": "Peikai Huang",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f08",
                    "name": "Ruiyang Jin",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f09",
                    "name": "Sitan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f0a",
                    "name": "Weihua Cheng",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f0b",
                    "name": "Yawei Li",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f0c",
                    "name": "Yichen Xiao",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f0d",
                    "name": "Yiying Zhou",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f0e",
                    "user": {
                        "_id": "64b655c3f44a33a87e73b866",
                        "avatarUrl": "/avatars/3a2c58eb10d4cf7040f63ea15284c574.svg",
                        "isPro": false,
                        "fullname": "yongmao zhang",
                        "user": "ymzhang0519",
                        "type": "user"
                    },
                    "name": "Yongmao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:06:55.523Z",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f0f",
                    "name": "Yuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68244ea3bfb1b25f60400f10",
                    "name": "Yucen He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T14:25:20.000Z",
            "submittedOnDailyAt": "2025-05-14T07:29:51.954Z",
            "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
            "submittedOnDailyBy": {
                "_id": "676e38ad04af5bec20bc9faf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
                "isPro": false,
                "fullname": "MiniMax",
                "user": "MiniMax-AI",
                "type": "user"
            },
            "summary": "We introduce MiniMax-Speech, an autoregressive Transformer-based\nText-to-Speech (TTS) model that generates high-quality speech. A key innovation\nis our learnable speaker encoder, which extracts timbre features from a\nreference audio without requiring its transcription. This enables\nMiniMax-Speech to produce highly expressive speech with timbre consistent with\nthe reference in a zero-shot manner, while also supporting one-shot voice\ncloning with exceptionally high similarity to the reference voice. In addition,\nthe overall quality of the synthesized audio is enhanced through the proposed\nFlow-VAE. Our model supports 32 languages and demonstrates excellent\nperformance across multiple objective and subjective evaluations metrics.\nNotably, it achieves state-of-the-art (SOTA) results on objective voice cloning\nmetrics (Word Error Rate and Speaker Similarity) and has secured the top\nposition on the public TTS Arena leaderboard. Another key strength of\nMiniMax-Speech, granted by the robust and disentangled representations from the\nspeaker encoder, is its extensibility without modifying the base model,\nenabling various applications such as: arbitrary voice emotion control via\nLoRA; text to voice (T2V) by synthesizing timbre features directly from text\ndescription; and professional voice cloning (PVC) by fine-tuning timbre\nfeatures with additional data. We encourage readers to visit\nhttps://minimax-ai.github.io/tts_tech_report for more examples.",
            "upvotes": 81,
            "discussionId": "68244ea4bfb1b25f60400f4c",
            "projectPage": "https://minimax-ai.github.io/tts_tech_report/",
            "githubRepo": "https://github.com/MiniMax-AI/MiniMax-AI.github.io",
            "ai_keywords": [
                "autoregressive Transformer",
                "Text-to-Speech (TTS)",
                "learnable speaker encoder",
                "timbre features",
                "zero-shot",
                "one-shot voice cloning",
                "Flow-VAE",
                "Word Error Rate",
                "Speaker Similarity",
                "TTS Arena leaderboard",
                "robust and disentangled representations",
                "arbitrary voice emotion control",
                "LoRA (Low-Rank Adaptation)",
                "text to voice (T2V)",
                "professional voice cloning (PVC)"
            ]
        },
        "publishedAt": "2025-05-12T10:25:20.000Z",
        "title": "MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable\n  Speaker Encoder",
        "summary": "We introduce MiniMax-Speech, an autoregressive Transformer-based\nText-to-Speech (TTS) model that generates high-quality speech. A key innovation\nis our learnable speaker encoder, which extracts timbre features from a\nreference audio without requiring its transcription. This enables\nMiniMax-Speech to produce highly expressive speech with timbre consistent with\nthe reference in a zero-shot manner, while also supporting one-shot voice\ncloning with exceptionally high similarity to the reference voice. In addition,\nthe overall quality of the synthesized audio is enhanced through the proposed\nFlow-VAE. Our model supports 32 languages and demonstrates excellent\nperformance across multiple objective and subjective evaluations metrics.\nNotably, it achieves state-of-the-art (SOTA) results on objective voice cloning\nmetrics (Word Error Rate and Speaker Similarity) and has secured the top\nposition on the public TTS Arena leaderboard. Another key strength of\nMiniMax-Speech, granted by the robust and disentangled representations from the\nspeaker encoder, is its extensibility without modifying the base model,\nenabling various applications such as: arbitrary voice emotion control via\nLoRA; text to voice (T2V) by synthesizing timbre features directly from text\ndescription; and professional voice cloning (PVC) by fine-tuning timbre\nfeatures with additional data. We encourage readers to visit\nhttps://minimax-ai.github.io/tts_tech_report for more examples.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07916.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "676e38ad04af5bec20bc9faf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
            "fullname": "MiniMax",
            "name": "MiniMax-AI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 133
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07591",
            "authors": [
                {
                    "_id": "6822e023b1df51252f95e958",
                    "user": {
                        "_id": "66384be673c2c55f2ded89fa",
                        "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
                        "isPro": false,
                        "fullname": "Junjie Ye",
                        "user": "Junjie-Ye",
                        "type": "user"
                    },
                    "name": "Junjie Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-14T07:35:57.239Z",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e959",
                    "name": "Caishuang Huang",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e95a",
                    "name": "Zhuohan Chen",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e95b",
                    "user": {
                        "_id": "636b5fd69560e7403d9150ff",
                        "avatarUrl": "/avatars/ffe3553a47624f6821b0b46f0da729dd.svg",
                        "isPro": false,
                        "fullname": "fuwenjie",
                        "user": "avonfwj",
                        "type": "user"
                    },
                    "name": "Wenjie Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:10:19.727Z",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e95c",
                    "name": "Chenyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e95d",
                    "name": "Leyi Yang",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e95e",
                    "user": {
                        "_id": "64e99648662874dbc9c53ee6",
                        "avatarUrl": "/avatars/10927024e137a3d43a5e8028c1d7c1c1.svg",
                        "isPro": false,
                        "fullname": "yilong",
                        "user": "wuyilong",
                        "type": "user"
                    },
                    "name": "Yilong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:11:11.451Z",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e95f",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e960",
                    "name": "Meng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e961",
                    "user": {
                        "_id": "643d91e737453b48a6febd9b",
                        "avatarUrl": "/avatars/dc5802c5b76239737fa182a6cdfdae1b.svg",
                        "isPro": false,
                        "fullname": "Xiaolong  yang",
                        "user": "sean-xl-y",
                        "type": "user"
                    },
                    "name": "Xiaolong Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:11:18.988Z",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e962",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e963",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e964",
                    "name": "Zhongchao Shi",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e965",
                    "name": "Jianping Fan",
                    "hidden": false
                },
                {
                    "_id": "6822e023b1df51252f95e966",
                    "user": {
                        "_id": "67f9c4ee171948c38302ae0f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Cqb3ijr_sZkpLhEEEEybK.png",
                        "isPro": false,
                        "fullname": "Xuanjing Huang",
                        "user": "xjhuang",
                        "type": "user"
                    },
                    "name": "Xuanjing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:11:25.379Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T14:16:55.000Z",
            "submittedOnDailyAt": "2025-05-14T05:33:58.516Z",
            "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "66384be673c2c55f2ded89fa",
                "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
                "isPro": false,
                "fullname": "Junjie Ye",
                "user": "Junjie-Ye",
                "type": "user"
            },
            "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.",
            "upvotes": 7,
            "discussionId": "6822e024b1df51252f95e9be",
            "githubRepo": "https://github.com/Junjie-Ye/MulDimIF",
            "ai_keywords": [
                "instruction-following",
                "constraint expansion",
                "conflict detection",
                "instruction rewriting",
                "code-verifiable",
                "attention modules"
            ]
        },
        "publishedAt": "2025-05-12T10:16:55.000Z",
        "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving\n  Instruction Following in Large Language Models",
        "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07591.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66384be673c2c55f2ded89fa",
            "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
            "fullname": "Junjie Ye",
            "name": "Junjie-Ye",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.08175",
            "authors": [
                {
                    "_id": "6823efb1b220ff8b82381de5",
                    "name": "Zachary Novack",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381de6",
                    "name": "Zach Evans",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381de7",
                    "name": "Zack Zukowski",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381de8",
                    "name": "Josiah Taylor",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381de9",
                    "name": "CJ Carr",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381dea",
                    "name": "Julian Parker",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381deb",
                    "name": "Adnan Al-Sinan",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381dec",
                    "name": "Gian Marco Iodice",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381ded",
                    "name": "Julian McAuley",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381dee",
                    "name": "Taylor Berg-Kirkpatrick",
                    "hidden": false
                },
                {
                    "_id": "6823efb1b220ff8b82381def",
                    "name": "Jordi Pons",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/__4MAPfkwYJmf5KoRxgwz.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/1YBQHolY-ZSvV0LPBqI-r.mp4"
            ],
            "publishedAt": "2025-05-13T02:25:47.000Z",
            "submittedOnDailyAt": "2025-05-14T15:59:41.175Z",
            "title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
            "submittedOnDailyBy": {
                "_id": "643060c6cb3fe707b24c53a2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
                "isPro": false,
                "fullname": "Zachary Novack",
                "user": "ZacharyNovack",
                "type": "user"
            },
            "summary": "Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating approx12s of 44.1kHz stereo audio in\napprox75ms on an H100, and approx7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge.",
            "upvotes": 6,
            "discussionId": "6823efb2b220ff8b82381e1c",
            "projectPage": "https://arc-text2audio.github.io/web/",
            "githubRepo": "https://github.com/Stability-AI/stable-audio-tools",
            "ai_keywords": [
                "Adversarial Relativistic-Contrastive (ARC) post-training",
                "diffusion/flow models",
                "relativistic adversarial formulation",
                "contrastive discriminator objective",
                "prompt adherence",
                "Stable Audio Open"
            ]
        },
        "publishedAt": "2025-05-12T22:25:47.000Z",
        "title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
        "summary": "Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating approx12s of 44.1kHz stereo audio in\napprox75ms on an H100, and approx7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/__4MAPfkwYJmf5KoRxgwz.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/1YBQHolY-ZSvV0LPBqI-r.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08175.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643060c6cb3fe707b24c53a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
            "fullname": "Zachary Novack",
            "name": "ZacharyNovack",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07215",
            "authors": [
                {
                    "_id": "68236b86102b1d3069ebafab",
                    "user": {
                        "_id": "64b88247e436bbca16603baf",
                        "avatarUrl": "/avatars/7bde6b0f75bccc3195fb72cbe5860a7e.svg",
                        "isPro": false,
                        "fullname": "Vivek Verma",
                        "user": "vivekverma",
                        "type": "user"
                    },
                    "name": "Vivek Verma",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-13T15:55:50.678Z",
                    "hidden": false
                },
                {
                    "_id": "68236b86102b1d3069ebafac",
                    "name": "David Huang",
                    "hidden": false
                },
                {
                    "_id": "68236b86102b1d3069ebafad",
                    "name": "William Chen",
                    "hidden": false
                },
                {
                    "_id": "68236b86102b1d3069ebafae",
                    "user": {
                        "_id": "632be88b3690fb57e70e0bf1",
                        "avatarUrl": "/avatars/74ff8f30b3662db2602495bdf493d397.svg",
                        "isPro": false,
                        "fullname": "Dan Klein",
                        "user": "danjklein",
                        "type": "user"
                    },
                    "name": "Dan Klein",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:09:04.358Z",
                    "hidden": false
                },
                {
                    "_id": "68236b86102b1d3069ebafaf",
                    "user": {
                        "_id": "6269d074a6a7bba9e46d8d50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651101782106-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Nicholas Tomlin",
                        "user": "nickatomlin",
                        "type": "user"
                    },
                    "name": "Nicholas Tomlin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-14T07:35:26.733Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6269d074a6a7bba9e46d8d50/RSzjacMbHw27QCpwl_Nte.png"
            ],
            "publishedAt": "2025-05-12T04:01:03.000Z",
            "submittedOnDailyAt": "2025-05-14T06:11:56.396Z",
            "title": "Measuring General Intelligence with Generated Games",
            "submittedOnDailyBy": {
                "_id": "6269d074a6a7bba9e46d8d50",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651101782106-noauth.jpeg",
                "isPro": false,
                "fullname": "Nicholas Tomlin",
                "user": "nickatomlin",
                "type": "user"
            },
            "summary": "We present gg-bench, a collection of game environments designed to evaluate\ngeneral reasoning capabilities in language models. Unlike most static\nbenchmarks, gg-bench is a data generating process where new evaluation\ninstances can be generated at will. In particular, gg-bench is synthetically\ngenerated by (1) using a large language model (LLM) to generate natural\nlanguage descriptions of novel games, (2) using the LLM to implement each game\nin code as a Gym environment, and (3) training reinforcement learning (RL)\nagents via self-play on the generated games. We evaluate language models by\ntheir winrate against these RL agents by prompting models with the game\ndescription, current board state, and a list of valid moves, after which models\noutput the moves they wish to take. gg-bench is challenging: state-of-the-art\nLLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench\nusing in-context learning, while reasoning models such as o1, o3-mini and\nDeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,\ndata generation process, and evaluation code in order to support future\nmodeling work and expansion of our benchmark.",
            "upvotes": 5,
            "discussionId": "68236b86102b1d3069ebb00e",
            "ai_keywords": [
                "large language model (LLM)",
                "Gym environment",
                "reinforcement learning (RL)",
                "self-play",
                "prompt",
                "in-context learning",
                "winrate"
            ]
        },
        "publishedAt": "2025-05-12T00:01:03.000Z",
        "title": "Measuring General Intelligence with Generated Games",
        "summary": "We present gg-bench, a collection of game environments designed to evaluate\ngeneral reasoning capabilities in language models. Unlike most static\nbenchmarks, gg-bench is a data generating process where new evaluation\ninstances can be generated at will. In particular, gg-bench is synthetically\ngenerated by (1) using a large language model (LLM) to generate natural\nlanguage descriptions of novel games, (2) using the LLM to implement each game\nin code as a Gym environment, and (3) training reinforcement learning (RL)\nagents via self-play on the generated games. We evaluate language models by\ntheir winrate against these RL agents by prompting models with the game\ndescription, current board state, and a list of valid moves, after which models\noutput the moves they wish to take. gg-bench is challenging: state-of-the-art\nLLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench\nusing in-context learning, while reasoning models such as o1, o3-mini and\nDeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,\ndata generation process, and evaluation code in order to support future\nmodeling work and expansion of our benchmark.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6269d074a6a7bba9e46d8d50/RSzjacMbHw27QCpwl_Nte.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07215.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6269d074a6a7bba9e46d8d50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651101782106-noauth.jpeg",
            "fullname": "Nicholas Tomlin",
            "name": "nickatomlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.05464",
            "authors": [
                {
                    "_id": "6824c5964fc74fcd88891f6f",
                    "name": "Shiqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f70",
                    "name": "Jinghan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f71",
                    "name": "Tongyao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f72",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f73",
                    "name": "Siyang Gao",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f74",
                    "name": "Miao Xiong",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f75",
                    "name": "Manling Li",
                    "hidden": false
                },
                {
                    "_id": "6824c5964fc74fcd88891f76",
                    "name": "Junxian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-08T17:56:23.000Z",
            "submittedOnDailyAt": "2025-05-14T15:05:56.498Z",
            "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
            "submittedOnDailyBy": {
                "_id": "642c5631ab0cc792e4379099",
                "avatarUrl": "/avatars/c660650950d4804f48329970226e63d1.svg",
                "isPro": false,
                "fullname": "Jinghan ZHANG",
                "user": "jinghan23",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
            "upvotes": 5,
            "discussionId": "6824c5994fc74fcd8889201b",
            "githubRepo": "https://github.com/shiqichen17/VLM_Merging",
            "ai_keywords": [
                "Vision-Language Models (VLMs)",
                "Large Language Models (LLMs)",
                "model merging",
                "multimodal integration",
                "early layers",
                "middle-to-late layers"
            ]
        },
        "publishedAt": "2025-05-08T13:56:23.000Z",
        "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
        "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05464.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642c5631ab0cc792e4379099",
            "avatarUrl": "/avatars/c660650950d4804f48329970226e63d1.svg",
            "fullname": "Jinghan ZHANG",
            "name": "jinghan23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.08638",
            "authors": [
                {
                    "_id": "6824bddc7ecdd81cdac3b346",
                    "name": "Darshan Deshpande",
                    "hidden": false
                },
                {
                    "_id": "6824bddc7ecdd81cdac3b347",
                    "name": "Varun Gangal",
                    "hidden": false
                },
                {
                    "_id": "6824bddc7ecdd81cdac3b348",
                    "name": "Hersh Mehta",
                    "hidden": false
                },
                {
                    "_id": "6824bddc7ecdd81cdac3b349",
                    "name": "Jitin Krishnan",
                    "hidden": false
                },
                {
                    "_id": "6824bddc7ecdd81cdac3b34a",
                    "name": "Anand Kannappan",
                    "hidden": false
                },
                {
                    "_id": "6824bddc7ecdd81cdac3b34b",
                    "name": "Rebecca Qian",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/LjLE8adfQ4nZXmMzLRZAi.png",
                "https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/sE-Zk-OB3GOXq7weVwRYR.png"
            ],
            "publishedAt": "2025-05-13T14:55:31.000Z",
            "submittedOnDailyAt": "2025-05-14T15:46:01.772Z",
            "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
            "submittedOnDailyBy": {
                "_id": "60390e04770949ef34f12d9b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632225571380-60390e04770949ef34f12d9b.jpeg",
                "isPro": false,
                "fullname": "Darshan Deshpande",
                "user": "DarshanDeshpande",
                "type": "user"
            },
            "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
            "upvotes": 4,
            "discussionId": "6824bddd7ecdd81cdac3b391",
            "githubRepo": "https://github.com/patronus-ai/trail-benchmark",
            "ai_keywords": [
                "long context LLMs",
                "trace debugging",
                "Gemini-2.5-pro"
            ]
        },
        "publishedAt": "2025-05-13T10:55:31.000Z",
        "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
        "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/LjLE8adfQ4nZXmMzLRZAi.png",
            "https://cdn-uploads.huggingface.co/production/uploads/60390e04770949ef34f12d9b/sE-Zk-OB3GOXq7weVwRYR.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08638.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60390e04770949ef34f12d9b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632225571380-60390e04770949ef34f12d9b.jpeg",
            "fullname": "Darshan Deshpande",
            "name": "DarshanDeshpande",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.08311",
            "authors": [
                {
                    "_id": "682404584254a325ece48c4e",
                    "name": "Yunjie Ji",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c4f",
                    "name": "Xiaoyu Tian",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c50",
                    "name": "Sitong Zhao",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c51",
                    "name": "Haotian Wang",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c52",
                    "name": "Shuaiting Chen",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c53",
                    "name": "Yiping Peng",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c54",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "682404584254a325ece48c55",
                    "name": "Xiangang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T07:41:15.000Z",
            "submittedOnDailyAt": "2025-05-14T12:05:28.577Z",
            "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\nhttps://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.",
            "upvotes": 3,
            "discussionId": "682404594254a325ece48cbd",
            "ai_keywords": [
                "dense language model",
                "reasoning",
                "DeepSeek-R1",
                "Mixture-of-Experts (MoE)",
                "Qwen3-235B-A22B",
                "Seed1.5-Thinking",
                "AIME 2024",
                "AIME 2025",
                "LiveCodeBench",
                "mathematical capabilities",
                "coding capabilities",
                "Qwen2.5-32B base model",
                "post-training pipeline",
                "supervised fine-tuning",
                "reinforcement learning",
                "reasoning boundaries"
            ]
        },
        "publishedAt": "2025-05-13T03:41:15.000Z",
        "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
        "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\nhttps://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08311.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.08751",
            "authors": [
                {
                    "_id": "68249d57f08a44ee1f1c7139",
                    "name": "Saurabh Dash",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c713a",
                    "name": "Yiyang Nan",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c713b",
                    "name": "John Dang",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c713c",
                    "name": "Arash Ahmadian",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c713d",
                    "name": "Shivalika Singh",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c713e",
                    "name": "Madeline Smith",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c713f",
                    "name": "Bharat Venkitesh",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7140",
                    "name": "Vlad Shmyhlo",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7141",
                    "name": "Viraat Aryabumi",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7142",
                    "name": "Walter Beller-Morales",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7143",
                    "name": "Jeremy Pekmez",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7144",
                    "name": "Jason Ozuzu",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7145",
                    "name": "Pierre Richemond",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7146",
                    "name": "Acyr Locatelli",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7147",
                    "name": "Nick Frosst",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7148",
                    "name": "Phil Blunsom",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7149",
                    "name": "Aidan Gomez",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c714a",
                    "name": "Ivan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c714b",
                    "name": "Marzieh Fadaee",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c714c",
                    "name": "Manoj Govindassamy",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c714d",
                    "name": "Sudip Roy",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c714e",
                    "name": "Matthias Gall",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c714f",
                    "name": "Beyza Ermis",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7150",
                    "name": "Ahmet stn",
                    "hidden": false
                },
                {
                    "_id": "68249d57f08a44ee1f1c7151",
                    "name": "Sara Hooker",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T17:03:48.000Z",
            "submittedOnDailyAt": "2025-05-14T12:11:04.410Z",
            "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance.",
            "upvotes": 2,
            "discussionId": "68249d5af08a44ee1f1c71d1",
            "ai_keywords": [
                "synthetic annotation framework",
                "cross-modal model merging technique",
                "catastrophic forgetting",
                "multimodal instruction data",
                "multimodal generative performance",
                "best-in-class performance",
                "multilingual multimodal models",
                "Aya Vision models"
            ]
        },
        "publishedAt": "2025-05-13T13:03:48.000Z",
        "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality",
        "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6842
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.08727",
            "authors": [
                {
                    "_id": "6824392a5d39467d53a7ca9d",
                    "user": {
                        "_id": "64d98ef7a4839890b25eb78b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
                        "isPro": true,
                        "fullname": "Fangyuan Yu",
                        "user": "Ksgk-fy",
                        "type": "user"
                    },
                    "name": "Fangyuan Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-14T07:33:49.661Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T16:37:54.000Z",
            "submittedOnDailyAt": "2025-05-14T15:05:43.645Z",
            "title": "Memorization-Compression Cycles Improve Generalization",
            "submittedOnDailyBy": {
                "_id": "64d98ef7a4839890b25eb78b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
                "isPro": true,
                "fullname": "Fangyuan Yu",
                "user": "Ksgk-fy",
                "type": "user"
            },
            "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.",
            "upvotes": 2,
            "discussionId": "6824392b5d39467d53a7cae2",
            "ai_keywords": [
                "Information Bottleneck Language Modeling (IBLM)",
                "constrained optimization",
                "representation entropy",
                "cross-entropy",
                "Matrix-Based Entropy (MBE)",
                "memorization-compression cycle",
                "Gated Phase Transition (GAPT)",
                "OOD generalization",
                "arithmetic multiplication",
                "catastrophic forgetting",
                "interference",
                "separation",
                "sleep consolidation"
            ]
        },
        "publishedAt": "2025-05-13T12:37:54.000Z",
        "title": "Memorization-Compression Cycles Improve Generalization",
        "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08727.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d98ef7a4839890b25eb78b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.08665",
            "authors": [
                {
                    "_id": "68243bddd08d8e01109d5680",
                    "user": {
                        "_id": "622dc11fe27c88667db093fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
                        "isPro": false,
                        "fullname": "Edoardo Bianchi",
                        "user": "EdBianchi",
                        "type": "user"
                    },
                    "name": "Edoardo Bianchi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-14T06:45:06.471Z",
                    "hidden": false
                },
                {
                    "_id": "68243bddd08d8e01109d5681",
                    "user": {
                        "_id": "66f2ab691e8b23ab0af8436e",
                        "avatarUrl": "/avatars/161a26e9444a860128282e553a95641c.svg",
                        "isPro": false,
                        "fullname": "Antonio Liotta",
                        "user": "ucaclio",
                        "type": "user"
                    },
                    "name": "Antonio Liotta",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:11:54.811Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T15:27:24.000Z",
            "submittedOnDailyAt": "2025-05-14T05:16:45.606Z",
            "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
            "submittedOnDailyBy": {
                "_id": "622dc11fe27c88667db093fc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
                "isPro": false,
                "fullname": "Edoardo Bianchi",
                "user": "EdBianchi",
                "type": "user"
            },
            "summary": "Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.",
            "upvotes": 2,
            "discussionId": "68243bded08d8e01109d56cc",
            "ai_keywords": [
                "parameter-efficient architecture",
                "TimeSformer backbone",
                "CrossViewFusion module",
                "multi-head cross-attention",
                "learnable gating",
                "adaptive self-calibration",
                "Low-Rank Adaptation",
                "fine-tune",
                "multi-view settings",
                "structured tasks",
                "multi-view integration"
            ]
        },
        "publishedAt": "2025-05-13T11:27:24.000Z",
        "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation",
        "summary": "Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08665.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622dc11fe27c88667db093fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667052350862-622dc11fe27c88667db093fc.jpeg",
            "fullname": "Edoardo Bianchi",
            "name": "EdBianchi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2504.21475",
            "authors": [
                {
                    "_id": "68248d2c5d4294a3345b3889",
                    "user": {
                        "_id": "63fc7fe6d44f50f559587f93",
                        "avatarUrl": "/avatars/0066af2fb1399a842c2ecca9e95b65dd.svg",
                        "isPro": false,
                        "fullname": "Serry Sibaee",
                        "user": "SerrySibaee",
                        "type": "user"
                    },
                    "name": "Serry Sibaee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:54:34.701Z",
                    "hidden": false
                },
                {
                    "_id": "68248d2c5d4294a3345b388a",
                    "name": "Samar Ahmed",
                    "hidden": false
                },
                {
                    "_id": "68248d2c5d4294a3345b388b",
                    "user": {
                        "_id": "651452834a0c7cab164af487",
                        "avatarUrl": "/avatars/f254d57e9a5c391827889c49df5cbc0e.svg",
                        "isPro": false,
                        "fullname": "Abdullah Alharbi",
                        "user": "harbiai",
                        "type": "user"
                    },
                    "name": "Abdullah Al Harbi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:54:54.777Z",
                    "hidden": false
                },
                {
                    "_id": "68248d2c5d4294a3345b388c",
                    "name": "Omer Nacar",
                    "hidden": false
                },
                {
                    "_id": "68248d2c5d4294a3345b388d",
                    "user": {
                        "_id": "647c60cd36e109abce3b3b15",
                        "avatarUrl": "/avatars/d702ec97cc2cde5ca7b5d16feb5f45c9.svg",
                        "isPro": false,
                        "fullname": "Adel Ammar",
                        "user": "ammaradel",
                        "type": "user"
                    },
                    "name": "Adel Ammar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:55:04.658Z",
                    "hidden": false
                },
                {
                    "_id": "68248d2c5d4294a3345b388e",
                    "name": "Yasser Habashi",
                    "hidden": false
                },
                {
                    "_id": "68248d2c5d4294a3345b388f",
                    "user": {
                        "_id": "67bde4936c7c7de3a68608d1",
                        "avatarUrl": "/avatars/e08afba50d8aae8f7698ccba4c1e7b4a.svg",
                        "isPro": false,
                        "fullname": "Wadii Boulila",
                        "user": "wboulila",
                        "type": "user"
                    },
                    "name": "Wadii Boulila",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:55:18.105Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-04-30T09:56:36.000Z",
            "submittedOnDailyAt": "2025-05-14T11:02:37.522Z",
            "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic.",
            "upvotes": 2,
            "discussionId": "68248d2d5d4294a3345b38d8",
            "ai_keywords": [
                "transformer-based approach",
                "semi-encoder neural network architecture",
                "geometrically decreasing layers",
                "state-of-the-art",
                "formal quality standards",
                "pre-trained models",
                "Arabic-specific models",
                "general multilingual embeddings",
                "ARBERTv2",
                "formal abstraction",
                "modular, extensible Python library",
                "RDTL",
                "configurable training pipelines",
                "dataset quality",
                "high-quality reverse dictionary resources",
                "Arabic computational linguistics",
                "language learning",
                "academic writing",
                "professional communication"
            ]
        },
        "publishedAt": "2025-04-30T05:56:36.000Z",
        "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based\n  Approach with Dataset Construction Guidelines",
        "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21475.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 96
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.08712",
            "authors": [
                {
                    "_id": "682451c487e04e8c4ee5d13b",
                    "user": {
                        "_id": "66a347adb839c8994e6cb641",
                        "avatarUrl": "/avatars/efdeff32628b6c531109e047b45b2627.svg",
                        "isPro": false,
                        "fullname": "Wenzhe Cai",
                        "user": "WadeCai",
                        "type": "user"
                    },
                    "name": "Wenzhe Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:12:04.232Z",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d13c",
                    "name": "Jiaqi Peng",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d13d",
                    "user": {
                        "_id": "670bbd8541e624a441f76306",
                        "avatarUrl": "/avatars/b606cabd30f1374b1ffa82ff1b7e9ae6.svg",
                        "isPro": false,
                        "fullname": "yuqiang yang",
                        "user": "fulifuli666",
                        "type": "user"
                    },
                    "name": "Yuqiang Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:12:17.389Z",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d13e",
                    "name": "Yujian Zhang",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d13f",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d140",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d141",
                    "name": "Yilun Chen",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d142",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "682451c487e04e8c4ee5d143",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:13:19.562Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T16:20:28.000Z",
            "submittedOnDailyAt": "2025-05-14T06:48:31.831Z",
            "title": "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance",
            "submittedOnDailyBy": {
                "_id": "64e6d9d229a548f66aff6e5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
                "isPro": false,
                "fullname": "Tai Wang",
                "user": "taiwang",
                "type": "user"
            },
            "summary": "Learning navigation in dynamic open-world environments is an important yet\nchallenging skill for robots. Most previous methods rely on precise\nlocalization and mapping or learn from expensive real-world demonstrations. In\nthis paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end\nframework trained solely in simulation and can zero-shot transfer to different\nembodiments in diverse real-world environments. The key ingredient of NavDP's\nnetwork is the combination of diffusion-based trajectory generation and a\ncritic function for trajectory selection, which are conditioned on only local\nobservation tokens encoded from a shared policy transformer. Given the\nprivileged information of the global environment in simulation, we scale up the\ndemonstrations of good quality to train the diffusion policy and formulate the\ncritic value function targets with contrastive negative samples. Our\ndemonstration generation approach achieves about 2,500 trajectories/GPU per\nday, 20times more efficient than real-world data collection, and results in\na large-scale navigation dataset with 363.2km trajectories across 1244 scenes.\nTrained with this simulation dataset, NavDP achieves state-of-the-art\nperformance and consistently outstanding generalization capability on\nquadruped, wheeled, and humanoid robots in diverse indoor and outdoor\nenvironments. In addition, we present a preliminary attempt at using Gaussian\nSplatting to make in-domain real-to-sim fine-tuning to further bridge the\nsim-to-real gap. Experiments show that adding such real-to-sim data can improve\nthe success rate by 30\\% without hurting its generalization capability.",
            "upvotes": 1,
            "discussionId": "682451c787e04e8c4ee5d203",
            "ai_keywords": [
                "Navigation Diffusion Policy (NavDP)",
                "diffusion-based trajectory generation",
                "critic function",
                "local observation tokens",
                "policy transformer",
                "contrastive negative samples",
                "Gaussian Splatting"
            ]
        },
        "publishedAt": "2025-05-13T12:20:28.000Z",
        "title": "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged\n  Information Guidance",
        "summary": "Learning navigation in dynamic open-world environments is an important yet\nchallenging skill for robots. Most previous methods rely on precise\nlocalization and mapping or learn from expensive real-world demonstrations. In\nthis paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end\nframework trained solely in simulation and can zero-shot transfer to different\nembodiments in diverse real-world environments. The key ingredient of NavDP's\nnetwork is the combination of diffusion-based trajectory generation and a\ncritic function for trajectory selection, which are conditioned on only local\nobservation tokens encoded from a shared policy transformer. Given the\nprivileged information of the global environment in simulation, we scale up the\ndemonstrations of good quality to train the diffusion policy and formulate the\ncritic value function targets with contrastive negative samples. Our\ndemonstration generation approach achieves about 2,500 trajectories/GPU per\nday, 20times more efficient than real-world data collection, and results in\na large-scale navigation dataset with 363.2km trajectories across 1244 scenes.\nTrained with this simulation dataset, NavDP achieves state-of-the-art\nperformance and consistently outstanding generalization capability on\nquadruped, wheeled, and humanoid robots in diverse indoor and outdoor\nenvironments. In addition, we present a preliminary attempt at using Gaussian\nSplatting to make in-domain real-to-sim fine-tuning to further bridge the\nsim-to-real gap. Experiments show that adding such real-to-sim data can improve\nthe success rate by 30\\% without hurting its generalization capability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08712.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e6d9d229a548f66aff6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
            "fullname": "Tai Wang",
            "name": "taiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.08445",
            "authors": [
                {
                    "_id": "68248d74e7c1cd4c9b255e16",
                    "user": {
                        "_id": "647c60cd36e109abce3b3b15",
                        "avatarUrl": "/avatars/d702ec97cc2cde5ca7b5d16feb5f45c9.svg",
                        "isPro": false,
                        "fullname": "Adel Ammar",
                        "user": "ammaradel",
                        "type": "user"
                    },
                    "name": "Adel Ammar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:54:00.928Z",
                    "hidden": false
                },
                {
                    "_id": "68248d74e7c1cd4c9b255e17",
                    "user": {
                        "_id": "6458b291d1d994ec8105e2b7",
                        "avatarUrl": "/avatars/1031491368645c5abf5b20af74dc627b.svg",
                        "isPro": false,
                        "fullname": "Anis Koubaa",
                        "user": "akoubaa",
                        "type": "user"
                    },
                    "name": "Anis Koubaa",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:54:09.177Z",
                    "hidden": false
                },
                {
                    "_id": "68248d74e7c1cd4c9b255e18",
                    "name": "Omer Nacar",
                    "hidden": false
                },
                {
                    "_id": "68248d74e7c1cd4c9b255e19",
                    "user": {
                        "_id": "67bde4936c7c7de3a68608d1",
                        "avatarUrl": "/avatars/e08afba50d8aae8f7698ccba4c1e7b4a.svg",
                        "isPro": false,
                        "fullname": "Wadii Boulila",
                        "user": "wboulila",
                        "type": "user"
                    },
                    "name": "Wadii Boulila",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T12:54:21.036Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T11:13:27.000Z",
            "submittedOnDailyAt": "2025-05-14T11:03:04.302Z",
            "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "Large language models achieve high task performance yet often hallucinate or\nrely on outdated knowledge. Retrieval-augmented generation (RAG) addresses\nthese gaps by coupling generation with external search. We analyse how\nhyperparameters influence speed and quality in RAG systems, covering Chroma and\nFaiss vector stores, chunking policies, cross-encoder re-ranking, and\ntemperature, and we evaluate six metrics: faithfulness, answer correctness,\nanswer relevancy, context precision, context recall, and answer similarity.\nChroma processes queries 13% faster, whereas Faiss yields higher retrieval\nprecision, revealing a clear speed-accuracy trade-off. Naive fixed-length\nchunking with small windows and minimal overlap outperforms semantic\nsegmentation while remaining the quickest option. Re-ranking provides modest\ngains in retrieval quality yet increases runtime by roughly a factor of 5, so\nits usefulness depends on latency constraints. These results help practitioners\nbalance computational cost and accuracy when tuning RAG systems for\ntransparent, up-to-date responses. Finally, we re-evaluate the top\nconfigurations with a corrective RAG workflow and show that their advantages\npersist when the model can iteratively request additional evidence. We obtain a\nnear-perfect context precision (99%), which demonstrates that RAG systems can\nachieve extremely high retrieval accuracy with the right combination of\nhyperparameters, with significant implications for applications where retrieval\nquality directly impacts downstream task performance, such as clinical decision\nsupport in healthcare.",
            "upvotes": 1,
            "discussionId": "68248d78e7c1cd4c9b255f77",
            "ai_keywords": [
                "retrieval-augmented generation (RAG)",
                "vector stores",
                "chunking policies",
                "cross-encoder re-ranking",
                "temperature",
                "faithfulness",
                "answer correctness",
                "answer relevancy",
                "context precision",
                "context recall",
                "answer similarity",
                "Chroma",
                "Faiss",
                "semantic segmentation",
                "corrective RAG workflow"
            ]
        },
        "publishedAt": "2025-05-13T07:13:27.000Z",
        "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter\n  Impact on Performance and Efficiency",
        "summary": "Large language models achieve high task performance yet often hallucinate or\nrely on outdated knowledge. Retrieval-augmented generation (RAG) addresses\nthese gaps by coupling generation with external search. We analyse how\nhyperparameters influence speed and quality in RAG systems, covering Chroma and\nFaiss vector stores, chunking policies, cross-encoder re-ranking, and\ntemperature, and we evaluate six metrics: faithfulness, answer correctness,\nanswer relevancy, context precision, context recall, and answer similarity.\nChroma processes queries 13% faster, whereas Faiss yields higher retrieval\nprecision, revealing a clear speed-accuracy trade-off. Naive fixed-length\nchunking with small windows and minimal overlap outperforms semantic\nsegmentation while remaining the quickest option. Re-ranking provides modest\ngains in retrieval quality yet increases runtime by roughly a factor of 5, so\nits usefulness depends on latency constraints. These results help practitioners\nbalance computational cost and accuracy when tuning RAG systems for\ntransparent, up-to-date responses. Finally, we re-evaluate the top\nconfigurations with a corrective RAG workflow and show that their advantages\npersist when the model can iteratively request additional evidence. We obtain a\nnear-perfect context precision (99%), which demonstrates that RAG systems can\nachieve extremely high retrieval accuracy with the right combination of\nhyperparameters, with significant implications for applications where retrieval\nquality directly impacts downstream task performance, such as clinical decision\nsupport in healthcare.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08445.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 96
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07416",
            "authors": [
                {
                    "_id": "68238a5124c55c2bd5bec8b5",
                    "user": {
                        "_id": "68238b250a4767fd1572ce33",
                        "avatarUrl": "/avatars/7e824a30f9d07ed992633aba8ad11b6c.svg",
                        "isPro": false,
                        "fullname": "Truc Mai-Thanh Nguyen",
                        "user": "trucnguyen28",
                        "type": "user"
                    },
                    "name": "Truc Mai-Thanh Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-14T07:35:04.131Z",
                    "hidden": false
                },
                {
                    "_id": "68238a5124c55c2bd5bec8b6",
                    "name": "Dat Minh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68238a5124c55c2bd5bec8b7",
                    "user": {
                        "_id": "60bb728e29800c34660339e3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60bb728e29800c34660339e3/kIscETb7-lF5u2jHOJ-dR.png",
                        "isPro": false,
                        "fullname": "Son T. Luu ",
                        "user": "sonlam1102",
                        "type": "user"
                    },
                    "name": "Son T. Luu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-14T10:13:35.878Z",
                    "hidden": false
                },
                {
                    "_id": "68238a5124c55c2bd5bec8b8",
                    "name": "Kiet Van Nguyen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T10:11:28.000Z",
            "submittedOnDailyAt": "2025-05-14T06:09:57.940Z",
            "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
            "submittedOnDailyBy": {
                "_id": "68238b250a4767fd1572ce33",
                "avatarUrl": "/avatars/7e824a30f9d07ed992633aba8ad11b6c.svg",
                "isPro": false,
                "fullname": "Truc Mai-Thanh Nguyen",
                "user": "trucnguyen28",
                "type": "user"
            },
            "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP",
            "upvotes": 1,
            "discussionId": "68238a5324c55c2bd5bec921",
            "githubRepo": "https://github.com/trng28/ViMRHP"
        },
        "publishedAt": "2025-05-12T06:11:28.000Z",
        "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness\n  Prediction via Human-AI Collaborative Annotation",
        "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07416.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68238b250a4767fd1572ce33",
            "avatarUrl": "/avatars/7e824a30f9d07ed992633aba8ad11b6c.svg",
            "fullname": "Truc Mai-Thanh Nguyen",
            "name": "trucnguyen28",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09027",
            "authors": [
                {
                    "_id": "6825421166de88551db7dab6",
                    "name": "Yi Cui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T23:47:12.000Z",
            "submittedOnDailyAt": "2025-05-14T23:53:51.417Z",
            "title": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code\n  Generation",
            "submittedOnDailyBy": {
                "_id": "669dbd709a4bf63e08f1ddc2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669dbd709a4bf63e08f1ddc2/aV10ZJPPzH5LbnHFZNqc7.png",
                "isPro": false,
                "fullname": "Yi Cui",
                "user": "onekq",
                "type": "user"
            },
            "summary": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios.",
            "upvotes": 0,
            "discussionId": "6825421266de88551db7dafd",
            "ai_keywords": [
                "large language models (LLMs)",
                "test-driven development (TDD)",
                "test cases",
                "prompt",
                "verification",
                "code generation",
                "natural language prompts",
                "functionality",
                "software development practices",
                "compact, functional code",
                "context length",
                "multi-feature complexity",
                "instruction following",
                "in-context learning",
                "general coding proficiency",
                "pretraining knowledge",
                "performance bottlenecks",
                "instruction loss",
                "error analysis"
            ]
        },
        "publishedAt": "2025-05-13T19:47:12.000Z",
        "title": "Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code\n  Generation",
        "summary": "We introduce WebApp1K, a novel benchmark for evaluating large language models\n(LLMs) in test-driven development (TDD) tasks, where test cases serve as both\nprompt and verification for code generation. Unlike traditional approaches\nrelying on natural language prompts, our benchmark emphasizes the ability of\nLLMs to interpret and implement functionality directly from test cases,\nreflecting real-world software development practices. Comprising 1000 diverse\nchallenges across 20 application domains, the benchmark evaluates LLMs on their\nability to generate compact, functional code under the constraints of context\nlength and multi-feature complexity. Our findings highlight instruction\nfollowing and in-context learning as critical capabilities for TDD success,\nsurpassing the importance of general coding proficiency or pretraining\nknowledge. Through comprehensive evaluation of 19 frontier models, we reveal\nperformance bottlenecks, such as instruction loss in long prompts, and provide\na detailed error analysis spanning multiple root causes. This work underscores\nthe practical value of TDD-specific benchmarks and lays the foundation for\nadvancing LLM capabilities in rigorous, application-driven coding scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09027.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "669dbd709a4bf63e08f1ddc2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669dbd709a4bf63e08f1ddc2/aV10ZJPPzH5LbnHFZNqc7.png",
            "fullname": "Yi Cui",
            "name": "onekq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    }
]