[
    {
        "paper": {
            "id": "2505.10554",
            "authors": [
                {
                    "_id": "6826a569ea77771e3880f793",
                    "user": {
                        "_id": "64351475901c5734bcb64248",
                        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan Hu",
                        "user": "zhiyuanhucs",
                        "type": "user"
                    },
                    "name": "Zhiyuan Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:02:17.850Z",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f794",
                    "name": "Yibo Wang",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f795",
                    "user": {
                        "_id": "63a3ff69f91ad3ea5703841d",
                        "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
                        "isPro": false,
                        "fullname": "Hanze Dong",
                        "user": "hendrydong",
                        "type": "user"
                    },
                    "name": "Hanze Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:08:50.665Z",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f796",
                    "user": {
                        "_id": "6602869253a0518b2a98cafd",
                        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
                        "isPro": false,
                        "fullname": "Yuhui Xu",
                        "user": "yuhuixu",
                        "type": "user"
                    },
                    "name": "Yuhui Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:09:07.114Z",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f797",
                    "user": {
                        "_id": "6461c2905dba83471db3be53",
                        "avatarUrl": "/avatars/6e36cf86201d590ac729a75d4a439cde.svg",
                        "isPro": false,
                        "fullname": "Amrita Saha",
                        "user": "amritasaha87",
                        "type": "user"
                    },
                    "name": "Amrita Saha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:09:22.879Z",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f798",
                    "user": {
                        "_id": "649dbcc4e0fff1ed099dc80a",
                        "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
                        "isPro": false,
                        "fullname": "Caiming Xiong",
                        "user": "cxiong",
                        "type": "user"
                    },
                    "name": "Caiming Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:09:29.160Z",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f799",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:09:35.640Z",
                    "hidden": false
                },
                {
                    "_id": "6826a569ea77771e3880f79a",
                    "user": {
                        "_id": "61f9d3b54ac99e8a1bae85f4",
                        "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
                        "isPro": false,
                        "fullname": "JunnanLi",
                        "user": "JunnanLi",
                        "type": "user"
                    },
                    "name": "Junnan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:09:57.841Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:58:33.000Z",
            "submittedOnDailyAt": "2025-05-16T01:09:52.437Z",
            "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "64351475901c5734bcb64248",
                "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
                "isPro": false,
                "fullname": "Zhiyuan Hu",
                "user": "zhiyuanhucs",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
            "upvotes": 74,
            "discussionId": "6826a56aea77771e3880f7c8",
            "githubRepo": "https://github.com/zhiyuanhubj/Meta-Ability-Alignment",
            "ai_keywords": [
                "large reasoning models",
                "long chain-of-thought reasoning",
                "outcome-based reinforcement learning",
                "RL",
                "self-correction",
                "backtracking",
                "verification phenomena",
                "aha moment",
                "meta-abilities",
                "deduction",
                "induction",
                "abduction",
                "automatically generated tasks",
                "self-verifiable tasks",
                "parameter-space merging",
                "domain-specific reinforcement learning",
                "performance ceiling",
                "math benchmarks",
                "coding benchmarks",
                "science benchmarks",
                "Meta-Ability-Alignment"
            ]
        },
        "publishedAt": "2025-05-15T13:58:33.000Z",
        "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
        "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10554.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64351475901c5734bcb64248",
            "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
            "fullname": "Zhiyuan Hu",
            "name": "zhiyuanhucs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09666",
            "authors": [
                {
                    "_id": "68269a1eaa8aded616d280a0",
                    "user": {
                        "_id": "64cfa0b9749587dbe01d0079",
                        "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
                        "isPro": false,
                        "fullname": "Yumin Choi",
                        "user": "YuminChoi",
                        "type": "user"
                    },
                    "name": "Yumin Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T07:12:06.309Z",
                    "hidden": false
                },
                {
                    "_id": "68269a1eaa8aded616d280a1",
                    "user": {
                        "_id": "63036b6c5c70c21d0ea79d48",
                        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                        "isPro": false,
                        "fullname": "Jinheon Baek",
                        "user": "jinheon",
                        "type": "user"
                    },
                    "name": "Jinheon Baek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:10:10.549Z",
                    "hidden": false
                },
                {
                    "_id": "68269a1eaa8aded616d280a2",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T16:46:15.000Z",
            "submittedOnDailyAt": "2025-05-16T01:05:44.315Z",
            "title": "System Prompt Optimization with Meta-Learning",
            "submittedOnDailyBy": {
                "_id": "63036b6c5c70c21d0ea79d48",
                "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                "isPro": false,
                "fullname": "Jinheon Baek",
                "user": "jinheon",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
            "upvotes": 48,
            "discussionId": "68269a1eaa8aded616d280d1",
            "githubRepo": "https://github.com/Dozi01/MetaSPO",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "bilevel system prompt optimization",
                "meta-learning framework",
                "system prompts",
                "user prompts",
                "unseen datasets",
                "domains",
                "rapid adaptation",
                "test-time user prompts"
            ]
        },
        "publishedAt": "2025-05-14T12:46:15.000Z",
        "title": "System Prompt Optimization with Meta-Learning",
        "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09666.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "fullname": "Jinheon Baek",
            "name": "jinheon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09723",
            "authors": [
                {
                    "_id": "6826b00c251d26fc0cd035cc",
                    "user": {
                        "_id": "63c20105726f62e411fbe882",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
                        "isPro": false,
                        "fullname": "Yuxin Jiang",
                        "user": "YuxinJiang",
                        "type": "user"
                    },
                    "name": "Yuxin Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:10:30.068Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035cd",
                    "user": {
                        "_id": "6575f9aeca03b6c514fe6e5c",
                        "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
                        "isPro": false,
                        "fullname": "Shengcong Chen",
                        "user": "Shengcong",
                        "type": "user"
                    },
                    "name": "Shengcong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:10:37.358Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035ce",
                    "user": {
                        "_id": "63c7a33121bd95f80ed74652",
                        "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
                        "isPro": false,
                        "fullname": "Siyuan Huang",
                        "user": "thuhsy",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:10:50.217Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035cf",
                    "user": {
                        "_id": "640b00555a9c21b95c6449b3",
                        "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
                        "isPro": false,
                        "fullname": "Liliang Chen",
                        "user": "pathcn",
                        "type": "user"
                    },
                    "name": "Liliang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:10:55.980Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d0",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d1",
                    "name": "Yue Liao",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d2",
                    "name": "Xindong He",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d3",
                    "name": "Chiming Liu",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d4",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:11:29.857Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d5",
                    "user": {
                        "_id": "67739bfa64e8b7438ae68eb4",
                        "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
                        "isPro": false,
                        "fullname": "Maoqing Yao",
                        "user": "AutobotZero",
                        "type": "user"
                    },
                    "name": "Maoqing Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:11:36.940Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00c251d26fc0cd035d6",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:11:44.407Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
            ],
            "publishedAt": "2025-05-14T18:30:53.000Z",
            "submittedOnDailyAt": "2025-05-16T02:11:01.174Z",
            "title": "EnerVerse-AC: Envisioning Embodied Environments with Action Condition",
            "submittedOnDailyBy": {
                "_id": "634e4120038b5879133552f5",
                "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                "isPro": true,
                "fullname": "Siyuan",
                "user": "SiyuanH",
                "type": "user"
            },
            "summary": "Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.",
            "upvotes": 18,
            "discussionId": "6826b013251d26fc0cd037ba",
            "githubRepo": "https://github.com/AgibotTech/EnerVerse-AC",
            "ai_keywords": [
                "action-conditional world model",
                "future visual observations",
                "multi-level action-conditioning mechanism",
                "ray map encoding",
                "dynamic multi-view image generation",
                "diverse failure trajectories",
                "data engine",
                "evaluator",
                "human-collected trajectories",
                "diverse datasets",
                "action-conditioned video observations",
                "robotic manipulation evaluation"
            ]
        },
        "publishedAt": "2025-05-14T14:30:53.000Z",
        "title": "EnerVerse-AC: Envisioning Embodied Environments with Action Condition",
        "summary": "Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09723.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634e4120038b5879133552f5",
            "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
            "fullname": "Siyuan",
            "name": "SiyuanH",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10475",
            "authors": [
                {
                    "_id": "68271b1e228dc6f5fd90b10f",
                    "user": {
                        "_id": "66d58df54b87a685ccb8e4a0",
                        "avatarUrl": "/avatars/2566c20d79088ba761215b9a0197cb8e.svg",
                        "isPro": false,
                        "fullname": "Mouxiang Chen",
                        "user": "chenmouxiang",
                        "type": "user"
                    },
                    "name": "Mouxiang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:23:18.035Z",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b110",
                    "user": {
                        "_id": "61e4c4ca1ab24785ac11ba69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
                        "isPro": false,
                        "fullname": "Binyuan Hui",
                        "user": "huybery",
                        "type": "user"
                    },
                    "name": "Binyuan Hui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:11:11.082Z",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b111",
                    "user": {
                        "_id": "672c25ca8cfb61188128eb6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
                        "isPro": false,
                        "fullname": "Zeyu Cui",
                        "user": "misakamage",
                        "type": "user"
                    },
                    "name": "Zeyu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:11:59.338Z",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b112",
                    "name": "Jiaxi Yang",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b113",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:11:40.391Z",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b114",
                    "name": "Jianling Sun",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b115",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:11:27.444Z",
                    "hidden": false
                },
                {
                    "_id": "68271b1e228dc6f5fd90b116",
                    "name": "Zhongxin Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T16:24:45.000Z",
            "submittedOnDailyAt": "2025-05-16T09:31:59.839Z",
            "title": "Parallel Scaling Law for Language Models",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply P diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the P outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with P parallel\nstreams is similar to scaling the parameters by O(log P) while showing\nsuperior inference efficiency. For example, ParScale can use up to 22times\nless memory increase and 6times less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.",
            "upvotes": 17,
            "discussionId": "68271b1f228dc6f5fd90b18d",
            "githubRepo": "https://github.com/QwenLM/ParScale",
            "ai_keywords": [
                "parallel computation",
                "parallel scaling (ParScale)",
                "scaling law",
                "forward passes",
                "dynamic aggregation",
                "off-the-shelf pre-trained model",
                "post-training"
            ]
        },
        "publishedAt": "2025-05-15T12:24:45.000Z",
        "title": "Parallel Scaling Law for Language Models",
        "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply P diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the P outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with P parallel\nstreams is similar to scaling the parameters by O(log P) while showing\nsuperior inference efficiency. For example, ParScale can use up to 22times\nless memory increase and 6times less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10475.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 865
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10185",
            "authors": [
                {
                    "_id": "68269f67a47cb2b87646b98c",
                    "user": {
                        "_id": "6550c4f27bbfce1878f5f280",
                        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
                        "isPro": false,
                        "fullname": "seongyun_lee",
                        "user": "Seongyun",
                        "type": "user"
                    },
                    "name": "Seongyun Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:14:03.612Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b98d",
                    "user": {
                        "_id": "6469949654873f0043b09c22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
                        "isPro": false,
                        "fullname": "Seungone Kim",
                        "user": "seungone",
                        "type": "user"
                    },
                    "name": "Seungone Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:14:19.025Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b98e",
                    "user": {
                        "_id": "66f10ac605182775917d8c5a",
                        "avatarUrl": "/avatars/21b21284d0a5a95413f91dde9dda346c.svg",
                        "isPro": false,
                        "fullname": "Minju Seo",
                        "user": "Minju2136",
                        "type": "user"
                    },
                    "name": "Minju Seo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:14:25.204Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b98f",
                    "user": {
                        "_id": "649e06313e5e7504763dfe03",
                        "avatarUrl": "/avatars/1c4d19de5f2950d3342480c4b3e01047.svg",
                        "isPro": false,
                        "fullname": "Yongrae Jo",
                        "user": "dreamgonfly",
                        "type": "user"
                    },
                    "name": "Yongrae Jo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:14:30.806Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b990",
                    "name": "Dongyoung Go",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b991",
                    "user": {
                        "_id": "647eaaf61a1fcad2fdc5d1ef",
                        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
                        "isPro": false,
                        "fullname": "Hyeonbin Hwang ",
                        "user": "hbin0701",
                        "type": "user"
                    },
                    "name": "Hyeonbin Hwang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:14:44.935Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b992",
                    "user": {
                        "_id": "638467ee8283412d401770dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638467ee8283412d401770dd/UAMwDHhSwf91XSsubfrS_.jpeg",
                        "isPro": false,
                        "fullname": "Jinho Park",
                        "user": "Br3ad",
                        "type": "user"
                    },
                    "name": "Jinho Park",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:14:51.443Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b993",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b994",
                    "user": {
                        "_id": "63e3f3c59db5da2dc1ef6889",
                        "avatarUrl": "/avatars/f7546f57a5fd69bc99ff1640cc4a4853.svg",
                        "isPro": false,
                        "fullname": "Sean Welleck",
                        "user": "wellecks",
                        "type": "user"
                    },
                    "name": "Sean Welleck",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:15:17.990Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b995",
                    "user": {
                        "_id": "60de14638bedd2315529d43f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625166923504-noauth.png",
                        "isPro": false,
                        "fullname": "Graham Neubig",
                        "user": "gneubig",
                        "type": "user"
                    },
                    "name": "Graham Neubig",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:15:24.404Z",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b996",
                    "name": "Moontae Lee",
                    "hidden": false
                },
                {
                    "_id": "68269f67a47cb2b87646b997",
                    "user": {
                        "_id": "621f05ba970615ad5861ceb1",
                        "avatarUrl": "/avatars/7e1902aa71369a524afda9b0a9e88e22.svg",
                        "isPro": false,
                        "fullname": "Minjoon Seo",
                        "user": "minjoon",
                        "type": "user"
                    },
                    "name": "Minjoon Seo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:15:33.521Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T11:31:02.000Z",
            "submittedOnDailyAt": "2025-05-16T00:44:19.223Z",
            "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
            "submittedOnDailyBy": {
                "_id": "6550c4f27bbfce1878f5f280",
                "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
                "isPro": false,
                "fullname": "seongyun_lee",
                "user": "Seongyun",
                "type": "user"
            },
            "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
            "upvotes": 16,
            "discussionId": "68269f68a47cb2b87646b9ed",
            "ai_keywords": [
                "long chain-of-thought (CoT)",
                "large language models",
                "reasoning strategies",
                "predefined strategy types",
                "CoT Encyclopedia",
                "bottom-up framework",
                "reasoning criteria",
                "semantic space",
                "contrastive rubrics",
                "reasoning behavior",
                "interpretability",
                "performance gains"
            ]
        },
        "publishedAt": "2025-05-15T07:31:02.000Z",
        "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
        "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10185.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6550c4f27bbfce1878f5f280",
            "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
            "fullname": "seongyun_lee",
            "name": "Seongyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09694",
            "authors": [
                {
                    "_id": "6826ae4611765454f5757d7c",
                    "name": "Hu Yue",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d7d",
                    "user": {
                        "_id": "63c7a33121bd95f80ed74652",
                        "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
                        "isPro": false,
                        "fullname": "Siyuan Huang",
                        "user": "thuhsy",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:16:33.829Z",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d7e",
                    "name": "Yue Liao",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d7f",
                    "user": {
                        "_id": "6575f9aeca03b6c514fe6e5c",
                        "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
                        "isPro": false,
                        "fullname": "Shengcong Chen",
                        "user": "Shengcong",
                        "type": "user"
                    },
                    "name": "Shengcong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:16:15.416Z",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d80",
                    "name": "Pengfei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d81",
                    "user": {
                        "_id": "640b00555a9c21b95c6449b3",
                        "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
                        "isPro": false,
                        "fullname": "Liliang Chen",
                        "user": "pathcn",
                        "type": "user"
                    },
                    "name": "Liliang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:15:58.407Z",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d82",
                    "user": {
                        "_id": "67739bfa64e8b7438ae68eb4",
                        "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
                        "isPro": false,
                        "fullname": "Maoqing Yao",
                        "user": "AutobotZero",
                        "type": "user"
                    },
                    "name": "Maoqing Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:15:51.039Z",
                    "hidden": false
                },
                {
                    "_id": "6826ae4611765454f5757d83",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:15:44.660Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T18:00:19.000Z",
            "submittedOnDailyAt": "2025-05-16T01:55:39.761Z",
            "title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models",
            "submittedOnDailyBy": {
                "_id": "634e4120038b5879133552f5",
                "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                "isPro": true,
                "fullname": "Siyuan",
                "user": "SiyuanH",
                "type": "user"
            },
            "summary": "Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.",
            "upvotes": 15,
            "discussionId": "6826ae4911765454f5757e32",
            "ai_keywords": [
                "text-to-video diffusion models",
                "embodied world models",
                "physically plausible scenes",
                "language commands",
                "perceptual metrics",
                "visual scene consistency",
                "motion correctness",
                "semantic alignment",
                "Embodied World Model Benchmark (EWMBench)",
                "multi-dimensional evaluation toolkit",
                "video generation models"
            ]
        },
        "publishedAt": "2025-05-14T14:00:19.000Z",
        "title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models",
        "summary": "Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634e4120038b5879133552f5",
            "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
            "fullname": "Siyuan",
            "name": "SiyuanH",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10562",
            "authors": [
                {
                    "_id": "6826a5d8154611642ada50da",
                    "user": {
                        "_id": "656428b5462e5ebcbf537d4e",
                        "avatarUrl": "/avatars/cbedecc9c6f2afee2ca6b72efb593561.svg",
                        "isPro": false,
                        "fullname": "Wenxuan Wang",
                        "user": "Rookielion",
                        "type": "user"
                    },
                    "name": "Wenxuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:24:09.061Z",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50db",
                    "user": {
                        "_id": "640ed40dc025ddf618950af7",
                        "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
                        "isPro": false,
                        "fullname": "Fan Zhang",
                        "user": "ryanzhangfan",
                        "type": "user"
                    },
                    "name": "Fan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T07:12:04.101Z",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50dc",
                    "user": {
                        "_id": "648683de623b5f050213f2be",
                        "avatarUrl": "/avatars/83ecbcf4a21f68d2893de79f0444d6e3.svg",
                        "isPro": false,
                        "fullname": "Yufeng Cui",
                        "user": "YufengCui",
                        "type": "user"
                    },
                    "name": "Yufeng Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:16:58.537Z",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50dd",
                    "user": {
                        "_id": "64b4a717aa03b6520839e9b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
                        "isPro": false,
                        "fullname": "Haiwen Diao",
                        "user": "Paranioar",
                        "type": "user"
                    },
                    "name": "Haiwen Diao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:17:04.879Z",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50de",
                    "user": {
                        "_id": "657187de7644d1128571495e",
                        "avatarUrl": "/avatars/89412c94fd6136f6680055551de3ddc4.svg",
                        "isPro": false,
                        "fullname": "Zhuoyan Luo",
                        "user": "RobertLuo1",
                        "type": "user"
                    },
                    "name": "Zhuoyan Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:17:11.336Z",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50df",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50e0",
                    "name": "Jing Liu",
                    "hidden": false
                },
                {
                    "_id": "6826a5d8154611642ada50e1",
                    "user": {
                        "_id": "63ca558304c979828311c5a5",
                        "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
                        "isPro": false,
                        "fullname": "Xinlong Wang",
                        "user": "xinlongwang",
                        "type": "user"
                    },
                    "name": "Xinlong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:17:32.558Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
            ],
            "publishedAt": "2025-05-15T17:59:39.000Z",
            "submittedOnDailyAt": "2025-05-16T01:26:23.739Z",
            "title": "End-to-End Vision Tokenizer Tuning",
            "submittedOnDailyBy": {
                "_id": "640ed40dc025ddf618950af7",
                "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
                "isPro": false,
                "fullname": "Fan Zhang",
                "user": "ryanzhangfan",
                "type": "user"
            },
            "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
            "upvotes": 13,
            "discussionId": "6826a5d9154611642ada5122",
            "ai_keywords": [
                "vision tokenization",
                "end-to-end vision tokenizer tuning (ETT)",
                "autoregressive tasks",
                "visual embeddings",
                "tokenizer codebook",
                "multimodal understanding",
                "visual generation tasks"
            ]
        },
        "publishedAt": "2025-05-15T13:59:39.000Z",
        "title": "End-to-End Vision Tokenizer Tuning",
        "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10562.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640ed40dc025ddf618950af7",
            "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
            "fullname": "Fan Zhang",
            "name": "ryanzhangfan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10527",
            "authors": [
                {
                    "_id": "682699da19c4a596dbcea4f5",
                    "user": {
                        "_id": "642127bbe40f66bcd1e7e047",
                        "avatarUrl": "/avatars/8da3b0ae9723af8d96906c7307e90bb8.svg",
                        "isPro": false,
                        "fullname": "wang binghai",
                        "user": "refrain-wbh",
                        "type": "user"
                    },
                    "name": "Binghai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:24:15.616Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4f6",
                    "name": "Runji Lin",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4f7",
                    "user": {
                        "_id": "6453fa96ed6d7fede94408e0",
                        "avatarUrl": "/avatars/e8c9025ef24cec958c87a1008bb54fd7.svg",
                        "isPro": false,
                        "fullname": "Keming Lu",
                        "user": "keminglu",
                        "type": "user"
                    },
                    "name": "Keming Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:40:18.414Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4f8",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4f9",
                    "user": {
                        "_id": "64704e973601bb7b06643e98",
                        "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
                        "isPro": false,
                        "fullname": "Zhenru Zhang",
                        "user": "Zhenru",
                        "type": "user"
                    },
                    "name": "Zhenru Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:40:11.604Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4fa",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4fb",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:24:17.629Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4fc",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4fd",
                    "name": "Yang Fan",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4fe",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea4ff",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea500",
                    "user": {
                        "_id": "61e4c4ca1ab24785ac11ba69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
                        "isPro": false,
                        "fullname": "Binyuan Hui",
                        "user": "huybery",
                        "type": "user"
                    },
                    "name": "Binyuan Hui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:39:55.653Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea501",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:39:49.138Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea502",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea503",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea504",
                    "user": {
                        "_id": "67f9c4ee171948c38302ae0f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Cqb3ijr_sZkpLhEEEEybK.png",
                        "isPro": false,
                        "fullname": "Xuanjing Huang",
                        "user": "xjhuang",
                        "type": "user"
                    },
                    "name": "Xuanjing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:39:38.400Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea505",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea506",
                    "user": {
                        "_id": "6583ab7983a9e1460c67d876",
                        "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
                        "isPro": false,
                        "fullname": "bowen",
                        "user": "bowenYu",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:39:20.178Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea507",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:39:13.182Z",
                    "hidden": false
                },
                {
                    "_id": "682699da19c4a596dbcea508",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:39:02.362Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:38:37.000Z",
            "submittedOnDailyAt": "2025-05-16T00:22:55.977Z",
            "title": "WorldPM: Scaling Human Preference Modeling",
            "submittedOnDailyBy": {
                "_id": "63d9d68c1cae35c27bf7a6a7",
                "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
                "isPro": false,
                "fullname": "Bowen Yu",
                "user": "Tigerph",
                "type": "user"
            },
            "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
            "upvotes": 13,
            "discussionId": "682699dd19c4a596dbcea604",
            "ai_keywords": [
                "preference modeling",
                "World Preference Modeling (WorldPM)",
                "World Preference",
                "unified representation",
                "human preferences",
                "preference data",
                "public forums",
                "user communities",
                "extensive training",
                "15M-scale data",
                "parameter-efficient fine-tuning",
                "Adversarial metrics",
                "deceptive features",
                "Objective metrics",
                "Subjective metrics",
                "preference fine-tuning",
                "generalization performance",
                "human preference datasets",
                "RLHF (Reinforcement Learning from Human Feedback)",
                "in-house evaluations",
                "public evaluation sets"
            ]
        },
        "publishedAt": "2025-05-15T13:38:37.000Z",
        "title": "WorldPM: Scaling Human Preference Modeling",
        "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10527.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d9d68c1cae35c27bf7a6a7",
            "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
            "fullname": "Bowen Yu",
            "name": "Tigerph",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07782",
            "authors": [
                {
                    "_id": "6822b3c8c10ac9c466c63e01",
                    "user": {
                        "_id": "6466e31a14e059dde8bbe4be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
                        "isPro": true,
                        "fullname": "Rushi Qiang",
                        "user": "Jerrycool",
                        "type": "user"
                    },
                    "name": "Rushi Qiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-14T07:35:59.704Z",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e02",
                    "user": {
                        "_id": "6471bddd609ae9f56368f132",
                        "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
                        "isPro": true,
                        "fullname": "Yuchen Zhuang",
                        "user": "yczhuang",
                        "type": "user"
                    },
                    "name": "Yuchen Zhuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:17:45.935Z",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e03",
                    "user": {
                        "_id": "68198e34612ca40b67abbf18",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Gj9eV6k3CcKJinTePC2CV.png",
                        "isPro": false,
                        "fullname": "Yinghao Li",
                        "user": "yinghaoli-yh",
                        "type": "user"
                    },
                    "name": "Yinghao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:17:54.487Z",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e04",
                    "name": "Dingu Sagar V K",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e05",
                    "user": {
                        "_id": "644a2c4e9a1c5faef7a5dbd8",
                        "avatarUrl": "/avatars/fbbbc1347f8e423b2477e2506fdb43d9.svg",
                        "isPro": false,
                        "fullname": "Rongzhi Zhang",
                        "user": "Solute",
                        "type": "user"
                    },
                    "name": "Rongzhi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T07:12:47.628Z",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e06",
                    "name": "Changhao Li",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e07",
                    "name": "Ian Shu-Hei Wong",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e08",
                    "name": "Sherry Yang",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e09",
                    "user": {
                        "_id": "6409651b9e9f790c905b2335",
                        "avatarUrl": "/avatars/1fb8c80b60f21f65a0a027319101f236.svg",
                        "isPro": false,
                        "fullname": "Percy Liang",
                        "user": "percyliang",
                        "type": "user"
                    },
                    "name": "Percy Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:18:20.107Z",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e0a",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6822b3c8c10ac9c466c63e0b",
                    "user": {
                        "_id": "635f93577c05eb9f59966209",
                        "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
                        "isPro": false,
                        "fullname": "Intelligent Digital Creation",
                        "user": "BoDai",
                        "type": "user"
                    },
                    "name": "Bo Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T08:18:42.045Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T17:35:43.000Z",
            "submittedOnDailyAt": "2025-05-16T00:26:20.796Z",
            "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
            "submittedOnDailyBy": {
                "_id": "6466e31a14e059dde8bbe4be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
                "isPro": true,
                "fullname": "Rushi Qiang",
                "user": "Jerrycool",
                "type": "user"
            },
            "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
            "upvotes": 12,
            "discussionId": "6822b3c9c10ac9c466c63e8a",
            "projectPage": "https://mle-dojo.github.io/MLE-Dojo-page/",
            "githubRepo": "https://github.com/MLE-Dojo/MLE-Dojo",
            "ai_keywords": [
                "reinforcement learning",
                "autonomous large language model (LLM) agents",
                "iterative machine learning engineering (MLE) workflows",
                "Kaggle challenges",
                "MLE tasks",
                "data processing",
                "architecture search",
                "hyperparameter tuning",
                "code debugging",
                "supervised fine-tuning",
                "model-based agent tuning"
            ]
        },
        "publishedAt": "2025-05-12T13:35:43.000Z",
        "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
        "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "fullname": "Rushi Qiang",
            "name": "Jerrycool",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06027",
            "authors": [
                {
                    "_id": "68270512001465610c496a53",
                    "user": {
                        "_id": "63a852ec353e10031a8be92e",
                        "avatarUrl": "/avatars/42ff79115c7b84fa00e7c5df373dc77d.svg",
                        "isPro": false,
                        "fullname": "Stefan Petkov Vasilev",
                        "user": "stefanvasilev",
                        "type": "user"
                    },
                    "name": "Stefan Vasilev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:23:20.122Z",
                    "hidden": false
                },
                {
                    "_id": "68270512001465610c496a54",
                    "name": "Christian Herold",
                    "hidden": false
                },
                {
                    "_id": "68270512001465610c496a55",
                    "name": "Baohao Liao",
                    "hidden": false
                },
                {
                    "_id": "68270512001465610c496a56",
                    "name": "Seyyed Hadi Hashemi",
                    "hidden": false
                },
                {
                    "_id": "68270512001465610c496a57",
                    "name": "Shahram Khadivi",
                    "hidden": false
                },
                {
                    "_id": "68270512001465610c496a58",
                    "name": "Christof Monz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-09T13:19:09.000Z",
            "submittedOnDailyAt": "2025-05-16T13:26:11.675Z",
            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation",
            "submittedOnDailyBy": {
                "_id": "63a852ec353e10031a8be92e",
                "avatarUrl": "/avatars/42ff79115c7b84fa00e7c5df373dc77d.svg",
                "isPro": false,
                "fullname": "Stefan Petkov Vasilev",
                "user": "stefanvasilev",
                "type": "user"
            },
            "summary": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning.",
            "upvotes": 11,
            "discussionId": "68270513001465610c496a83",
            "ai_keywords": [
                "self-distillation",
                "machine unlearning",
                "Large Language Models",
                "target logits",
                "self-distillation targets",
                "golden targets",
                "NPO",
                "UnDIAL"
            ]
        },
        "publishedAt": "2025-05-09T09:19:09.000Z",
        "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation",
        "summary": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06027.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a852ec353e10031a8be92e",
            "avatarUrl": "/avatars/42ff79115c7b84fa00e7c5df373dc77d.svg",
            "fullname": "Stefan Petkov Vasilev",
            "name": "stefanvasilev",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10320",
            "authors": [
                {
                    "_id": "6826a180cad9000ebc70f038",
                    "name": "Chenxi Whitehouse",
                    "hidden": false
                },
                {
                    "_id": "6826a180cad9000ebc70f039",
                    "user": {
                        "_id": "63f6a0e6b4c9a104f4b95d86",
                        "avatarUrl": "/avatars/660c3e44bea976f3e0560bf95466b03c.svg",
                        "isPro": false,
                        "fullname": "Tianlu Wang",
                        "user": "Tianlu",
                        "type": "user"
                    },
                    "name": "Tianlu Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:42:23.084Z",
                    "hidden": false
                },
                {
                    "_id": "6826a180cad9000ebc70f03a",
                    "name": "Ping Yu",
                    "hidden": false
                },
                {
                    "_id": "6826a180cad9000ebc70f03b",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "6826a180cad9000ebc70f03c",
                    "user": {
                        "_id": "62f023a36a027498eaa2f9cc",
                        "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                        "isPro": false,
                        "fullname": "Jason Weston",
                        "user": "spermwhale",
                        "type": "user"
                    },
                    "name": "Jason Weston",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:42:55.728Z",
                    "hidden": false
                },
                {
                    "_id": "6826a180cad9000ebc70f03d",
                    "user": {
                        "_id": "65d645913bc37244c55c39d5",
                        "avatarUrl": "/avatars/508372876a654957650ee9429886d10c.svg",
                        "isPro": false,
                        "fullname": "ilia kulikov",
                        "user": "uralik",
                        "type": "user"
                    },
                    "name": "Ilia Kulikov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T13:43:03.749Z",
                    "hidden": false
                },
                {
                    "_id": "6826a180cad9000ebc70f03e",
                    "user": {
                        "_id": "64b75f4b037d6452a30f71aa",
                        "avatarUrl": "/avatars/5a0322e7ecda05164e45526d605e3619.svg",
                        "isPro": false,
                        "fullname": "Swarnadeep Saha",
                        "user": "swarna92",
                        "type": "user"
                    },
                    "name": "Swarnadeep Saha",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-16T02:22:57.318Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T14:05:15.000Z",
            "submittedOnDailyAt": "2025-05-16T00:58:47.493Z",
            "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64b6feee17681d64b19b112b",
                "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
                "isPro": false,
                "fullname": "Swarnadeep Saha",
                "user": "swarnaNLP",
                "type": "user"
            },
            "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
            "upvotes": 10,
            "discussionId": "6826a181cad9000ebc70f0a3",
            "ai_keywords": [
                "reinforcement learning",
                "chain-of-thought reasoning",
                "judgment tasks",
                "judgment bias",
                "Pairwise-J1",
                "Pointwise-J1",
                "offline training",
                "online training",
                "reward strategies",
                "seed prompts",
                "evaluation criteria",
                "reference answers",
                "correctness of model responses"
            ]
        },
        "publishedAt": "2025-05-15T10:05:15.000Z",
        "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
        "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10320.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b6feee17681d64b19b112b",
            "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
            "fullname": "Swarnadeep Saha",
            "name": "swarnaNLP",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.09990",
            "authors": [
                {
                    "_id": "6826dce4682e62074d0ed686",
                    "name": "Long Cheng",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed687",
                    "name": "Jiafei Duan",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed688",
                    "user": {
                        "_id": "665769972387046cf6196029",
                        "avatarUrl": "/avatars/7ddd519572ba197b0c03e175f142a1bc.svg",
                        "isPro": false,
                        "fullname": "yiru wang",
                        "user": "yiruwang0324",
                        "type": "user"
                    },
                    "name": "Yi Ru Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:09:18.022Z",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed689",
                    "user": {
                        "_id": "6402a262cad36eb2b0fdd14b",
                        "avatarUrl": "/avatars/3c137e9897c80c479cfd72950aa20fde.svg",
                        "isPro": false,
                        "fullname": "Haoquan Fang",
                        "user": "hqfang",
                        "type": "user"
                    },
                    "name": "Haoquan Fang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:09:12.281Z",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed68a",
                    "user": {
                        "_id": "635d3e57abb916fca6af907d",
                        "avatarUrl": "/avatars/dc4db02e37eddd6b14a058349b218b44.svg",
                        "isPro": false,
                        "fullname": "Albert Li",
                        "user": "boyangli",
                        "type": "user"
                    },
                    "name": "Boyang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:09:06.599Z",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed68b",
                    "user": {
                        "_id": "66236ecff1b64cfb49c236dc",
                        "avatarUrl": "/avatars/8328233527d6785ef7ee8f726e696e72.svg",
                        "isPro": false,
                        "fullname": "Yu Shan Huang",
                        "user": "hys990996",
                        "type": "user"
                    },
                    "name": "Yushan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:08:59.603Z",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed68c",
                    "name": "Elvis Wang",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed68d",
                    "user": {
                        "_id": "64272345d039ee7fa4e0b015",
                        "avatarUrl": "/avatars/d8933fb0f171e1f752f64a59c1745da5.svg",
                        "isPro": false,
                        "fullname": "Ainaz Eftekhar",
                        "user": "Ainaz99",
                        "type": "user"
                    },
                    "name": "Ainaz Eftekhar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:08:48.493Z",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed68e",
                    "name": "Jason Lee",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed68f",
                    "name": "Wentao Yuan",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed690",
                    "user": {
                        "_id": "627598f47128d87e9f92f487",
                        "avatarUrl": "/avatars/1f9ae7236b25f573f62e298436c7422a.svg",
                        "isPro": false,
                        "fullname": "Rose Hendrix",
                        "user": "roseh",
                        "type": "user"
                    },
                    "name": "Rose Hendrix",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:08:36.316Z",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed691",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed692",
                    "name": "Fei Xia",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed693",
                    "name": "Dieter Fox",
                    "hidden": false
                },
                {
                    "_id": "6826dce4682e62074d0ed694",
                    "user": {
                        "_id": "66429868ab89e3a3a85668b0",
                        "avatarUrl": "/avatars/170e0daa454838deee2bf946f7118651.svg",
                        "isPro": false,
                        "fullname": "Ranjay Krishna",
                        "user": "ranjaykrishna",
                        "type": "user"
                    },
                    "name": "Ranjay Krishna",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:08:19.914Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
            ],
            "publishedAt": "2025-05-15T06:04:42.000Z",
            "submittedOnDailyAt": "2025-05-16T05:07:15.117Z",
            "title": "PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing",
            "submittedOnDailyBy": {
                "_id": "632b42626110e37dba3d5bcb",
                "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
                "isPro": false,
                "fullname": "Duan",
                "user": "Jiafei1224",
                "type": "user"
            },
            "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/",
            "upvotes": 9,
            "discussionId": "6826dce8682e62074d0ed7eb",
            "projectPage": "https://pointarena.github.io/",
            "githubRepo": "https://github.com/pointarena/pointarena",
            "ai_keywords": [
                "multimodal models",
                "referential object localization tasks",
                "PointArena",
                "Point-Bench",
                "reasoning categories",
                "Point-Battle",
                "Point-Act",
                "blind, pairwise model comparisons",
                "Molmo-72B",
                "supervised training",
                "precise pointing capabilities"
            ]
        },
        "publishedAt": "2025-05-15T02:04:42.000Z",
        "title": "PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing",
        "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09990.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632b42626110e37dba3d5bcb",
            "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
            "fullname": "Duan",
            "name": "Jiafei1224",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10565",
            "authors": [
                {
                    "_id": "6826bda8caf89edf94b736dc",
                    "user": {
                        "_id": "6425761a175bd295228311a0",
                        "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
                        "isPro": false,
                        "fullname": "zehan wang",
                        "user": "sleetwang6",
                        "type": "user"
                    },
                    "name": "Zehan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T07:12:01.498Z",
                    "hidden": false
                },
                {
                    "_id": "6826bda8caf89edf94b736dd",
                    "user": {
                        "_id": "6728865953709f441b66d901",
                        "avatarUrl": "/avatars/5f06b4e335357a77fa702f00a4306124.svg",
                        "isPro": false,
                        "fullname": "Siyu Chen",
                        "user": "SiyuChen",
                        "type": "user"
                    },
                    "name": "Siyu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:10:23.599Z",
                    "hidden": false
                },
                {
                    "_id": "6826bda8caf89edf94b736de",
                    "user": {
                        "_id": "65a3a0342548c41ad9f4e4e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a3a0342548c41ad9f4e4e7/80Yod9z7O95nC-Y0o-5UI.jpeg",
                        "isPro": false,
                        "fullname": "Lihe Yang",
                        "user": "LiheYoung",
                        "type": "user"
                    },
                    "name": "Lihe Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:10:32.115Z",
                    "hidden": false
                },
                {
                    "_id": "6826bda8caf89edf94b736df",
                    "user": {
                        "_id": "661b66368ad066992421a8c9",
                        "avatarUrl": "/avatars/6d970c07b7366563fd7a73d48ac496ae.svg",
                        "isPro": false,
                        "fullname": "Jialei Wang",
                        "user": "UncleWang233",
                        "type": "user"
                    },
                    "name": "Jialei Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:10:39.079Z",
                    "hidden": false
                },
                {
                    "_id": "6826bda8caf89edf94b736e0",
                    "name": "Ziang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6826bda8caf89edf94b736e1",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6826bda8caf89edf94b736e2",
                    "name": "Zhou Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:59:50.000Z",
            "submittedOnDailyAt": "2025-05-16T02:55:45.280Z",
            "title": "Depth Anything with Any Prior",
            "submittedOnDailyBy": {
                "_id": "663b4d6aa55b0634634cd302",
                "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
                "isPro": false,
                "fullname": "ZehanWang",
                "user": "ZehanWang",
                "type": "user"
            },
            "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.",
            "upvotes": 8,
            "discussionId": "6826bdaacaf89edf94b73753",
            "projectPage": "https://prior-depth-anything.github.io/",
            "githubRepo": "https://github.com/SpatialVision/Prior-Depth-Anything",
            "ai_keywords": [
                "metric depth maps",
                "coarse-to-fine pipeline",
                "pixel-level metric alignment",
                "distance-aware weighting",
                "metric priors",
                "domain gap",
                "generalization",
                "conditioned monocular depth estimation (MDE)",
                "zero-shot generalization",
                "depth completion",
                "super-resolution",
                "inpainting",
                "test-time improvements",
                "accuracy-efficiency trade-off"
            ]
        },
        "publishedAt": "2025-05-15T13:59:50.000Z",
        "title": "Depth Anything with Any Prior",
        "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10565.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "663b4d6aa55b0634634cd302",
            "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
            "fullname": "ZehanWang",
            "name": "ZehanWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10558",
            "authors": [
                {
                    "_id": "6826bc3cf032d8147549ac6b",
                    "user": {
                        "_id": "635eac5ea81c7f7424a23b8c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
                        "isPro": false,
                        "fullname": "intchous",
                        "user": "intchous",
                        "type": "user"
                    },
                    "name": "Peiying Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
                    "hidden": false
                },
                {
                    "_id": "6826bc3cf032d8147549ac6c",
                    "user": {
                        "_id": "6412b90e6e51a8e21887ff30",
                        "avatarUrl": "/avatars/b0c3a8624a686481b9f609be96b1307c.svg",
                        "isPro": false,
                        "fullname": "Z",
                        "user": "CHERRY-Z",
                        "type": "user"
                    },
                    "name": "Nanxuan Zhao",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
                    "hidden": false
                },
                {
                    "_id": "6826bc3cf032d8147549ac6d",
                    "name": "Jing Liao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:59:21.000Z",
            "submittedOnDailyAt": "2025-05-16T02:49:19.469Z",
            "title": "Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors",
            "submittedOnDailyBy": {
                "_id": "635eac5ea81c7f7424a23b8c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
                "isPro": false,
                "fullname": "intchous",
                "user": "intchous",
                "type": "user"
            },
            "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.",
            "upvotes": 8,
            "discussionId": "6826bc3df032d8147549acac",
            "ai_keywords": [
                "T2V (text-to-vector)",
                "SVG (Scalable Vector Graphics)",
                "T2I (text-to-image)",
                "diffusion model",
                "path-level representation",
                "structural regularity",
                "expressive capabilities"
            ]
        },
        "publishedAt": "2025-05-15T13:59:21.000Z",
        "title": "Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors",
        "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10558.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "635eac5ea81c7f7424a23b8c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
            "fullname": "intchous",
            "name": "intchous",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09738",
            "authors": [
                {
                    "_id": "6826c755f032d814754cadfe",
                    "name": "Shaurya Sharthak",
                    "hidden": false
                },
                {
                    "_id": "6826c755f032d814754cadff",
                    "name": "Vinayak Pahalwan",
                    "hidden": false
                },
                {
                    "_id": "6826c755f032d814754cae00",
                    "user": {
                        "_id": "6523d85b27d1f3d84ab3a0a4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6523d85b27d1f3d84ab3a0a4/GF159gtSvxSWR9TAJIQby.jpeg",
                        "isPro": false,
                        "fullname": "Adithya Kamath",
                        "user": "adi-kmt",
                        "type": "user"
                    },
                    "name": "Adithya Kamath",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T07:11:49.739Z",
                    "hidden": false
                },
                {
                    "_id": "6826c755f032d814754cae01",
                    "user": {
                        "_id": "644bf6ef778ecbfb977e8e84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
                        "isPro": true,
                        "fullname": "Adarsh AS",
                        "user": "adarshxs",
                        "type": "user"
                    },
                    "name": "Adarsh Shirawalmath",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-16T05:19:45.785Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
            ],
            "publishedAt": "2025-05-14T19:00:27.000Z",
            "submittedOnDailyAt": "2025-05-16T03:53:49.621Z",
            "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning",
            "submittedOnDailyBy": {
                "_id": "644bf6ef778ecbfb977e8e84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
                "isPro": true,
                "fullname": "Adarsh AS",
                "user": "adarshxs",
                "type": "user"
            },
            "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
            "upvotes": 8,
            "discussionId": "6826c756f032d814754cae4d",
            "githubRepo": "https://github.com/Tinycompany-AI/tokenadapt",
            "ai_keywords": [
                "Tokenadapt",
                "Supertokens",
                "subword decomposition",
                "semantically similar tokens",
                "zero-shot perplexity",
                "perplexity ratios"
            ]
        },
        "publishedAt": "2025-05-14T15:00:27.000Z",
        "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning",
        "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09738.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644bf6ef778ecbfb977e8e84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
            "fullname": "Adarsh AS",
            "name": "adarshxs",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 33
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.08617",
            "authors": [
                {
                    "_id": "6826aa068caf98415c50897f",
                    "user": {
                        "_id": "64264095ba51f8a2136946a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
                        "isPro": false,
                        "fullname": "Zhaochen Su",
                        "user": "Warrieryes",
                        "type": "user"
                    },
                    "name": "Zhaochen Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:12:13.472Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508980",
                    "user": {
                        "_id": "63db16fff03c3d71ef397206",
                        "avatarUrl": "/avatars/bfb7e0d730b7d03302799d5d2828d97d.svg",
                        "isPro": false,
                        "fullname": "Linjie Li",
                        "user": "linjieli222",
                        "type": "user"
                    },
                    "name": "Linjie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:12:27.963Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508981",
                    "user": {
                        "_id": "66aca01e33f6b27979856f6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
                        "isPro": false,
                        "fullname": "Mingyang Song",
                        "user": "hitsmy",
                        "type": "user"
                    },
                    "name": "Mingyang Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:12:40.890Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508982",
                    "user": {
                        "_id": "653dd16277c2f09452ad37cd",
                        "avatarUrl": "/avatars/a95f9527722845a5414d86180c8e945d.svg",
                        "isPro": false,
                        "fullname": "Yunzhuo Hao",
                        "user": "luckychao",
                        "type": "user"
                    },
                    "name": "Yunzhuo Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:12:47.552Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508983",
                    "user": {
                        "_id": "630713411801ecc7d2592a7c",
                        "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
                        "isPro": false,
                        "fullname": "Zhengyuan Yang",
                        "user": "zyang39",
                        "type": "user"
                    },
                    "name": "Zhengyuan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:12:53.296Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508984",
                    "name": "Jun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508985",
                    "user": {
                        "_id": "653a5ae21e1140b0c7d7f8f0",
                        "avatarUrl": "/avatars/2b839aad010bcd9364eed396e19f3700.svg",
                        "isPro": false,
                        "fullname": "Guanjie Chen",
                        "user": "GuanjieChen",
                        "type": "user"
                    },
                    "name": "Guanjie Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:13:01.061Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508986",
                    "name": "Jiawei Gu",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508987",
                    "user": {
                        "_id": "6670e285b0c03c4e9d6e0985",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
                        "isPro": false,
                        "fullname": "Juntao Li",
                        "user": "douvleplus",
                        "type": "user"
                    },
                    "name": "Juntao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:13:16.466Z",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508988",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "6826aa068caf98415c508989",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-13T14:35:51.000Z",
            "submittedOnDailyAt": "2025-05-16T01:29:32.761Z",
            "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64264095ba51f8a2136946a0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
                "isPro": false,
                "fullname": "Zhaochen Su",
                "user": "Warrieryes",
                "type": "user"
            },
            "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".",
            "upvotes": 6,
            "discussionId": "6826aa078caf98415c5089d6",
            "githubRepo": "https://github.com/zhaochen0110/OpenThinkIMG",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "OpenThinkIMG",
                "vision tool interfaces",
                "scalable trajectory generation",
                "policy initialization",
                "flexible training environment",
                "supervised fine-tuning (SFT)",
                "reinforcement learning (RL)",
                "V-ToolRL",
                "adaptive policies",
                "external vision tools",
                "optimal tool-usage strategies",
                "task success",
                "feedback from tool interactions",
                "chart reasoning tasks",
                "Qwen2-VL-2B",
                "Taco",
                "CogCom",
                "GPT-4.1"
            ]
        },
        "publishedAt": "2025-05-13T10:35:51.000Z",
        "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning",
        "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08617.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "fullname": "Zhaochen Su",
            "name": "Warrieryes",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.08581",
            "authors": [
                {
                    "_id": "6825c47fbe0e3c3bfbf4d285",
                    "user": {
                        "_id": "66bf75432777c05070bf49dc",
                        "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
                        "isPro": false,
                        "fullname": "Haofeng Liu",
                        "user": "HeverLaw",
                        "type": "user"
                    },
                    "name": "Haofeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T07:12:36.358Z",
                    "hidden": false
                },
                {
                    "_id": "6825c47fbe0e3c3bfbf4d286",
                    "name": "Mingqi Gao",
                    "hidden": false
                },
                {
                    "_id": "6825c47fbe0e3c3bfbf4d287",
                    "name": "Xuxiao Luo",
                    "hidden": false
                },
                {
                    "_id": "6825c47fbe0e3c3bfbf4d288",
                    "name": "Ziyue Wang",
                    "hidden": false
                },
                {
                    "_id": "6825c47fbe0e3c3bfbf4d289",
                    "name": "Guanyi Qin",
                    "hidden": false
                },
                {
                    "_id": "6825c47fbe0e3c3bfbf4d28a",
                    "user": {
                        "_id": "6317257fc92fd6fee317ff7c",
                        "avatarUrl": "/avatars/2f460a2f28562c987becb2acad8d93e7.svg",
                        "isPro": false,
                        "fullname": "Junde Wu",
                        "user": "morson",
                        "type": "user"
                    },
                    "name": "Junde Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:20:21.901Z",
                    "hidden": false
                },
                {
                    "_id": "6825c47fbe0e3c3bfbf4d28b",
                    "name": "Yueming Jin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
            ],
            "publishedAt": "2025-05-13T13:56:10.000Z",
            "submittedOnDailyAt": "2025-05-16T02:23:20.718Z",
            "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
            "submittedOnDailyBy": {
                "_id": "66bf75432777c05070bf49dc",
                "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
                "isPro": false,
                "fullname": "Haofeng Liu",
                "user": "HeverLaw",
                "type": "user"
            },
            "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
            "upvotes": 6,
            "discussionId": "6825c480be0e3c3bfbf4d2c0",
            "githubRepo": "https://github.com/jinlab-imvr/ReSurgSAM2",
            "ai_keywords": [
                "Segment Anything Model 2",
                "text-referred target detection",
                "cross-modal spatial-temporal Mamba",
                "initial frame identification",
                "diversity-driven long-term memory",
                "diversity-driven memory mechanism",
                "memory bank",
                "real-time tracking",
                "real-time operation"
            ]
        },
        "publishedAt": "2025-05-13T09:56:10.000Z",
        "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
        "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08581.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66bf75432777c05070bf49dc",
            "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
            "fullname": "Haofeng Liu",
            "name": "HeverLaw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10167",
            "authors": [
                {
                    "_id": "68271b6ed5568917361eb8d8",
                    "user": {
                        "_id": "653425f4ed74ace63395826c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QJlB0DOEel6U9b-95wasK.png",
                        "isPro": false,
                        "fullname": "Saikat Barua",
                        "user": "AlignAI",
                        "type": "user"
                    },
                    "name": "Saikat Barua",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-16T11:03:11.305Z",
                    "hidden": false
                },
                {
                    "_id": "68271b6ed5568917361eb8d9",
                    "user": {
                        "_id": "63827fb4544214606661792d",
                        "avatarUrl": "/avatars/bc47b0733cc034ec006ebaca803fef62.svg",
                        "isPro": false,
                        "fullname": "Mostafizur Rahman",
                        "user": "imostafizur",
                        "type": "user"
                    },
                    "name": "Mostafizur Rahman",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-16T11:03:11.305Z",
                    "hidden": false
                },
                {
                    "_id": "68271b6ed5568917361eb8da",
                    "name": "Shehenaz Khaled",
                    "hidden": false
                },
                {
                    "_id": "68271b6ed5568917361eb8db",
                    "user": {
                        "_id": "63c99ab3dfac8071d01b61d4",
                        "avatarUrl": "/avatars/9151241b8af4d64d7771740587d1b7a5.svg",
                        "isPro": false,
                        "fullname": "MD Jafor Sadek Khan",
                        "user": "Jafor",
                        "type": "user"
                    },
                    "name": "Md Jafor Sadek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:18:14.383Z",
                    "hidden": false
                },
                {
                    "_id": "68271b6ed5568917361eb8dc",
                    "name": "Rafiul Islam",
                    "hidden": false
                },
                {
                    "_id": "68271b6ed5568917361eb8dd",
                    "name": "Shahnewaz Siddique",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T10:51:34.000Z",
            "submittedOnDailyAt": "2025-05-16T10:14:45.855Z",
            "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models",
            "submittedOnDailyBy": {
                "_id": "653425f4ed74ace63395826c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QJlB0DOEel6U9b-95wasK.png",
                "isPro": false,
                "fullname": "Saikat Barua",
                "user": "AlignAI",
                "type": "user"
            },
            "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.",
            "upvotes": 5,
            "discussionId": "68271b6fd5568917361eb911",
            "githubRepo": "https://github.com/GitsSaikat/QuXAI",
            "ai_keywords": [
                "hybrid quantum-classical machine learning (HQML)",
                "quantum feature encoding",
                "Q-MEDLEY",
                "feature importance",
                "quantum feature maps",
                "ablation studies",
                "interpretability",
                "XAI (Explainable AI)",
                "classical validation settings"
            ]
        },
        "publishedAt": "2025-05-15T06:51:34.000Z",
        "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models",
        "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10167.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653425f4ed74ace63395826c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QJlB0DOEel6U9b-95wasK.png",
            "fullname": "Saikat Barua",
            "name": "AlignAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10566",
            "authors": [
                {
                    "_id": "6826a515cf1804ab4c37cde8",
                    "user": {
                        "_id": "634cbe8888c6b621d3577fe6",
                        "avatarUrl": "/avatars/e8108ccb3602a5e387a03aaa06d7fdc3.svg",
                        "isPro": false,
                        "fullname": "Yen-Chi Cheng",
                        "user": "yenchic",
                        "type": "user"
                    },
                    "name": "Yen-Chi Cheng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-16T18:15:41.266Z",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cde9",
                    "name": "Krishna Kumar Singh",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cdea",
                    "name": "Jae Shin Yoon",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cdeb",
                    "name": "Alex Schwing",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cdec",
                    "name": "Liangyan Gui",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cded",
                    "name": "Matheus Gadelha",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cdee",
                    "name": "Paul Guerrero",
                    "hidden": false
                },
                {
                    "_id": "6826a515cf1804ab4c37cdef",
                    "name": "Nanxuan Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:59:51.000Z",
            "submittedOnDailyAt": "2025-05-16T16:48:22.073Z",
            "title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
            "submittedOnDailyBy": {
                "_id": "634cbe8888c6b621d3577fe6",
                "avatarUrl": "/avatars/e8108ccb3602a5e387a03aaa06d7fdc3.svg",
                "isPro": false,
                "fullname": "Yen-Chi Cheng",
                "user": "yenchic",
                "type": "user"
            },
            "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/",
            "upvotes": 4,
            "discussionId": "6826a516cf1804ab4c37ce47",
            "ai_keywords": [
                "diffusion models",
                "3D-Fixup",
                "3D priors",
                "3D-aware image editing",
                "object translation",
                "3D rotation",
                "training-based approach",
                "generative power",
                "video data",
                "source and target frames",
                "Image-to-3D model",
                "2D information into 3D space",
                "data generation pipeline",
                "identity coherent 3D-aware edits",
                "realistic image manipulation"
            ]
        },
        "publishedAt": "2025-05-15T13:59:51.000Z",
        "title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
        "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10566.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634cbe8888c6b621d3577fe6",
            "avatarUrl": "/avatars/e8108ccb3602a5e387a03aaa06d7fdc3.svg",
            "fullname": "Yen-Chi Cheng",
            "name": "yenchic",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10046",
            "authors": [
                {
                    "_id": "6826b00ed4c8b864e5ed1c0f",
                    "user": {
                        "_id": "666de415ea11ab8f28560962",
                        "avatarUrl": "/avatars/c910b35e3128cf17ba1e45afff551820.svg",
                        "isPro": false,
                        "fullname": "Bingda Tang",
                        "user": "ooutlierr",
                        "type": "user"
                    },
                    "name": "Bingda Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:21:21.171Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00ed4c8b864e5ed1c10",
                    "user": {
                        "_id": "6434226da4c9c55871a78052",
                        "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg",
                        "isPro": false,
                        "fullname": "BoYang Zheng",
                        "user": "bytetriper",
                        "type": "user"
                    },
                    "name": "Boyang Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:21:27.353Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00ed4c8b864e5ed1c11",
                    "user": {
                        "_id": "63172831c92fd6fee3181f50",
                        "avatarUrl": "/avatars/0f57068a138cb181e9451bfc1ed3d1c0.svg",
                        "isPro": false,
                        "fullname": "Xichen Pan",
                        "user": "xcpan",
                        "type": "user"
                    },
                    "name": "Xichen Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:21:34.655Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00ed4c8b864e5ed1c12",
                    "user": {
                        "_id": "5f7fbd813e94f16a85448745",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
                        "isPro": false,
                        "fullname": "Sayak Paul",
                        "user": "sayakpaul",
                        "type": "user"
                    },
                    "name": "Sayak Paul",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:23:23.708Z",
                    "hidden": false
                },
                {
                    "_id": "6826b00ed4c8b864e5ed1c13",
                    "user": {
                        "_id": "6596422646624a86ff3b3bda",
                        "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
                        "isPro": false,
                        "fullname": "Saining Xie",
                        "user": "sainx",
                        "type": "user"
                    },
                    "name": "Saining Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:21:41.504Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T07:43:23.000Z",
            "submittedOnDailyAt": "2025-05-16T01:55:37.654Z",
            "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
            "submittedOnDailyBy": {
                "_id": "5f7fbd813e94f16a85448745",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
                "isPro": false,
                "fullname": "Sayak Paul",
                "user": "sayakpaul",
                "type": "user"
            },
            "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
            "upvotes": 4,
            "discussionId": "6826b00ed4c8b864e5ed1c4f",
            "githubRepo": "https://github.com/tang-bd/fuse-dit",
            "ai_keywords": [
                "large language models (LLMs)",
                "diffusion transformers (DiTs)",
                "multi-modal generation",
                "text-to-image synthesis",
                "controlled comparisons",
                "established baselines",
                "important design choices",
                "training recipes",
                "reproducible recipe",
                "training at scale"
            ]
        },
        "publishedAt": "2025-05-15T03:43:23.000Z",
        "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
        "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "fullname": "Sayak Paul",
            "name": "sayakpaul",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 619
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09926",
            "authors": [
                {
                    "_id": "68268c3408f7cb26defd82fc",
                    "user": {
                        "_id": "648972ff99f6c45ff6bbd295",
                        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                        "isPro": false,
                        "fullname": "Bin-Bin Gao",
                        "user": "csgaobb",
                        "type": "user"
                    },
                    "name": "Bin-Bin Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:18:39.295Z",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd82fd",
                    "name": "Yue Zhu",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd82fe",
                    "name": "Jiangtao Yan",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd82ff",
                    "name": "Yuezhi Cai",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd8300",
                    "name": "Weixi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd8301",
                    "name": "Meng Wang",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd8302",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd8303",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd8304",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "68268c3408f7cb26defd8305",
                    "name": "Chengjie Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T03:24:28.000Z",
            "submittedOnDailyAt": "2025-05-16T00:07:29.541Z",
            "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
            "submittedOnDailyBy": {
                "_id": "648972ff99f6c45ff6bbd295",
                "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                "isPro": false,
                "fullname": "Bin-Bin Gao",
                "user": "csgaobb",
                "type": "user"
            },
            "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
            "upvotes": 4,
            "discussionId": "68268c3808f7cb26defd83bf",
            "ai_keywords": [
                "CLIP",
                "pre-trained vision-language models",
                "prompt templates",
                "token interactions",
                "fine-tuning",
                "adaptive visual and textual representations",
                "comparative learning",
                "query and normal image prompt",
                "contextual and aligned residual features",
                "residual features",
                "visual adapter",
                "textual adapter",
                "prompt-query adapter",
                "zero-/few-shot generalization",
                "training-free",
                "anomaly detection benchmarks",
                "industrial domains",
                "medical domains"
            ]
        },
        "publishedAt": "2025-05-14T23:24:28.000Z",
        "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
        "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09926.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "fullname": "Bin-Bin Gao",
            "name": "csgaobb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09265",
            "authors": [
                {
                    "_id": "682547e6501a31b392e78f6a",
                    "user": {
                        "_id": "648972ff99f6c45ff6bbd295",
                        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                        "isPro": false,
                        "fullname": "Bin-Bin Gao",
                        "user": "csgaobb",
                        "type": "user"
                    },
                    "name": "Bin-Bin Gao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-15T01:54:50.125Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T10:25:26.000Z",
            "submittedOnDailyAt": "2025-05-16T00:08:33.750Z",
            "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
            "submittedOnDailyBy": {
                "_id": "648972ff99f6c45ff6bbd295",
                "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                "isPro": false,
                "fullname": "Bin-Bin Gao",
                "user": "csgaobb",
                "type": "user"
            },
            "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
            "upvotes": 3,
            "discussionId": "682547eb501a31b392e79038",
            "githubRepo": "https://github.com/gaobb/MetaUAS",
            "ai_keywords": [
                "Meta-learning",
                "Universal Anomaly Segmentation (MetaUAS)",
                "synthetic image pairs",
                "object-level changes",
                "local region changes",
                "prompt",
                "query images",
                "soft feature alignment module",
                "paired-image change perception",
                "single-image semantic segmentation",
                "universal anomaly segmentation",
                "pure vision model",
                "zero-shot",
                "few-shot",
                "full-shot anomaly segmentation"
            ]
        },
        "publishedAt": "2025-05-14T06:25:26.000Z",
        "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
        "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09265.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "fullname": "Bin-Bin Gao",
            "name": "csgaobb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09264",
            "authors": [
                {
                    "_id": "682548bff4997d78fe92cd57",
                    "user": {
                        "_id": "648972ff99f6c45ff6bbd295",
                        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                        "isPro": false,
                        "fullname": "Bin-Bin Gao",
                        "user": "csgaobb",
                        "type": "user"
                    },
                    "name": "Bin-Bin Gao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-15T01:54:51.290Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T10:25:14.000Z",
            "submittedOnDailyAt": "2025-05-16T00:10:20.034Z",
            "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
            "submittedOnDailyBy": {
                "_id": "648972ff99f6c45ff6bbd295",
                "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                "isPro": false,
                "fullname": "Bin-Bin Gao",
                "user": "csgaobb",
                "type": "user"
            },
            "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
            "upvotes": 3,
            "discussionId": "682548c1f4997d78fe92cdbc",
            "githubRepo": "https://github.com/gaobb/OneNIP",
            "ai_keywords": [
                "self-attention transformers",
                "multi-class anomaly detection",
                "self-attention reconstruction models",
                "target features",
                "context",
                "latent space",
                "One Normal Image Prompt (OneNIP)",
                "supervised refiner",
                "reconstruction errors",
                "MVTec",
                "BTAD",
                "VisA"
            ]
        },
        "publishedAt": "2025-05-14T06:25:14.000Z",
        "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
        "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09264.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "fullname": "Bin-Bin Gao",
            "name": "csgaobb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09263",
            "authors": [
                {
                    "_id": "682549a75b5784e023ed7d8a",
                    "name": "Guan Gui",
                    "hidden": false
                },
                {
                    "_id": "682549a75b5784e023ed7d8b",
                    "user": {
                        "_id": "648972ff99f6c45ff6bbd295",
                        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                        "isPro": false,
                        "fullname": "Bin-Bin Gao",
                        "user": "csgaobb",
                        "type": "user"
                    },
                    "name": "Bin-Bin Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-15T10:31:36.508Z",
                    "hidden": false
                },
                {
                    "_id": "682549a75b5784e023ed7d8c",
                    "name": "Jun Liu",
                    "hidden": false
                },
                {
                    "_id": "682549a75b5784e023ed7d8d",
                    "user": {
                        "_id": "65729087379284b904a6d81d",
                        "avatarUrl": "/avatars/0e7968b1d9221d523a54e3ac787e449d.svg",
                        "isPro": false,
                        "fullname": "Chengjie Wang",
                        "user": "chengjie-wang",
                        "type": "user"
                    },
                    "name": "Chengjie Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-16T14:22:17.616Z",
                    "hidden": false
                },
                {
                    "_id": "682549a75b5784e023ed7d8e",
                    "name": "Yunsheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T10:25:06.000Z",
            "submittedOnDailyAt": "2025-05-16T00:12:32.405Z",
            "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "648972ff99f6c45ff6bbd295",
                "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
                "isPro": false,
                "fullname": "Bin-Bin Gao",
                "user": "csgaobb",
                "type": "user"
            },
            "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
            "upvotes": 3,
            "discussionId": "682549ac5b5784e023ed7e72",
            "ai_keywords": [
                "Anomaly-driven Generation (AnoGen)",
                "diffusion model",
                "anomaly distribution",
                "embedding",
                "bounding boxes",
                "weakly-supervised anomaly detection",
                "DRAEM",
                "DesTSeg",
                "MVTec",
                "AU-PR metric"
            ]
        },
        "publishedAt": "2025-05-14T06:25:06.000Z",
        "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
        "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09263.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "fullname": "Bin-Bin Gao",
            "name": "csgaobb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10468",
            "authors": [
                {
                    "_id": "682733b0d7b648c3eb78c9fd",
                    "user": {
                        "_id": "67ddd80896ac367438d400a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                        "isPro": false,
                        "fullname": "Ranjan Sapkota",
                        "user": "RanjanSapkota",
                        "type": "user"
                    },
                    "name": "Ranjan Sapkota",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-16T13:23:12.733Z",
                    "hidden": false
                },
                {
                    "_id": "682733b0d7b648c3eb78c9fe",
                    "name": "Konstantinos I. Roumeliotis",
                    "hidden": false
                },
                {
                    "_id": "682733b0d7b648c3eb78c9ff",
                    "name": "Manoj Karkee",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/uLphbVQxqrhKLEkvvmN7N.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/6qjQorHf7Yiiz55rZm2Ea.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ZUCWQkhxm7fOB32GvJNzl.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ZweC-77DcuALb-VususJ6.jpeg"
            ],
            "publishedAt": "2025-05-15T16:21:33.000Z",
            "submittedOnDailyAt": "2025-05-16T11:20:47.004Z",
            "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge",
            "submittedOnDailyBy": {
                "_id": "67ddd80896ac367438d400a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                "isPro": false,
                "fullname": "Ranjan Sapkota",
                "user": "RanjanSapkota",
                "type": "user"
            },
            "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications",
            "upvotes": 2,
            "discussionId": "682733b2d7b648c3eb78cac6",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Large Image Models (LIMs)",
                "Generative AI",
                "multi-agent collaboration",
                "dynamic task decomposition",
                "persistent memory",
                "orchestrated autonomy",
                "ReAct loops",
                "RAG (Retrieval-Augmented Generation)",
                "orchestration layers",
                "causal modeling",
                "Agentic AI Decision Support System"
            ]
        },
        "publishedAt": "2025-05-15T12:21:33.000Z",
        "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge",
        "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/uLphbVQxqrhKLEkvvmN7N.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/6qjQorHf7Yiiz55rZm2Ea.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ZUCWQkhxm7fOB32GvJNzl.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/ZweC-77DcuALb-VususJ6.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10468.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ddd80896ac367438d400a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
            "fullname": "Ranjan Sapkota",
            "name": "RanjanSapkota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09601",
            "authors": [
                {
                    "_id": "6827b0940908de1cafc267bb",
                    "name": "Justin Yu",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267bc",
                    "name": "Letian Fu",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267bd",
                    "name": "Huang Huang",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267be",
                    "name": "Karim El-Refai",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267bf",
                    "name": "Rares Andrei Ambrus",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267c0",
                    "name": "Richard Cheng",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267c1",
                    "name": "Muhammad Zubair Irshad",
                    "hidden": false
                },
                {
                    "_id": "6827b0940908de1cafc267c2",
                    "name": "Ken Goldberg",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T17:50:35.000Z",
            "submittedOnDailyAt": "2025-05-16T20:12:03.343Z",
            "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware",
            "submittedOnDailyBy": {
                "_id": "64908d6b0c18343a09361b93",
                "avatarUrl": "/avatars/2fa44344fe760f68f19de775b76f45b8.svg",
                "isPro": false,
                "fullname": "Max (Letian) Fu",
                "user": "mlfu7",
                "type": "user"
            },
            "summary": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing\ndata collection paradigm-human teleoperation-remains costly and constrained by\nmanual effort and physical robot access. We introduce Real2Render2Real (R2R2R),\na novel approach for generating robot training data without relying on object\ndynamics simulation or teleoperation of robot hardware. The input is a\nsmartphone-captured scan of one or more objects and a single video of a human\ndemonstration. R2R2R renders thousands of high visual fidelity robot-agnostic\ndemonstrations by reconstructing detailed 3D object geometry and appearance,\nand tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to\nenable flexible asset generation and trajectory synthesis for both rigid and\narticulated objects, converting these representations to meshes to maintain\ncompatibility with scalable rendering engines like IsaacLab but with collision\nmodeling off. Robot demonstration data generated by R2R2R integrates directly\nwith models that operate on robot proprioceptive states and image observations,\nsuch as vision-language-action models (VLA) and imitation learning policies.\nPhysical experiments suggest that models trained on R2R2R data from a single\nhuman demonstration can match the performance of models trained on 150 human\nteleoperation demonstrations. Project page: https://real2render2real.com",
            "upvotes": 2,
            "discussionId": "6827b0950908de1cafc2683d",
            "projectPage": "https://real2render2real.com",
            "githubRepo": "https://github.com/uynitsuj/real2render2real",
            "ai_keywords": [
                "Real2Render2Real (R2R2R)",
                "3D Gaussian Splatting (3DGS)",
                "6-DoF",
                "mesh",
                "scalable rendering engines (e.g., IsaacLab)",
                "vision-language-action models (VLA)",
                "imitation learning policies",
                "proprioceptive states"
            ]
        },
        "publishedAt": "2025-05-14T13:50:35.000Z",
        "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware",
        "summary": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing\ndata collection paradigm-human teleoperation-remains costly and constrained by\nmanual effort and physical robot access. We introduce Real2Render2Real (R2R2R),\na novel approach for generating robot training data without relying on object\ndynamics simulation or teleoperation of robot hardware. The input is a\nsmartphone-captured scan of one or more objects and a single video of a human\ndemonstration. R2R2R renders thousands of high visual fidelity robot-agnostic\ndemonstrations by reconstructing detailed 3D object geometry and appearance,\nand tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to\nenable flexible asset generation and trajectory synthesis for both rigid and\narticulated objects, converting these representations to meshes to maintain\ncompatibility with scalable rendering engines like IsaacLab but with collision\nmodeling off. Robot demonstration data generated by R2R2R integrates directly\nwith models that operate on robot proprioceptive states and image observations,\nsuch as vision-language-action models (VLA) and imitation learning policies.\nPhysical experiments suggest that models trained on R2R2R data from a single\nhuman demonstration can match the performance of models trained on 150 human\nteleoperation demonstrations. Project page: https://real2render2real.com",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09601.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64908d6b0c18343a09361b93",
            "avatarUrl": "/avatars/2fa44344fe760f68f19de775b76f45b8.svg",
            "fullname": "Max (Letian) Fu",
            "name": "mlfu7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.07096",
            "authors": [
                {
                    "_id": "68276829c73b5a259249b459",
                    "name": "Prithwish Dan",
                    "hidden": false
                },
                {
                    "_id": "68276829c73b5a259249b45a",
                    "name": "Kushal Kedia",
                    "hidden": false
                },
                {
                    "_id": "68276829c73b5a259249b45b",
                    "name": "Angela Chao",
                    "hidden": false
                },
                {
                    "_id": "68276829c73b5a259249b45c",
                    "name": "Edward Weiyi Duan",
                    "hidden": false
                },
                {
                    "_id": "68276829c73b5a259249b45d",
                    "name": "Maximus Adrian Pace",
                    "hidden": false
                },
                {
                    "_id": "68276829c73b5a259249b45e",
                    "name": "Wei-Chiu Ma",
                    "hidden": false
                },
                {
                    "_id": "68276829c73b5a259249b45f",
                    "name": "Sanjiban Choudhury",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-11T19:04:00.000Z",
            "submittedOnDailyAt": "2025-05-16T15:02:00.023Z",
            "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
            "submittedOnDailyBy": {
                "_id": "65e8c6cae214f37d85e341c0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e8c6cae214f37d85e341c0/CtpYeBEqap9TwomncZI8V.png",
                "isPro": false,
                "fullname": "Prithwish Dan",
                "user": "prithwishdan",
                "type": "user"
            },
            "summary": "Human videos offer a scalable way to train robot manipulation policies, but\nlack the action labels needed by standard imitation learning algorithms.\nExisting cross-embodiment approaches try to map human motion to robot actions,\nbut often fail when the embodiments differ significantly. We propose X-Sim, a\nreal-to-sim-to-real framework that uses object motion as a dense and\ntransferable signal for learning robot policies. X-Sim starts by reconstructing\na photorealistic simulation from an RGBD human video and tracking object\ntrajectories to define object-centric rewards. These rewards are used to train\na reinforcement learning (RL) policy in simulation. The learned policy is then\ndistilled into an image-conditioned diffusion policy using synthetic rollouts\nrendered with varied viewpoints and lighting. To transfer to the real world,\nX-Sim introduces an online domain adaptation technique that aligns real and\nsimulated observations during deployment. Importantly, X-Sim does not require\nany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2\nenvironments and show that it: (1) improves task progress by 30% on average\nover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with\n10x less data collection time, and (3) generalizes to new camera viewpoints and\ntest-time changes. Code and videos are available at\nhttps://portal-cornell.github.io/X-Sim/.",
            "upvotes": 1,
            "discussionId": "6827682bc73b5a259249b4c2",
            "ai_keywords": [
                "photorealistic simulation",
                "RGBD",
                "object-centric rewards",
                "reinforcement learning (RL)",
                "diffusion policy",
                "synthetic rollouts",
                "domain adaptation",
                "hand-tracking",
                "sim-to-real",
                "behavior cloning"
            ]
        },
        "publishedAt": "2025-05-11T15:04:00.000Z",
        "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
        "summary": "Human videos offer a scalable way to train robot manipulation policies, but\nlack the action labels needed by standard imitation learning algorithms.\nExisting cross-embodiment approaches try to map human motion to robot actions,\nbut often fail when the embodiments differ significantly. We propose X-Sim, a\nreal-to-sim-to-real framework that uses object motion as a dense and\ntransferable signal for learning robot policies. X-Sim starts by reconstructing\na photorealistic simulation from an RGBD human video and tracking object\ntrajectories to define object-centric rewards. These rewards are used to train\na reinforcement learning (RL) policy in simulation. The learned policy is then\ndistilled into an image-conditioned diffusion policy using synthetic rollouts\nrendered with varied viewpoints and lighting. To transfer to the real world,\nX-Sim introduces an online domain adaptation technique that aligns real and\nsimulated observations during deployment. Importantly, X-Sim does not require\nany robot teleoperation data. We evaluate it across 5 manipulation tasks in 2\nenvironments and show that it: (1) improves task progress by 30% on average\nover hand-tracking and sim-to-real baselines, (2) matches behavior cloning with\n10x less data collection time, and (3) generalizes to new camera viewpoints and\ntest-time changes. Code and videos are available at\nhttps://portal-cornell.github.io/X-Sim/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07096.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e8c6cae214f37d85e341c0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e8c6cae214f37d85e341c0/CtpYeBEqap9TwomncZI8V.png",
            "fullname": "Prithwish Dan",
            "name": "prithwishdan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]