[
    {
        "paper": {
            "id": "2505.09388",
            "authors": [
                {
                    "_id": "68299e3128752b51372d31ea",
                    "user": {
                        "_id": "62088594a5943c8a8fc94560",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
                        "isPro": false,
                        "fullname": "An Yang",
                        "user": "yangapku",
                        "type": "user"
                    },
                    "name": "An Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:43:00.733Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31eb",
                    "user": {
                        "_id": "6799128b9da39716ab1ebd95",
                        "avatarUrl": "/avatars/677d8ae2087137134c3f0e58f4cf769f.svg",
                        "isPro": false,
                        "fullname": "Anfeng Li",
                        "user": "laf070810",
                        "type": "user"
                    },
                    "name": "Anfeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:15:44.771Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ec",
                    "user": {
                        "_id": "64b0a77df12b47366663884c",
                        "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg",
                        "isPro": false,
                        "fullname": "Baosong Yang",
                        "user": "Baosong",
                        "type": "user"
                    },
                    "name": "Baosong Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:15:37.853Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ed",
                    "user": {
                        "_id": "64b93578ee257c3a4cfceed1",
                        "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
                        "isPro": false,
                        "fullname": "Beichen Zhang",
                        "user": "BeichenZhang",
                        "type": "user"
                    },
                    "name": "Beichen Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:16:13.672Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ee",
                    "user": {
                        "_id": "61e4c4ca1ab24785ac11ba69",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
                        "isPro": false,
                        "fullname": "Binyuan Hui",
                        "user": "huybery",
                        "type": "user"
                    },
                    "name": "Binyuan Hui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:16:22.151Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ef",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f0",
                    "user": {
                        "_id": "6583ab7983a9e1460c67d876",
                        "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
                        "isPro": false,
                        "fullname": "bowen",
                        "user": "bowenYu",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:16:31.453Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f1",
                    "name": "Chang Gao",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f2",
                    "name": "Chengen Huang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f3",
                    "name": "Chenxu Lv",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f4",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:43:04.798Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f5",
                    "user": {
                        "_id": "6434d4989bd5a84b5dd0b0f5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
                        "isPro": false,
                        "fullname": "Dayiheng Liu",
                        "user": "Losin94",
                        "type": "user"
                    },
                    "name": "Dayiheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:17:32.677Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f6",
                    "name": "Fan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f7",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f8",
                    "name": "Feng Hu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31f9",
                    "name": "Hao Ge",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fa",
                    "user": {
                        "_id": "6436618aeef1f55654a9f458",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436618aeef1f55654a9f458/OvxGtuDg2GAFG9As-2hzW.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Wei",
                        "user": "HaoranWei",
                        "type": "user"
                    },
                    "name": "Haoran Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:17:56.110Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fb",
                    "name": "Huan Lin",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fc",
                    "user": {
                        "_id": "63281d05ac205d01918b5fc7",
                        "avatarUrl": "/avatars/fc3e0f7285bb2869a92670f764dfc535.svg",
                        "isPro": false,
                        "fullname": "Jialong Tang",
                        "user": "Jialong",
                        "type": "user"
                    },
                    "name": "Jialong Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:18:16.959Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fd",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31fe",
                    "user": {
                        "_id": "654bead777401b47e6424f88",
                        "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
                        "isPro": false,
                        "fullname": "Jianhong Tu",
                        "user": "ToviTu",
                        "type": "user"
                    },
                    "name": "Jianhong Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:18:30.045Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d31ff",
                    "name": "Jianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3200",
                    "name": "Jianxin Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3201",
                    "name": "Jiaxi Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3202",
                    "name": "Jing Zhou",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3203",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:20:51.253Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3204",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3205",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3206",
                    "name": "Keqin Bao",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3207",
                    "name": "Kexin Yang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3208",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3209",
                    "name": "Lianghao Deng",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320a",
                    "name": "Mei Li",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320b",
                    "user": {
                        "_id": "5f8946925d083370c711f296",
                        "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
                        "isPro": false,
                        "fullname": "Mingfeng Xue",
                        "user": "mingfengxue",
                        "type": "user"
                    },
                    "name": "Mingfeng Xue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:21:56.048Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320c",
                    "name": "Mingze Li",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320d",
                    "name": "Pei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320e",
                    "user": {
                        "_id": "62f220ccee7d7af44979efc7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f220ccee7d7af44979efc7/RImNglMumGCpAKB5gin6k.jpeg",
                        "isPro": false,
                        "fullname": "Peng Wang",
                        "user": "ZJUPeng",
                        "type": "user"
                    },
                    "name": "Peng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:43:02.813Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d320f",
                    "name": "Qin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3210",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3211",
                    "user": {
                        "_id": "6629ed94aabce1b25c3db90c",
                        "avatarUrl": "/avatars/cbc39db81c8e8f950d3bd2c2e03f71c8.svg",
                        "isPro": false,
                        "fullname": "Ruize Gao",
                        "user": "gaoruize",
                        "type": "user"
                    },
                    "name": "Ruize Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:21:46.295Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3212",
                    "name": "Shixuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3213",
                    "name": "Shuang Luo",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3214",
                    "name": "Tianhao Li",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3215",
                    "name": "Tianyi Tang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3216",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3217",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3218",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3219",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321a",
                    "name": "Xuancheng Ren",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321b",
                    "name": "Yang Fan",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321c",
                    "name": "Yang Su",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321d",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321e",
                    "name": "Yinger Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d321f",
                    "name": "Yu Wan",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3220",
                    "user": {
                        "_id": "666aacfb918ba11c7c598194",
                        "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
                        "isPro": false,
                        "fullname": "Yuqiong Liu",
                        "user": "lyq333",
                        "type": "user"
                    },
                    "name": "Yuqiong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:20:06.363Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3221",
                    "name": "Zekun Wang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3222",
                    "user": {
                        "_id": "672c25ca8cfb61188128eb6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
                        "isPro": false,
                        "fullname": "Zeyu Cui",
                        "user": "misakamage",
                        "type": "user"
                    },
                    "name": "Zeyu Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:19:43.843Z",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3223",
                    "name": "Zhenru Zhang",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3224",
                    "name": "Zhipeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68299e3128752b51372d3225",
                    "user": {
                        "_id": "647ccbd6e07cf9bb2d485244",
                        "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
                        "isPro": false,
                        "fullname": "Zihan Qiu",
                        "user": "QwQZh",
                        "type": "user"
                    },
                    "name": "Zihan Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:18:58.545Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T13:41:34.000Z",
            "submittedOnDailyAt": "2025-05-19T01:23:20.310Z",
            "title": "Qwen3 Technical Report",
            "submittedOnDailyBy": {
                "_id": "610b70452719facd4ea85e28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                "isPro": false,
                "fullname": "Chujie Zheng",
                "user": "chujiezheng",
                "type": "user"
            },
            "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
            "upvotes": 114,
            "discussionId": "68299e3228752b51372d325f",
            "projectPage": "https://qwenlm.github.io/blog/qwen3/",
            "githubRepo": "https://github.com/QwenLM/Qwen3",
            "ai_keywords": [
                "large language models (LLMs)",
                "Mixture-of-Expert (MoE) architectures",
                "thinking mode",
                "non-thinking mode",
                "chat-optimized models",
                "dedicated reasoning models",
                "thinking budget mechanism",
                "computational resources adaptively",
                "inference",
                "latency",
                "performance",
                "code generation",
                "mathematical reasoning",
                "agent tasks",
                "multilingual support",
                "cross-lingual understanding",
                "generation capabilities"
            ]
        },
        "publishedAt": "2025-05-14T09:41:34.000Z",
        "title": "Qwen3 Technical Report",
        "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09388.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "fullname": "Chujie Zheng",
            "name": "chujiezheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 38
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11049",
            "authors": [
                {
                    "_id": "682af4241286a7273c5bfd09",
                    "user": {
                        "_id": "6650c77a74664a42ddfb9187",
                        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                        "isPro": false,
                        "fullname": "yueliu1999",
                        "user": "yueliu1999",
                        "type": "user"
                    },
                    "name": "Yue Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:09.117Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0a",
                    "user": {
                        "_id": "6366429195204b4649c658b8",
                        "avatarUrl": "/avatars/5d80e9ebe0b57fd815f36796b9187248.svg",
                        "isPro": false,
                        "fullname": "Shengfang Zhai",
                        "user": "zsf",
                        "type": "user"
                    },
                    "name": "Shengfang Zhai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:16.514Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0b",
                    "user": {
                        "_id": "61711f02e0b1ddb56eb9b526",
                        "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
                        "isPro": true,
                        "fullname": "Mingzhe Du",
                        "user": "Elfsong",
                        "type": "user"
                    },
                    "name": "Mingzhe Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:22.634Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0c",
                    "user": {
                        "_id": "65efc25828426de60f977dfc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8ZcIoo58JPLdnjm-jZeo.png",
                        "isPro": false,
                        "fullname": "Yulin Chen",
                        "user": "CallMeChen",
                        "type": "user"
                    },
                    "name": "Yulin Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:29.818Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0d",
                    "name": "Tri Cao",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0e",
                    "user": {
                        "_id": "62728f4f6253fe2068da1021",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                        "isPro": false,
                        "fullname": "Hongcheng Gao",
                        "user": "HongchengGao",
                        "type": "user"
                    },
                    "name": "Hongcheng Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:18:48.924Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd0f",
                    "name": "Cheng Wang",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd10",
                    "name": "Xinfeng Li",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd11",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd12",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd13",
                    "user": {
                        "_id": "669e19e5dac1eb34c0f5f505",
                        "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Zhang",
                        "user": "jiaheng233",
                        "type": "user"
                    },
                    "name": "Jiaheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:19:28.414Z",
                    "hidden": false
                },
                {
                    "_id": "682af4241286a7273c5bfd14",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:19:35.772Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T09:46:10.000Z",
            "submittedOnDailyAt": "2025-05-19T07:36:35.140Z",
            "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
            "submittedOnDailyBy": {
                "_id": "6650c77a74664a42ddfb9187",
                "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
                "isPro": false,
                "fullname": "yueliu1999",
                "user": "yueliu1999",
                "type": "user"
            },
            "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
            "upvotes": 41,
            "discussionId": "682af42c1286a7273c5bfed9",
            "ai_keywords": [
                "GuardReasoner-VL",
                "online RL",
                "GuardReasoner-VLTrain",
                "reasoning corpus",
                "SFT",
                "rejection sampling",
                "data augmentation",
                "safety-aware data concatenation",
                "dynamic clipping parameter",
                "length-aware safety reward",
                "F1 score"
            ]
        },
        "publishedAt": "2025-05-16T05:46:10.000Z",
        "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
        "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11049.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "fullname": "yueliu1999",
            "name": "yueliu1999",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10610",
            "authors": [
                {
                    "_id": "682adaf581c740ab4aabc5a3",
                    "user": {
                        "_id": "62281c11236b7b2eefa7f198",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62281c11236b7b2eefa7f198/xuTCVMUahpg4Mfb7L62sm.jpeg",
                        "isPro": false,
                        "fullname": "Zhaowei Wang",
                        "user": "ZhaoweiWang",
                        "type": "user"
                    },
                    "name": "Zhaowei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:12.290Z",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a4",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a5",
                    "name": "Xiyu Ren",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a6",
                    "name": "Jipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a7",
                    "user": {
                        "_id": "654a500d9b8bd6406d431c0d",
                        "avatarUrl": "/avatars/a6b76441bbc6f4b71d49c52e454c9ef7.svg",
                        "isPro": false,
                        "fullname": "Yu Zhao",
                        "user": "yuzhaouoe",
                        "type": "user"
                    },
                    "name": "Yu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:10.234Z",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a8",
                    "user": {
                        "_id": "657ccbf2869d5bb0e53b482f",
                        "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
                        "isPro": false,
                        "fullname": "Rohit Saxena",
                        "user": "rohitsaxena",
                        "type": "user"
                    },
                    "name": "Rohit Saxena",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:08.362Z",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5a9",
                    "name": "Liang Cheng",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5aa",
                    "name": "Ginny Wong",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ab",
                    "name": "Simon See",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ac",
                    "name": "Pasquale Minervini",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ad",
                    "name": "Yangqiu Song",
                    "hidden": false
                },
                {
                    "_id": "682adaf581c740ab4aabc5ae",
                    "name": "Mark Steedman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T17:52:54.000Z",
            "submittedOnDailyAt": "2025-05-19T08:37:50.522Z",
            "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
            "submittedOnDailyBy": {
                "_id": "657ccbf2869d5bb0e53b482f",
                "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
                "isPro": false,
                "fullname": "Rohit Saxena",
                "user": "rohitsaxena",
                "type": "user"
            },
            "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
            "upvotes": 41,
            "discussionId": "682adaf681c740ab4aabc5e2",
            "projectPage": "https://zhaowei-wang-nlp.github.io/MMLongBench-page/",
            "githubRepo": "https://github.com/EdinburghNLP/MMLongBench",
            "ai_keywords": [
                "long-context vision-language models (LCVLMs)",
                "MMLongBench",
                "Visual RAG",
                "Many-Shot ICL",
                "vision patches",
                "cross-modal tokenization scheme",
                "long-context vision-language tasks",
                "reasoning ability"
            ]
        },
        "publishedAt": "2025-05-15T13:52:54.000Z",
        "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
        "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10610.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "657ccbf2869d5bb0e53b482f",
            "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
            "fullname": "Rohit Saxena",
            "name": "rohitsaxena",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11409",
            "authors": [
                {
                    "_id": "682abb7984695084c1a48eab",
                    "name": "Yi Xu",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eac",
                    "user": {
                        "_id": "650e9b0288cdfe73a8575923",
                        "avatarUrl": "/avatars/0fc7fcd0776f63ea5f50a310e7def2f5.svg",
                        "isPro": false,
                        "fullname": "Chengzu Li",
                        "user": "chengzu",
                        "type": "user"
                    },
                    "name": "Chengzu Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T13:09:14.188Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48ead",
                    "user": {
                        "_id": "62b279e92375526ae51a537b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
                        "isPro": false,
                        "fullname": "Han Zhou",
                        "user": "hzhouml",
                        "type": "user"
                    },
                    "name": "Han Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:42:16.276Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eae",
                    "user": {
                        "_id": "65bf213f8467e2a3d6374d4b",
                        "avatarUrl": "/avatars/0194cdba95d7a4c01fbbdd505e384a3d.svg",
                        "isPro": false,
                        "fullname": "X Wan",
                        "user": "masonxw",
                        "type": "user"
                    },
                    "name": "Xingchen Wan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-19T05:02:52.536Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eaf",
                    "user": {
                        "_id": "63920dfac47e36ddeb8f1864",
                        "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
                        "isPro": false,
                        "fullname": "Caiqi Zhang",
                        "user": "caiqizh",
                        "type": "user"
                    },
                    "name": "Caiqi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:23:48.005Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eb0",
                    "user": {
                        "_id": "617a6284941993035fbaf299",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635410461794-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Anna Korhonen",
                        "user": "akorhonen",
                        "type": "user"
                    },
                    "name": "Anna Korhonen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:23:42.059Z",
                    "hidden": false
                },
                {
                    "_id": "682abb7984695084c1a48eb1",
                    "user": {
                        "_id": "6273e70dc8d55dd434bd8e52",
                        "avatarUrl": "/avatars/3483eeda218e95b1eb00c3dc63c7d000.svg",
                        "isPro": false,
                        "fullname": "Ivan Vulić",
                        "user": "ivulic",
                        "type": "user"
                    },
                    "name": "Ivan Vulić",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:23:36.111Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
            ],
            "publishedAt": "2025-05-16T16:17:22.000Z",
            "submittedOnDailyAt": "2025-05-19T03:37:48.826Z",
            "title": "Visual Planning: Let's Think Only with Images",
            "submittedOnDailyBy": {
                "_id": "62b279e92375526ae51a537b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
                "isPro": false,
                "fullname": "Han Zhou",
                "user": "hzhouml",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
            "upvotes": 32,
            "discussionId": "682abb7c84695084c1a48fb4",
            "githubRepo": "https://github.com/yix8/VisualPlanning",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "multimodal extensions (MLLMs)",
                "machine reasoning",
                "visual information",
                "Visual Planning",
                "purely visual representations",
                "sequences of images",
                "step-by-step inference",
                "Visual Planning via Reinforcement Learning (VPRL)",
                "GRPO",
                "post-training large vision models",
                "planning",
                "visual navigation tasks",
                "FrozenLake",
                "Maze",
                "MiniBehavior",
                "text-only space",
                "intuitive, image-based inference"
            ]
        },
        "publishedAt": "2025-05-16T12:17:22.000Z",
        "title": "Visual Planning: Let's Think Only with Images",
        "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11409.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "62b279e92375526ae51a537b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
            "fullname": "Han Zhou",
            "name": "hzhouml",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11107",
            "authors": [
                {
                    "_id": "682ad96cdc6d7453624831b9",
                    "user": {
                        "_id": "6213410828005421265b27d3",
                        "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
                        "isPro": false,
                        "fullname": "許湛然",
                        "user": "Splend1dchan",
                        "type": "user"
                    },
                    "name": "Chan-Jan Hsu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T07:23:15.798Z",
                    "hidden": false
                },
                {
                    "_id": "682ad96cdc6d7453624831ba",
                    "name": "Davide Buffelli",
                    "hidden": false
                },
                {
                    "_id": "682ad96cdc6d7453624831bb",
                    "user": {
                        "_id": "633ee7834be90e06da1ea244",
                        "avatarUrl": "/avatars/c981cb4250e9646fe8bf11045cfa715d.svg",
                        "isPro": false,
                        "fullname": "Jamie McGowan",
                        "user": "jamie-mcg",
                        "type": "user"
                    },
                    "name": "Jamie McGowan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:20:45.715Z",
                    "hidden": false
                },
                {
                    "_id": "682ad96cdc6d7453624831bc",
                    "user": {
                        "_id": "643fb7332397d8eef5b844cd",
                        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
                        "isPro": false,
                        "fullname": "Liao",
                        "user": "FengTing",
                        "type": "user"
                    },
                    "name": "Feng-Ting Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:20:58.111Z",
                    "hidden": false
                },
                {
                    "_id": "682ad96cdc6d7453624831bd",
                    "user": {
                        "_id": "64a5fa024a594ea7ccfe9ec5",
                        "avatarUrl": "/avatars/2071e6078489b3d09737585d33002063.svg",
                        "isPro": false,
                        "fullname": "Yi-Chang Chen",
                        "user": "scsnake",
                        "type": "user"
                    },
                    "name": "Yi-Chang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:21:05.466Z",
                    "hidden": false
                },
                {
                    "_id": "682ad96cdc6d7453624831be",
                    "user": {
                        "_id": "64c242ce129617dbaba8d439",
                        "avatarUrl": "/avatars/947cf4330ed1ce2cbdd5252cd91c9024.svg",
                        "isPro": false,
                        "fullname": "Sattar Vakili",
                        "user": "sattar-vakili",
                        "type": "user"
                    },
                    "name": "Sattar Vakili",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:21:12.208Z",
                    "hidden": false
                },
                {
                    "_id": "682ad96cdc6d7453624831bf",
                    "user": {
                        "_id": "6811b1294119e4ecc92fc93b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/srmY2yyzOg9KRDSLYXJKf.png",
                        "isPro": false,
                        "fullname": "Dashan Shiu",
                        "user": "dsshiu",
                        "type": "user"
                    },
                    "name": "Da-shan Shiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:21:22.352Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T10:40:35.000Z",
            "submittedOnDailyAt": "2025-05-19T05:58:53.531Z",
            "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
            "submittedOnDailyBy": {
                "_id": "6213410828005421265b27d3",
                "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
                "isPro": false,
                "fullname": "許湛然",
                "user": "Splend1dchan",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
            "upvotes": 16,
            "discussionId": "682ad96ddc6d7453624831f3",
            "ai_keywords": [
                "large language models (LLMs)",
                "reasoning through self-generated chains of thought",
                "reasoning agents",
                "turn-based manner",
                "Group Think",
                "concurrent reasoning agents",
                "think ers",
                "shared visibility",
                "reasoning trajectories",
                "token level",
                "reasoning thread",
                "fine-grained, token-level collaboration",
                "redundant reasoning",
                "edge inference",
                "modification",
                "LLMs",
                "local GPU",
                "evaluation strategy",
                "reasoning latency"
            ]
        },
        "publishedAt": "2025-05-16T06:40:35.000Z",
        "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
        "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11107.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6213410828005421265b27d3",
            "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
            "fullname": "許湛然",
            "name": "Splend1dchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.07675",
            "authors": [
                {
                    "_id": "6829dcab0daa5ccc817e6ec8",
                    "user": {
                        "_id": "6357a08f8ed056fa1ccd3b38",
                        "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
                        "isPro": false,
                        "fullname": "erjui",
                        "user": "erjui",
                        "type": "user"
                    },
                    "name": "Seongjae Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T17:47:06.348Z",
                    "hidden": false
                },
                {
                    "_id": "6829dcab0daa5ccc817e6ec9",
                    "user": {
                        "_id": "64f000769e7770db74d44bba",
                        "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
                        "isPro": false,
                        "fullname": "Dong-Bok Lee",
                        "user": "dongboklee",
                        "type": "user"
                    },
                    "name": "Dong Bok Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:42:58.152Z",
                    "hidden": false
                },
                {
                    "_id": "6829dcab0daa5ccc817e6eca",
                    "user": {
                        "_id": "633ad9e4976a7d6910e84d15",
                        "avatarUrl": "/avatars/bb464e7503d6e865d1c351430982f7dd.svg",
                        "isPro": false,
                        "fullname": "Hyungjoon Jang",
                        "user": "hjoon",
                        "type": "user"
                    },
                    "name": "Hyungjoon Jang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:20:23.560Z",
                    "hidden": false
                },
                {
                    "_id": "6829dcab0daa5ccc817e6ecb",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-12T15:39:51.000Z",
            "submittedOnDailyAt": "2025-05-19T06:17:24.942Z",
            "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
            "submittedOnDailyBy": {
                "_id": "64f000769e7770db74d44bba",
                "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
                "isPro": false,
                "fullname": "Dong-Bok Lee",
                "user": "dongboklee",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
            "upvotes": 15,
            "discussionId": "6829dcad0daa5ccc817e6f40",
            "ai_keywords": [
                "Vision-language models (VLMs)",
                "knowledge distillation (KD)",
                "dual prediction heads",
                "gradient conflicts",
                "feature learning",
                "semi-supervised settings",
                "state-of-the-art performance",
                "ImageNet",
                "accuracy"
            ]
        },
        "publishedAt": "2025-05-12T11:39:51.000Z",
        "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
        "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07675.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64f000769e7770db74d44bba",
            "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
            "fullname": "Dong-Bok Lee",
            "name": "dongboklee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11427",
            "authors": [
                {
                    "_id": "682ad9809506a7e45a93be00",
                    "user": {
                        "_id": "6318e7a2acffc70bd4e057ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6318e7a2acffc70bd4e057ec/2m3XSbNLwv7Kmo8qfWq3L.jpeg",
                        "isPro": false,
                        "fullname": "Adrian Robert Minut",
                        "user": "adrianrob",
                        "type": "user"
                    },
                    "name": "Adrian Robert Minut",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:25:22.059Z",
                    "hidden": false
                },
                {
                    "_id": "682ad9809506a7e45a93be01",
                    "user": {
                        "_id": "63ab16a6d7ee953f604ecd52",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
                        "isPro": false,
                        "fullname": "Tommaso Mencattini",
                        "user": "tmencatt",
                        "type": "user"
                    },
                    "name": "Tommaso Mencattini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T07:22:21.898Z",
                    "hidden": false
                },
                {
                    "_id": "682ad9809506a7e45a93be02",
                    "user": {
                        "_id": "5e8ef1f14957053f606489e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
                        "isPro": false,
                        "fullname": "Andrea Santilli",
                        "user": "teelinsan",
                        "type": "user"
                    },
                    "name": "Andrea Santilli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T07:22:26.518Z",
                    "hidden": false
                },
                {
                    "_id": "682ad9809506a7e45a93be03",
                    "user": {
                        "_id": "64256584daa3502ee3570b86",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64256584daa3502ee3570b86/kui0eb59S5aTUeZIjawUj.jpeg",
                        "isPro": false,
                        "fullname": "Donato Crisostomi",
                        "user": "crisostomi",
                        "type": "user"
                    },
                    "name": "Donato Crisostomi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:25:28.737Z",
                    "hidden": false
                },
                {
                    "_id": "682ad9809506a7e45a93be04",
                    "user": {
                        "_id": "652681664e066bf73f8e2bd1",
                        "avatarUrl": "/avatars/084dec4765d9996d74901b8df95ec35f.svg",
                        "isPro": false,
                        "fullname": "Emanuele Rodola'",
                        "user": "erodola",
                        "type": "user"
                    },
                    "name": "Emanuele Rodolà",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:25:35.566Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T16:43:23.000Z",
            "submittedOnDailyAt": "2025-05-19T05:45:27.421Z",
            "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
            "submittedOnDailyBy": {
                "_id": "5e8ef1f14957053f606489e6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
                "isPro": false,
                "fullname": "Andrea Santilli",
                "user": "teelinsan",
                "type": "user"
            },
            "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
            "upvotes": 10,
            "discussionId": "682ad9819506a7e45a93be38",
            "githubRepo": "https://github.com/tommasomncttn/mergenetic",
            "ai_keywords": [
                "model merging",
                "evolutionary algorithms",
                "Mergenetic",
                "fitness estimators",
                "evaluation costs"
            ]
        },
        "publishedAt": "2025-05-16T12:43:23.000Z",
        "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
        "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11427.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "fullname": "Andrea Santilli",
            "name": "teelinsan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10518",
            "authors": [
                {
                    "_id": "68271c682f2e31ef0667bfaf",
                    "user": {
                        "_id": "668e4d1b446c8736208d99e1",
                        "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
                        "isPro": false,
                        "fullname": "Anastasios Gerontopoulos",
                        "user": "nasos10",
                        "type": "user"
                    },
                    "name": "Anastasios Gerontopoulos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-18T19:39:25.735Z",
                    "hidden": false
                },
                {
                    "_id": "68271c682f2e31ef0667bfb0",
                    "name": "Spyros Gidaris",
                    "hidden": false
                },
                {
                    "_id": "68271c682f2e31ef0667bfb1",
                    "name": "Nikos Komodakis",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
            ],
            "publishedAt": "2025-05-15T17:25:03.000Z",
            "submittedOnDailyAt": "2025-05-19T07:50:27.978Z",
            "title": "Multi-Token Prediction Needs Registers",
            "submittedOnDailyBy": {
                "_id": "668e4d1b446c8736208d99e1",
                "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
                "isPro": false,
                "fullname": "Anastasios Gerontopoulos",
                "user": "nasos10",
                "type": "user"
            },
            "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.",
            "upvotes": 9,
            "discussionId": "68271c692f2e31ef0667bff6",
            "githubRepo": "https://github.com/nasosger/MuToR",
            "ai_keywords": [
                "register tokens",
                "multi-token prediction",
                "next-token pretraining",
                "parameter-efficient fine-tuning (PEFT)",
                "generative tasks"
            ]
        },
        "publishedAt": "2025-05-15T13:25:03.000Z",
        "title": "Multi-Token Prediction Needs Registers",
        "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10518.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668e4d1b446c8736208d99e1",
            "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
            "fullname": "Anastasios Gerontopoulos",
            "name": "nasos10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10962",
            "authors": [
                {
                    "_id": "682ab4fe7a9f1a7ec9779dd6",
                    "user": {
                        "_id": "62ffa3f8311cad266f9af236",
                        "avatarUrl": "/avatars/4c88cb518e000a475f8381573f21aa7f.svg",
                        "isPro": false,
                        "fullname": "Zhenwen Liang",
                        "user": "invokerliang",
                        "type": "user"
                    },
                    "name": "Zhenwen Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:24:26.692Z",
                    "hidden": false
                },
                {
                    "_id": "682ab4fe7a9f1a7ec9779dd7",
                    "user": {
                        "_id": "64c94eddcb2f1bf0e7db5a4d",
                        "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
                        "isPro": false,
                        "fullname": "Linfeng Song",
                        "user": "freesunshine0316",
                        "type": "user"
                    },
                    "name": "Linfeng Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:24:45.999Z",
                    "hidden": false
                },
                {
                    "_id": "682ab4fe7a9f1a7ec9779dd8",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "682ab4fe7a9f1a7ec9779dd9",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "682ab4fe7a9f1a7ec9779dda",
                    "name": "Feng Zhang",
                    "hidden": false
                },
                {
                    "_id": "682ab4fe7a9f1a7ec9779ddb",
                    "user": {
                        "_id": "65147a1426fbd558dbd08f1b",
                        "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
                        "isPro": false,
                        "fullname": "Haitao Mi",
                        "user": "haitaominlp",
                        "type": "user"
                    },
                    "name": "Haitao Mi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:24:56.781Z",
                    "hidden": false
                },
                {
                    "_id": "682ab4fe7a9f1a7ec9779ddc",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T07:56:03.000Z",
            "submittedOnDailyAt": "2025-05-19T03:06:11.065Z",
            "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
            "submittedOnDailyBy": {
                "_id": "64c94eddcb2f1bf0e7db5a4d",
                "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
                "isPro": false,
                "fullname": "Linfeng Song",
                "user": "freesunshine0316",
                "type": "user"
            },
            "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
            "upvotes": 7,
            "discussionId": "682ab4ff7a9f1a7ec9779e71",
            "ai_keywords": [
                "Automated Theorem Proving (ATP)",
                "large language models (LLMs)",
                "biased search guidance",
                "Multi-Perspective Search Prover (MPS-Prover)",
                "post-training data curation strategy",
                "multi-perspective tree search mechanism",
                "learned critic model",
                "heuristic rules",
                "tactic selection",
                "search robustness",
                "miniF2F",
                "ProofNet",
                "state-of-the-art performance",
                "formal reasoning"
            ]
        },
        "publishedAt": "2025-05-16T03:56:03.000Z",
        "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
        "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10962.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "fullname": "Linfeng Song",
            "name": "freesunshine0316",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11140",
            "authors": [
                {
                    "_id": "682ad417500638b80a43471d",
                    "user": {
                        "_id": "60d33fbbd7b174177faabd4f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
                        "isPro": true,
                        "fullname": "Mike Zhang",
                        "user": "jjzha",
                        "type": "user"
                    },
                    "name": "Mike Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T07:23:04.053Z",
                    "hidden": false
                },
                {
                    "_id": "682ad417500638b80a43471e",
                    "user": {
                        "_id": "678fa79005ae7fe48d03ba47",
                        "avatarUrl": "/avatars/a78ab2b37fa3e18ace783f6f71f5a361.svg",
                        "isPro": false,
                        "fullname": "Johannes Bjerva",
                        "user": "bjerva",
                        "type": "user"
                    },
                    "name": "Johannes Bjerva",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:25:58.827Z",
                    "hidden": false
                },
                {
                    "_id": "682ad417500638b80a43471f",
                    "user": {
                        "_id": "60ed4c56abab3c2620df8ac8",
                        "avatarUrl": "/avatars/ad5508c1c94a96f6d1290e4735e81b73.svg",
                        "isPro": false,
                        "fullname": "Russa Biswas",
                        "user": "rubis",
                        "type": "user"
                    },
                    "name": "Russa Biswas",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:26:04.749Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T11:39:33.000Z",
            "submittedOnDailyAt": "2025-05-19T05:25:57.460Z",
            "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "60d33fbbd7b174177faabd4f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
                "isPro": true,
                "fullname": "Mike Zhang",
                "user": "jjzha",
                "type": "user"
            },
            "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
            "upvotes": 5,
            "discussionId": "682ad418500638b80a434770",
            "githubRepo": "https://github.com/jjzha/fs1",
            "ai_keywords": [
                "large language model (LLM)",
                "reasoning capabilities",
                "mathematical reasoning",
                "length thinking process",
                "computational resources",
                "inference",
                "complex open-domain question-answering (QA)",
                "reasoning traces",
                "reasoning models",
                "QwQ-32B",
                "DeepSeek-R1-671B",
                "instruction-tuned variants",
                "Qwen2.5",
                "knowledge graphs",
                "paths",
                "reasoning traces",
                "baseline approaches",
                "instruction-tuned models",
                "benchmark",
                "datasets",
                "experimental runs",
                "factual accuracy",
                "test-time compute",
                "token budgets",
                "test-time scaling",
                "reasoning accuracy"
            ]
        },
        "publishedAt": "2025-05-16T07:39:33.000Z",
        "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
        "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11140.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60d33fbbd7b174177faabd4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
            "fullname": "Mike Zhang",
            "name": "jjzha",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 56
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11011",
            "authors": [
                {
                    "_id": "682ae098730bd40a0755f87c",
                    "name": "Darija Barak",
                    "hidden": false
                },
                {
                    "_id": "682ae098730bd40a0755f87d",
                    "name": "Miguel Costa-Gomes",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T09:01:09.000Z",
            "submittedOnDailyAt": "2025-05-19T06:12:42.874Z",
            "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
            "upvotes": 4,
            "discussionId": "682ae099730bd40a0755f8b9",
            "ai_keywords": [
                "p-beauty contest",
                "Nash-equilibrium choices",
                "strategic reasoning ability",
                "reasoning ability",
                "propensity towards cooperation",
                "mechanism design"
            ]
        },
        "publishedAt": "2025-05-16T05:01:09.000Z",
        "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
        "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11011.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10852",
            "authors": [
                {
                    "_id": "682a8dfbc8a4b019406bd8b0",
                    "user": {
                        "_id": "6508469df3060ea840fd888f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508469df3060ea840fd888f/3vwlKo0Cd9UWOPlwOvlJw.jpeg",
                        "isPro": false,
                        "fullname": "Siyu Liu",
                        "user": "SiyuLiu",
                        "type": "user"
                    },
                    "name": "Siyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:42:45.155Z",
                    "hidden": false
                },
                {
                    "_id": "682a8dfbc8a4b019406bd8b1",
                    "user": {
                        "_id": "656cf0757c934a7b3c2302c0",
                        "avatarUrl": "/avatars/4d4f585df63f0e2a23a56f9cbd067ba0.svg",
                        "isPro": false,
                        "fullname": "Jiamin Xu",
                        "user": "jiaminxu1019",
                        "type": "user"
                    },
                    "name": "Jiamin Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:22:06.474Z",
                    "hidden": false
                },
                {
                    "_id": "682a8dfbc8a4b019406bd8b2",
                    "name": "Beilin Ye",
                    "hidden": false
                },
                {
                    "_id": "682a8dfbc8a4b019406bd8b3",
                    "name": "Bo Hu",
                    "hidden": false
                },
                {
                    "_id": "682a8dfbc8a4b019406bd8b4",
                    "name": "David J. Srolovitz",
                    "hidden": false
                },
                {
                    "_id": "682a8dfbc8a4b019406bd8b5",
                    "name": "Tongqi Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T04:43:05.000Z",
            "submittedOnDailyAt": "2025-05-19T09:20:48.570Z",
            "title": "MatTools: Benchmarking Large Language Models for Materials Science Tools",
            "submittedOnDailyBy": {
                "_id": "6508469df3060ea840fd888f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508469df3060ea840fd888f/3vwlKo0Cd9UWOPlwOvlJw.jpeg",
                "isPro": false,
                "fullname": "Siyu Liu",
                "user": "SiyuLiu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.",
            "upvotes": 4,
            "discussionId": "682a8dfcc8a4b019406bd90e",
            "githubRepo": "https://github.com/Grenzlinie/MatTools",
            "ai_keywords": [
                "Large language models (LLMs)",
                "literature comprehension",
                "property prediction",
                "materials discovery",
                "alloy design",
                "physics-based computational approaches",
                "materials properties",
                "MatTools",
                "materials simulation tool question-answer (QA) benchmark",
                "real-world tool-usage benchmark",
                "automated methodology",
                "pymatgen (Python Materials Genomics)",
                "QA pairs",
                "functional Python code",
                "materials property calculations",
                "Generalists",
                "specialists"
            ]
        },
        "publishedAt": "2025-05-16T00:43:05.000Z",
        "title": "MatTools: Benchmarking Large Language Models for Materials Science Tools",
        "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10852.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6508469df3060ea840fd888f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6508469df3060ea840fd888f/3vwlKo0Cd9UWOPlwOvlJw.jpeg",
            "fullname": "Siyu Liu",
            "name": "SiyuLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11480",
            "authors": [
                {
                    "_id": "682b5efc970fc73c32ddc4e4",
                    "name": "Anjiang Wei",
                    "hidden": false
                },
                {
                    "_id": "682b5efc970fc73c32ddc4e5",
                    "name": "Tarun Suresh",
                    "hidden": false
                },
                {
                    "_id": "682b5efc970fc73c32ddc4e6",
                    "name": "Huanmi Tan",
                    "hidden": false
                },
                {
                    "_id": "682b5efc970fc73c32ddc4e7",
                    "name": "Yinglun Xu",
                    "hidden": false
                },
                {
                    "_id": "682b5efc970fc73c32ddc4e8",
                    "name": "Gagandeep Singh",
                    "hidden": false
                },
                {
                    "_id": "682b5efc970fc73c32ddc4e9",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "682b5efc970fc73c32ddc4ea",
                    "name": "Alex Aiken",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T17:40:45.000Z",
            "submittedOnDailyAt": "2025-05-19T15:10:48.516Z",
            "title": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.",
            "upvotes": 2,
            "discussionId": "682b5efc970fc73c32ddc518",
            "ai_keywords": [
                "reinforcement learning framework",
                "Proximal Policy Optimization (PPO)",
                "reward function",
                "test cases",
                "execution performance",
                "gcc -O3",
                "benchmark",
                "Qwen2.5-Coder-7B-PPO",
                "test pass rates",
                "speedup"
            ]
        },
        "publishedAt": "2025-05-16T13:40:45.000Z",
        "title": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning",
        "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11480.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6885
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.11152",
            "authors": [
                {
                    "_id": "682a9a3f5e6f0c59f4d8a0e5",
                    "user": {
                        "_id": "65601c6ee23401f82005e361",
                        "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
                        "isPro": false,
                        "fullname": "Daniel Sungho Jung",
                        "user": "dqj5182",
                        "type": "user"
                    },
                    "name": "Daniel Sungho Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-19T06:42:43.502Z",
                    "hidden": false
                },
                {
                    "_id": "682a9a3f5e6f0c59f4d8a0e6",
                    "user": {
                        "_id": "656056b21392aa3beb5de0bd",
                        "avatarUrl": "/avatars/07f25b750ef308d65f2e6c82506e7816.svg",
                        "isPro": false,
                        "fullname": "Kyoung Mu  Lee ",
                        "user": "kyoungmu",
                        "type": "user"
                    },
                    "name": "Kyoung Mu Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T07:25:48.640Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T11:54:25.000Z",
            "submittedOnDailyAt": "2025-05-19T01:11:35.713Z",
            "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
            "submittedOnDailyBy": {
                "_id": "65601c6ee23401f82005e361",
                "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
                "isPro": false,
                "fullname": "Daniel Sungho Jung",
                "user": "dqj5182",
                "type": "user"
            },
            "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
            "upvotes": 2,
            "discussionId": "682a9a405e6f0c59f4d8a125",
            "projectPage": "https://haco-release.github.io/",
            "githubRepo": "https://github.com/dqj5182/HACO_RELEASE",
            "ai_keywords": [
                "dense hand contact estimation",
                "class imbalance issue",
                "spatial imbalance issue",
                "finger tips",
                "balanced contact sampling",
                "vertex-level class-balanced (VCB) loss",
                "contact distribution",
                "contact frequency"
            ]
        },
        "publishedAt": "2025-05-16T07:54:25.000Z",
        "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
        "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11152.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65601c6ee23401f82005e361",
            "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
            "fullname": "Daniel Sungho Jung",
            "name": "dqj5182",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10769",
            "authors": [
                {
                    "_id": "682bb1d99476a97d3b983da0",
                    "name": "Manyu Li",
                    "hidden": false
                },
                {
                    "_id": "682bb1d99476a97d3b983da1",
                    "name": "Ruian He",
                    "hidden": false
                },
                {
                    "_id": "682bb1d99476a97d3b983da2",
                    "name": "Zixian Zhang",
                    "hidden": false
                },
                {
                    "_id": "682bb1d99476a97d3b983da3",
                    "name": "Weimin Tan",
                    "hidden": false
                },
                {
                    "_id": "682bb1d99476a97d3b983da4",
                    "name": "Bo Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T00:55:56.000Z",
            "submittedOnDailyAt": "2025-05-19T21:04:29.926Z",
            "title": "Unifying Segment Anything in Microscopy with Multimodal Large Language\n  Model",
            "submittedOnDailyBy": {
                "_id": "650871aeb44445e9b3625c7b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
                "isPro": false,
                "fullname": "James Burgess",
                "user": "jmhb",
                "type": "user"
            },
            "summary": "Accurate segmentation of regions of interest in biomedical images holds\nsubstantial value in image analysis. Although several foundation models for\nbiomedical segmentation have currently achieved excellent performance on\ncertain datasets, they typically demonstrate sub-optimal performance on unseen\ndomain data. We owe the deficiency to lack of vision-language knowledge before\nsegmentation. Multimodal Large Language Models (MLLMs) bring outstanding\nunderstanding and reasoning capabilities to multimodal tasks, which inspires us\nto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling\nvision models to demonstrate superior generalization capabilities on\ncross-domain datasets. In this paper, we propose using MLLMs to guide SAM in\nlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,\nnamed uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment\n(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find\nthat after SAM receives global VLK prompts, its performance improves\nsignificantly, but there are deficiencies in boundary contour perception.\nTherefore, we further propose Semantic Boundary Regularization (SBR) to prompt\nSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%\nin SA across 9 in-domain microscopy datasets, achieving state-of-the-art\nperformance. Our method also demonstrates improvements of 6.79% in Dice and\n10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization\ncapabilities. Code is available at https://github.com/ieellee/uLLSAM.",
            "upvotes": 2,
            "discussionId": "682bb1da9476a97d3b983dd2",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "Vision-Language Knowledge (VLK)",
                "Segment Anything Model (SAM)",
                "Vision-Language Semantic Alignment (VLSA)",
                "Semantic Boundary Regularization (SBR)",
                "Dice",
                "SA (Surface Area)"
            ]
        },
        "publishedAt": "2025-05-15T20:55:56.000Z",
        "title": "Unifying Segment Anything in Microscopy with Multimodal Large Language\n  Model",
        "summary": "Accurate segmentation of regions of interest in biomedical images holds\nsubstantial value in image analysis. Although several foundation models for\nbiomedical segmentation have currently achieved excellent performance on\ncertain datasets, they typically demonstrate sub-optimal performance on unseen\ndomain data. We owe the deficiency to lack of vision-language knowledge before\nsegmentation. Multimodal Large Language Models (MLLMs) bring outstanding\nunderstanding and reasoning capabilities to multimodal tasks, which inspires us\nto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling\nvision models to demonstrate superior generalization capabilities on\ncross-domain datasets. In this paper, we propose using MLLMs to guide SAM in\nlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,\nnamed uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment\n(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find\nthat after SAM receives global VLK prompts, its performance improves\nsignificantly, but there are deficiencies in boundary contour perception.\nTherefore, we further propose Semantic Boundary Regularization (SBR) to prompt\nSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%\nin SA across 9 in-domain microscopy datasets, achieving state-of-the-art\nperformance. Our method also demonstrates improvements of 6.79% in Dice and\n10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization\ncapabilities. Code is available at https://github.com/ieellee/uLLSAM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10769.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650871aeb44445e9b3625c7b/mtx3EnkuNF4z29IosnhaQ.png",
            "fullname": "James Burgess",
            "name": "jmhb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10496",
            "authors": [
                {
                    "_id": "6827c35bbc09f10228e2315e",
                    "user": {
                        "_id": "655cc4fd905b1390a44b19b8",
                        "avatarUrl": "/avatars/bbf5144468ba96317b6dc10e2d465d05.svg",
                        "isPro": false,
                        "fullname": "Raman Dutt",
                        "user": "raman07",
                        "type": "user"
                    },
                    "name": "Raman Dutt",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:22:37.406Z",
                    "hidden": false
                },
                {
                    "_id": "6827c35bbc09f10228e2315f",
                    "user": {
                        "_id": "633bf51720ceb9fcae5abda3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633bf51720ceb9fcae5abda3/2GlqQb-EPv_MiR3xraBen.jpeg",
                        "isPro": false,
                        "fullname": "Pedro Sanchez",
                        "user": "PedroSanchez",
                        "type": "user"
                    },
                    "name": "Pedro Sanchez",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:22:47.584Z",
                    "hidden": false
                },
                {
                    "_id": "6827c35bbc09f10228e23160",
                    "name": "Yongchen Yao",
                    "hidden": false
                },
                {
                    "_id": "6827c35bbc09f10228e23161",
                    "user": {
                        "_id": "67373673d50f5c2e018b4ed6",
                        "avatarUrl": "/avatars/5ebf329d917eac0be02dc5e7fb16a7b4.svg",
                        "isPro": false,
                        "fullname": "Steven McDonagh",
                        "user": "smcdonagh",
                        "type": "user"
                    },
                    "name": "Steven McDonagh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-19T14:23:02.159Z",
                    "hidden": false
                },
                {
                    "_id": "6827c35bbc09f10228e23162",
                    "name": "Sotirios A. Tsaftaris",
                    "hidden": false
                },
                {
                    "_id": "6827c35bbc09f10228e23163",
                    "name": "Timothy Hospedales",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655cc4fd905b1390a44b19b8/MP7mGslsxFJQRpJGU3Ktv.png"
            ],
            "publishedAt": "2025-05-15T16:59:17.000Z",
            "submittedOnDailyAt": "2025-05-19T11:23:36.811Z",
            "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs",
            "submittedOnDailyBy": {
                "_id": "655cc4fd905b1390a44b19b8",
                "avatarUrl": "/avatars/bbf5144468ba96317b6dc10e2d465d05.svg",
                "isPro": false,
                "fullname": "Raman Dutt",
                "user": "raman07",
                "type": "user"
            },
            "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/",
            "upvotes": 2,
            "discussionId": "6827c35cbc09f10228e2319d",
            "projectPage": "https://raman1121.github.io/CheXGenBench/",
            "githubRepo": "https://github.com/Raman1121/CheXGenBench",
            "ai_keywords": [
                "text-to-image generative models",
                "synthetic chest radiograph generation",
                "fidelity",
                "privacy risks",
                "clinical utility",
                "standardised data partitioning",
                "unified evaluation protocol",
                "quantitative metrics",
                "generation quality",
                "privacy vulnerabilities",
                "downstream clinical applicability",
                "high-quality, synthetic dataset",
                "SynthCheX-75K",
                "radiographs",
                "Sana 0.6B"
            ]
        },
        "publishedAt": "2025-05-15T12:59:17.000Z",
        "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs",
        "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655cc4fd905b1390a44b19b8/MP7mGslsxFJQRpJGU3Ktv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10496.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655cc4fd905b1390a44b19b8",
            "avatarUrl": "/avatars/bbf5144468ba96317b6dc10e2d465d05.svg",
            "fullname": "Raman Dutt",
            "name": "raman07",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09924",
            "authors": [
                {
                    "_id": "6829e74e54b087448a2805c5",
                    "user": {
                        "_id": "654da9621aeef70f9af2e7cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654da9621aeef70f9af2e7cb/kZ8D3isElQ_hmxbzZnmVw.jpeg",
                        "isPro": false,
                        "fullname": "Yidan Wang",
                        "user": "redwyd",
                        "type": "user"
                    },
                    "name": "Yidan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-18T19:39:08.834Z",
                    "hidden": false
                },
                {
                    "_id": "6829e74e54b087448a2805c6",
                    "name": "Yubing Ren",
                    "hidden": false
                },
                {
                    "_id": "6829e74e54b087448a2805c7",
                    "name": "Yanan Cao",
                    "hidden": false
                },
                {
                    "_id": "6829e74e54b087448a2805c8",
                    "name": "Binxing Fang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T03:12:36.000Z",
            "submittedOnDailyAt": "2025-05-19T10:56:25.190Z",
            "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "654da9621aeef70f9af2e7cb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654da9621aeef70f9af2e7cb/kZ8D3isElQ_hmxbzZnmVw.jpeg",
                "isPro": false,
                "fullname": "Yidan Wang",
                "user": "redwyd",
                "type": "user"
            },
            "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\nhttps://github.com/redwyd/SymMark{https://github.com/redwyd/SymMark}.",
            "upvotes": 2,
            "discussionId": "6829e75054b087448a280667",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "watermarking",
                "logits-based",
                "sampling-based",
                "token entropy",
                "semantic entropy",
                "symbiotic watermarking framework",
                "state-of-the-art (SOTA) performance"
            ]
        },
        "publishedAt": "2025-05-14T23:12:36.000Z",
        "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models",
        "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\nhttps://github.com/redwyd/SymMark{https://github.com/redwyd/SymMark}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654da9621aeef70f9af2e7cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654da9621aeef70f9af2e7cb/kZ8D3isElQ_hmxbzZnmVw.jpeg",
            "fullname": "Yidan Wang",
            "name": "redwyd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11493",
            "authors": [
                {
                    "_id": "682b70a7ebc1db7ecb14ee63",
                    "name": "Yusu Qian",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee64",
                    "name": "Jiasen Lu",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee65",
                    "name": "Tsu-Jui Fu",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee66",
                    "name": "Xinze Wang",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee67",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee68",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee69",
                    "name": "Wenze Hu",
                    "hidden": false
                },
                {
                    "_id": "682b70a7ebc1db7ecb14ee6a",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T17:55:54.000Z",
            "submittedOnDailyAt": "2025-05-19T16:26:06.969Z",
            "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing",
            "submittedOnDailyBy": {
                "_id": "65d5d6e590119106db895670",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d5d6e590119106db895670/SphTxh2E0MtT8T_bH0_zb.png",
                "isPro": false,
                "fullname": "Yusu Qian",
                "user": "YusuQian",
                "type": "user"
            },
            "summary": "Editing images using natural language instructions has become a natural and\nexpressive way to modify visual content; yet, evaluating the performance of\nsuch models remains challenging. Existing evaluation approaches often rely on\nimage-text similarity metrics like CLIP, which lack precision. In this work, we\nintroduce a new benchmark designed to evaluate text-guided image editing models\nin a more grounded manner, along two critical dimensions: (i) functional\ncorrectness, assessed via automatically generated multiple-choice questions\nthat verify whether the intended change was successfully applied; and (ii)\nimage content preservation, which ensures that non-targeted regions of the\nimage remain visually consistent using an object-aware masking technique and\npreservation scoring. The benchmark includes over 1000 high-quality editing\nexamples across 20 diverse content categories, each annotated with detailed\nediting instructions, evaluation questions, and spatial object masks. We\nconduct a large-scale study comparing GPT-Image-1, the latest flagship in the\ntext-guided image editing space, against several state-of-the-art editing\nmodels, and validate our automatic metrics against human ratings. Results show\nthat GPT-Image-1 leads in instruction-following accuracy, but often\nover-modifies irrelevant image regions, highlighting a key trade-off in the\ncurrent model behavior. GIE-Bench provides a scalable, reproducible framework\nfor advancing more accurate evaluation of text-guided image editing.",
            "upvotes": 1,
            "discussionId": "682b70acebc1db7ecb14efe8",
            "ai_keywords": [
                "text-guided image editing",
                "automatic generated multiple-choice questions",
                "object-aware masking technique",
                "preservation scoring",
                "GPT-Image-1",
                "instruction-following accuracy"
            ]
        },
        "publishedAt": "2025-05-16T13:55:54.000Z",
        "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing",
        "summary": "Editing images using natural language instructions has become a natural and\nexpressive way to modify visual content; yet, evaluating the performance of\nsuch models remains challenging. Existing evaluation approaches often rely on\nimage-text similarity metrics like CLIP, which lack precision. In this work, we\nintroduce a new benchmark designed to evaluate text-guided image editing models\nin a more grounded manner, along two critical dimensions: (i) functional\ncorrectness, assessed via automatically generated multiple-choice questions\nthat verify whether the intended change was successfully applied; and (ii)\nimage content preservation, which ensures that non-targeted regions of the\nimage remain visually consistent using an object-aware masking technique and\npreservation scoring. The benchmark includes over 1000 high-quality editing\nexamples across 20 diverse content categories, each annotated with detailed\nediting instructions, evaluation questions, and spatial object masks. We\nconduct a large-scale study comparing GPT-Image-1, the latest flagship in the\ntext-guided image editing space, against several state-of-the-art editing\nmodels, and validate our automatic metrics against human ratings. Results show\nthat GPT-Image-1 leads in instruction-following accuracy, but often\nover-modifies irrelevant image regions, highlighting a key trade-off in the\ncurrent model behavior. GIE-Bench provides a scalable, reproducible framework\nfor advancing more accurate evaluation of text-guided image editing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11493.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d5d6e590119106db895670",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d5d6e590119106db895670/SphTxh2E0MtT8T_bH0_zb.png",
            "fullname": "Yusu Qian",
            "name": "YusuQian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.05678",
            "authors": [
                {
                    "_id": "6828f2e4159fce854b16f5fa",
                    "user": {
                        "_id": "63b3e4b4b7fec0adf64ded69",
                        "avatarUrl": "/avatars/55804581c263c1d68b6fc59a1b0a3df7.svg",
                        "isPro": false,
                        "fullname": "Etai  Sella",
                        "user": "etaisella",
                        "type": "user"
                    },
                    "name": "Etai Sella",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-17T20:34:50.662Z",
                    "hidden": false
                },
                {
                    "_id": "6828f2e4159fce854b16f5fb",
                    "name": "Yanir Kleiman",
                    "hidden": false
                },
                {
                    "_id": "6828f2e4159fce854b16f5fc",
                    "name": "Hadar Averbuch-Elor",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63b3e4b4b7fec0adf64ded69/LET5cyH8WeoSdDfolb36w.mp4"
            ],
            "publishedAt": "2025-05-08T22:31:23.000Z",
            "submittedOnDailyAt": "2025-05-19T18:15:26.214Z",
            "title": "InstanceGen: Image Generation with Instance-level Instructions",
            "submittedOnDailyBy": {
                "_id": "63b3e4b4b7fec0adf64ded69",
                "avatarUrl": "/avatars/55804581c263c1d68b6fc59a1b0a3df7.svg",
                "isPro": false,
                "fullname": "Etai  Sella",
                "user": "etaisella",
                "type": "user"
            },
            "summary": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, typically in the form of coarse bounding\nboxes, to better guide the generation process in such challenging cases. In\nthis work, we take the idea of structural guidance a step further by making the\nobservation that contemporary image generation models can directly provide a\nplausible fine-grained structural initialization. We propose a technique that\ncouples this image-based structural guidance with LLM-based instance-level\ninstructions, yielding output images that adhere to all parts of the text\nprompt, including object counts, instance-level attributes, and spatial\nrelations between instances.",
            "upvotes": 1,
            "discussionId": "6828f2ea159fce854b16f7d7",
            "projectPage": "https://tau-vailab.github.io/InstanceGen/",
            "ai_keywords": [
                "generative models",
                "text-to-image models",
                "semantics",
                "prompts",
                "object counts",
                "instance-level attributes",
                "spatial relations",
                "bounding boxes",
                "image generation models",
                "fine-grained structural initialization",
                "LLM-based",
                "instance-level instructions"
            ]
        },
        "publishedAt": "2025-05-08T18:31:23.000Z",
        "title": "InstanceGen: Image Generation with Instance-level Instructions",
        "summary": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, typically in the form of coarse bounding\nboxes, to better guide the generation process in such challenging cases. In\nthis work, we take the idea of structural guidance a step further by making the\nobservation that contemporary image generation models can directly provide a\nplausible fine-grained structural initialization. We propose a technique that\ncouples this image-based structural guidance with LLM-based instance-level\ninstructions, yielding output images that adhere to all parts of the text\nprompt, including object counts, instance-level attributes, and spatial\nrelations between instances.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63b3e4b4b7fec0adf64ded69/LET5cyH8WeoSdDfolb36w.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05678.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b3e4b4b7fec0adf64ded69",
            "avatarUrl": "/avatars/55804581c263c1d68b6fc59a1b0a3df7.svg",
            "fullname": "Etai  Sella",
            "name": "etaisella",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11315",
            "authors": [
                {
                    "_id": "682b539908b49a9e5088330f",
                    "name": "Chin-Yun Yu",
                    "hidden": false
                },
                {
                    "_id": "682b539908b49a9e50883310",
                    "name": "Marco A. Martínez-Ramírez",
                    "hidden": false
                },
                {
                    "_id": "682b539908b49a9e50883311",
                    "name": "Junghyun Koo",
                    "hidden": false
                },
                {
                    "_id": "682b539908b49a9e50883312",
                    "name": "Wei-Hsiang Liao",
                    "hidden": false
                },
                {
                    "_id": "682b539908b49a9e50883313",
                    "name": "Yuki Mitsufuji",
                    "hidden": false
                },
                {
                    "_id": "682b539908b49a9e50883314",
                    "name": "György Fazekas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T14:40:31.000Z",
            "submittedOnDailyAt": "2025-05-19T14:23:13.760Z",
            "title": "Improving Inference-Time Optimisation for Vocal Effects Style Transfer\n  with a Gaussian Prior",
            "submittedOnDailyBy": {
                "_id": "621d85a10e35b2fbbf3e6196",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
                "isPro": false,
                "fullname": "Chin-Yun Yu",
                "user": "yoyolicoris",
                "type": "user"
            },
            "summary": "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach\nfor transferring the applied effects of a reference audio to a raw audio track.\nIt optimises the effect parameters to minimise the distance between the style\nembeddings of the processed audio and the reference. However, this method\ntreats all possible configurations equally and relies solely on the embedding\nspace, which can lead to unrealistic or biased results. We address this pitfall\nby introducing a Gaussian prior derived from a vocal preset dataset, DiffVox,\nover the parameter space. The resulting optimisation is equivalent to\nmaximum-a-posteriori estimation. Evaluations on vocal effects transfer on the\nMedleyDB dataset show significant improvements across metrics compared to\nbaselines, including a blind audio effects estimator, nearest-neighbour\napproaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter\nmean squared error by up to 33% and matches the reference style better.\nSubjective evaluations with 16 participants confirm our method's superiority,\nespecially in limited data regimes. This work demonstrates how incorporating\nprior knowledge in inference time enhances audio effects transfer, paving the\nway for more effective and realistic audio processing systems.",
            "upvotes": 0,
            "discussionId": "682b539a08b49a9e50883337",
            "ai_keywords": [
                "Gaussian prior",
                "DiffVox",
                "maximum-a-posteriori estimation",
                "MedleyDB",
                "blind audio effects estimator",
                "nearest-neighbour approaches",
                "parameter mean squared error"
            ]
        },
        "publishedAt": "2025-05-16T10:40:31.000Z",
        "title": "Improving Inference-Time Optimisation for Vocal Effects Style Transfer\n  with a Gaussian Prior",
        "summary": "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach\nfor transferring the applied effects of a reference audio to a raw audio track.\nIt optimises the effect parameters to minimise the distance between the style\nembeddings of the processed audio and the reference. However, this method\ntreats all possible configurations equally and relies solely on the embedding\nspace, which can lead to unrealistic or biased results. We address this pitfall\nby introducing a Gaussian prior derived from a vocal preset dataset, DiffVox,\nover the parameter space. The resulting optimisation is equivalent to\nmaximum-a-posteriori estimation. Evaluations on vocal effects transfer on the\nMedleyDB dataset show significant improvements across metrics compared to\nbaselines, including a blind audio effects estimator, nearest-neighbour\napproaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter\nmean squared error by up to 33% and matches the reference style better.\nSubjective evaluations with 16 participants confirm our method's superiority,\nespecially in limited data regimes. This work demonstrates how incorporating\nprior knowledge in inference time enhances audio effects transfer, paving the\nway for more effective and realistic audio processing systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11315.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "621d85a10e35b2fbbf3e6196",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
            "fullname": "Chin-Yun Yu",
            "name": "yoyolicoris",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]