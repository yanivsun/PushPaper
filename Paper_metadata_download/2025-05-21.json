[
    {
        "paper": {
            "id": "2505.14683",
            "authors": [
                {
                    "_id": "682d2fd84540abccd3b835e8",
                    "name": "Chaorui Deng",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835e9",
                    "name": "Deyao Zhu",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835ea",
                    "user": {
                        "_id": "61fb81006374891646732f37",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
                        "isPro": false,
                        "fullname": "Kunchang Li",
                        "user": "Andy1621",
                        "type": "user"
                    },
                    "name": "Kunchang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:41:06.469Z",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835eb",
                    "user": {
                        "_id": "652e9c5774d1b0d7ff73d091",
                        "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg",
                        "isPro": true,
                        "fullname": "Chenhui Gou",
                        "user": "gouc",
                        "type": "user"
                    },
                    "name": "Chenhui Gou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:41:08.903Z",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835ec",
                    "name": "Feng Li",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835ed",
                    "name": "Zeyu Wang",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835ee",
                    "name": "Shu Zhong",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835ef",
                    "user": {
                        "_id": "5df833bdda6d0311fd3d5403",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png",
                        "isPro": false,
                        "fullname": "Weihao Yu",
                        "user": "whyu",
                        "type": "user"
                    },
                    "name": "Weihao Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T09:31:55.569Z",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835f0",
                    "user": {
                        "_id": "64b6b81142134e053233c3c0",
                        "avatarUrl": "/avatars/5c7455d99a7a2648f77a531c9a71eb98.svg",
                        "isPro": false,
                        "fullname": "Xiaonan Nie",
                        "user": "codecaution",
                        "type": "user"
                    },
                    "name": "Xiaonan Nie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:06:14.057Z",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835f1",
                    "user": {
                        "_id": "617fe76105423df678cef199",
                        "avatarUrl": "/avatars/64c94a4d743edab18ecb4bb7c550f049.svg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "Ziang",
                        "type": "user"
                    },
                    "name": "Ziang Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:06:07.780Z",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835f2",
                    "name": "Guang Shi",
                    "hidden": false
                },
                {
                    "_id": "682d2fd84540abccd3b835f3",
                    "name": "Haoqi Fan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
            ],
            "publishedAt": "2025-05-20T17:59:30.000Z",
            "submittedOnDailyAt": "2025-05-21T00:38:53.960Z",
            "title": "Emerging Properties in Unified Multimodal Pretraining",
            "submittedOnDailyBy": {
                "_id": "61fb81006374891646732f37",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
                "isPro": false,
                "fullname": "Kunchang Li",
                "user": "Andy1621",
                "type": "user"
            },
            "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
            "upvotes": 86,
            "discussionId": "682d2fdc4540abccd3b836ee",
            "ai_summary": "BAGEL, an open-source foundational model trained on diverse multimodal data, significantly outperforms existing models in both generation and understanding tasks.",
            "ai_keywords": [
                "multimodal understanding",
                "multimodal generation",
                "foundational model",
                "decoder-only model",
                "trillions of tokens",
                "large-scale interleaved data",
                "complex multimodal reasoning",
                "free-form image manipulation",
                "future frame prediction",
                "3D manipulation",
                "world navigation"
            ]
        },
        "publishedAt": "2025-05-20T13:59:30.000Z",
        "title": "Emerging Properties in Unified Multimodal Pretraining",
        "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61fb81006374891646732f37/HQOfWqrOf9B97hWczL489.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14683.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "61fb81006374891646732f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1643872995181-61fb81006374891646732f37.jpeg",
            "fullname": "Kunchang Li",
            "name": "Andy1621",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11594",
            "authors": [
                {
                    "_id": "682d426251ce04237318cfe5",
                    "user": {
                        "_id": "66c0a08bac74db25de8427ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                        "isPro": false,
                        "fullname": "Jintao Zhang",
                        "user": "jt-zhang",
                        "type": "user"
                    },
                    "name": "Jintao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:46.065Z",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfe6",
                    "name": "Jia Wei",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfe7",
                    "user": {
                        "_id": "62cc11a4f1d37c16280a2923",
                        "avatarUrl": "/avatars/265b3cfb80f0a7b11a2ef67c49e29cf7.svg",
                        "isPro": false,
                        "fullname": "Pengle Zhang",
                        "user": "Guyan",
                        "type": "user"
                    },
                    "name": "Pengle Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:07:09.573Z",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfe8",
                    "name": "Xiaoming Xu",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfe9",
                    "user": {
                        "_id": "67ea1f6693f71dd8167a2d22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/H_upra_XVG1AoBKUe9ArV.png",
                        "isPro": false,
                        "fullname": "haofeng huang",
                        "user": "haofeng666",
                        "type": "user"
                    },
                    "name": "Haofeng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:06:48.450Z",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfea",
                    "user": {
                        "_id": "658c1802a1105f8157ad1db9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658c1802a1105f8157ad1db9/WzjY29SkngxkKfiTYcssh.jpeg",
                        "isPro": false,
                        "fullname": "whx1003",
                        "user": "whx1003",
                        "type": "user"
                    },
                    "name": "Haoxu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T10:30:12.064Z",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfeb",
                    "name": "Kai Jiang",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfec",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "682d426251ce04237318cfed",
                    "user": {
                        "_id": "65fcad0ba0d7adc40b54fac2",
                        "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
                        "isPro": false,
                        "fullname": "Jianfei Chen",
                        "user": "surfingtomchen",
                        "type": "user"
                    },
                    "name": "Jianfei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:06:40.981Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
            ],
            "publishedAt": "2025-05-16T18:01:54.000Z",
            "submittedOnDailyAt": "2025-05-21T01:35:25.101Z",
            "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
            "submittedOnDailyBy": {
                "_id": "66c0a08bac74db25de8427ec",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                "isPro": false,
                "fullname": "Jintao Zhang",
                "user": "jt-zhang",
                "type": "user"
            },
            "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
            "upvotes": 48,
            "discussionId": "682d426551ce04237318d0b9",
            "projectPage": "https://github.com/thu-ml/SageAttention",
            "githubRepo": "https://github.com/thu-ml/SageAttention",
            "ai_summary": "Efficiency enhancements for attention mechanisms, including leveraging FP4 Tensor Cores and developing an 8-bit attention method, improve inference and training performance.",
            "ai_keywords": [
                "attention",
                "FP4 Tensor Cores",
                "Blackwell GPUs",
                "RTX5090",
                "TOPS",
                "FlashAttention",
                "low-bit attention",
                "forward propagation",
                "backward propagation",
                "fine-tuning",
                "pretraining"
            ]
        },
        "publishedAt": "2025-05-16T14:01:54.000Z",
        "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
        "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/Tb20E3IJSV6PjcD9Nkvfg.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11594.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "fullname": "Jintao Zhang",
            "name": "jt-zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13438",
            "authors": [
                {
                    "_id": "682d84d6aa4903837eeac1dc",
                    "user": {
                        "_id": "63885f1d0bebb233d8ad6e5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Penghui Qi",
                        "user": "QPHutu",
                        "type": "user"
                    },
                    "name": "Penghui Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:35.138Z",
                    "hidden": false
                },
                {
                    "_id": "682d84d6aa4903837eeac1dd",
                    "user": {
                        "_id": "65f5392c68b8e0cb3c9977a2",
                        "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
                        "isPro": false,
                        "fullname": "Zichen",
                        "user": "lkevinzc",
                        "type": "user"
                    },
                    "name": "Zichen Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T09:31:12.541Z",
                    "hidden": false
                },
                {
                    "_id": "682d84d6aa4903837eeac1de",
                    "user": {
                        "_id": "63d91b6d255ef6add20e1b38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "P2333",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:12:08.028Z",
                    "hidden": false
                },
                {
                    "_id": "682d84d6aa4903837eeac1df",
                    "user": {
                        "_id": "632407c892e07e3ca20aca28",
                        "avatarUrl": "/avatars/23b51b37b12b51a0947f687d1de4d3b5.svg",
                        "isPro": false,
                        "fullname": "Chao Du",
                        "user": "duchao",
                        "type": "user"
                    },
                    "name": "Chao Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:12:18.649Z",
                    "hidden": false
                },
                {
                    "_id": "682d84d6aa4903837eeac1e0",
                    "name": "Wee Sun Lee",
                    "hidden": false
                },
                {
                    "_id": "682d84d6aa4903837eeac1e1",
                    "name": "Min Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png"
            ],
            "publishedAt": "2025-05-19T17:58:44.000Z",
            "submittedOnDailyAt": "2025-05-21T06:36:34.577Z",
            "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "63885f1d0bebb233d8ad6e5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
                "isPro": false,
                "fullname": "Penghui Qi",
                "user": "QPHutu",
                "type": "user"
            },
            "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
            "upvotes": 29,
            "discussionId": "682d84d7aa4903837eeac215",
            "githubRepo": "https://github.com/sail-sg/AnytimeReasoner",
            "ai_summary": "The AnytimeReasoner framework optimizes token efficiency and reasoning flexibility for large language models by introducing verifiable dense rewards and a decoupled policy optimization technique.",
            "ai_keywords": [
                "large language models (LLMs)",
                "reinforcement learning (RL)",
                "token budget",
                "anytime reasoning performance",
                "verifiable dense rewards",
                "thinking and summary policies",
                "Budget Relative Policy Optimization (BRPO)",
                "mathematical reasoning tasks",
                "GRPO"
            ]
        },
        "publishedAt": "2025-05-19T13:58:44.000Z",
        "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
        "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/UeoRcEi-bKgq6eecwvo8o.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63885f1d0bebb233d8ad6e5b/JLzRbfpomaF02gHLNkON6.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13438.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63885f1d0bebb233d8ad6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
            "fullname": "Penghui Qi",
            "name": "QPHutu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14460",
            "authors": [
                {
                    "_id": "682d54b0396c1e613eaac5ef",
                    "user": {
                        "_id": "655de51982afda0fc479fb91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
                        "isPro": false,
                        "fullname": "Tianhe Wu",
                        "user": "TianheWu",
                        "type": "user"
                    },
                    "name": "Tianhe Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:26.204Z",
                    "hidden": false
                },
                {
                    "_id": "682d54b0396c1e613eaac5f0",
                    "name": "Jian Zou",
                    "hidden": false
                },
                {
                    "_id": "682d54b0396c1e613eaac5f1",
                    "name": "Jie Liang",
                    "hidden": false
                },
                {
                    "_id": "682d54b0396c1e613eaac5f2",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d54b0396c1e613eaac5f3",
                    "name": "Kede Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T14:56:50.000Z",
            "submittedOnDailyAt": "2025-05-21T07:48:51.670Z",
            "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
            "submittedOnDailyBy": {
                "_id": "655de51982afda0fc479fb91",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
                "isPro": false,
                "fullname": "Tianhe Wu",
                "user": "TianheWu",
                "type": "user"
            },
            "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.",
            "upvotes": 26,
            "discussionId": "682d54b0396c1e613eaac62a",
            "githubRepo": "https://github.com/TianheWu/VisualQuality-R1",
            "ai_summary": "VisualQuality-R1, a reasoning-induced no-reference IQA model trained via reinforcement learning, outperforms discriminative models in visual quality assessment by generating human-aligned quality descriptions and supporting multi-dataset training.",
            "ai_keywords": [
                "reinforcement learning",
                "reasoning-induced no-reference IQA (NR-IQA)",
                "group relative policy optimization",
                "Thurstone model",
                "continuous fidelity measures",
                "super-resolution",
                "image generation"
            ]
        },
        "publishedAt": "2025-05-20T10:56:50.000Z",
        "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
        "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14460.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "655de51982afda0fc479fb91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/99oxuFD3OjlReuGfDZluh.png",
            "fullname": "Tianhe Wu",
            "name": "TianheWu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14246",
            "authors": [
                {
                    "_id": "682d7a2340a42d1538fada76",
                    "user": {
                        "_id": "66fe1334ff3ee1f7569fab6d",
                        "avatarUrl": "/avatars/6868b1a545028a9b8bbded52490dc093.svg",
                        "isPro": false,
                        "fullname": "ziyuliu",
                        "user": "ziyuliu",
                        "type": "user"
                    },
                    "name": "Ziyu Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:08:39.517Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada77",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:42.962Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada78",
                    "user": {
                        "_id": "6671341b4eee852a8b25888f",
                        "avatarUrl": "/avatars/635d1821bbc960b4ea845e606883eb16.svg",
                        "isPro": false,
                        "fullname": "yushan zou",
                        "user": "zyshan",
                        "type": "user"
                    },
                    "name": "Yushan Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:08:45.972Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada79",
                    "user": {
                        "_id": "652768eeb723b49e8c8865da",
                        "avatarUrl": "/avatars/491e02da9ec81e439ccda8a181634bca.svg",
                        "isPro": false,
                        "fullname": "Zijian Liang",
                        "user": "steins1096",
                        "type": "user"
                    },
                    "name": "Zijian Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:08:51.938Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada7a",
                    "user": {
                        "_id": "67c0849ee08c178ef8d4e05c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mQ6VdnjZnRhb0H_waPclo.png",
                        "isPro": false,
                        "fullname": "Xiaoyi Dong",
                        "user": "sweetFruit",
                        "type": "user"
                    },
                    "name": "Xiaoyi Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:09:00.912Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada7b",
                    "user": {
                        "_id": "65000bef18830fabea469fdd",
                        "avatarUrl": "/avatars/b320c77dfad039d9f9c54127f610d44f.svg",
                        "isPro": false,
                        "fullname": "Cao Yuhang",
                        "user": "yhcao",
                        "type": "user"
                    },
                    "name": "Yuhang Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:09:22.645Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada7c",
                    "user": {
                        "_id": "63ee1379190ddd6214efd73a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
                        "isPro": false,
                        "fullname": "HAODONG DUAN",
                        "user": "KennyUTC",
                        "type": "user"
                    },
                    "name": "Haodong Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:09:31.442Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada7d",
                    "user": {
                        "_id": "636317ed80c1a705a6eff396",
                        "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg",
                        "isPro": false,
                        "fullname": "Dahua Lin",
                        "user": "lindahua",
                        "type": "user"
                    },
                    "name": "Dahua Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:09:37.103Z",
                    "hidden": false
                },
                {
                    "_id": "682d7a2340a42d1538fada7e",
                    "user": {
                        "_id": "64b4eec4faa3181a5eab9c46",
                        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                        "isPro": true,
                        "fullname": "Jiaqi Wang",
                        "user": "myownskyW7",
                        "type": "user"
                    },
                    "name": "Jiaqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:45.391Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T11:59:25.000Z",
            "submittedOnDailyAt": "2025-05-21T05:32:01.442Z",
            "title": "Visual Agentic Reinforcement Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "64b4eec4faa3181a5eab9c46",
                "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                "isPro": true,
                "fullname": "Jiaqi Wang",
                "user": "myownskyW7",
                "type": "user"
            },
            "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
            "upvotes": 25,
            "discussionId": "682d7a2440a42d1538fadac0",
            "githubRepo": "https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT",
            "ai_summary": "Visual Agentic Reinforcement Fine-Tuning enhances Large Vision-Language Models for flexible image manipulation and web-based reasoning, outperforming existing models on multi-modal benchmarks.",
            "ai_keywords": [
                "Visual Agentic Reinforcement Fine-Tuning",
                "Large Vision-Language Models",
                "Multi-modal Agentic Tool Bench",
                "MAT-Search",
                "MAT-Coding",
                "multi-hop QA benchmarks"
            ]
        },
        "publishedAt": "2025-05-20T07:59:25.000Z",
        "title": "Visual Agentic Reinforcement Fine-Tuning",
        "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14246.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13138",
            "authors": [
                {
                    "_id": "682da62a975206d1caa8c9d9",
                    "user": {
                        "_id": "62cd4ae83e5ba89c40f15156",
                        "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
                        "isPro": false,
                        "fullname": "Emile van Krieken",
                        "user": "HEmile",
                        "type": "user"
                    },
                    "name": "Emile van Krieken",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T10:14:44.208Z",
                    "hidden": false
                },
                {
                    "_id": "682da62a975206d1caa8c9da",
                    "user": {
                        "_id": "61001311e043e15c13412d30",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61001311e043e15c13412d30/6yAbTweYR16XtxMBEyOWl.png",
                        "isPro": false,
                        "fullname": "Pasquale Minervini",
                        "user": "pminervini",
                        "type": "user"
                    },
                    "name": "Pasquale Minervini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T10:16:57.876Z",
                    "hidden": false
                },
                {
                    "_id": "682da62a975206d1caa8c9db",
                    "user": {
                        "_id": "60809ad44ad99100d63ce36a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1619040921084-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Edoardo Maria Ponti",
                        "user": "ducdauge",
                        "type": "user"
                    },
                    "name": "Edoardo Ponti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T10:30:14.820Z",
                    "hidden": false
                },
                {
                    "_id": "682da62a975206d1caa8c9dc",
                    "name": "Antonio Vergari",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif"
            ],
            "publishedAt": "2025-05-19T14:07:47.000Z",
            "submittedOnDailyAt": "2025-05-21T08:42:10.409Z",
            "title": "Neurosymbolic Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "62cd4ae83e5ba89c40f15156",
                "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
                "isPro": false,
                "fullname": "Emile van Krieken",
                "user": "HEmile",
                "type": "user"
            },
            "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.",
            "upvotes": 25,
            "discussionId": "682da62a975206d1caa8ca06",
            "projectPage": "https://x.com/EmilevanKrieken/status/1925143347758252478",
            "githubRepo": "https://github.com/HEmile/neurosymbolic-diffusion",
            "ai_summary": "Neurosymbolic diffusion models address limitations of standard neurosymbolic predictors by modeling dependencies between symbols using discrete diffusion, leading to improved accuracy and calibration.",
            "ai_keywords": [
                "neurosymbolic predictors",
                "symbolic reasoning",
                "conditional independence",
                "discrete diffusion",
                "symbol dependencies",
                "uncertainty quantification",
                "synthetic benchmarks",
                "real-world benchmarks",
                "high-dimensional visual path planning",
                "rule-based autonomous driving",
                "state-of-the-art accuracy",
                "strong calibration"
            ]
        },
        "publishedAt": "2025-05-19T10:07:47.000Z",
        "title": "Neurosymbolic Diffusion Models",
        "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62cd4ae83e5ba89c40f15156/6TK9qXUQWm6Yk3bQW_0ht.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13138.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cd4ae83e5ba89c40f15156",
            "avatarUrl": "/avatars/f70aca85f8836edaedcee324a18140b5.svg",
            "fullname": "Emile van Krieken",
            "name": "HEmile",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.04388",
            "authors": [
                {
                    "_id": "682d83494c4685831f85ec92",
                    "name": "Dario Garcia-Gasulla",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec93",
                    "user": {
                        "_id": "661e8559b0338c03bd6e5054",
                        "avatarUrl": "/avatars/044d69afa40ddef4485aebfe984da96b.svg",
                        "isPro": false,
                        "fullname": "Bayarri",
                        "user": "JordiBayarri-bsc",
                        "type": "user"
                    },
                    "name": "Jordi Bayarri-Planas",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec94",
                    "name": "Ashwin Kumar Gururajan",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec95",
                    "name": "Enrique Lopez-Cuena",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec96",
                    "user": {
                        "_id": "6622352d9fcf61dee8a1f24d",
                        "avatarUrl": "/avatars/dd164c50b3ecb8861e2294337b942e6f.svg",
                        "isPro": false,
                        "fullname": "Adrian Tormos",
                        "user": "adriantormos",
                        "type": "user"
                    },
                    "name": "Adrian Tormos",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T11:04:23.232Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec97",
                    "user": {
                        "_id": "65b8e66f078c543033e49672",
                        "avatarUrl": "/avatars/aa81fd59b4ec624245a4a84d2708962d.svg",
                        "isPro": false,
                        "fullname": "Daniel Hinjos Garca",
                        "user": "danihinjos",
                        "type": "user"
                    },
                    "name": "Daniel Hinjos",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:40.380Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec98",
                    "user": {
                        "_id": "620683e7eeb1b73d904c96e5",
                        "avatarUrl": "/avatars/d0309ac9408530a74f1799e175cc5fad.svg",
                        "isPro": false,
                        "fullname": "Pablo Bernabeu",
                        "user": "pabberpe",
                        "type": "user"
                    },
                    "name": "Pablo Bernabeu-Perez",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T07:39:54.713Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec99",
                    "user": {
                        "_id": "65d71603ca16ef9ba7fb2efb",
                        "avatarUrl": "/avatars/7d3e1436427f7f58c86fb1f8724c4244.svg",
                        "isPro": false,
                        "fullname": "Anna",
                        "user": "annariasdu",
                        "type": "user"
                    },
                    "name": "Anna Arias-Duart",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T07:59:53.756Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec9a",
                    "user": {
                        "_id": "6565c2a4131d13ccc5df1435",
                        "avatarUrl": "/avatars/218ceac504772e7f0fb3ee4d46d4fab7.svg",
                        "isPro": false,
                        "fullname": "Pablo Agustin Martin Torres",
                        "user": "PabloMartinTorres",
                        "type": "user"
                    },
                    "name": "Pablo Agustin Martin-Torres",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T09:03:22.597Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec9b",
                    "name": "Marta Gonzalez-Mallo",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec9c",
                    "user": {
                        "_id": "62d16c742ad06bbc89217797",
                        "avatarUrl": "/avatars/11ce629fbcb33f2431164d8a3e54c876.svg",
                        "isPro": false,
                        "fullname": "Sergio Alvarez-Napagao",
                        "user": "tranchis",
                        "type": "user"
                    },
                    "name": "Sergio Alvarez-Napagao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T11:38:27.934Z",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec9d",
                    "name": "Eduard Ayguad-Parra",
                    "hidden": false
                },
                {
                    "_id": "682d83494c4685831f85ec9e",
                    "name": "Ulises Corts",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
            ],
            "publishedAt": "2025-05-07T13:13:14.000Z",
            "submittedOnDailyAt": "2025-05-21T06:12:19.909Z",
            "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
            "submittedOnDailyBy": {
                "_id": "62f7a16192950415b637e201",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
                "isPro": false,
                "fullname": "Dario",
                "user": "dariog",
                "type": "user"
            },
            "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
            "upvotes": 20,
            "discussionId": "682d834a4c4685831f85ed09",
            "ai_summary": "Aloe Beta models improve open-source medical Large Language Models through enhanced preprocessing, Direct Preference Optimization, Retrieval-Augmented Generation, and a rigorous evaluation methodology, achieving competitive performance and high safety standards.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Direct Preference Optimization",
                "DPO",
                "Retrieval-Augmented Generation",
                "RAG",
                "Chain of Thought examples",
                "jailbreaking attacks",
                "safety assessments",
                "human assessments"
            ]
        },
        "publishedAt": "2025-05-07T09:13:14.000Z",
        "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
        "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/SdO6Jth8V1sz0wvfL9Nxg.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/Efb0dFT2ULJC-TvdQ1ERR.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04388.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f7a16192950415b637e201",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7a16192950415b637e201/4IIqYap43vujvSuql68Vj.jpeg",
            "fullname": "Dario",
            "name": "dariog",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14513",
            "authors": [
                {
                    "_id": "682d334862cadf615f5f73e6",
                    "user": {
                        "_id": "63ad217b9fc40b14560e9e06",
                        "avatarUrl": "/avatars/c2d33830b141fc9c73ad8302ff35ed9d.svg",
                        "isPro": false,
                        "fullname": "Yen-Chen Wu",
                        "user": "yenchen",
                        "type": "user"
                    },
                    "name": "Yen-Chen Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:11:09.090Z",
                    "hidden": false
                },
                {
                    "_id": "682d334862cadf615f5f73e7",
                    "user": {
                        "_id": "643fb7332397d8eef5b844cd",
                        "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
                        "isPro": false,
                        "fullname": "Feng-Ting Liao",
                        "user": "FengTing",
                        "type": "user"
                    },
                    "name": "Feng-Ting Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:41:00.821Z",
                    "hidden": false
                },
                {
                    "_id": "682d334862cadf615f5f73e8",
                    "user": {
                        "_id": "66018c8eb1e509e1e4d9196f",
                        "avatarUrl": "/avatars/5d6cbfc7b6c435264d271c958607630f.svg",
                        "isPro": false,
                        "fullname": "Meng-Hsi Chen",
                        "user": "menghsichen",
                        "type": "user"
                    },
                    "name": "Meng-Hsi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:11:15.604Z",
                    "hidden": false
                },
                {
                    "_id": "682d334862cadf615f5f73e9",
                    "name": "Pei-Chen Ho",
                    "hidden": false
                },
                {
                    "_id": "682d334862cadf615f5f73ea",
                    "name": "Farhang Nabiei",
                    "hidden": false
                },
                {
                    "_id": "682d334862cadf615f5f73eb",
                    "user": {
                        "_id": "6811b1294119e4ecc92fc93b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/srmY2yyzOg9KRDSLYXJKf.png",
                        "isPro": false,
                        "fullname": "Dashan Shiu",
                        "user": "dsshiu",
                        "type": "user"
                    },
                    "name": "Da-shan Shiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:11:37.639Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T15:41:05.000Z",
            "submittedOnDailyAt": "2025-05-21T00:30:57.355Z",
            "title": "Latent Flow Transformer",
            "submittedOnDailyBy": {
                "_id": "643fb7332397d8eef5b844cd",
                "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
                "isPro": false,
                "fullname": "Feng-Ting Liao",
                "user": "FengTing",
                "type": "user"
            },
            "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in preserving\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
            "upvotes": 19,
            "discussionId": "682d334962cadf615f5f743f",
            "githubRepo": "https://github.com/mtkresearch/latent-flow-transformer",
            "ai_summary": "The Latent Flow Transformer (LFT) compresses layers by replacing them with learned transport operators using flow matching and Flow Walking, showing improved performance over layer skipping and reducing the gap between autoregressive and flow-based models.",
            "ai_keywords": [
                "Latent Flow Transformer",
                "LFT",
                "flow matching",
                "Flow Walking",
                "KL Divergence",
                "LM logits",
                "autoregressive",
                "flow-based generation paradigms"
            ]
        },
        "publishedAt": "2025-05-20T11:41:05.000Z",
        "title": "Latent Flow Transformer",
        "summary": "Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in preserving\ncoupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14513.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643fb7332397d8eef5b844cd",
            "avatarUrl": "/avatars/e403a19fc13e478d5929c67028230b0e.svg",
            "fullname": "Feng-Ting Liao",
            "name": "FengTing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14674",
            "authors": [
                {
                    "_id": "682d4145514c96fbf03f5f76",
                    "name": "Jiaxin Guo",
                    "hidden": false
                },
                {
                    "_id": "682d4145514c96fbf03f5f77",
                    "name": "Zewen Chi",
                    "hidden": false
                },
                {
                    "_id": "682d4145514c96fbf03f5f78",
                    "user": {
                        "_id": "5df85abada6d0311fd3d5408",
                        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                        "isPro": false,
                        "fullname": "Li Dong",
                        "user": "unilm",
                        "type": "user"
                    },
                    "name": "Li Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:52.470Z",
                    "hidden": false
                },
                {
                    "_id": "682d4145514c96fbf03f5f79",
                    "name": "Qingxiu Dong",
                    "hidden": false
                },
                {
                    "_id": "682d4145514c96fbf03f5f7a",
                    "user": {
                        "_id": "62d1227384bfbee86b6eec56",
                        "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
                        "isPro": false,
                        "fullname": "Xun Wu",
                        "user": "YUSHUIWX",
                        "type": "user"
                    },
                    "name": "Xun Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:50.085Z",
                    "hidden": false
                },
                {
                    "_id": "682d4145514c96fbf03f5f7b",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "682d4145514c96fbf03f5f7c",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
            ],
            "publishedAt": "2025-05-20T17:58:03.000Z",
            "submittedOnDailyAt": "2025-05-21T02:57:46.082Z",
            "title": "Reward Reasoning Model",
            "submittedOnDailyBy": {
                "_id": "5df85abada6d0311fd3d5408",
                "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
                "isPro": false,
                "fullname": "Li Dong",
                "user": "unilm",
                "type": "user"
            },
            "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.",
            "upvotes": 17,
            "discussionId": "682d4146514c96fbf03f5fab",
            "ai_summary": "RRMs, employing chain-of-thought reasoning and reinforcement learning, enhance reward model performance by adaptively utilizing test-time compute.",
            "ai_keywords": [
                "reward models",
                "large language models",
                "chain-of-thought reasoning",
                "reinforcement learning",
                "test-time compute",
                "reward accuracy"
            ]
        },
        "publishedAt": "2025-05-20T13:58:03.000Z",
        "title": "Reward Reasoning Model",
        "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/wpApND0JXiL6s6MUpBdg7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14674.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "fullname": "Li Dong",
            "name": "unilm",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 29
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14489",
            "authors": [
                {
                    "_id": "682d55ecea67e90811b09b6b",
                    "user": {
                        "_id": "617f679fb15f8a665f3999fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
                        "isPro": false,
                        "fullname": "Dongkeun Yoon",
                        "user": "DKYoon",
                        "type": "user"
                    },
                    "name": "Dongkeun Yoon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:23.897Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b6c",
                    "user": {
                        "_id": "6469949654873f0043b09c22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
                        "isPro": false,
                        "fullname": "Seungone Kim",
                        "user": "seungone",
                        "type": "user"
                    },
                    "name": "Seungone Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:15:44.937Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b6d",
                    "user": {
                        "_id": "606ae0adb579bb0e88515311",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617617060380-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Sohee Yang",
                        "user": "soheeyang",
                        "type": "user"
                    },
                    "name": "Sohee Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:15:51.388Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b6e",
                    "user": {
                        "_id": "628efeff6d8dcc7ab5263881",
                        "avatarUrl": "/avatars/1e894ccd7ba206a755b0b9af9f22ead1.svg",
                        "isPro": true,
                        "fullname": "Sunkyoung Kim",
                        "user": "Sunkyoung",
                        "type": "user"
                    },
                    "name": "Sunkyoung Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:16:00.139Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b6f",
                    "name": "Soyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b70",
                    "user": {
                        "_id": "66a9c79cf59bc4c77baac0d5",
                        "avatarUrl": "/avatars/57e9943b339a5b9be21b411e5286821c.svg",
                        "isPro": false,
                        "fullname": "Yongil Kim",
                        "user": "YongilKim",
                        "type": "user"
                    },
                    "name": "Yongil Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:17:46.138Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b71",
                    "user": {
                        "_id": "6044fd39e6aa3e130cb92867",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg",
                        "isPro": false,
                        "fullname": "Eunbi Choi",
                        "user": "unbiarirang",
                        "type": "user"
                    },
                    "name": "Eunbi Choi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:17:29.667Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b72",
                    "user": {
                        "_id": "660260cf1737e5cd4a826550",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660260cf1737e5cd4a826550/AlSfoM2WtqPjLtYR6x7Wf.jpeg",
                        "isPro": false,
                        "fullname": "Yireun Kim",
                        "user": "yireun",
                        "type": "user"
                    },
                    "name": "Yireun Kim",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T04:27:50.114Z",
                    "hidden": false
                },
                {
                    "_id": "682d55ecea67e90811b09b73",
                    "user": {
                        "_id": "621f05ba970615ad5861ceb1",
                        "avatarUrl": "/avatars/7e1902aa71369a524afda9b0a9e88e22.svg",
                        "isPro": false,
                        "fullname": "Minjoon Seo",
                        "user": "minjoon",
                        "type": "user"
                    },
                    "name": "Minjoon Seo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:17:22.628Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
                "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
                "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
            ],
            "publishedAt": "2025-05-20T15:19:00.000Z",
            "submittedOnDailyAt": "2025-05-21T02:58:45.807Z",
            "title": "Reasoning Models Better Express Their Confidence",
            "submittedOnDailyBy": {
                "_id": "617f679fb15f8a665f3999fc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
                "isPro": false,
                "fullname": "Dongkeun Yoon",
                "user": "DKYoon",
                "type": "user"
            },
            "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
            "upvotes": 14,
            "discussionId": "682d55edea67e90811b09ba1",
            "githubRepo": "https://github.com/MattYoon/reasoning-models-confidence",
            "ai_summary": "Reasoning models, which perform chain-of-thought reasoning, exhibit superior confidence calibration compared to non-reasoning models by dynamically adjusting their confidence during the reasoning process.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "chain-of-thought",
                "CoT reasoning",
                "confidence calibration",
                "benchmarking",
                "datasets",
                "alternative approaches",
                "backtracking",
                "in-context learning"
            ]
        },
        "publishedAt": "2025-05-20T11:19:00.000Z",
        "title": "Reasoning Models Better Express Their Confidence",
        "summary": "Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/uZZgyWD0lw8jektQyDZfV.png",
            "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/_Kq3BF1HC_aFc6mee08zl.png",
            "https://cdn-uploads.huggingface.co/production/uploads/617f679fb15f8a665f3999fc/bpz_J8XujNicCvMH2n2iK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14489.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "617f679fb15f8a665f3999fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617f679fb15f8a665f3999fc/NW1vkLsGAlWpAQYTux05X.jpeg",
            "fullname": "Dongkeun Yoon",
            "name": "DKYoon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14652",
            "authors": [
                {
                    "_id": "682d474782567fffe1f99b7a",
                    "user": {
                        "_id": "5ec82854968f6028e0559f70",
                        "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
                        "isPro": true,
                        "fullname": "Xueguang Ma",
                        "user": "MrLight",
                        "type": "user"
                    },
                    "name": "Xueguang Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:41.675Z",
                    "hidden": false
                },
                {
                    "_id": "682d474782567fffe1f99b7b",
                    "user": {
                        "_id": "612ee6a7b960e78c6d2319d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:39.513Z",
                    "hidden": false
                },
                {
                    "_id": "682d474782567fffe1f99b7c",
                    "user": {
                        "_id": "62567c86d444a9b5a0ec51c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
                        "isPro": false,
                        "fullname": "Dongfu Jiang",
                        "user": "DongfuJiang",
                        "type": "user"
                    },
                    "name": "Dongfu Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:15:22.975Z",
                    "hidden": false
                },
                {
                    "_id": "682d474782567fffe1f99b7d",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:15:29.552Z",
                    "hidden": false
                },
                {
                    "_id": "682d474782567fffe1f99b7e",
                    "name": "Zejun Ma",
                    "hidden": false
                },
                {
                    "_id": "682d474782567fffe1f99b7f",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T03:23:52.529Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:41:33.000Z",
            "submittedOnDailyAt": "2025-05-21T01:56:42.807Z",
            "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
            "submittedOnDailyBy": {
                "_id": "5ec82854968f6028e0559f70",
                "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
                "isPro": true,
                "fullname": "Xueguang Ma",
                "user": "MrLight",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
            "upvotes": 13,
            "discussionId": "682d474882567fffe1f99bc5",
            "projectPage": "https://tiger-ai-lab.github.io/General-Reasoner/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/General-Reasoner",
            "ai_summary": "General-Reasoner enhances large language model reasoning across diverse domains by using a large-scale dataset and generative model-based answer verification, outperforming existing methods.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "Deepseek-R1-Zero",
                "Generative model",
                "chain-of-thought",
                "context-awareness"
            ]
        },
        "publishedAt": "2025-05-20T13:41:33.000Z",
        "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
        "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14652.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "5ec82854968f6028e0559f70",
            "avatarUrl": "/avatars/45b58d912f7d00cb351947cd79d5eeb4.svg",
            "fullname": "Xueguang Ma",
            "name": "MrLight",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13547",
            "authors": [
                {
                    "_id": "682d6b06ec3b65b35772c0af",
                    "user": {
                        "_id": "668f440894dfc0ed1a7006ed",
                        "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
                        "isPro": false,
                        "fullname": "Pengxin Guo",
                        "user": "gpx333",
                        "type": "user"
                    },
                    "name": "Pengxin Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:11.271Z",
                    "hidden": false
                },
                {
                    "_id": "682d6b06ec3b65b35772c0b0",
                    "user": {
                        "_id": "641b065a1911d3be6742ef04",
                        "avatarUrl": "/avatars/bceb4ad2a278b6e2a40af0b89c4fb48e.svg",
                        "isPro": false,
                        "fullname": "Yinong Wang",
                        "user": "jcccy",
                        "type": "user"
                    },
                    "name": "Yinong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:12:52.463Z",
                    "hidden": false
                },
                {
                    "_id": "682d6b06ec3b65b35772c0b1",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "682d6b06ec3b65b35772c0b2",
                    "user": {
                        "_id": "678f8d4b1a5e393c1c285165",
                        "avatarUrl": "/avatars/533eb74376915eb63c03dc9b4df421f6.svg",
                        "isPro": false,
                        "fullname": "MENGTINGLIU",
                        "user": "MENGTINGLIU",
                        "type": "user"
                    },
                    "name": "Mengting Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:13:13.691Z",
                    "hidden": false
                },
                {
                    "_id": "682d6b06ec3b65b35772c0b3",
                    "user": {
                        "_id": "637f0eb22438d7485b8ef5d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
                        "isPro": false,
                        "fullname": "Ming Li",
                        "user": "limingcv",
                        "type": "user"
                    },
                    "name": "Ming Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:13:40.959Z",
                    "hidden": false
                },
                {
                    "_id": "682d6b06ec3b65b35772c0b4",
                    "name": "Jinkai Zheng",
                    "hidden": false
                },
                {
                    "_id": "682d6b06ec3b65b35772c0b5",
                    "user": {
                        "_id": "663058bc2653ec94f4a6235f",
                        "avatarUrl": "/avatars/f55b8c3c8100d6b6d65ba61abc4fb014.svg",
                        "isPro": false,
                        "fullname": "Liangqiong Qu",
                        "user": "Liangqiong-QU",
                        "type": "user"
                    },
                    "name": "Liangqiong Qu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:12:59.427Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T03:41:54.000Z",
            "submittedOnDailyAt": "2025-05-21T04:27:08.535Z",
            "title": "Exploring Federated Pruning for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "668f440894dfc0ed1a7006ed",
                "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
                "isPro": false,
                "fullname": "Pengxin Guo",
                "user": "gpx333",
                "type": "user"
            },
            "summary": "LLM pruning has emerged as a promising technology for compressing LLMs,\nenabling their deployment on resource-limited devices. However, current\nmethodologies typically require access to public calibration samples, which can\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\nto calculate a pruning mask matrix based on its local calibration data and\nshare it with the server to prune the global model. This approach allows for\ncollaborative pruning of the global model with the knowledge of each client\nwhile maintaining local data privacy. Additionally, we conduct extensive\nexperiments to explore various possibilities within the FedPrLLM framework,\nincluding different comparison groups, pruning strategies, and the decision to\nscale weights. Our extensive evaluation reveals that one-shot pruning with\nlayer comparison and no weight scaling is the optimal choice within the\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\nLLMs in privacy-sensitive fields. Our code is available at\nhttps://github.com/Pengxin-Guo/FedPrLLM.",
            "upvotes": 13,
            "discussionId": "682d6b07ec3b65b35772c0f3",
            "githubRepo": "https://github.com/Pengxin-Guo/FedPrLLM",
            "ai_summary": "FedPrLLM is a federated pruning framework that enables privacy-preserving compression of large language models (LLMs) using local client data.",
            "ai_keywords": [
                "FedPrLLM",
                "federated pruning",
                "privacy-preserving compression",
                "LLMs",
                "calibration data",
                "pruning mask matrix",
                "layer comparison",
                "weight scaling"
            ]
        },
        "publishedAt": "2025-05-18T23:41:54.000Z",
        "title": "Exploring Federated Pruning for Large Language Models",
        "summary": "LLM pruning has emerged as a promising technology for compressing LLMs,\nenabling their deployment on resource-limited devices. However, current\nmethodologies typically require access to public calibration samples, which can\nbe challenging to obtain in privacy-sensitive domains. To address this issue,\nwe introduce FedPrLLM, a comprehensive federated pruning framework designed for\nthe privacy-preserving compression of LLMs. In FedPrLLM, each client only needs\nto calculate a pruning mask matrix based on its local calibration data and\nshare it with the server to prune the global model. This approach allows for\ncollaborative pruning of the global model with the knowledge of each client\nwhile maintaining local data privacy. Additionally, we conduct extensive\nexperiments to explore various possibilities within the FedPrLLM framework,\nincluding different comparison groups, pruning strategies, and the decision to\nscale weights. Our extensive evaluation reveals that one-shot pruning with\nlayer comparison and no weight scaling is the optimal choice within the\nFedPrLLM framework. We hope our work will help guide future efforts in pruning\nLLMs in privacy-sensitive fields. Our code is available at\nhttps://github.com/Pengxin-Guo/FedPrLLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13547.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668f440894dfc0ed1a7006ed",
            "avatarUrl": "/avatars/fa0d328300b03bcbbf9b3a7532f28458.svg",
            "fullname": "Pengxin Guo",
            "name": "gpx333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14677",
            "authors": [
                {
                    "_id": "682d718306291bf11fcf69a3",
                    "user": {
                        "_id": "664da76e4eb4c91c8c32cc06",
                        "avatarUrl": "/avatars/050bb6e4136f8a1645fef277ad08c7fc.svg",
                        "isPro": false,
                        "fullname": "Jiaer Xia",
                        "user": "Jiaer-Xia",
                        "type": "user"
                    },
                    "name": "Jiaer Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:14:20.493Z",
                    "hidden": false
                },
                {
                    "_id": "682d718306291bf11fcf69a4",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:56.735Z",
                    "hidden": false
                },
                {
                    "_id": "682d718306291bf11fcf69a5",
                    "name": "Peng Gao",
                    "hidden": false
                },
                {
                    "_id": "682d718306291bf11fcf69a6",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "682d718306291bf11fcf69a7",
                    "user": {
                        "_id": "62ac6656de8bfbb93094b8fd",
                        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                        "isPro": false,
                        "fullname": "Kaiyang Zhou",
                        "user": "kaiyangzhou",
                        "type": "user"
                    },
                    "name": "Kaiyang Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:14:28.433Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:58:35.000Z",
            "submittedOnDailyAt": "2025-05-21T04:54:13.466Z",
            "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "62ac6656de8bfbb93094b8fd",
                "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                "isPro": false,
                "fullname": "Kaiyang Zhou",
                "user": "kaiyangzhou",
                "type": "user"
            },
            "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
            "upvotes": 12,
            "discussionId": "682d718406291bf11fcf69df",
            "githubRepo": "https://github.com/maifoundations/Visionary-R1",
            "ai_summary": "Reinforcement learning applied to visual language models with image captions and reasoning chains leads to improved performance on visual reasoning benchmarks compared to multimodal models.",
            "ai_keywords": [
                "reinforcement learning",
                "visual language models",
                "visual question-answer pairs",
                "chain-of-thought",
                "caption-reason-answer",
                "visual reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-05-20T13:58:35.000Z",
        "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
        "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ac6656de8bfbb93094b8fd",
            "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
            "fullname": "Kaiyang Zhou",
            "name": "kaiyangzhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14631",
            "authors": [
                {
                    "_id": "682d69ed056cf7b86cd8d6ec",
                    "user": {
                        "_id": "66ab80e9bfb7d73a56bc293c",
                        "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
                        "isPro": false,
                        "fullname": "Jack",
                        "user": "lingjie23",
                        "type": "user"
                    },
                    "name": "Lingjie Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:17.384Z",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6ed",
                    "user": {
                        "_id": "62d1227384bfbee86b6eec56",
                        "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
                        "isPro": false,
                        "fullname": "Xun Wu",
                        "user": "YUSHUIWX",
                        "type": "user"
                    },
                    "name": "Xun Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:20.741Z",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6ee",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6ef",
                    "name": "Qingxiu Dong",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6f0",
                    "name": "Zewen Chi",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6f1",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6f2",
                    "name": "Xingxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6f3",
                    "name": "Tengchao Lv",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6f4",
                    "name": "Lei Cui",
                    "hidden": false
                },
                {
                    "_id": "682d69ed056cf7b86cd8d6f5",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:23:25.000Z",
            "submittedOnDailyAt": "2025-05-21T04:22:54.134Z",
            "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "62d1227384bfbee86b6eec56",
                "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
                "isPro": false,
                "fullname": "Xun Wu",
                "user": "YUSHUIWX",
                "type": "user"
            },
            "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
            "upvotes": 12,
            "discussionId": "682d69ee056cf7b86cd8d730",
            "ai_summary": "Large Hybrid-Reasoning Models dynamically choose between thinking modes based on query context, improving reasoning and efficiency compared to existing models.",
            "ai_keywords": [
                "Large Reasoning Models",
                "Large Language Models",
                "Hybrid Fine-Tuning",
                "Hybrid Group Policy Optimization",
                "Hybrid Accuracy",
                "hybrid thinking"
            ]
        },
        "publishedAt": "2025-05-20T13:23:25.000Z",
        "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
        "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14631.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d1227384bfbee86b6eec56",
            "avatarUrl": "/avatars/84435f9768a76c0fe9d404dfc2d70be3.svg",
            "fullname": "Xun Wu",
            "name": "YUSHUIWX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14673",
            "authors": [
                {
                    "_id": "682d713e6c66e25ab59fc800",
                    "user": {
                        "_id": "65c38108425a226a29f00365",
                        "avatarUrl": "/avatars/c276a0d2b108df446246c31a396496f0.svg",
                        "isPro": false,
                        "fullname": "tongyu",
                        "user": "yutchina02",
                        "type": "user"
                    },
                    "name": "Yu Tong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:01.048Z",
                    "hidden": false
                },
                {
                    "_id": "682d713e6c66e25ab59fc801",
                    "user": {
                        "_id": "65ad57da57f263e3d030187a",
                        "avatarUrl": "/avatars/e1e3c4119180aee5fb660d1e5f745e99.svg",
                        "isPro": false,
                        "fullname": "",
                        "user": "Apostle723",
                        "type": "user"
                    },
                    "name": "Zihao Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:58.888Z",
                    "hidden": false
                },
                {
                    "_id": "682d713e6c66e25ab59fc802",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "682d713e6c66e25ab59fc803",
                    "user": {
                        "_id": "62ac6656de8bfbb93094b8fd",
                        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                        "isPro": false,
                        "fullname": "Kaiyang Zhou",
                        "user": "kaiyangzhou",
                        "type": "user"
                    },
                    "name": "Kaiyang Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:20:44.575Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:58:02.000Z",
            "submittedOnDailyAt": "2025-05-21T04:53:16.661Z",
            "title": "Training-Free Watermarking for Autoregressive Image Generation",
            "submittedOnDailyBy": {
                "_id": "62ac6656de8bfbb93094b8fd",
                "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                "isPro": false,
                "fullname": "Kaiyang Zhou",
                "user": "kaiyangzhou",
                "type": "user"
            },
            "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
            "upvotes": 11,
            "discussionId": "682d71426c66e25ab59fc946",
            "githubRepo": "https://github.com/maifoundations/IndexMark",
            "ai_summary": "IndexMark is a training-free watermarking framework for autoregressive image generation models that embeds watermarks by replacing generated indices with similar ones, maintaining image quality and robustness against various perturbations.",
            "ai_keywords": [
                "autoregressive image generation models",
                "IndexMark",
                "codebook",
                "token similarity",
                "watermark tokens",
                "token replacement",
                "Index Encoder",
                "cropping attacks",
                "verification accuracy",
                "Gaussian blur",
                "random erasing",
                "color jittering",
                "JPEG compression"
            ]
        },
        "publishedAt": "2025-05-20T13:58:02.000Z",
        "title": "Training-Free Watermarking for Autoregressive Image Generation",
        "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14673.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ac6656de8bfbb93094b8fd",
            "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
            "fullname": "Kaiyang Zhou",
            "name": "kaiyangzhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14640",
            "authors": [
                {
                    "_id": "682d488557686b8c44f257fa",
                    "user": {
                        "_id": "65c387c807a1445dfe1e9452",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c387c807a1445dfe1e9452/t0VnwQh2wRZ9W_UGTZ8zt.jpeg",
                        "isPro": false,
                        "fullname": "Wentao Ma",
                        "user": "tonymwt",
                        "type": "user"
                    },
                    "name": "Wentao Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:31.165Z",
                    "hidden": false
                },
                {
                    "_id": "682d488557686b8c44f257fb",
                    "user": {
                        "_id": "64405a9d518271b0d1beea38",
                        "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
                        "isPro": false,
                        "fullname": "Weiming Ren",
                        "user": "wren93",
                        "type": "user"
                    },
                    "name": "Weiming Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:28.689Z",
                    "hidden": false
                },
                {
                    "_id": "682d488557686b8c44f257fc",
                    "name": "Yiming Jia",
                    "hidden": false
                },
                {
                    "_id": "682d488557686b8c44f257fd",
                    "user": {
                        "_id": "66349404f2c753240d02952a",
                        "avatarUrl": "/avatars/4f207cf5807d9629b9f4f7d13875b840.svg",
                        "isPro": false,
                        "fullname": "ZhuofengLi",
                        "user": "ZhuofengLi",
                        "type": "user"
                    },
                    "name": "Zhuofeng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:35.151Z",
                    "hidden": false
                },
                {
                    "_id": "682d488557686b8c44f257fe",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "682d488557686b8c44f257ff",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d488557686b8c44f25800",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:26:32.000Z",
            "submittedOnDailyAt": "2025-05-21T02:03:04.298Z",
            "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
            "submittedOnDailyBy": {
                "_id": "64405a9d518271b0d1beea38",
                "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
                "isPro": false,
                "fullname": "Weiming Ren",
                "user": "wren93",
                "type": "user"
            },
            "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.",
            "upvotes": 11,
            "discussionId": "682d488857686b8c44f258b7",
            "projectPage": "https://tiger-ai-lab.github.io/VideoEval-Pro",
            "githubRepo": "https://github.com/TIGER-AI-Lab/VideoEval-Pro",
            "ai_summary": "VideoEval-Pro, a benchmark using open-ended questions, provides a more accurate measure of long video understanding compared to existing multiple-choice question benchmarks.",
            "ai_keywords": [
                "large multimodal models",
                "long video understanding",
                "LVU benchmarks",
                "multiple-choice questions",
                "Video-MME",
                "VideoEval-Pro",
                "segment-level understanding",
                "full-video understanding",
                "perception tasks",
                "reasoning tasks"
            ]
        },
        "publishedAt": "2025-05-20T13:26:32.000Z",
        "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
        "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14640.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64405a9d518271b0d1beea38",
            "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg",
            "fullname": "Weiming Ren",
            "name": "wren93",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14135",
            "authors": [
                {
                    "_id": "682d43cf85d5e40c81ed313d",
                    "user": {
                        "_id": "648a822341c90440f32b316c",
                        "avatarUrl": "/avatars/be74f02c54cc4072a9d1ac2f14f4fb96.svg",
                        "isPro": false,
                        "fullname": "ruihuang li",
                        "user": "lslrh",
                        "type": "user"
                    },
                    "name": "Ruihuang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:32:30.513Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed313e",
                    "name": "Caijin Zhou",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed313f",
                    "user": {
                        "_id": "64d450cc52101a10fa64ba23",
                        "avatarUrl": "/avatars/eccba117e286864fa2c8cbca27ba79e0.svg",
                        "isPro": false,
                        "fullname": "Zheng Shoujian",
                        "user": "zhengsj",
                        "type": "user"
                    },
                    "name": "Shoujian Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:32:54.604Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3140",
                    "name": "Jianxiang Lu",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3141",
                    "user": {
                        "_id": "641c139b73296f7ee256970c",
                        "avatarUrl": "/avatars/5a2550d95e686640242840ad3bd0e680.svg",
                        "isPro": false,
                        "fullname": "Jiabin Huang",
                        "user": "YellowAddice",
                        "type": "user"
                    },
                    "name": "Jiabin Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:33:08.493Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3142",
                    "name": "Comi Chen",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3143",
                    "user": {
                        "_id": "634e67f5d049354d7ee0caa4",
                        "avatarUrl": "/avatars/13f7a6f66060590e0394eb888148716c.svg",
                        "isPro": false,
                        "fullname": "junshu tang",
                        "user": "tangjs",
                        "type": "user"
                    },
                    "name": "Junshu Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:33:22.811Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3144",
                    "user": {
                        "_id": "6539c938f84e87099cef67b7",
                        "avatarUrl": "/avatars/283ed16bd6b414c68e8c90b1b2aef5c4.svg",
                        "isPro": false,
                        "fullname": "Xu Guangzheng",
                        "user": "vcvcvn",
                        "type": "user"
                    },
                    "name": "Guangzheng Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:33:32.698Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3145",
                    "user": {
                        "_id": "64f3406b0abb66646fb68cb9",
                        "avatarUrl": "/avatars/c835eb87fda447551130929078f555d3.svg",
                        "isPro": false,
                        "fullname": "",
                        "user": "TaoJiaLe",
                        "type": "user"
                    },
                    "name": "Jiale Tao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:33:44.533Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3146",
                    "name": "Hongmei Wang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3147",
                    "user": {
                        "_id": "66f33e713c412b900bac6f2e",
                        "avatarUrl": "/avatars/9496d54a5a394d0082cb80b505fa26ec.svg",
                        "isPro": false,
                        "fullname": "Donghao Li",
                        "user": "panda12345pa",
                        "type": "user"
                    },
                    "name": "Donghao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:34:06.502Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3148",
                    "name": "Wenqing Yu",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3149",
                    "name": "Senbo Wang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed314a",
                    "name": "Zhimin Li",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed314b",
                    "user": {
                        "_id": "668cea284587d0a82bd22e37",
                        "avatarUrl": "/avatars/ef592494b339b5ab93078fdc1d847653.svg",
                        "isPro": false,
                        "fullname": "yetshuanshi",
                        "user": "yetshuan",
                        "type": "user"
                    },
                    "name": "Yetshuan Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:34:53.532Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed314c",
                    "name": "Haoyu Yang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed314d",
                    "name": "Yukun Wang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed314e",
                    "user": {
                        "_id": "6035f299564a521e3b4c73e1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614152410729-6035f299564a521e3b4c73e1.jpeg",
                        "isPro": false,
                        "fullname": "Wenxun Dai",
                        "user": "Askar101",
                        "type": "user"
                    },
                    "name": "Wenxun Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:35:08.579Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed314f",
                    "name": "Jiaqi Li",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3150",
                    "user": {
                        "_id": "64250b2bd476e4ad5568ea1b",
                        "avatarUrl": "/avatars/c770cd8a095140b33b9dcec9911fba13.svg",
                        "isPro": false,
                        "fullname": "WangLinqing",
                        "user": "WangLinqing",
                        "type": "user"
                    },
                    "name": "Linqing Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:35:21.856Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3151",
                    "user": {
                        "_id": "65e2e93bfcaff433f7a87b43",
                        "avatarUrl": "/avatars/020e008a6a748df75227c51f331d3bce.svg",
                        "isPro": false,
                        "fullname": "Qixun Wang",
                        "user": "NOVAglow646",
                        "type": "user"
                    },
                    "name": "Qixun Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:35:28.984Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3152",
                    "user": {
                        "_id": "67206bf75ff1582170e07153",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ktS3nDRZwIW04mAbqqmFY.png",
                        "isPro": false,
                        "fullname": "Zhiyong Xu",
                        "user": "TeddyXu2",
                        "type": "user"
                    },
                    "name": "Zhiyong Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:35:35.753Z",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3153",
                    "name": "Yingfang Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3154",
                    "name": "Jiangfeng Xiong",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3155",
                    "name": "Weijie Kong",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3156",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3157",
                    "name": "Hongxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3158",
                    "name": "Qiaoling Zheng",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3159",
                    "name": "Weiting Guo",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed315a",
                    "name": "Xinchi Deng",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed315b",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed315c",
                    "name": "Renjia Wei",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed315d",
                    "name": "Yulin Jian",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed315e",
                    "name": "Duojun Huang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed315f",
                    "name": "Xuhua Ren",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3160",
                    "name": "Sihuan Lin",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3161",
                    "name": "Yifu Sun",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3162",
                    "name": "Yuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3163",
                    "name": "Joey Wang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3164",
                    "name": "Qin Lin",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3165",
                    "name": "Jingmiao Yu",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3166",
                    "name": "Jihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3167",
                    "name": "Caesar Zhong",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3168",
                    "name": "Di Wang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed3169",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed316a",
                    "name": "Linus",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed316b",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed316c",
                    "name": "Longhuang Wu",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed316d",
                    "name": "Shuai Shao",
                    "hidden": false
                },
                {
                    "_id": "682d43cf85d5e40c81ed316e",
                    "name": "Qinglin Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T09:39:48.000Z",
            "submittedOnDailyAt": "2025-05-21T01:40:24.172Z",
            "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.",
            "upvotes": 10,
            "discussionId": "682d43d585d5e40c81ed3302",
            "ai_summary": "Hunyuan-Game leverages generative models to produce high-fidelity game assets, including images and videos, addressing player preferences and enhancing designer efficiency through specialized models for various game scenarios.",
            "ai_keywords": [
                "generative artificial intelligence",
                "image generation",
                "video generation",
                "General Text-to-Image Generation",
                "Game Visual Effects Generation",
                "Transparent Image Generation",
                "Game Character Generation",
                "Image-to-Video Generation",
                "360 A/T Pose Avatar Video Synthesis",
                "Dynamic Illustration Generation",
                "Generative Video Super-Resolution",
                "Interactive Game Video Generation"
            ]
        },
        "publishedAt": "2025-05-20T05:39:48.000Z",
        "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
        "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14135.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6902
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.13559",
            "authors": [
                {
                    "_id": "682d4ccc3b5f51f4218e12b4",
                    "user": {
                        "_id": "62f0e457bc8201db9ef47f89",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
                        "isPro": false,
                        "fullname": "Sathya Krishnan",
                        "user": "SkAndMl",
                        "type": "user"
                    },
                    "name": "Sathya Krishnan Suresh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:18:55.833Z",
                    "hidden": false
                },
                {
                    "_id": "682d4ccc3b5f51f4218e12b5",
                    "name": "Tanmay Surana",
                    "hidden": false
                },
                {
                    "_id": "682d4ccc3b5f51f4218e12b6",
                    "name": "Lim Zhi Hao",
                    "hidden": false
                },
                {
                    "_id": "682d4ccc3b5f51f4218e12b7",
                    "name": "Eng Siong Chng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T09:18:14.000Z",
            "submittedOnDailyAt": "2025-05-21T02:18:05.886Z",
            "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "62f0e457bc8201db9ef47f89",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
                "isPro": false,
                "fullname": "Sathya Krishnan",
                "user": "SkAndMl",
                "type": "user"
            },
            "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.",
            "upvotes": 10,
            "discussionId": "682d4ccd3b5f51f4218e12f4",
            "ai_summary": "LLMs exhibit high automated metric scores but make subtle errors in code-switching dialogue summarization, highlighting the need for specialized training on CS data.",
            "ai_keywords": [
                "LLMs",
                "code-switching (CS)",
                "CS-Sum",
                "Mandarin-English",
                "Tamil-English",
                "Malay-English",
                "dialogue summarization",
                "few-shot",
                "translate-summarize",
                "fine-tuning",
                "LoRA",
                "QLoRA",
                "synthetic data"
            ]
        },
        "publishedAt": "2025-05-19T05:18:14.000Z",
        "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
        "summary": "Code-switching (CS) poses a significant challenge for Large Language Models\n(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce\nCS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue\nto English summarization. CS-Sum is the first benchmark for CS dialogue\nsummarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and\nMalay-English (EN-MS), with 900-1300 human-annotated dialogues per language\npair. Evaluating ten LLMs, including open and closed-source models, we analyze\nperformance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA\non synthetic data) approaches. Our findings show that though the scores on\nautomated metrics are high, LLMs make subtle mistakes that alter the complete\nmeaning of the dialogue. To this end, we introduce 3 most common type of errors\nthat LLMs make when handling CS input. Error rates vary across CS pairs and\nLLMs, with some LLMs showing more frequent errors on certain language pairs,\nunderscoring the need for specialized training on code-switched data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13559.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62f0e457bc8201db9ef47f89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f0e457bc8201db9ef47f89/zOhDptwZpDGaugKCBOWB2.jpeg",
            "fullname": "Sathya Krishnan",
            "name": "SkAndMl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14681",
            "authors": [
                {
                    "_id": "682db6f3975206d1caadf1cd",
                    "name": "Mengru Wang",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1ce",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1cf",
                    "name": "Yue Wang",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d0",
                    "name": "Zhiwei He",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d1",
                    "name": "Jiahao Xu",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d2",
                    "name": "Tian Liang",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d3",
                    "name": "Qiuzhi Liu",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d4",
                    "name": "Yunzhi Yao",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d5",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d6",
                    "name": "Ruotian Ma",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d7",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d8",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T12:26:50.565Z",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1d9",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1da",
                    "name": "Xiaolong Li",
                    "hidden": false
                },
                {
                    "_id": "682db6f3975206d1caadf1db",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:59:16.000Z",
            "submittedOnDailyAt": "2025-05-21T09:51:44.990Z",
            "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.",
            "upvotes": 9,
            "discussionId": "682db6f4975206d1caadf21c",
            "ai_summary": "Reinforcing Cognitive Experts (RICE) improves reasoning performance and efficiency in Mixture-of-Experts architectures by identifying and utilizing specialized cognitive experts without requiring additional training or complex heuristics.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "Large Reasoning Models (LRMs)",
                "Reinforcing Cognitive Experts (RICE)",
                "normalized Pointwise Mutual Information (nPMI)",
                "meta-level reasoning",
                "cognitive efficiency",
                "overthinking",
                "underthinking",
                "prompt design",
                "decoding constraints",
                "cross-domain generalization"
            ]
        },
        "publishedAt": "2025-05-20T13:59:16.000Z",
        "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training",
        "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14681.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14680",
            "authors": [
                {
                    "_id": "682d30a37812103582f50de4",
                    "user": {
                        "_id": "64db88993725f8d9a908c077",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                        "isPro": false,
                        "fullname": "Sunhao Dai",
                        "user": "KID-22",
                        "type": "user"
                    },
                    "name": "Sunhao Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:19:10.865Z",
                    "hidden": false
                },
                {
                    "_id": "682d30a37812103582f50de5",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "682d30a37812103582f50de6",
                    "user": {
                        "_id": "5fc356c5ea82dd667bb0ffd1",
                        "avatarUrl": "/avatars/20519be09267bb429f3ca0f996f9fdc1.svg",
                        "isPro": false,
                        "fullname": "Liang Pang",
                        "user": "pl8787",
                        "type": "user"
                    },
                    "name": "Liang Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:19:41.530Z",
                    "hidden": false
                },
                {
                    "_id": "682d30a37812103582f50de7",
                    "name": "Jun Xu",
                    "hidden": false
                },
                {
                    "_id": "682d30a37812103582f50de8",
                    "name": "See-Kiong Ng",
                    "hidden": false
                },
                {
                    "_id": "682d30a37812103582f50de9",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:19:55.385Z",
                    "hidden": false
                },
                {
                    "_id": "682d30a37812103582f50dea",
                    "user": {
                        "_id": "6570ae84c4993b8fb96f41a8",
                        "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
                        "isPro": false,
                        "fullname": "Tat-Seng Chua",
                        "user": "chuats",
                        "type": "user"
                    },
                    "name": "Tat-Seng Chua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:19:49.244Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:59:13.000Z",
            "submittedOnDailyAt": "2025-05-21T00:18:37.037Z",
            "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
            "submittedOnDailyBy": {
                "_id": "64db88993725f8d9a908c077",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
                "isPro": false,
                "fullname": "Sunhao Dai",
                "user": "KID-22",
                "type": "user"
            },
            "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
            "upvotes": 9,
            "discussionId": "682d30a47812103582f50e19",
            "ai_summary": "NExT-Search seeks to enhance generative AI search by introducing fine-grained feedback mechanisms at each stage of the pipeline using User Debug Mode and Shadow User Mode, facilitating continuous improvement.",
            "ai_keywords": [
                "User Debug Mode",
                "Shadow User Mode",
                "online adaptation",
                "offline update",
                "generative AI search",
                "fine-grained feedback"
            ]
        },
        "publishedAt": "2025-05-20T13:59:13.000Z",
        "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
        "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64db88993725f8d9a908c077",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64db88993725f8d9a908c077/JZEGk0kius6mrANlwOWw9.jpeg",
            "fullname": "Sunhao Dai",
            "name": "KID-22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13430",
            "authors": [
                {
                    "_id": "682d70c598e3ea9be315ed85",
                    "user": {
                        "_id": "66fd024b2f3b4b95701b3055",
                        "avatarUrl": "/avatars/7efa17e0ba1b8e2f5afabeba59ef7a6f.svg",
                        "isPro": false,
                        "fullname": "Sifeng SHANG",
                        "user": "sifengshang",
                        "type": "user"
                    },
                    "name": "Sifeng Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:38:00.611Z",
                    "hidden": false
                },
                {
                    "_id": "682d70c598e3ea9be315ed86",
                    "name": "Jiayi Zhou",
                    "hidden": false
                },
                {
                    "_id": "682d70c598e3ea9be315ed87",
                    "user": {
                        "_id": "644b40a9a49c03b1b68b7e48",
                        "avatarUrl": "/avatars/0fbe390e34a45441905f2c55c17ca1ea.svg",
                        "isPro": false,
                        "fullname": "lin",
                        "user": "chenyulin",
                        "type": "user"
                    },
                    "name": "Chenyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:38:15.674Z",
                    "hidden": false
                },
                {
                    "_id": "682d70c598e3ea9be315ed88",
                    "name": "Minxian Li",
                    "hidden": false
                },
                {
                    "_id": "682d70c598e3ea9be315ed89",
                    "user": {
                        "_id": "62ac6656de8bfbb93094b8fd",
                        "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                        "isPro": false,
                        "fullname": "Kaiyang Zhou",
                        "user": "kaiyangzhou",
                        "type": "user"
                    },
                    "name": "Kaiyang Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:38:21.725Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T17:55:15.000Z",
            "submittedOnDailyAt": "2025-05-21T04:51:33.821Z",
            "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
            "submittedOnDailyBy": {
                "_id": "62ac6656de8bfbb93094b8fd",
                "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                "isPro": false,
                "fullname": "Kaiyang Zhou",
                "user": "kaiyangzhou",
                "type": "user"
            },
            "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
            "upvotes": 9,
            "discussionId": "682d70c898e3ea9be315ee64",
            "githubRepo": "https://github.com/maifoundations/QZO",
            "ai_summary": "Quantized Zeroth-order Optimization (QZO) enables memory-efficient fine-tuning of large language models by minimizing memory usage on model weights, gradients, and optimizer states.",
            "ai_keywords": [
                "zeroth-order optimization",
                "model quantization",
                "bfloat16",
                "int4",
                "Quantized Zeroth-order Optimization",
                "QZO",
                "directional derivative clipping",
                "full-parameter fine-tuning",
                "Llama-2-13B",
                "Stable Diffusion 3.5 Large"
            ]
        },
        "publishedAt": "2025-05-19T13:55:15.000Z",
        "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
        "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13430.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ac6656de8bfbb93094b8fd",
            "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
            "fullname": "Kaiyang Zhou",
            "name": "kaiyangzhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14464",
            "authors": [
                {
                    "_id": "682d39e6fa24196dcd10d5e8",
                    "user": {
                        "_id": "621499d72be42a56cca7afad",
                        "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
                        "isPro": false,
                        "fullname": "TianXiaoyu",
                        "user": "Emperorizzis",
                        "type": "user"
                    },
                    "name": "Xiaoyu Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:56.337Z",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5e9",
                    "name": "Yunjie Ji",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5ea",
                    "name": "Haotian Wang",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5eb",
                    "name": "Shuaiting Chen",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5ec",
                    "name": "Sitong Zhao",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5ed",
                    "name": "Yiping Peng",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5ee",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "682d39e6fa24196dcd10d5ef",
                    "name": "Xiangang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T15:00:51.000Z",
            "submittedOnDailyAt": "2025-05-21T01:40:28.265Z",
            "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
            "submittedOnDailyBy": {
                "_id": "621499d72be42a56cca7afad",
                "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
                "isPro": false,
                "fullname": "TianXiaoyu",
                "user": "Emperorizzis",
                "type": "user"
            },
            "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging FaceDatasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
            "upvotes": 8,
            "discussionId": "682d39e8fa24196dcd10d643",
            "ai_summary": "Distilling reasoning data from advanced language models improves student model performance across various benchmarks.",
            "ai_keywords": [
                "distillation",
                "reasoning data",
                "teacher models",
                "AM-Thinking-v1",
                "Qwen3-235B-A22B",
                "DeepSeek-R1",
                "token length diversity",
                "perplexity",
                "student models",
                "reasoning benchmarks",
                "AIME2024",
                "AIME2025",
                "MATH500",
                "LiveCodeBench",
                "adaptive output behavior",
                "high-quality",
                "verified reasoning traces"
            ]
        },
        "publishedAt": "2025-05-20T11:00:51.000Z",
        "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
        "summary": "Distillation has emerged as a practical and effective approach to enhance the\nreasoning capabilities of open-source language models. In this work, we conduct\na large-scale empirical study on reasoning data distillation by collecting\nverified outputs from three state-of-the-art teacher models-AM-Thinking-v1,\nQwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We\nconstruct three parallel datasets and analyze their distributions, revealing\nthat AM-Thinking-v1-distilled data exhibits greater token length diversity and\nlower perplexity. Student models trained on each dataset are evaluated on\nreasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.\nThe AM-based model consistently achieves the best performance (e.g., 84.3 on\nAIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and\ndemonstrates adaptive output behavior-producing longer responses for harder\ntasks and shorter ones for simpler tasks. These findings highlight the value of\nhigh-quality, verified reasoning traces. We release the AM-Thinking-v1 and\nQwen3-235B-A22B distilled datasets to support future research on open and\nhigh-performing reasoning-oriented language models. The datasets are publicly\navailable on Hugging FaceDatasets are available on Hugging Face:\n\\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled},\nhttps://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14464.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "621499d72be42a56cca7afad",
            "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg",
            "fullname": "TianXiaoyu",
            "name": "Emperorizzis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.12448",
            "authors": [
                {
                    "_id": "682d47aa64daf8623f1f5604",
                    "user": {
                        "_id": "6586817e509bcae23f3dfc60",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pwEStoey1XJYDi3F0Z930.png",
                        "isPro": false,
                        "fullname": "Yang Liu",
                        "user": "yliu-cs",
                        "type": "user"
                    },
                    "name": "Yang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:37.342Z",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f5605",
                    "name": "Ming Ma",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f5606",
                    "user": {
                        "_id": "64084fa192033c150738e4f2",
                        "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg",
                        "isPro": false,
                        "fullname": "Yu_xm",
                        "user": "Yu2020",
                        "type": "user"
                    },
                    "name": "Xiaomin Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T15:17:46.958Z",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f5607",
                    "name": "Pengxiang Ding",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f5608",
                    "name": "Han Zhao",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f5609",
                    "name": "Mingyang Sun",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f560a",
                    "name": "Siteng Huang",
                    "hidden": false
                },
                {
                    "_id": "682d47aa64daf8623f1f560b",
                    "name": "Donglin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-18T14:40:16.000Z",
            "submittedOnDailyAt": "2025-05-21T01:56:36.099Z",
            "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
            "submittedOnDailyBy": {
                "_id": "65fd82762bf2cd20ddaa193f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                "isPro": false,
                "fullname": "Siteng Huang",
                "user": "huangsiteng",
                "type": "user"
            },
            "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
            "upvotes": 8,
            "discussionId": "682d47ab64daf8623f1f5634",
            "projectPage": "https://yliu-cs.github.io/SSR/",
            "githubRepo": "https://github.com/yliu-cs/SSR",
            "ai_summary": "A novel method transforms depth data into textual rationales to enhance spatial reasoning in Visual-Language Models, improving depth utilization and human-like multi-modal understanding.",
            "ai_keywords": [
                "Spatial Sense and Reasoning",
                "SSR",
                "structured",
                "interpretable textual rationales",
                "knowledge distillation",
                "latent embeddings",
                "SSR-CoT",
                "SSRBench",
                "multi-task benchmark",
                "visual-language reasoning"
            ]
        },
        "publishedAt": "2025-05-18T10:40:16.000Z",
        "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
        "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12448.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14352",
            "authors": [
                {
                    "_id": "682d72b0d57ba1e4d132148d",
                    "user": {
                        "_id": "6422f416a73327caad9d1d86",
                        "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
                        "isPro": false,
                        "fullname": "Bartosz Cywiski",
                        "user": "bcywinski",
                        "type": "user"
                    },
                    "name": "Bartosz Cywiski",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:39:48.008Z",
                    "hidden": false
                },
                {
                    "_id": "682d72b0d57ba1e4d132148e",
                    "name": "Emil Ryd",
                    "hidden": false
                },
                {
                    "_id": "682d72b0d57ba1e4d132148f",
                    "user": {
                        "_id": "6477122c99a5ce743ccf2f55",
                        "avatarUrl": "/avatars/dee617374fc609e07eba3bcb2cd16810.svg",
                        "isPro": false,
                        "fullname": "Senthooran Rajamanoharan",
                        "user": "srdm",
                        "type": "user"
                    },
                    "name": "Senthooran Rajamanoharan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:37:33.691Z",
                    "hidden": false
                },
                {
                    "_id": "682d72b0d57ba1e4d1321490",
                    "user": {
                        "_id": "62669380c8bc5cf80ca97350",
                        "avatarUrl": "/avatars/6d5cd2261163308b82341c1ce28984d1.svg",
                        "isPro": false,
                        "fullname": "Neel Nanda",
                        "user": "NeelNanda",
                        "type": "user"
                    },
                    "name": "Neel Nanda",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:37:18.664Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T13:36:37.000Z",
            "submittedOnDailyAt": "2025-05-21T04:59:37.379Z",
            "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
            "submittedOnDailyBy": {
                "_id": "6422f416a73327caad9d1d86",
                "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
                "isPro": false,
                "fullname": "Bartosz Cywiski",
                "user": "bcywinski",
                "type": "user"
            },
            "summary": "As language models become more powerful and sophisticated, it is crucial that\nthey remain trustworthy and reliable. There is concerning preliminary evidence\nthat models may attempt to deceive or keep secrets from their operators. To\nexplore the ability of current techniques to elicit such hidden knowledge, we\ntrain a Taboo model: a language model that describes a specific secret word\nwithout explicitly stating it. Importantly, the secret word is not presented to\nthe model in its training data or prompt. We then investigate methods to\nuncover this secret. First, we evaluate non-interpretability (black-box)\napproaches. Subsequently, we develop largely automated strategies based on\nmechanistic interpretability techniques, including logit lens and sparse\nautoencoders. Evaluation shows that both approaches are effective in eliciting\nthe secret word in our proof-of-concept setting. Our findings highlight the\npromise of these approaches for eliciting hidden knowledge and suggest several\npromising avenues for future work, including testing and refining these methods\non more complex model organisms. This work aims to be a step towards addressing\nthe crucial problem of eliciting secret knowledge from language models, thereby\ncontributing to their safe and reliable deployment.",
            "upvotes": 7,
            "discussionId": "682d72b1d57ba1e4d13214c6",
            "githubRepo": "https://github.com/EmilRyd/eliciting-secrets",
            "ai_summary": "Methods using logit lens and sparse autoencoders effectively uncover hidden knowledge in language models trained to keep secrets.",
            "ai_keywords": [
                "logit lens",
                "sparse autoencoders",
                "mechanistic interpretability",
                "Taboo model"
            ]
        },
        "publishedAt": "2025-05-20T09:36:37.000Z",
        "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
        "summary": "As language models become more powerful and sophisticated, it is crucial that\nthey remain trustworthy and reliable. There is concerning preliminary evidence\nthat models may attempt to deceive or keep secrets from their operators. To\nexplore the ability of current techniques to elicit such hidden knowledge, we\ntrain a Taboo model: a language model that describes a specific secret word\nwithout explicitly stating it. Importantly, the secret word is not presented to\nthe model in its training data or prompt. We then investigate methods to\nuncover this secret. First, we evaluate non-interpretability (black-box)\napproaches. Subsequently, we develop largely automated strategies based on\nmechanistic interpretability techniques, including logit lens and sparse\nautoencoders. Evaluation shows that both approaches are effective in eliciting\nthe secret word in our proof-of-concept setting. Our findings highlight the\npromise of these approaches for eliciting hidden knowledge and suggest several\npromising avenues for future work, including testing and refining these methods\non more complex model organisms. This work aims to be a step towards addressing\nthe crucial problem of eliciting secret knowledge from language models, thereby\ncontributing to their safe and reliable deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14352.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6422f416a73327caad9d1d86",
            "avatarUrl": "/avatars/aa3639277cd1732504402fc64a57eff8.svg",
            "fullname": "Bartosz Cywiski",
            "name": "bcywinski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14534",
            "authors": [
                {
                    "_id": "682d7f009a06bc9f9106480b",
                    "user": {
                        "_id": "65ef0b4b325d9aaef87eb33b",
                        "avatarUrl": "/avatars/ec27dba9de7e74c1f6cdcacf5aa25528.svg",
                        "isPro": false,
                        "fullname": "C Shi",
                        "user": "chongyangs",
                        "type": "user"
                    },
                    "name": "Chongyang Shi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T07:21:37.326Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f9106480c",
                    "name": "Sharon Lin",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f9106480d",
                    "name": "Shuang Song",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f9106480e",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f9106480f",
                    "user": {
                        "_id": "6475c2794766357252e69e9f",
                        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                        "isPro": false,
                        "fullname": "i",
                        "user": "iliashum",
                        "type": "user"
                    },
                    "name": "Ilia Shumailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:31:09.839Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064810",
                    "user": {
                        "_id": "62697edaa6a7bba9e46ae4ad",
                        "avatarUrl": "/avatars/3009bf769a09d946447a0e8c833a04f3.svg",
                        "isPro": false,
                        "fullname": "Itay Yona",
                        "user": "tux",
                        "type": "user"
                    },
                    "name": "Itay Yona",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:31:16.965Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064811",
                    "user": {
                        "_id": "657d17d43480ce8aae628d40",
                        "avatarUrl": "/avatars/7f8cd9953f82e5a7ad5db1f82cfdb8f1.svg",
                        "isPro": false,
                        "fullname": "juliette pluto",
                        "user": "julsh",
                        "type": "user"
                    },
                    "name": "Juliette Pluto",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:31:22.677Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064812",
                    "name": "Aneesh Pappu",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064813",
                    "user": {
                        "_id": "649c5307e89a7650158f4965",
                        "avatarUrl": "/avatars/3c71b501db27c86ed686ae26ad1391cd.svg",
                        "isPro": false,
                        "fullname": "Christopher A. Choquette-Choo",
                        "user": "cchoquette",
                        "type": "user"
                    },
                    "name": "Christopher A. Choquette-Choo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:31:33.340Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064814",
                    "name": "Milad Nasr",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064815",
                    "user": {
                        "_id": "623e3ebef3be5bae969680c0",
                        "avatarUrl": "/avatars/590c264bf9964225bd67f61a3a5d53ea.svg",
                        "isPro": false,
                        "fullname": "Chawin Sitawarin",
                        "user": "chawins",
                        "type": "user"
                    },
                    "name": "Chawin Sitawarin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:31:58.593Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064816",
                    "name": "Gena Gibson",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064817",
                    "name": "Andreas Terzis",
                    "hidden": false
                },
                {
                    "_id": "682d7f009a06bc9f91064818",
                    "name": "John \"Four\" Flynn",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T15:54:45.000Z",
            "submittedOnDailyAt": "2025-05-21T05:52:22.988Z",
            "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.",
            "upvotes": 6,
            "discussionId": "682d7f019a06bc9f9106486e",
            "ai_summary": "Google DeepMind evaluates the adversarial robustness of Gemini through continuous testing with adaptive attack techniques to enhance its resilience.",
            "ai_keywords": [
                "adversarial robustness",
                "model evaluation",
                "adaptive attack techniques"
            ]
        },
        "publishedAt": "2025-05-20T11:54:45.000Z",
        "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
        "summary": "Gemini is increasingly used to perform tasks on behalf of users, where\nfunction-calling and tool-use capabilities enable the model to access user\ndata. Some tools, however, require access to untrusted data introducing risk.\nAdversaries can embed malicious instructions in untrusted data which cause the\nmodel to deviate from the user's expectations and mishandle their data or\npermissions. In this report, we set out Google DeepMind's approach to\nevaluating the adversarial robustness of Gemini models and describe the main\nlessons learned from the process. We test how Gemini performs against a\nsophisticated adversary through an adversarial evaluation framework, which\ndeploys a suite of adaptive attack techniques to run continuously against past,\ncurrent, and future versions of Gemini. We describe how these ongoing\nevaluations directly help make Gemini more resilient against manipulation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14534.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13988",
            "authors": [
                {
                    "_id": "682d355b89b5f79a1d6546f9",
                    "name": "Linxin Song",
                    "hidden": false
                },
                {
                    "_id": "682d355b89b5f79a1d6546fa",
                    "name": "Taiwei Shi",
                    "hidden": false
                },
                {
                    "_id": "682d355b89b5f79a1d6546fb",
                    "name": "Jieyu Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T06:36:45.000Z",
            "submittedOnDailyAt": "2025-05-21T18:53:18.798Z",
            "title": "The Hallucination Tax of Reinforcement Finetuning",
            "submittedOnDailyBy": {
                "_id": "62e1b3cb3eb0730f621a83f6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
                "isPro": false,
                "fullname": "Taiwei Shi",
                "user": "MaksimSTW",
                "type": "user"
            },
            "summary": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.",
            "upvotes": 6,
            "discussionId": "682d355b89b5f79a1d65472d",
            "ai_summary": "Reinforcement fine-tuning can degrade model refusal behavior, leading to increased hallucination; incorporating a synthetic dataset of unanswerable math problems during fine-tuning can restore appropriate refusal behavior with minimal accuracy loss and improve generalization.",
            "ai_keywords": [
                "Reinforcement finetuning (RFT)",
                "large language models (LLMs)",
                "hallucination tax",
                "refusal behavior",
                "SUM (Synthetic Unanswerable Math)",
                "unanswerable math problems",
                "solvable tasks",
                "inference-time compute",
                "knowledge boundaries",
                "generalization",
                "factual question answering tasks"
            ]
        },
        "publishedAt": "2025-05-20T02:36:45.000Z",
        "title": "The Hallucination Tax of Reinforcement Finetuning",
        "summary": "Reinforcement finetuning (RFT) has become a standard approach for enhancing\nthe reasoning capabilities of large language models (LLMs). However, its impact\non model trustworthiness remains underexplored. In this work, we identify and\nsystematically study a critical side effect of RFT, which we term the\nhallucination tax: a degradation in refusal behavior causing models to produce\nhallucinated answers to unanswerable questions confidently. To investigate\nthis, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of\nunanswerable math problems designed to probe models' ability to recognize an\nunanswerable question by reasoning from the insufficient or ambiguous\ninformation. Our results show that standard RFT training could reduce model\nrefusal rates by more than 80%, which significantly increases model's tendency\nto hallucinate. We further demonstrate that incorporating just 10% SUM during\nRFT substantially restores appropriate refusal behavior, with minimal accuracy\ntrade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage\ninference-time compute to reason about their own uncertainty and knowledge\nboundaries, improving generalization not only to out-of-domain math problems\nbut also to factual question answering tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13988.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e1b3cb3eb0730f621a83f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
            "fullname": "Taiwei Shi",
            "name": "MaksimSTW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.13718",
            "authors": [
                {
                    "_id": "682d6ccc26146f27d1ab3d83",
                    "user": {
                        "_id": "64cb922ec7f30fbf7b91a9a7",
                        "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
                        "isPro": false,
                        "fullname": "Safal Shrestha",
                        "user": "safal312",
                        "type": "user"
                    },
                    "name": "Safal Shrestha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:07.374Z",
                    "hidden": false
                },
                {
                    "_id": "682d6ccc26146f27d1ab3d84",
                    "user": {
                        "_id": "64e77b47d96966317b45eeb3",
                        "avatarUrl": "/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg",
                        "isPro": false,
                        "fullname": "Minwu Kim",
                        "user": "guactastesgood",
                        "type": "user"
                    },
                    "name": "Minwu Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:27:16.736Z",
                    "hidden": false
                },
                {
                    "_id": "682d6ccc26146f27d1ab3d85",
                    "user": {
                        "_id": "67b5b0800918c8645fea09d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_6pKdkxjsC0dOBbKAN3ok.png",
                        "isPro": false,
                        "fullname": "Aadim Nepal",
                        "user": "AadimNepal",
                        "type": "user"
                    },
                    "name": "Aadim Nepal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:27:23.895Z",
                    "hidden": false
                },
                {
                    "_id": "682d6ccc26146f27d1ab3d86",
                    "user": {
                        "_id": "66a0394001cfae79a11b54ea",
                        "avatarUrl": "/avatars/e135cca9b9027cfb762e7ff86bd5ab5a.svg",
                        "isPro": false,
                        "fullname": "Anubhav Shrestha",
                        "user": "xanubhav81",
                        "type": "user"
                    },
                    "name": "Anubhav Shrestha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:27:30.010Z",
                    "hidden": false
                },
                {
                    "_id": "682d6ccc26146f27d1ab3d87",
                    "name": "Keith Ross",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T20:29:15.000Z",
            "submittedOnDailyAt": "2025-05-21T04:35:24.229Z",
            "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
            "submittedOnDailyBy": {
                "_id": "64cb922ec7f30fbf7b91a9a7",
                "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
                "isPro": false,
                "fullname": "Safal Shrestha",
                "user": "safal312",
                "type": "user"
            },
            "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: (i) the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset (leq100 examples), the warmed-up model consistently outperforms the\nbase model; (iii) Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; (iv)\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.",
            "upvotes": 6,
            "discussionId": "682d6ccd26146f27d1ab3dbb",
            "githubRepo": "https://github.com/safal312/warmup-before-you-train",
            "ai_summary": "A two-stage training strategy involving a warm-up phase with distillation of logic puzzles followed by RLVR training on limited domain-specific data improves reasoning capabilities and sample efficiency in LLMs.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Long Chain of Thoughts (CoT)",
                "Knights & Knaves (K&K) logic puzzles",
                "warm-up phase",
                "generalized reasoning",
                "MATH",
                "HumanEval+",
                "MMLU-Pro",
                "sample efficiency",
                "cross-domain generalizability"
            ]
        },
        "publishedAt": "2025-05-19T16:29:15.000Z",
        "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
        "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: (i) the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset (leq100 examples), the warmed-up model consistently outperforms the\nbase model; (iii) Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; (iv)\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13718.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cb922ec7f30fbf7b91a9a7",
            "avatarUrl": "/avatars/457eae5e56b9641ee5543146447d1755.svg",
            "fullname": "Safal Shrestha",
            "name": "safal312",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13103",
            "authors": [
                {
                    "_id": "682d7f7a176087390484a412",
                    "name": "Han Zheng",
                    "hidden": false
                },
                {
                    "_id": "682d7f7a176087390484a413",
                    "user": {
                        "_id": "6475c2794766357252e69e9f",
                        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                        "isPro": false,
                        "fullname": "i",
                        "user": "iliashum",
                        "type": "user"
                    },
                    "name": "Ilia Shumailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:25:55.989Z",
                    "hidden": false
                },
                {
                    "_id": "682d7f7a176087390484a414",
                    "name": "Tianqi Fan",
                    "hidden": false
                },
                {
                    "_id": "682d7f7a176087390484a415",
                    "name": "Aiden Hall",
                    "hidden": false
                },
                {
                    "_id": "682d7f7a176087390484a416",
                    "name": "Mathias Payer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T13:32:51.000Z",
            "submittedOnDailyAt": "2025-05-21T05:55:51.238Z",
            "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
            "upvotes": 6,
            "discussionId": "682d7f7a176087390484a448",
            "ai_summary": "Crash-site repair and template-guided patch generation reduce token costs and improve bug-fixing rates in automated program repair.",
            "ai_keywords": [
                "crash-site repair",
                "template-guided patch generation",
                "Large Language Models",
                "token cost",
                "Automated Program Repair"
            ]
        },
        "publishedAt": "2025-05-19T09:32:51.000Z",
        "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
        "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13103.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.12182",
            "authors": [
                {
                    "_id": "682d31dd17608739046e1169",
                    "user": {
                        "_id": "634cabd104491d9f7111eea3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
                        "isPro": false,
                        "fullname": "Haohang Li",
                        "user": "Acatsama",
                        "type": "user"
                    },
                    "name": "Haohang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:29:09.565Z",
                    "hidden": false
                },
                {
                    "_id": "682d31dd17608739046e116a",
                    "user": {
                        "_id": "62dd8f328456396d4f8aa894",
                        "avatarUrl": "/avatars/af8f5dc7ff937e3e849ecdfd9ca4750b.svg",
                        "isPro": false,
                        "fullname": "Yupeng Cao",
                        "user": "YupengCao",
                        "type": "user"
                    },
                    "name": "Yupeng Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:29:23.338Z",
                    "hidden": false
                },
                {
                    "_id": "682d31dd17608739046e116b",
                    "user": {
                        "_id": "64f757c6016d60f3199ef5e6",
                        "avatarUrl": "/avatars/2659ba698081265d0480b08161718013.svg",
                        "isPro": false,
                        "fullname": "Yangyang Yu",
                        "user": "ShirleyY",
                        "type": "user"
                    },
                    "name": "Yangyang Yu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:29:31.615Z",
                    "hidden": false
                },
                {
                    "_id": "682d31dd17608739046e116c",
                    "user": {
                        "_id": "631a8c9e0867652f538a64e5",
                        "avatarUrl": "/avatars/09e1cef9e7d6f7c51579448d4605850e.svg",
                        "isPro": false,
                        "fullname": "Jordan Suchow",
                        "user": "jordansuchow",
                        "type": "user"
                    },
                    "name": "Jordan W. Suchow",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:29:41.392Z",
                    "hidden": false
                },
                {
                    "_id": "682d31dd17608739046e116d",
                    "user": {
                        "_id": "62d63a9dc5ada8ef841b4787",
                        "avatarUrl": "/avatars/a79a4ac07984d9a8623c99bdce9add54.svg",
                        "isPro": false,
                        "fullname": "Zining Zhu",
                        "user": "ZiningZhu",
                        "type": "user"
                    },
                    "name": "Zining Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:29:50.430Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-18T00:47:21.000Z",
            "submittedOnDailyAt": "2025-05-21T00:23:07.653Z",
            "title": "Truth Neurons",
            "submittedOnDailyBy": {
                "_id": "634cabd104491d9f7111eea3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
                "isPro": false,
                "fullname": "Haohang Li",
                "user": "Acatsama",
                "type": "user"
            },
            "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
            "upvotes": 6,
            "discussionId": "682d31de17608739046e11c9",
            "ai_summary": "Truth neurons encode truthfulness in language models at a subject-agnostic level, and suppressing these neurons degrades performance across benchmarks.",
            "ai_keywords": [
                "truth neurons",
                "language models",
                "truthfulness",
                "neuron level",
                "geometry of truthfulness",
                "TruthfulQA dataset"
            ]
        },
        "publishedAt": "2025-05-17T20:47:21.000Z",
        "title": "Truth Neurons",
        "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12182.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634cabd104491d9f7111eea3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cabd104491d9f7111eea3/JoqlugwfD1aGkd-wZTmP7.jpeg",
            "fullname": "Haohang Li",
            "name": "Acatsama",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.09569",
            "authors": [
                {
                    "_id": "682563f807f74666ec373332",
                    "user": {
                        "_id": "682573f5a8792137ebca0a70",
                        "avatarUrl": "/avatars/358767039db06c41c2bcee55a081cb21.svg",
                        "isPro": false,
                        "fullname": "Linbo Liu",
                        "user": "linboliu",
                        "type": "user"
                    },
                    "name": "Linbo Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:28:01.555Z",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373333",
                    "user": {
                        "_id": "636c32ae181c81c337f086b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
                        "isPro": false,
                        "fullname": "Xinle Sheila Liu",
                        "user": "sliuxl",
                        "type": "user"
                    },
                    "name": "Xinle Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-15T10:30:40.773Z",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373334",
                    "name": "Qiang Zhou",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373335",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373336",
                    "name": "Yihan Liu",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373337",
                    "name": "Hoan Nguyen",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373338",
                    "user": {
                        "_id": "63aa8f1b8780edbabb18f9ad",
                        "avatarUrl": "/avatars/a184f9816bedf5f9f49975f3334fd24a.svg",
                        "isPro": false,
                        "fullname": "Behrooz Omidvar-Tehrani",
                        "user": "omidvarb",
                        "type": "user"
                    },
                    "name": "Behrooz Omidvar-Tehrani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:28:28.110Z",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec373339",
                    "name": "Xi Shen",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec37333a",
                    "name": "Jun Huan",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec37333b",
                    "name": "Omer Tripp",
                    "hidden": false
                },
                {
                    "_id": "682563f807f74666ec37333c",
                    "name": "Anoop Deoras",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-14T17:11:23.000Z",
            "submittedOnDailyAt": "2025-05-21T04:22:33.758Z",
            "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
            "submittedOnDailyBy": {
                "_id": "636c32ae181c81c337f086b9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
                "isPro": false,
                "fullname": "Xinle Sheila Liu",
                "user": "sliuxl",
                "type": "user"
            },
            "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with 5,102 and 300 repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
            "upvotes": 6,
            "discussionId": "682563f907f74666ec373369",
            "projectPage": "https://github.com/amazon-science/SDFeedback",
            "githubRepo": "https://github.com/amazon-science/MigrationBench",
            "ai_summary": "A new benchmark MIGRATION-BENCH evaluates large language models on Java code migration tasks, providing a dataset and framework for repository-level migration assessment.",
            "ai_keywords": [
                "large language models",
                "code migration",
                "MIGRATION-BENCH",
                "Java 8",
                "Java 17",
                "Java 21",
                "repository-level migration",
                "SD-Feedback"
            ]
        },
        "publishedAt": "2025-05-14T13:11:23.000Z",
        "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
        "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with 5,102 and 300 repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09569.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636c32ae181c81c337f086b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636c32ae181c81c337f086b9/9zHTHmwzSeLMJWuwHCqAe.jpeg",
            "fullname": "Xinle Sheila Liu",
            "name": "sliuxl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14648",
            "authors": [
                {
                    "_id": "682e0839d8a198bf9e184ef2",
                    "user": {
                        "_id": "64092a1ab6a334f53e278b3b",
                        "avatarUrl": "/avatars/ff8f9b88599c1d6ffe496a4d6063d5c8.svg",
                        "isPro": false,
                        "fullname": "Tiantian Feng",
                        "user": "tiantiaf",
                        "type": "user"
                    },
                    "name": "Tiantian Feng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T17:07:37.217Z",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef3",
                    "name": "Jihwan Lee",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef4",
                    "name": "Anfeng Xu",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef5",
                    "name": "Yoonjeong Lee",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef6",
                    "name": "Thanathai Lertpetchpun",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef7",
                    "name": "Xuan Shi",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef8",
                    "name": "Helin Wang",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184ef9",
                    "name": "Thomas Thebaud",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184efa",
                    "name": "Laureano Moro-Velazquez",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184efb",
                    "name": "Dani Byrd",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184efc",
                    "name": "Najim Dehak",
                    "hidden": false
                },
                {
                    "_id": "682e0839d8a198bf9e184efd",
                    "name": "Shrikanth Narayanan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:36:41.000Z",
            "submittedOnDailyAt": "2025-05-21T15:41:42.211Z",
            "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing\n  Diverse Speaker and Speech Traits",
            "submittedOnDailyBy": {
                "_id": "64092a1ab6a334f53e278b3b",
                "avatarUrl": "/avatars/ff8f9b88599c1d6ffe496a4d6063d5c8.svg",
                "isPro": false,
                "fullname": "Tiantian Feng",
                "user": "tiantiaf",
                "type": "user"
            },
            "summary": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich\nspeaker and speech traits using speech foundation models. Unlike existing works\nthat focus on a single dimension of speaker traits, Vox-Profile provides\nholistic and multi-dimensional profiles that reflect both static speaker traits\n(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech\nflow). This benchmark is grounded in speech science and linguistics, developed\nwith domain experts to accurately index speaker and speech characteristics. We\nreport benchmark experiments using over 15 publicly available speech datasets\nand several widely used speech foundation models that target various static and\ndynamic speaker and speech properties. In addition to benchmark experiments, we\nshowcase several downstream applications supported by Vox-Profile. First, we\nshow that Vox-Profile can augment existing speech recognition datasets to\nanalyze ASR performance variability. Vox-Profile is also used as a tool to\nevaluate the performance of speech generation systems. Finally, we assess the\nquality of our automated profiles through comparison with human evaluation and\nshow convergent validity. Vox-Profile is publicly available at:\nhttps://github.com/tiantiaf0627/vox-profile-release.",
            "upvotes": 5,
            "discussionId": "682e083ad8a198bf9e184f47",
            "ai_summary": "Vox-Profile is a benchmark for evaluating multi-dimensional speaker and speech traits using speech foundation models, offering applications in ASR and speech generation performance analysis.",
            "ai_keywords": [
                "speech foundation models",
                "speaker traits",
                "speech properties",
                "emotion",
                "speech flow",
                "speaker characteristics",
                "ASR performance variability",
                "speech generation systems",
                "human evaluation",
                "convergent validity"
            ]
        },
        "publishedAt": "2025-05-20T13:36:41.000Z",
        "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing\n  Diverse Speaker and Speech Traits",
        "summary": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich\nspeaker and speech traits using speech foundation models. Unlike existing works\nthat focus on a single dimension of speaker traits, Vox-Profile provides\nholistic and multi-dimensional profiles that reflect both static speaker traits\n(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech\nflow). This benchmark is grounded in speech science and linguistics, developed\nwith domain experts to accurately index speaker and speech characteristics. We\nreport benchmark experiments using over 15 publicly available speech datasets\nand several widely used speech foundation models that target various static and\ndynamic speaker and speech properties. In addition to benchmark experiments, we\nshowcase several downstream applications supported by Vox-Profile. First, we\nshow that Vox-Profile can augment existing speech recognition datasets to\nanalyze ASR performance variability. Vox-Profile is also used as a tool to\nevaluate the performance of speech generation systems. Finally, we assess the\nquality of our automated profiles through comparison with human evaluation and\nshow convergent validity. Vox-Profile is publicly available at:\nhttps://github.com/tiantiaf0627/vox-profile-release.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14648.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64092a1ab6a334f53e278b3b",
            "avatarUrl": "/avatars/ff8f9b88599c1d6ffe496a4d6063d5c8.svg",
            "fullname": "Tiantian Feng",
            "name": "tiantiaf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13946",
            "authors": [
                {
                    "_id": "682e33fa7a4bdea890eb25d0",
                    "name": "Changdae Oh",
                    "hidden": false
                },
                {
                    "_id": "682e33fa7a4bdea890eb25d1",
                    "name": "Jiatong Li",
                    "hidden": false
                },
                {
                    "_id": "682e33fa7a4bdea890eb25d2",
                    "name": "Shawn Im",
                    "hidden": false
                },
                {
                    "_id": "682e33fa7a4bdea890eb25d3",
                    "name": "Yixuan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T05:24:53.000Z",
            "submittedOnDailyAt": "2025-05-21T18:47:40.248Z",
            "title": "Visual Instruction Bottleneck Tuning",
            "submittedOnDailyBy": {
                "_id": "672fc8ede7c89e44c9757259",
                "avatarUrl": "/avatars/caa0d0de519ea96992c81328c89b3843.svg",
                "isPro": false,
                "fullname": "Changdae Oh",
                "user": "changdae",
                "type": "user"
            },
            "summary": "Despite widespread adoption, multimodal large language models (MLLMs) suffer\nperformance degradation when encountering unfamiliar queries under distribution\nshifts. Existing methods to improve MLLM generalization typically require\neither more instruction data or larger advanced model architectures, both of\nwhich incur non-trivial human labor or computational costs. In this work, we\ntake an alternative approach to enhance the robustness of MLLMs under\ndistribution shifts, from a representation learning perspective. Inspired by\nthe information bottleneck (IB) principle, we derive a variational lower bound\nof the IB for MLLMs and devise a practical implementation, Visual Instruction\nBottleneck Tuning (Vittle). We then provide a theoretical justification of\nVittle by revealing its connection to an information-theoretic robustness\nmetric of MLLM. Empirical validation of three MLLMs on open-ended and\nclosed-form question answering and object hallucination detection tasks over 45\ndatasets, including 30 shift scenarios, demonstrates that Vittle consistently\nimproves the MLLM's robustness under shifts by pursuing the learning of a\nminimal sufficient representation.",
            "upvotes": 5,
            "discussionId": "682e33fc7a4bdea890eb2654",
            "ai_summary": "A variational lower bound of the information bottleneck principle is implemented to enhance the robustness of multimodal large language models under distribution shifts.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "distribution shifts",
                "information bottleneck principle",
                "variational lower bound",
                "Visual Instruction Bottleneck Tuning",
                "Vittle",
                "information-theoretic robustness metric",
                "minimal sufficient representation"
            ]
        },
        "publishedAt": "2025-05-20T01:24:53.000Z",
        "title": "Visual Instruction Bottleneck Tuning",
        "summary": "Despite widespread adoption, multimodal large language models (MLLMs) suffer\nperformance degradation when encountering unfamiliar queries under distribution\nshifts. Existing methods to improve MLLM generalization typically require\neither more instruction data or larger advanced model architectures, both of\nwhich incur non-trivial human labor or computational costs. In this work, we\ntake an alternative approach to enhance the robustness of MLLMs under\ndistribution shifts, from a representation learning perspective. Inspired by\nthe information bottleneck (IB) principle, we derive a variational lower bound\nof the IB for MLLMs and devise a practical implementation, Visual Instruction\nBottleneck Tuning (Vittle). We then provide a theoretical justification of\nVittle by revealing its connection to an information-theoretic robustness\nmetric of MLLM. Empirical validation of three MLLMs on open-ended and\nclosed-form question answering and object hallucination detection tasks over 45\ndatasets, including 30 shift scenarios, demonstrates that Vittle consistently\nimproves the MLLM's robustness under shifts by pursuing the learning of a\nminimal sufficient representation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13946.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672fc8ede7c89e44c9757259",
            "avatarUrl": "/avatars/caa0d0de519ea96992c81328c89b3843.svg",
            "fullname": "Changdae Oh",
            "name": "changdae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.13380",
            "authors": [
                {
                    "_id": "682d3787265177367e119f04",
                    "user": {
                        "_id": "64c2bea2ada7df214276913b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
                        "isPro": false,
                        "fullname": "Nguyen Van Nam",
                        "user": "DavidNguyen",
                        "type": "user"
                    },
                    "name": "Nam V. Nguyen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:58.584Z",
                    "hidden": false
                },
                {
                    "_id": "682d3787265177367e119f05",
                    "name": "Huy Nguyen",
                    "hidden": false
                },
                {
                    "_id": "682d3787265177367e119f06",
                    "name": "Quang Pham",
                    "hidden": false
                },
                {
                    "_id": "682d3787265177367e119f07",
                    "name": "Van Nguyen",
                    "hidden": false
                },
                {
                    "_id": "682d3787265177367e119f08",
                    "name": "Savitha Ramasamy",
                    "hidden": false
                },
                {
                    "_id": "682d3787265177367e119f09",
                    "user": {
                        "_id": "66216209da5d83251916f301",
                        "avatarUrl": "/avatars/dd9d8977cbca95ae3309738b805474d4.svg",
                        "isPro": false,
                        "fullname": "Nhat Ho",
                        "user": "nhatho",
                        "type": "user"
                    },
                    "name": "Nhat Ho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:26:55.401Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T17:24:26.000Z",
            "submittedOnDailyAt": "2025-05-21T00:48:58.161Z",
            "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
            "submittedOnDailyBy": {
                "_id": "64c2bea2ada7df214276913b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
                "isPro": false,
                "fullname": "Nguyen Van Nam",
                "user": "DavidNguyen",
                "type": "user"
            },
            "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526",
            "upvotes": 5,
            "discussionId": "682d3788265177367e119f71",
            "githubRepo": "https://github.com/Fsoft-AIC/CompeteSMoE",
            "ai_summary": "CompeteSMoE enhances sparse mixture of experts (SMoE) by introducing a competition mechanism to improve routing efficiency and model performance in large language models.",
            "ai_keywords": [
                "sparse mixture of experts",
                "SMoE",
                "competition mechanism",
                "sample efficiency",
                "softmax routing",
                "CompeteSMoE",
                "visual instruction tuning",
                "language pre-training"
            ]
        },
        "publishedAt": "2025-05-19T13:24:26.000Z",
        "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
        "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13380.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c2bea2ada7df214276913b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c2bea2ada7df214276913b/QFCtmCn439Afsr7uqyoMT.jpeg",
            "fullname": "Nguyen Van Nam",
            "name": "DavidNguyen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11365",
            "authors": [
                {
                    "_id": "682d7ae082567fffe108b31a",
                    "user": {
                        "_id": "6596ca5cce76219628b8eab4",
                        "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
                        "isPro": false,
                        "fullname": "Pierre Le Jeune",
                        "user": "pierlj",
                        "type": "user"
                    },
                    "name": "Pierre Le Jeune",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T07:16:41.247Z",
                    "hidden": false
                },
                {
                    "_id": "682d7ae082567fffe108b31b",
                    "user": {
                        "_id": "65bccff4c1a44b6ef1c100da",
                        "avatarUrl": "/avatars/bf91000c78b83167958dc44c582397f0.svg",
                        "isPro": false,
                        "fullname": "benoit",
                        "user": "bmalezieux",
                        "type": "user"
                    },
                    "name": "Benot Malzieux",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
                    "hidden": false
                },
                {
                    "_id": "682d7ae082567fffe108b31c",
                    "user": {
                        "_id": "649f00fc37bfb5202be464a9",
                        "avatarUrl": "/avatars/89f6c6a92c076099f5450c3cd2057619.svg",
                        "isPro": false,
                        "fullname": "Inoki at Giskard",
                        "user": "inoki-giskard",
                        "type": "user"
                    },
                    "name": "Weixuan Xiao",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T07:04:02.669Z",
                    "hidden": false
                },
                {
                    "_id": "682d7ae082567fffe108b31d",
                    "name": "Matteo Dora",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T15:31:08.000Z",
            "submittedOnDailyAt": "2025-05-21T05:35:45.784Z",
            "title": "Phare: A Safety Probe for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6596ca5cce76219628b8eab4",
                "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
                "isPro": false,
                "fullname": "Pierre Le Jeune",
                "user": "pierlj",
                "type": "user"
            },
            "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
            "upvotes": 5,
            "discussionId": "682d7ae282567fffe108b40f",
            "projectPage": "https://phare.giskard.ai/",
            "githubRepo": "https://github.com/Giskard-AI/phare",
            "ai_summary": "Phare evaluates large language models across safety dimensions to uncover specific failure modes, offering insights for building more robust systems.",
            "ai_keywords": [
                "large language models",
                "hallucination",
                "reliability",
                "social biases",
                "harmful content generation",
                "sycophancy",
                "prompt sensitivity",
                "stereotype reproduction"
            ]
        },
        "publishedAt": "2025-05-16T11:31:08.000Z",
        "title": "Phare: A Safety Probe for Large Language Models",
        "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11365.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6596ca5cce76219628b8eab4",
            "avatarUrl": "/avatars/51cdea4e1e0e53260d403ceb7bc6de90.svg",
            "fullname": "Pierre Le Jeune",
            "name": "pierlj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11966",
            "authors": [
                {
                    "_id": "682d42808560f4baf596643b",
                    "user": {
                        "_id": "6608fa4f5baec84322ec85ea",
                        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
                        "isPro": false,
                        "fullname": "Zhong",
                        "user": "Jianyuan1",
                        "type": "user"
                    },
                    "name": "Jianyuan Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T08:40:43.900Z",
                    "hidden": false
                },
                {
                    "_id": "682d42808560f4baf596643c",
                    "name": "Zeju Li",
                    "hidden": false
                },
                {
                    "_id": "682d42808560f4baf596643d",
                    "name": "Zhijian Xu",
                    "hidden": false
                },
                {
                    "_id": "682d42808560f4baf596643e",
                    "user": {
                        "_id": "641b1b36a5f876fe30c49542",
                        "avatarUrl": "/avatars/ac9267925f45d325c2adb2eb0e38077b.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Wen",
                        "user": "XiangyuWen",
                        "type": "user"
                    },
                    "name": "Xiangyu Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:24:05.361Z",
                    "hidden": false
                },
                {
                    "_id": "682d42808560f4baf596643f",
                    "name": "Kezhi Li",
                    "hidden": false
                },
                {
                    "_id": "682d42808560f4baf5966440",
                    "name": "Qiang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-17T11:41:44.000Z",
            "submittedOnDailyAt": "2025-05-21T01:36:54.831Z",
            "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
            "submittedOnDailyBy": {
                "_id": "6608fa4f5baec84322ec85ea",
                "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
                "isPro": false,
                "fullname": "Zhong",
                "user": "Jianyuan1",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
            "upvotes": 4,
            "discussionId": "682d42808560f4baf5966480",
            "ai_summary": "FlexiVe, a novel generative verifier, optimally balances computational resources for enhanced LLM reasoning, improving accuracy and efficiency on complex tasks.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "verification",
                "Generative Reward Models (GenRMs)",
                "Flexible Allocation of Verification Budget",
                "Solve-Detect-Verify pipeline",
                "ProcessBench",
                "mathematical reasoning benchmarks",
                "self-consistency"
            ]
        },
        "publishedAt": "2025-05-17T07:41:44.000Z",
        "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
        "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11966.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "fullname": "Zhong",
            "name": "Jianyuan1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.10176",
            "authors": [
                {
                    "_id": "682dbb6ed675b2cd48589a13",
                    "user": {
                        "_id": "647f1e2ed26579210f5a6982",
                        "avatarUrl": "/avatars/742f3c701de9c846f58e693aec07cfc0.svg",
                        "isPro": false,
                        "fullname": "hexiang",
                        "user": "xianghe",
                        "type": "user"
                    },
                    "name": "Xiang He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T12:26:46.945Z",
                    "hidden": false
                },
                {
                    "_id": "682dbb6ed675b2cd48589a14",
                    "name": "Dongcheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "682dbb6ed675b2cd48589a15",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "682dbb6ed675b2cd48589a16",
                    "name": "Qingqun Kong",
                    "hidden": false
                },
                {
                    "_id": "682dbb6ed675b2cd48589a17",
                    "name": "Xin Yang",
                    "hidden": false
                },
                {
                    "_id": "682dbb6ed675b2cd48589a18",
                    "name": "Yi Zeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T11:08:50.000Z",
            "submittedOnDailyAt": "2025-05-21T10:10:04.519Z",
            "title": "Incorporating brain-inspired mechanisms for multimodal learning in\n  artificial intelligence",
            "submittedOnDailyBy": {
                "_id": "647f1e2ed26579210f5a6982",
                "avatarUrl": "/avatars/742f3c701de9c846f58e693aec07cfc0.svg",
                "isPro": false,
                "fullname": "hexiang",
                "user": "xianghe",
                "type": "user"
            },
            "summary": "Multimodal learning enhances the perceptual capabilities of cognitive systems\nby integrating information from different sensory modalities. However, existing\nmultimodal fusion research typically assumes static integration, not fully\nincorporating key dynamic mechanisms found in the brain. Specifically, the\nbrain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal\ncues yield stronger multisensory integration benefits; conversely, when\nindividual modal cues are stronger, the effect of fusion is diminished. This\nmechanism enables biological systems to achieve robust cognition even with\nscarce or noisy perceptual cues. Inspired by this biological mechanism, we\nexplore the relationship between multimodal output and information from\nindividual modalities, proposing an inverse effectiveness driven multimodal\nfusion (IEMF) strategy. By incorporating this strategy into neural networks, we\nachieve more efficient integration with improved model performance and\ncomputational efficiency, demonstrating up to 50% reduction in computational\ncost across diverse fusion methods. We conduct experiments on audio-visual\nclassification, continual learning, and question answering tasks to validate\nour method. Results consistently demonstrate that our method performs\nexcellently in these tasks. To verify universality and generalization, we also\nconduct experiments on Artificial Neural Networks (ANN) and Spiking Neural\nNetworks (SNN), with results showing good adaptability to both network types.\nOur research emphasizes the potential of incorporating biologically inspired\nmechanisms into multimodal networks and provides promising directions for the\nfuture development of multimodal artificial intelligence. The code is available\nat https://github.com/Brain-Cog-Lab/IEMF.",
            "upvotes": 3,
            "discussionId": "682dbb70d675b2cd48589acc",
            "githubRepo": "https://github.com/Brain-Cog-Lab/IEMF",
            "ai_summary": "Incorporating a biologically inspired inverse effectiveness mechanism into neural networks improves multimodal fusion performance and efficiency across various tasks and network types.",
            "ai_keywords": [
                "multimodal learning",
                "inverse effectiveness phenomenon",
                "multimodal fusion",
                "neural networks",
                "computational efficiency",
                "audio-visual classification",
                "continual learning",
                "question answering",
                "Artificial Neural Networks",
                "Spiking Neural Networks"
            ]
        },
        "publishedAt": "2025-05-15T07:08:50.000Z",
        "title": "Incorporating brain-inspired mechanisms for multimodal learning in\n  artificial intelligence",
        "summary": "Multimodal learning enhances the perceptual capabilities of cognitive systems\nby integrating information from different sensory modalities. However, existing\nmultimodal fusion research typically assumes static integration, not fully\nincorporating key dynamic mechanisms found in the brain. Specifically, the\nbrain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal\ncues yield stronger multisensory integration benefits; conversely, when\nindividual modal cues are stronger, the effect of fusion is diminished. This\nmechanism enables biological systems to achieve robust cognition even with\nscarce or noisy perceptual cues. Inspired by this biological mechanism, we\nexplore the relationship between multimodal output and information from\nindividual modalities, proposing an inverse effectiveness driven multimodal\nfusion (IEMF) strategy. By incorporating this strategy into neural networks, we\nachieve more efficient integration with improved model performance and\ncomputational efficiency, demonstrating up to 50% reduction in computational\ncost across diverse fusion methods. We conduct experiments on audio-visual\nclassification, continual learning, and question answering tasks to validate\nour method. Results consistently demonstrate that our method performs\nexcellently in these tasks. To verify universality and generalization, we also\nconduct experiments on Artificial Neural Networks (ANN) and Spiking Neural\nNetworks (SNN), with results showing good adaptability to both network types.\nOur research emphasizes the potential of incorporating biologically inspired\nmechanisms into multimodal networks and provides promising directions for the\nfuture development of multimodal artificial intelligence. The code is available\nat https://github.com/Brain-Cog-Lab/IEMF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10176.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f1e2ed26579210f5a6982",
            "avatarUrl": "/avatars/742f3c701de9c846f58e693aec07cfc0.svg",
            "fullname": "hexiang",
            "name": "xianghe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14633",
            "authors": [
                {
                    "_id": "682e09d9aa26daa3c1dfa562",
                    "name": "Yu Ying Chiu",
                    "hidden": false
                },
                {
                    "_id": "682e09d9aa26daa3c1dfa563",
                    "name": "Zhilin Wang",
                    "hidden": false
                },
                {
                    "_id": "682e09d9aa26daa3c1dfa564",
                    "name": "Sharan Maiya",
                    "hidden": false
                },
                {
                    "_id": "682e09d9aa26daa3c1dfa565",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "682e09d9aa26daa3c1dfa566",
                    "name": "Kyle Fish",
                    "hidden": false
                },
                {
                    "_id": "682e09d9aa26daa3c1dfa567",
                    "name": "Sydney Levine",
                    "hidden": false
                },
                {
                    "_id": "682e09d9aa26daa3c1dfa568",
                    "name": "Evan Hubinger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:24:09.000Z",
            "submittedOnDailyAt": "2025-05-21T15:45:19.302Z",
            "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas",
            "submittedOnDailyBy": {
                "_id": "65fcaae6e5dc5b0ec1b726cf",
                "avatarUrl": "/avatars/d5e696d19d91550be1421d46c2a3573b.svg",
                "isPro": true,
                "fullname": "Kelly Chiu",
                "user": "kellycyy",
                "type": "user"
            },
            "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.",
            "upvotes": 2,
            "discussionId": "682e09dbaa26daa3c1dfa5fc",
            "githubRepo": "https://github.com/kellycyy/LitmusValues",
            "ai_summary": "Identifying values within AI models using LitmusValues and evaluating them through AIRiskDilemmas and HarmBench can predict both known and unknown risky behaviors.",
            "ai_keywords": [
                "LitmusValues",
                "AI value classes",
                "AIRiskDilemmas",
                "value prioritization",
                "Power Seeking",
                "HarmBench"
            ]
        },
        "publishedAt": "2025-05-20T13:24:09.000Z",
        "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas",
        "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14633.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fcaae6e5dc5b0ec1b726cf",
            "avatarUrl": "/avatars/d5e696d19d91550be1421d46c2a3573b.svg",
            "fullname": "Kelly Chiu",
            "name": "kellycyy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14178",
            "authors": [
                {
                    "_id": "682d388c57686b8c44ede70b",
                    "user": {
                        "_id": "656553d89bf6665f10e3a92d",
                        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                        "isPro": false,
                        "fullname": "xiang wyatt zhang",
                        "user": "Wyattz23",
                        "type": "user"
                    },
                    "name": "Xiang Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T02:21:00.902Z",
                    "hidden": false
                },
                {
                    "_id": "682d388c57686b8c44ede70c",
                    "name": "Juntai Cao",
                    "hidden": false
                },
                {
                    "_id": "682d388c57686b8c44ede70d",
                    "name": "Jiaqi Wei",
                    "hidden": false
                },
                {
                    "_id": "682d388c57686b8c44ede70e",
                    "name": "Yiwei Xu",
                    "hidden": false
                },
                {
                    "_id": "682d388c57686b8c44ede70f",
                    "user": {
                        "_id": "6466d463060756d2854ab3e1",
                        "avatarUrl": "/avatars/4401387180c16472a6823f78aaa86d54.svg",
                        "isPro": false,
                        "fullname": "Chenyu You",
                        "user": "Charlesyooo",
                        "type": "user"
                    },
                    "name": "Chenyu You",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T02:30:12.849Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T10:32:30.000Z",
            "submittedOnDailyAt": "2025-05-21T00:51:16.514Z",
            "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
            "submittedOnDailyBy": {
                "_id": "656553d89bf6665f10e3a92d",
                "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                "isPro": false,
                "fullname": "xiang wyatt zhang",
                "user": "Wyattz23",
                "type": "user"
            },
            "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
            "upvotes": 2,
            "discussionId": "682d388c57686b8c44ede753",
            "ai_summary": "Tokenization structure significantly impacts symbolic reasoning in language models by affecting token granularity and alignment, influencing performance even with Chain-of-Thought prompting.",
            "ai_keywords": [
                "tokenization",
                "Chain-of-Thought (CoT)",
                "transformer models",
                "subword-based methods",
                "byte-pair encoding (BPE)",
                "Token Awareness",
                "symbolic computation",
                "logical alignment",
                "structured reasoning",
                "large language models (LLMs)"
            ]
        },
        "publishedAt": "2025-05-20T06:32:30.000Z",
        "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
        "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13731",
            "authors": [
                {
                    "_id": "682e1cd37a7eb705972c2cb7",
                    "name": "Pengyue Jia",
                    "hidden": false
                },
                {
                    "_id": "682e1cd37a7eb705972c2cb8",
                    "name": "Seongheon Park",
                    "hidden": false
                },
                {
                    "_id": "682e1cd37a7eb705972c2cb9",
                    "name": "Song Gao",
                    "hidden": false
                },
                {
                    "_id": "682e1cd37a7eb705972c2cba",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "682e1cd37a7eb705972c2cbb",
                    "name": "Yixuan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T21:04:46.000Z",
            "submittedOnDailyAt": "2025-05-21T17:09:01.897Z",
            "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization",
            "submittedOnDailyBy": {
                "_id": "64a53edb1adc23da725f3afd",
                "avatarUrl": "/avatars/d3cd53862f0b11fe2d532653062ac7cf.svg",
                "isPro": false,
                "fullname": "jiapengyue",
                "user": "Jia-py",
                "type": "user"
            },
            "summary": "Worldwide image geolocalization-the task of predicting GPS coordinates from\nimages taken anywhere on Earth-poses a fundamental challenge due to the vast\ndiversity in visual content across regions. While recent approaches adopt a\ntwo-stage pipeline of retrieving candidates and selecting the best match, they\ntypically rely on simplistic similarity heuristics and point-wise supervision,\nfailing to model spatial relationships among candidates. In this paper, we\npropose GeoRanker, a distance-aware ranking framework that leverages large\nvision-language models to jointly encode query-candidate interactions and\npredict geographic proximity. In addition, we introduce a multi-order distance\nloss that ranks both absolute and relative distances, enabling the model to\nreason over structured spatial relationships. To support this, we curate\nGeoRanking, the first dataset explicitly designed for geographic ranking tasks\nwith multimodal candidate information. GeoRanker achieves state-of-the-art\nresults on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly\noutperforming current best methods.",
            "upvotes": 2,
            "discussionId": "682e1cd57a7eb705972c2d42",
            "ai_summary": "GeoRanker uses large vision-language models to predict geographic proximity in image geolocalization by ranking multimodal candidate information.",
            "ai_keywords": [
                "vision-language models",
                "distance-aware ranking",
                "geographic proximity",
                "spatial relationships",
                "multi-order distance loss",
                "GeoRanking",
                "IM2GPS3K",
                "YFCC4K"
            ]
        },
        "publishedAt": "2025-05-19T17:04:46.000Z",
        "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization",
        "summary": "Worldwide image geolocalization-the task of predicting GPS coordinates from\nimages taken anywhere on Earth-poses a fundamental challenge due to the vast\ndiversity in visual content across regions. While recent approaches adopt a\ntwo-stage pipeline of retrieving candidates and selecting the best match, they\ntypically rely on simplistic similarity heuristics and point-wise supervision,\nfailing to model spatial relationships among candidates. In this paper, we\npropose GeoRanker, a distance-aware ranking framework that leverages large\nvision-language models to jointly encode query-candidate interactions and\npredict geographic proximity. In addition, we introduce a multi-order distance\nloss that ranks both absolute and relative distances, enabling the model to\nreason over structured spatial relationships. To support this, we curate\nGeoRanking, the first dataset explicitly designed for geographic ranking tasks\nwith multimodal candidate information. GeoRanker achieves state-of-the-art\nresults on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly\noutperforming current best methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13731.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a53edb1adc23da725f3afd",
            "avatarUrl": "/avatars/d3cd53862f0b11fe2d532653062ac7cf.svg",
            "fullname": "jiapengyue",
            "name": "Jia-py",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.13010",
            "authors": [
                {
                    "_id": "682c46c70f622b7afc2e6f8f",
                    "user": {
                        "_id": "67d5c51817d572b960f6982a",
                        "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
                        "isPro": false,
                        "fullname": "Himel Ghosh",
                        "user": "himel7",
                        "type": "user"
                    },
                    "name": "Himel Ghosh",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T09:42:53.022Z",
                    "hidden": false
                },
                {
                    "_id": "682c46c70f622b7afc2e6f90",
                    "user": {
                        "_id": "62d0859afb896639b29c45c8",
                        "avatarUrl": "/avatars/5309851f11c875f15f80a7415a4ae7f3.svg",
                        "isPro": false,
                        "fullname": "Ahmed Mosharafa",
                        "user": "amosharafa",
                        "type": "user"
                    },
                    "name": "Ahmed Mosharafa",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:24:54.616Z",
                    "hidden": false
                },
                {
                    "_id": "682c46c70f622b7afc2e6f91",
                    "user": {
                        "_id": "662a6560925d2af5b0e13a34",
                        "avatarUrl": "/avatars/d45a057e6bbde14f54811be34ae6bc64.svg",
                        "isPro": false,
                        "fullname": "Georg Groh",
                        "user": "grohg",
                        "type": "user"
                    },
                    "name": "Georg Groh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-21T10:25:00.667Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png"
            ],
            "publishedAt": "2025-05-19T11:54:39.000Z",
            "submittedOnDailyAt": "2025-05-21T08:14:27.462Z",
            "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
            "submittedOnDailyBy": {
                "_id": "67d5c51817d572b960f6982a",
                "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
                "isPro": false,
                "fullname": "Himel Ghosh",
                "user": "himel7",
                "type": "user"
            },
            "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.",
            "upvotes": 2,
            "discussionId": "682c46c80f622b7afc2e6fcf",
            "githubRepo": "https://github.com/Himel1996/NewsBiasDetector",
            "ai_summary": "A RoBERTa-based model fine-tuned on the BABE dataset shows improved performance in sentence-level media bias detection compared to a domain-adaptively pre-trained DA-RoBERTa baseline, with improved attention to contextually relevant tokens.",
            "ai_keywords": [
                "RoBERTa",
                "BABE dataset",
                "McNemar's test",
                "5x2 cross-validation paired t-test",
                "DA-RoBERTa",
                "attention-based analysis",
                "context-aware modeling",
                "bias neutralization",
                "bias type classification"
            ]
        },
        "publishedAt": "2025-05-19T07:54:39.000Z",
        "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
        "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67d5c51817d572b960f6982a/YKwZeahX2EReZyYUdKvtE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13010.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d5c51817d572b960f6982a",
            "avatarUrl": "/avatars/2c22663b27326b2300444bcc84ebbdd7.svg",
            "fullname": "Himel Ghosh",
            "name": "himel7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.12154",
            "authors": [
                {
                    "_id": "682dfacb59d73d96c5f2dfd4",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfd5",
                    "name": "Ruohan Gao",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfd6",
                    "name": "J. M. F. Tsang",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfd7",
                    "name": "Jan Kurcius",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfd8",
                    "name": "Cagdas Bilen",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfd9",
                    "name": "Chenliang Xu",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfda",
                    "name": "Anurag Kumar",
                    "hidden": false
                },
                {
                    "_id": "682dfacb59d73d96c5f2dfdb",
                    "name": "Sanjeel Parekh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-17T22:03:57.000Z",
            "submittedOnDailyAt": "2025-05-21T14:42:45.132Z",
            "title": "Learning to Highlight Audio by Watching Movies",
            "submittedOnDailyBy": {
                "_id": "67257ee0938e718957c9c100",
                "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
                "isPro": false,
                "fullname": "Chao Huang",
                "user": "ChaoHuangCS",
                "type": "user"
            },
            "summary": "Recent years have seen a significant increase in video content creation and\nconsumption. Crafting engaging content requires the careful curation of both\nvisual and audio elements. While visual cue curation, through techniques like\noptimal viewpoint selection or post-editing, has been central to media\nproduction, its natural counterpart, audio, has not undergone equivalent\nadvancements. This often results in a disconnect between visual and acoustic\nsaliency. To bridge this gap, we introduce a novel task: visually-guided\nacoustic highlighting, which aims to transform audio to deliver appropriate\nhighlighting effects guided by the accompanying video, ultimately creating a\nmore harmonious audio-visual experience. We propose a flexible,\ntransformer-based multimodal framework to solve this task. To train our model,\nwe also introduce a new dataset -- the muddy mix dataset, leveraging the\nmeticulous audio and video crafting found in movies, which provides a form of\nfree supervision. We develop a pseudo-data generation process to simulate\npoorly mixed audio, mimicking real-world scenarios through a three-step process\n-- separation, adjustment, and remixing. Our approach consistently outperforms\nseveral baselines in both quantitative and subjective evaluation. We also\nsystematically study the impact of different types of contextual guidance and\ndifficulty levels of the dataset. Our project page is here:\nhttps://wikichao.github.io/VisAH/.",
            "upvotes": 2,
            "discussionId": "682dfacd59d73d96c5f2e0bb",
            "ai_summary": "A transformer-based multimodal framework addresses the challenge of visually-guided acoustic highlighting, creating harmonious audio-visual experiences using a newly introduced dataset and pseudo-data generation process.",
            "ai_keywords": [
                "visually-guided acoustic highlighting",
                "transformer-based",
                "multimodal framework",
                "muddy mix dataset",
                "audio visualization",
                "contextual guidance"
            ]
        },
        "publishedAt": "2025-05-17T18:03:57.000Z",
        "title": "Learning to Highlight Audio by Watching Movies",
        "summary": "Recent years have seen a significant increase in video content creation and\nconsumption. Crafting engaging content requires the careful curation of both\nvisual and audio elements. While visual cue curation, through techniques like\noptimal viewpoint selection or post-editing, has been central to media\nproduction, its natural counterpart, audio, has not undergone equivalent\nadvancements. This often results in a disconnect between visual and acoustic\nsaliency. To bridge this gap, we introduce a novel task: visually-guided\nacoustic highlighting, which aims to transform audio to deliver appropriate\nhighlighting effects guided by the accompanying video, ultimately creating a\nmore harmonious audio-visual experience. We propose a flexible,\ntransformer-based multimodal framework to solve this task. To train our model,\nwe also introduce a new dataset -- the muddy mix dataset, leveraging the\nmeticulous audio and video crafting found in movies, which provides a form of\nfree supervision. We develop a pseudo-data generation process to simulate\npoorly mixed audio, mimicking real-world scenarios through a three-step process\n-- separation, adjustment, and remixing. Our approach consistently outperforms\nseveral baselines in both quantitative and subjective evaluation. We also\nsystematically study the impact of different types of contextual guidance and\ndifficulty levels of the dataset. Our project page is here:\nhttps://wikichao.github.io/VisAH/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12154.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67257ee0938e718957c9c100",
            "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
            "fullname": "Chao Huang",
            "name": "ChaoHuangCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.11754",
            "authors": [
                {
                    "_id": "682bdc203fff9e115285b2b6",
                    "user": {
                        "_id": "64c4d03ca684146b1cdb8237",
                        "avatarUrl": "/avatars/82b513c686ab20c859ca900fd59dfac7.svg",
                        "isPro": false,
                        "fullname": "Wenyu Huang",
                        "user": "hwy9855",
                        "type": "user"
                    },
                    "name": "Wenyu Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-20T07:21:08.965Z",
                    "hidden": false
                },
                {
                    "_id": "682bdc203fff9e115285b2b7",
                    "user": {
                        "_id": "641c83a492cd2530299714c7",
                        "avatarUrl": "/avatars/928195163f9040a2649953c86b9d6c79.svg",
                        "isPro": false,
                        "fullname": "Pavlos V",
                        "user": "pvougiou",
                        "type": "user"
                    },
                    "name": "Pavlos Vougiouklis",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T11:43:19.081Z",
                    "hidden": false
                },
                {
                    "_id": "682bdc203fff9e115285b2b8",
                    "name": "Mirella Lapata",
                    "hidden": false
                },
                {
                    "_id": "682bdc203fff9e115285b2b9",
                    "name": "Jeff Z. Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T23:29:47.000Z",
            "submittedOnDailyAt": "2025-05-21T10:11:19.053Z",
            "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with\n  Context Permutation",
            "submittedOnDailyBy": {
                "_id": "64c4d03ca684146b1cdb8237",
                "avatarUrl": "/avatars/82b513c686ab20c859ca900fd59dfac7.svg",
                "isPro": false,
                "fullname": "Wenyu Huang",
                "user": "hwy9855",
                "type": "user"
            },
            "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "upvotes": 2,
            "discussionId": "682bdc223fff9e115285b354",
            "githubRepo": "https://github.com/hwy9855/multiHopQA-Reasoning/",
            "ai_summary": "Encoder-decoder models outperform causal decoder-only models in multi-hop question answering by leveraging permutation of search results and bi-directional attention.",
            "ai_keywords": [
                "multi-hop question answering",
                "encoder-decoder models",
                "causal decoder-only models",
                "Flan-T5",
                "multi-hop reasoning",
                "causal mask",
                "bi-directional attention",
                "attention weights"
            ]
        },
        "publishedAt": "2025-05-16T19:29:47.000Z",
        "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with\n  Context Permutation",
        "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11754.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c4d03ca684146b1cdb8237",
            "avatarUrl": "/avatars/82b513c686ab20c859ca900fd59dfac7.svg",
            "fullname": "Wenyu Huang",
            "name": "hwy9855",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11730",
            "authors": [
                {
                    "_id": "682e032304c8bebe87d6e2bf",
                    "name": "Hao Mark Chen",
                    "hidden": false
                },
                {
                    "_id": "682e032304c8bebe87d6e2c0",
                    "name": "Guanxi Lu",
                    "hidden": false
                },
                {
                    "_id": "682e032304c8bebe87d6e2c1",
                    "name": "Yasuyuki Okoshi",
                    "hidden": false
                },
                {
                    "_id": "682e032304c8bebe87d6e2c2",
                    "name": "Zhiwen Mo",
                    "hidden": false
                },
                {
                    "_id": "682e032304c8bebe87d6e2c3",
                    "name": "Masato Motomura",
                    "hidden": false
                },
                {
                    "_id": "682e032304c8bebe87d6e2c4",
                    "name": "Hongxiang Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T22:24:48.000Z",
            "submittedOnDailyAt": "2025-05-21T15:16:27.231Z",
            "title": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "664f635ce58928eeecd3fcf9",
                "avatarUrl": "/avatars/8cfe0f72f0145a76ea4b8d09a5e94033.svg",
                "isPro": false,
                "fullname": "Hao Mark Chen",
                "user": "hmarkc",
                "type": "user"
            },
            "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.",
            "upvotes": 2,
            "discussionId": "682e032404c8bebe87d6e303",
            "ai_summary": "Variable Granularity Search improves test-time scaling of large language models by optimizing verification frequency, enhancing performance and compute efficiency.",
            "ai_keywords": [
                "test-time scaling (TTS)",
                "large language models (LLMs)",
                "verification",
                "reasoning performance",
                "compute efficiency",
                "generator-verifier configurations",
                "Variable Granularity Search (VG-Search)",
                "beam search",
                "Best-of-N sampling",
                "adaptive VG-Search strategies",
                "FLOPs"
            ]
        },
        "publishedAt": "2025-05-16T18:24:48.000Z",
        "title": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling",
        "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11730.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664f635ce58928eeecd3fcf9",
            "avatarUrl": "/avatars/8cfe0f72f0145a76ea4b8d09a5e94033.svg",
            "fullname": "Hao Mark Chen",
            "name": "hmarkc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14629",
            "authors": [
                {
                    "_id": "682dd6a9e3e00ded42a93674",
                    "user": {
                        "_id": "615711e6a1fdc424aed1eb48",
                        "avatarUrl": "/avatars/88279cb7ca39985f618a444128826893.svg",
                        "isPro": false,
                        "fullname": "Fnu Mohbat",
                        "user": "mohbattharani",
                        "type": "user"
                    },
                    "name": "Fnu Mohbat",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T13:35:37.613Z",
                    "hidden": false
                },
                {
                    "_id": "682dd6a9e3e00ded42a93675",
                    "name": "Mohammed J Zaki",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T17:19:57.000Z",
            "submittedOnDailyAt": "2025-05-21T12:06:10.015Z",
            "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "615711e6a1fdc424aed1eb48",
                "avatarUrl": "/avatars/88279cb7ca39985f618a444128826893.svg",
                "isPro": false,
                "fullname": "Fnu Mohbat",
                "user": "mohbattharani",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.",
            "upvotes": 1,
            "discussionId": "682dd6a9e3e00ded42a93695",
            "ai_summary": "KERL, integrating food knowledge graphs with large language models, offers personalized food recommendations and recipe generation with nutritional information.",
            "ai_keywords": [
                "large language models",
                "knowledge graphs",
                "KERL",
                "recipe generation",
                "nutritional analysis"
            ]
        },
        "publishedAt": "2025-05-20T13:19:57.000Z",
        "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
        "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14629.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "615711e6a1fdc424aed1eb48",
            "avatarUrl": "/avatars/88279cb7ca39985f618a444128826893.svg",
            "fullname": "Fnu Mohbat",
            "name": "mohbattharani",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14556",
            "authors": [
                {
                    "_id": "682ddb634e8859195a89ee05",
                    "name": "Marlne Careil",
                    "hidden": false
                },
                {
                    "_id": "682ddb634e8859195a89ee06",
                    "user": {
                        "_id": "6679bc0467be4d7b18a2896d",
                        "avatarUrl": "/avatars/9cba4040a9e90f2230d8b3c9412ecc26.svg",
                        "isPro": false,
                        "fullname": "Yohann Benchetrit",
                        "user": "ybenchetrit",
                        "type": "user"
                    },
                    "name": "Yohann Benchetrit",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T13:55:52.729Z",
                    "hidden": false
                },
                {
                    "_id": "682ddb634e8859195a89ee07",
                    "name": "Jean-Rmi King",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T16:14:37.000Z",
            "submittedOnDailyAt": "2025-05-21T12:28:19.958Z",
            "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI",
            "submittedOnDailyBy": {
                "_id": "630620e1df993a789e644129",
                "avatarUrl": "/avatars/315c41ecad079ae57350b7662382b2cb.svg",
                "isPro": false,
                "fullname": "Careil",
                "user": "Marl",
                "type": "user"
            },
            "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.",
            "upvotes": 1,
            "discussionId": "682ddb684e8859195a89eefd",
            "ai_summary": "Dynadiff, a single-stage diffusion model, enhances time-resolved brain-to-image decoding from fMRI recordings by simplifying training and improving high-level semantic image reconstruction.",
            "ai_keywords": [
                "diffusion model",
                "Dynadiff",
                "fMRI",
                "brain-to-image decoding",
                "time-resolved",
                "image reconstruction",
                "semantic image reconstruction"
            ]
        },
        "publishedAt": "2025-05-20T12:14:37.000Z",
        "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI",
        "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14556.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630620e1df993a789e644129",
            "avatarUrl": "/avatars/315c41ecad079ae57350b7662382b2cb.svg",
            "fullname": "Careil",
            "name": "Marl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.13778",
            "authors": [
                {
                    "_id": "682e0b38d8a198bf9e193d7a",
                    "name": "Guoheng Sun",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d7b",
                    "name": "Ziyao Wang",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d7c",
                    "name": "Bowei Tian",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d7d",
                    "name": "Meng Liu",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d7e",
                    "name": "Zheyu Shen",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d7f",
                    "name": "Shwai He",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d80",
                    "name": "Yexiao He",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d81",
                    "name": "Wanghao Ye",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d82",
                    "name": "Yiting Wang",
                    "hidden": false
                },
                {
                    "_id": "682e0b38d8a198bf9e193d83",
                    "user": {
                        "_id": "64a842b1221e6292c9fa7a45",
                        "avatarUrl": "/avatars/7f67f357c830ed6dade7ed7548ac1f05.svg",
                        "isPro": false,
                        "fullname": "Ang Li",
                        "user": "charleslipku",
                        "type": "user"
                    },
                    "name": "Ang Li",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T17:20:57.748Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T23:39:23.000Z",
            "submittedOnDailyAt": "2025-05-21T15:51:21.366Z",
            "title": "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM\n  APIs",
            "submittedOnDailyBy": {
                "_id": "64a842b1221e6292c9fa7a45",
                "avatarUrl": "/avatars/7f67f357c830ed6dade7ed7548ac1f05.svg",
                "isPro": false,
                "fullname": "Ang Li",
                "user": "charleslipku",
                "type": "user"
            },
            "summary": "As post-training techniques evolve, large language models (LLMs) are\nincreasingly augmented with structured multi-step reasoning abilities, often\noptimized through reinforcement learning. These reasoning-enhanced models\noutperform standard LLMs on complex tasks and now underpin many commercial LLM\nAPIs. However, to protect proprietary behavior and reduce verbosity, providers\ntypically conceal the reasoning traces while returning only the final answer.\nThis opacity introduces a critical transparency gap: users are billed for\ninvisible reasoning tokens, which often account for the majority of the cost,\nyet have no means to verify their authenticity. This opens the door to token\ncount inflation, where providers may overreport token usage or inject\nsynthetic, low-effort tokens to inflate charges. To address this issue, we\npropose CoIn, a verification framework that audits both the quantity and\nsemantic validity of hidden tokens. CoIn constructs a verifiable hash tree from\ntoken embedding fingerprints to check token counts, and uses embedding-based\nrelevance matching to detect fabricated reasoning content. Experiments\ndemonstrate that CoIn, when deployed as a trusted third-party auditor, can\neffectively detect token count inflation with a success rate reaching up to\n94.7%, showing the strong ability to restore billing transparency in opaque LLM\nservices. The dataset and code are available at\nhttps://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.",
            "upvotes": 1,
            "discussionId": "682e0b39d8a198bf9e193dbd",
            "ai_summary": "A verification framework named CoIn is proposed to audit and validate the quantity and authenticity of hidden tokens in large language models to ensure billing transparency.",
            "ai_keywords": [
                "large language models",
                "reinforcement learning",
                "structured multi-step reasoning",
                "token count inflation",
                "verifiable hash tree",
                "token embedding fingerprints",
                "embedding-based relevance matching"
            ]
        },
        "publishedAt": "2025-05-19T19:39:23.000Z",
        "title": "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM\n  APIs",
        "summary": "As post-training techniques evolve, large language models (LLMs) are\nincreasingly augmented with structured multi-step reasoning abilities, often\noptimized through reinforcement learning. These reasoning-enhanced models\noutperform standard LLMs on complex tasks and now underpin many commercial LLM\nAPIs. However, to protect proprietary behavior and reduce verbosity, providers\ntypically conceal the reasoning traces while returning only the final answer.\nThis opacity introduces a critical transparency gap: users are billed for\ninvisible reasoning tokens, which often account for the majority of the cost,\nyet have no means to verify their authenticity. This opens the door to token\ncount inflation, where providers may overreport token usage or inject\nsynthetic, low-effort tokens to inflate charges. To address this issue, we\npropose CoIn, a verification framework that audits both the quantity and\nsemantic validity of hidden tokens. CoIn constructs a verifiable hash tree from\ntoken embedding fingerprints to check token counts, and uses embedding-based\nrelevance matching to detect fabricated reasoning content. Experiments\ndemonstrate that CoIn, when deployed as a trusted third-party auditor, can\neffectively detect token count inflation with a success rate reaching up to\n94.7%, showing the strong ability to restore billing transparency in opaque LLM\nservices. The dataset and code are available at\nhttps://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13778.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a842b1221e6292c9fa7a45",
            "avatarUrl": "/avatars/7f67f357c830ed6dade7ed7548ac1f05.svg",
            "fullname": "Ang Li",
            "name": "charleslipku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14467",
            "authors": [
                {
                    "_id": "682e32a6a8253020d05e242d",
                    "user": {
                        "_id": "6109845486580d4580767223",
                        "avatarUrl": "/avatars/9192fda36782a3f208daaff701884544.svg",
                        "isPro": false,
                        "fullname": "Mani shemranifar",
                        "user": "Manishemi",
                        "type": "user"
                    },
                    "name": "Mani Shemiranifar",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T20:12:25.545Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T15:01:56.000Z",
            "submittedOnDailyAt": "2025-05-21T18:40:46.698Z",
            "title": "Void in Language Models",
            "submittedOnDailyBy": {
                "_id": "6109845486580d4580767223",
                "avatarUrl": "/avatars/9192fda36782a3f208daaff701884544.svg",
                "isPro": false,
                "fullname": "Mani shemranifar",
                "user": "Manishemi",
                "type": "user"
            },
            "summary": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.",
            "upvotes": 0,
            "discussionId": "682e32a6a8253020d05e2461",
            "githubRepo": "https://github.com/manishemirani/void_in_language_models",
            "ai_summary": "Analyzing layer activation using L2 Adaptive Computation reveals that not all layers in transformer-based language models are used during inference, and selectively skipping unactivated layers can improve performance.",
            "ai_keywords": [
                "transformer-based language models",
                "L2 Adaptive Computation",
                "Voids",
                "Prompt Processing",
                "Response Generation",
                "MMLU",
                "GPQA Diamond",
                "BoolQ",
                "zero-shot setting"
            ]
        },
        "publishedAt": "2025-05-20T11:01:56.000Z",
        "title": "Void in Language Models",
        "summary": "Despite advances in transformer-based language models (LMs), a fundamental\nquestion remains largely unanswered: Are all layers activated during inference?\nWe investigate this question by detecting unactivated layers (which we refer to\nas Voids) using a non-trainable and parameter-free adaptive computation method\ncalled L2 Adaptive Computation (LAC). We adapt LAC from its original\nefficiency-focused application to trace activated layers during inference. This\nmethod monitors changes in the L2-norm of activations to identify voids. We\nanalyze layer activation in instruction-tuned LMs across two phases: Prompt\nProcessing (PP), where we trace activated layers for each token in the input\nprompts, and Response Generation (RG), where we trace activated layers for each\ngenerated token. We further demonstrate that distinct layers are activated\nduring these two phases. To show the effectiveness of our method, we evaluated\nthree distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families\non three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a\nzero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an\nimprovement from 69.24 to 71.29 while the model uses only 30% of the layers.\nSimilarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to\n18.36 when using 70% of the layers during both the PP and RG phases. These\nresults show that not all layers contribute equally during inference, and that\nselectively skipping most of them can improve the performance of models on\ncertain tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14467.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6109845486580d4580767223",
            "avatarUrl": "/avatars/9192fda36782a3f208daaff701884544.svg",
            "fullname": "Mani shemranifar",
            "name": "Manishemi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14366",
            "authors": [
                {
                    "_id": "682dd1aaf3e4b74b546b6db4",
                    "user": {
                        "_id": "6793cc6e09bb7d2e0441a335",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6793cc6e09bb7d2e0441a335/QtAEDNeEJFvJuqbvvkvj3.jpeg",
                        "isPro": false,
                        "fullname": "Joel Currie",
                        "user": "jwgcurrie",
                        "type": "user"
                    },
                    "name": "Joel Currie",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-21T13:15:52.516Z",
                    "hidden": false
                },
                {
                    "_id": "682dd1aaf3e4b74b546b6db5",
                    "name": "Gioele Migno",
                    "hidden": false
                },
                {
                    "_id": "682dd1aaf3e4b74b546b6db6",
                    "user": {
                        "_id": "67d9304651046c588116e552",
                        "avatarUrl": "/avatars/796426f7a7e024fba594b0c56ece8a57.svg",
                        "isPro": false,
                        "fullname": "Enrico Piacenti",
                        "user": "ep10102",
                        "type": "user"
                    },
                    "name": "Enrico Piacenti",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T13:14:19.458Z",
                    "hidden": false
                },
                {
                    "_id": "682dd1aaf3e4b74b546b6db7",
                    "name": "Maria Elena Giannaccini",
                    "hidden": false
                },
                {
                    "_id": "682dd1aaf3e4b74b546b6db8",
                    "name": "Patric Bach",
                    "hidden": false
                },
                {
                    "_id": "682dd1aaf3e4b74b546b6db9",
                    "name": "Davide De Tommaso",
                    "hidden": false
                },
                {
                    "_id": "682dd1aaf3e4b74b546b6dba",
                    "name": "Agnieszka Wykowska",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T13:49:09.000Z",
            "submittedOnDailyAt": "2025-05-21T13:52:01.114Z",
            "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds",
            "submittedOnDailyBy": {
                "_id": "6793cc6e09bb7d2e0441a335",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6793cc6e09bb7d2e0441a335/QtAEDNeEJFvJuqbvvkvj3.jpeg",
                "isPro": false,
                "fullname": "Joel Currie",
                "user": "jwgcurrie",
                "type": "user"
            },
            "summary": "We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.",
            "upvotes": 0,
            "discussionId": "682dd1abf3e4b74b546b6ddb",
            "ai_summary": "A synthetic dataset in NVIDIA Omniverse aids in training Vision-Language Models for Visual Perspective Taking by providing supervised learning for spatial reasoning tasks.",
            "ai_keywords": [
                "Vision-Language Models",
                "Visual Perspective Taking",
                "embodied cognition",
                "Human-Robot Interaction",
                "NVIDIA Omniverse",
                "spatial reasoning",
                "RGB image",
                "natural language description",
                "4X4 transformation matrix",
                "object pose",
                "Z-axis distance",
                "Degrees Of Freedom"
            ]
        },
        "publishedAt": "2025-05-20T09:49:09.000Z",
        "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds",
        "summary": "We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14366.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6793cc6e09bb7d2e0441a335",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6793cc6e09bb7d2e0441a335/QtAEDNeEJFvJuqbvvkvj3.jpeg",
            "fullname": "Joel Currie",
            "name": "jwgcurrie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11563",
            "authors": [
                {
                    "_id": "682db967a9a344c017078ee3",
                    "user": {
                        "_id": "63ba99e3d90985e7acd820d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg",
                        "isPro": false,
                        "fullname": "Alexandre Chapin",
                        "user": "Beegbrain",
                        "type": "user"
                    },
                    "name": "Alexandre Chapin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T12:26:48.791Z",
                    "hidden": false
                },
                {
                    "_id": "682db967a9a344c017078ee4",
                    "name": "Bruno Machado",
                    "hidden": false
                },
                {
                    "_id": "682db967a9a344c017078ee5",
                    "name": "Emmanuel Dellandrea",
                    "hidden": false
                },
                {
                    "_id": "682db967a9a344c017078ee6",
                    "name": "Liming Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T07:06:37.000Z",
            "submittedOnDailyAt": "2025-05-21T11:23:54.496Z",
            "title": "Object-Centric Representations Improve Policy Generalization in Robot\n  Manipulation",
            "submittedOnDailyBy": {
                "_id": "63ba99e3d90985e7acd820d8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg",
                "isPro": false,
                "fullname": "Alexandre Chapin",
                "user": "Beegbrain",
                "type": "user"
            },
            "summary": "Visual representations are central to the learning and generalization\ncapabilities of robotic manipulation policies. While existing methods rely on\nglobal or dense features, such representations often entangle task-relevant and\nirrelevant scene information, limiting robustness under distribution shifts. In\nthis work, we investigate object-centric representations (OCR) as a structured\nalternative that segments visual input into a finished set of entities,\nintroducing inductive biases that align more naturally with manipulation tasks.\nWe benchmark a range of visual encoders-object-centric, global and dense\nmethods-across a suite of simulated and real-world manipulation tasks ranging\nfrom simple to complex, and evaluate their generalization under diverse visual\nconditions including changes in lighting, texture, and the presence of\ndistractors. Our findings reveal that OCR-based policies outperform dense and\nglobal representations in generalization settings, even without task-specific\npretraining. These insights suggest that OCR is a promising direction for\ndesigning visual systems that generalize effectively in dynamic, real-world\nrobotic environments.",
            "upvotes": 0,
            "discussionId": "682db968a9a344c017078f41",
            "ai_summary": "Object-centric representations (OCR) demonstrate superior generalization in robotic manipulation tasks compared to global or dense visual features under various visual conditions.",
            "ai_keywords": [
                "object-centric representations",
                "OCR",
                "visual encoders",
                "global features",
                "dense features",
                "manipulation tasks",
                "generalization",
                "distribution shifts",
                "visual conditions",
                "lighting changes",
                "texture changes",
                "distractors"
            ]
        },
        "publishedAt": "2025-05-16T03:06:37.000Z",
        "title": "Object-Centric Representations Improve Policy Generalization in Robot\n  Manipulation",
        "summary": "Visual representations are central to the learning and generalization\ncapabilities of robotic manipulation policies. While existing methods rely on\nglobal or dense features, such representations often entangle task-relevant and\nirrelevant scene information, limiting robustness under distribution shifts. In\nthis work, we investigate object-centric representations (OCR) as a structured\nalternative that segments visual input into a finished set of entities,\nintroducing inductive biases that align more naturally with manipulation tasks.\nWe benchmark a range of visual encoders-object-centric, global and dense\nmethods-across a suite of simulated and real-world manipulation tasks ranging\nfrom simple to complex, and evaluate their generalization under diverse visual\nconditions including changes in lighting, texture, and the presence of\ndistractors. Our findings reveal that OCR-based policies outperform dense and\nglobal representations in generalization settings, even without task-specific\npretraining. These insights suggest that OCR is a promising direction for\ndesigning visual systems that generalize effectively in dynamic, real-world\nrobotic environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11563.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ba99e3d90985e7acd820d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ba99e3d90985e7acd820d8/9Ywt1MY9UBdmlnVuuSIm-.jpeg",
            "fullname": "Alexandre Chapin",
            "name": "Beegbrain",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.06914",
            "authors": [
                {
                    "_id": "682acf157ee0ab9140c94cb8",
                    "name": "Chen Amiraz",
                    "hidden": false
                },
                {
                    "_id": "682acf157ee0ab9140c94cb9",
                    "user": {
                        "_id": "62bfff6788fdef8ecde8c45b",
                        "avatarUrl": "/avatars/637da88b8dce01bdc2a4c1319cc882e3.svg",
                        "isPro": false,
                        "fullname": "Florin Cuconasu",
                        "user": "florin-hf",
                        "type": "user"
                    },
                    "name": "Florin Cuconasu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-21T15:19:03.169Z",
                    "hidden": false
                },
                {
                    "_id": "682acf157ee0ab9140c94cba",
                    "user": {
                        "_id": "652028f9974423bd3ea33ae2",
                        "avatarUrl": "/avatars/50b749f9a95a3b0fb7a2bfcf17c34295.svg",
                        "isPro": false,
                        "fullname": "Simone Filice",
                        "user": "filice",
                        "type": "user"
                    },
                    "name": "Simone Filice",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-19T07:32:13.228Z",
                    "hidden": false
                },
                {
                    "_id": "682acf157ee0ab9140c94cbb",
                    "name": "Zohar Karnin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62bfff6788fdef8ecde8c45b/MysTPK9FNVtXvzK797_d4.png"
            ],
            "publishedAt": "2025-05-11T09:25:05.000Z",
            "submittedOnDailyAt": "2025-05-21T13:48:10.749Z",
            "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG",
            "submittedOnDailyBy": {
                "_id": "62bfff6788fdef8ecde8c45b",
                "avatarUrl": "/avatars/637da88b8dce01bdc2a4c1319cc882e3.svg",
                "isPro": false,
                "fullname": "Florin Cuconasu",
                "user": "florin-hf",
                "type": "user"
            },
            "summary": "A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages.",
            "upvotes": 0,
            "discussionId": "682acf167ee0ab9140c94ce8",
            "ai_summary": "Methods for identifying and utilizing hard distracting passages in Retrieval Augmented Generation (RAG) systems improve answer-generating LLMs by up to 7.5% in accuracy.",
            "ai_keywords": [
                "Retrieval Augmented Generation",
                "RAG",
                "answer-generating LLM",
                "distracting effect",
                "fine-tuning",
                "conventional RAG datasets",
                "binary classification",
                "hard distracting passages"
            ]
        },
        "publishedAt": "2025-05-11T05:25:05.000Z",
        "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG",
        "summary": "A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62bfff6788fdef8ecde8c45b/MysTPK9FNVtXvzK797_d4.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.06914.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bfff6788fdef8ecde8c45b",
            "avatarUrl": "/avatars/637da88b8dce01bdc2a4c1319cc882e3.svg",
            "fullname": "Florin Cuconasu",
            "name": "florin-hf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    }
]