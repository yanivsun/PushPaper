[
    {
        "paper": {
            "id": "2505.18125",
            "authors": [
                {
                    "_id": "6833f8b419852283c4b3bbd6",
                    "user": {
                        "_id": "60d84af7eac5e05d4594f010",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d84af7eac5e05d4594f010/KnGxUR7OUOAGg0S67tRaY.png",
                        "isPro": false,
                        "fullname": "Alan Arazi",
                        "user": "alana89",
                        "type": "user"
                    },
                    "name": "Alan Arazi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T14:07:38.679Z",
                    "hidden": false
                },
                {
                    "_id": "6833f8b419852283c4b3bbd7",
                    "user": {
                        "_id": "64802fb6c57f629056c59966",
                        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
                        "isPro": false,
                        "fullname": "Eilam Shapira",
                        "user": "EilamSha",
                        "type": "user"
                    },
                    "name": "Eilam Shapira",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:08.206Z",
                    "hidden": false
                },
                {
                    "_id": "6833f8b419852283c4b3bbd8",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T17:34:28.000Z",
            "submittedOnDailyAt": "2025-05-26T03:50:53.260Z",
            "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
            "submittedOnDailyBy": {
                "_id": "64802fb6c57f629056c59966",
                "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
                "isPro": false,
                "fullname": "Eilam Shapira",
                "user": "EilamSha",
                "type": "user"
            },
            "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
            "upvotes": 95,
            "discussionId": "6833f8b419852283c4b3bc02",
            "projectPage": "https://eilamshapira.com/TabSTAR",
            "ai_summary": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.",
            "ai_keywords": [
                "TabSTAR",
                "foundation tabular model",
                "semantically target-aware representations",
                "transfer learning",
                "pretrained text encoder",
                "target tokens",
                "task-specific embeddings",
                "scaling laws"
            ]
        },
        "publishedAt": "2025-05-23T13:34:28.000Z",
        "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
        "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64802fb6c57f629056c59966",
            "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
            "fullname": "Eilam Shapira",
            "name": "EilamSha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17667",
            "authors": [
                {
                    "_id": "6833d7c5a3262d6b1e4d358e",
                    "user": {
                        "_id": "62ecbffd99112e99c5f7fded",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
                        "isPro": false,
                        "fullname": "Fanqi Wan",
                        "user": "Wanfq",
                        "type": "user"
                    },
                    "name": "Fanqi Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:44.880Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d358f",
                    "user": {
                        "_id": "64777a346e6c7ac608c1e9bf",
                        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
                        "isPro": false,
                        "fullname": "Weizhou Shen",
                        "user": "shenwzh3",
                        "type": "user"
                    },
                    "name": "Weizhou Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:12:42.577Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3590",
                    "user": {
                        "_id": "64a69c1a9704e9888c1da827",
                        "avatarUrl": "/avatars/096aefab0ca99af76097cdfc5e752402.svg",
                        "isPro": false,
                        "fullname": "LiaoShengyi",
                        "user": "LsyLsyLsyyy",
                        "type": "user"
                    },
                    "name": "Shengyi Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:12:56.397Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3591",
                    "name": "Yingcheng Shi",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3592",
                    "user": {
                        "_id": "645df9a28c179ee09e156876",
                        "avatarUrl": "/avatars/2171aac5196b4a68776174ca921912c9.svg",
                        "isPro": false,
                        "fullname": "li",
                        "user": "chenliangli",
                        "type": "user"
                    },
                    "name": "Chenliang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:13:23.931Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3593",
                    "user": {
                        "_id": "64c9b0f28d2d187c24d1e6c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png",
                        "isPro": false,
                        "fullname": "ZiYi Yang",
                        "user": "AALF",
                        "type": "user"
                    },
                    "name": "Ziyi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:13:43.167Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3594",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3595",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3596",
                    "user": {
                        "_id": "602f88f5e8149a962412a667",
                        "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Jingren",
                        "type": "user"
                    },
                    "name": "Jingren Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:14:03.511Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7c5a3262d6b1e4d3597",
                    "name": "Ming Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T09:31:55.000Z",
            "submittedOnDailyAt": "2025-05-26T03:36:36.885Z",
            "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "62ecbffd99112e99c5f7fded",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
                "isPro": false,
                "fullname": "Fanqi Wan",
                "user": "Wanfq",
                "type": "user"
            },
            "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
            "upvotes": 60,
            "discussionId": "6833d7c6a3262d6b1e4d35c5",
            "githubRepo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
            "ai_summary": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "long-context reasoning",
                "short-context reasoning",
                "training efficiency",
                "optimization process",
                "QwenLong-L1",
                "progressive context scaling",
                "supervised fine-tuning",
                "curriculum-guided phased RL",
                "difficulty-aware retrospective sampling",
                "document question-answering benchmarks",
                "OpenAI-o3-mini",
                "Qwen3-235B-A22B",
                "Claude-3.7-Sonnet-Thinking"
            ]
        },
        "publishedAt": "2025-05-23T05:31:55.000Z",
        "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
        "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "fullname": "Fanqi Wan",
            "name": "Wanfq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 29
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14669",
            "authors": [
                {
                    "_id": "682da9d3781210358218a950",
                    "name": "Roberto L. Castro",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a951",
                    "user": {
                        "_id": "623753b5eddd7763adc9346a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
                        "isPro": false,
                        "fullname": "Andrei Panferov",
                        "user": "BlackSamorez",
                        "type": "user"
                    },
                    "name": "Andrei Panferov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T14:07:46.802Z",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a952",
                    "user": {
                        "_id": "632a2e325f2ff1958c0103be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg",
                        "isPro": false,
                        "fullname": "Soroush Tabesh",
                        "user": "soroushtabesh",
                        "type": "user"
                    },
                    "name": "Soroush Tabesh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:25:12.072Z",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a953",
                    "user": {
                        "_id": "659ddfb45673a33b5db22d57",
                        "avatarUrl": "/avatars/ae1dce603b4cae2659d6070e8ce98b15.svg",
                        "isPro": false,
                        "fullname": "Oliver Sieberling",
                        "user": "OliverSieberling",
                        "type": "user"
                    },
                    "name": "Oliver Sieberling",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:25:18.736Z",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a954",
                    "name": "Jiale Chen",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a955",
                    "user": {
                        "_id": "6526b8ebba9a8279c139616b",
                        "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
                        "isPro": false,
                        "fullname": "Mahdi Nikdan",
                        "user": "mnikdan97",
                        "type": "user"
                    },
                    "name": "Mahdi Nikdan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:25:45.476Z",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a956",
                    "name": "Saleh Ashkboos",
                    "hidden": false
                },
                {
                    "_id": "682da9d3781210358218a957",
                    "user": {
                        "_id": "64d100c5d8d0927372e3d4c0",
                        "avatarUrl": "/avatars/91d9e4f1dab25b70d901783cdfcd2fd1.svg",
                        "isPro": false,
                        "fullname": "Dan Alistarh",
                        "user": "dalistarh",
                        "type": "user"
                    },
                    "name": "Dan Alistarh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:25:55.295Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
            ],
            "publishedAt": "2025-05-20T17:55:50.000Z",
            "submittedOnDailyAt": "2025-05-26T08:34:57.302Z",
            "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "623753b5eddd7763adc9346a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
                "isPro": false,
                "fullname": "Andrei Panferov",
                "user": "BlackSamorez",
                "type": "user"
            },
            "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
            "upvotes": 55,
            "discussionId": "682da9d4781210358218a982",
            "githubRepo": "https://github.com/IST-DASLab/Quartet",
            "ai_summary": "Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.",
            "ai_keywords": [
                "large language models",
                "low-precision arithmetic",
                "Blackwell architecture",
                "FP4",
                "mixed-precision",
                "linear layers",
                "low-precision scaling law",
                "CUDA kernels"
            ]
        },
        "publishedAt": "2025-05-20T13:55:50.000Z",
        "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
        "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14669.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "623753b5eddd7763adc9346a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
            "fullname": "Andrei Panferov",
            "name": "BlackSamorez",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 38
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18129",
            "authors": [
                {
                    "_id": "6833cf89df7cbb5c087a4caa",
                    "name": "Yan Ma",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cab",
                    "name": "Linge Du",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cac",
                    "user": {
                        "_id": "642e4d4d6748dd4f8eeb7732",
                        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
                        "isPro": false,
                        "fullname": "Xuyang Shen",
                        "user": "Ryan1122",
                        "type": "user"
                    },
                    "name": "Xuyang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:03.920Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cad",
                    "user": {
                        "_id": "6829f5feb461b85f5dd0f036",
                        "avatarUrl": "/avatars/a1308c337ee366ad06819cbd81dbdcb3.svg",
                        "isPro": false,
                        "fullname": "shaoxiang",
                        "user": "chenshaoxiang1",
                        "type": "user"
                    },
                    "name": "Shaoxiang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:27:11.208Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cae",
                    "name": "Pengfei Li",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4caf",
                    "user": {
                        "_id": "642cede11f576acdab6520e8",
                        "avatarUrl": "/avatars/95c57579beea2bfe703380a370dc2322.svg",
                        "isPro": false,
                        "fullname": "renqibing",
                        "user": "renqibing",
                        "type": "user"
                    },
                    "name": "Qibing Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:26:48.219Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cb0",
                    "name": "Lizhuang Ma",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cb1",
                    "user": {
                        "_id": "647df7670ed7d0c8760b00ed",
                        "avatarUrl": "/avatars/180dfa736af2aac64cb459ee4563dc61.svg",
                        "isPro": false,
                        "fullname": "Yuchao Dai",
                        "user": "daiyuchao",
                        "type": "user"
                    },
                    "name": "Yuchao Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:26:30.646Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cb2",
                    "user": {
                        "_id": "6144a0c4ff1146bbd84d9865",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png",
                        "isPro": false,
                        "fullname": "Pengfei Liu",
                        "user": "Pengfei",
                        "type": "user"
                    },
                    "name": "Pengfei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:26:36.896Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf89df7cbb5c087a4cb3",
                    "name": "Junjie Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T17:41:14.000Z",
            "submittedOnDailyAt": "2025-05-26T00:54:44.420Z",
            "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "642e4d4d6748dd4f8eeb7732",
                "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
                "isPro": false,
                "fullname": "Xuyang Shen",
                "user": "Ryan1122",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
            "upvotes": 50,
            "discussionId": "6833cf8adf7cbb5c087a4d0c",
            "githubRepo": "https://github.com/MiniMax-AI/One-RL-to-See-Them-All",
            "ai_summary": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.",
            "ai_keywords": [
                "visual triple unified reinforcement learning",
                "sample-level data formatting",
                "verifier-level reward computation",
                "source-level metric monitoring",
                "dynamic IoU reward",
                "reinforcement learning",
                "vision-language models",
                "object detection",
                "grounding",
                "Orsta",
                "MEGA-Bench Core"
            ]
        },
        "publishedAt": "2025-05-23T13:41:14.000Z",
        "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
        "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "fullname": "Xuyang Shen",
            "name": "Ryan1122",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17225",
            "authors": [
                {
                    "_id": "6833c65d49b9e903d3ddbd11",
                    "user": {
                        "_id": "62845957b410bd779033759c",
                        "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
                        "isPro": false,
                        "fullname": "Doohyuk Jang",
                        "user": "jadohu",
                        "type": "user"
                    },
                    "name": "Doohyuk Jang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:25.026Z",
                    "hidden": false
                },
                {
                    "_id": "6833c65d49b9e903d3ddbd12",
                    "user": {
                        "_id": "61b15ce1a5dd7dc7024406dc",
                        "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
                        "isPro": false,
                        "fullname": "Yoonjeon Kim",
                        "user": "yjyjyj98",
                        "type": "user"
                    },
                    "name": "Yoonjeon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:27.112Z",
                    "hidden": false
                },
                {
                    "_id": "6833c65d49b9e903d3ddbd13",
                    "name": "Chanjae Park",
                    "hidden": false
                },
                {
                    "_id": "6833c65d49b9e903d3ddbd14",
                    "user": {
                        "_id": "666d506fc0f3d5afc24dd5ca",
                        "avatarUrl": "/avatars/eeb98947415d08a26815fd139c76a071.svg",
                        "isPro": false,
                        "fullname": "Hyun Ryu",
                        "user": "hyun1905",
                        "type": "user"
                    },
                    "name": "Hyun Ryu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:24:47.602Z",
                    "hidden": false
                },
                {
                    "_id": "6833c65d49b9e903d3ddbd15",
                    "name": "Eunho Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T19:00:01.000Z",
            "submittedOnDailyAt": "2025-05-26T00:11:09.797Z",
            "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "61b15ce1a5dd7dc7024406dc",
                "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
                "isPro": false,
                "fullname": "Yoonjeon Kim",
                "user": "yjyjyj98",
                "type": "user"
            },
            "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
            "upvotes": 50,
            "discussionId": "6833c65e49b9e903d3ddbd6a",
            "projectPage": "https://reasoningtrap.github.io/",
            "githubRepo": "https://github.com/ReasoningTrap/ReasoningTrap",
            "ai_summary": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.",
            "ai_keywords": [
                "reasoning rigidity",
                "large language models",
                "long and complex reasoning tasks",
                "reasoning trajectories",
                "diagnostic set",
                "AIME",
                "MATH500",
                "Interpretation Overload",
                "Input Distrust",
                "Partial Instruction Attention"
            ]
        },
        "publishedAt": "2025-05-22T15:00:01.000Z",
        "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
        "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61b15ce1a5dd7dc7024406dc",
            "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
            "fullname": "Yoonjeon Kim",
            "name": "yjyjyj98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17612",
            "authors": [
                {
                    "_id": "6833c9fd298a7bec9c3da3b0",
                    "user": {
                        "_id": "64b74920fe6a108d03fed767",
                        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
                        "isPro": false,
                        "fullname": "Minki Kang",
                        "user": "Nardien",
                        "type": "user"
                    },
                    "name": "Minki Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T14:42:07.255Z",
                    "hidden": false
                },
                {
                    "_id": "6833c9fd298a7bec9c3da3b1",
                    "name": "Jongwon Jeong",
                    "hidden": false
                },
                {
                    "_id": "6833c9fd298a7bec9c3da3b2",
                    "user": {
                        "_id": "64ad5f59b7e4b2c1ce47eb43",
                        "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
                        "isPro": false,
                        "fullname": "Seanie Lee",
                        "user": "Seanie-lee",
                        "type": "user"
                    },
                    "name": "Seanie Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T14:07:42.794Z",
                    "hidden": false
                },
                {
                    "_id": "6833c9fd298a7bec9c3da3b3",
                    "name": "Jaewoong Cho",
                    "hidden": false
                },
                {
                    "_id": "6833c9fd298a7bec9c3da3b4",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T08:20:15.000Z",
            "submittedOnDailyAt": "2025-05-26T00:25:44.604Z",
            "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
            "submittedOnDailyBy": {
                "_id": "64b74920fe6a108d03fed767",
                "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
                "isPro": false,
                "fullname": "Minki Kang",
                "user": "Nardien",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
            "upvotes": 48,
            "discussionId": "6833ca00298a7bec9c3da444",
            "githubRepo": "https://github.com/Nardien/agent-distillation",
            "ai_summary": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.",
            "ai_keywords": [
                "Large language models",
                "small language models",
                "chain-of-thought",
                "agent distillation",
                "prompting method",
                "first-thought prefix",
                "self-consistent action generation",
                "task-solving behavior",
                "retrieval tools",
                "code tools",
                "in-domain generalization",
                "out-of-domain generalization"
            ]
        },
        "publishedAt": "2025-05-23T04:20:15.000Z",
        "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
        "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64b74920fe6a108d03fed767",
            "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
            "fullname": "Minki Kang",
            "name": "Nardien",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18092",
            "authors": [
                {
                    "_id": "6833ea049f968fc5c6b64486",
                    "name": "Weizhou Shen",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64487",
                    "name": "Chenliang Li",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64488",
                    "user": {
                        "_id": "62ecbffd99112e99c5f7fded",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
                        "isPro": false,
                        "fullname": "Fanqi Wan",
                        "user": "Wanfq",
                        "type": "user"
                    },
                    "name": "Fanqi Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:25.991Z",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64489",
                    "name": "Shengyi Liao",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b6448a",
                    "name": "Shaopeng Lai",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b6448b",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b6448c",
                    "name": "Yingcheng Shi",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b6448d",
                    "name": "Yuning Wu",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b6448e",
                    "name": "Gang Fu",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b6448f",
                    "name": "Zhansheng Li",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64490",
                    "name": "Bin Yang",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64491",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64492",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64493",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "6833ea049f968fc5c6b64494",
                    "name": "Ming Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T16:47:00.000Z",
            "submittedOnDailyAt": "2025-05-26T04:43:04.143Z",
            "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
            "submittedOnDailyBy": {
                "_id": "64777a346e6c7ac608c1e9bf",
                "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
                "isPro": false,
                "fullname": "Weizhou Shen",
                "user": "shenwzh3",
                "type": "user"
            },
            "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
            "upvotes": 38,
            "discussionId": "6833ea059f968fc5c6b644c1",
            "ai_summary": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.",
            "ai_keywords": [
                "context compression",
                "dynamic context optimization",
                "bidirectional reasoning layers",
                "token critic mechanisms",
                "window-parallel inference",
                "Qwen",
                "RAG",
                "sparse attention",
                "large language models",
                "SOTA performance"
            ]
        },
        "publishedAt": "2025-05-23T12:47:00.000Z",
        "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
        "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64777a346e6c7ac608c1e9bf",
            "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
            "fullname": "Weizhou Shen",
            "name": "shenwzh3",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.15929",
            "authors": [
                {
                    "_id": "6830404effb59afb6569273a",
                    "name": "Hui Shen",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569273b",
                    "user": {
                        "_id": "6621cea88850e38ffbb1854f",
                        "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
                        "isPro": false,
                        "fullname": "Taki WU",
                        "user": "taki555",
                        "type": "user"
                    },
                    "name": "Taiqiang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:14:30.851Z",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569273c",
                    "name": "Qi Han",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569273d",
                    "name": "Yunta Hsieh",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569273e",
                    "user": {
                        "_id": "67fe265f9698ae4f5f4db718",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0PLJEbUyJbM9BacFxcScP.png",
                        "isPro": false,
                        "fullname": "Jizhou Wang",
                        "user": "John-ai-bee",
                        "type": "user"
                    },
                    "name": "Jizhou Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:14:33.171Z",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569273f",
                    "name": "Yuyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692740",
                    "name": "Yuxin Cheng",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692741",
                    "name": "Zijian Hao",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692742",
                    "name": "Yuansheng Ni",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692743",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692744",
                    "name": "Zhongwei Wan",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692745",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692746",
                    "name": "Wendong Xu",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692747",
                    "name": "Jing Xiong",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692748",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb65692749",
                    "name": "Wenhu Chen",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569274a",
                    "name": "Chaofan Tao",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569274b",
                    "name": "Zhuoqing Mao",
                    "hidden": false
                },
                {
                    "_id": "6830404effb59afb6569274c",
                    "name": "Ngai Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T18:33:50.000Z",
            "submittedOnDailyAt": "2025-05-26T05:21:02.238Z",
            "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
            "submittedOnDailyBy": {
                "_id": "6621cea88850e38ffbb1854f",
                "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
                "isPro": false,
                "fullname": "Taki WU",
                "user": "taki555",
                "type": "user"
            },
            "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
            "upvotes": 38,
            "discussionId": "68304052ffb59afb6569282f",
            "projectPage": "https://phyx-bench.github.io/",
            "githubRepo": "https://github.com/NastyMarcus/PhyX",
            "ai_summary": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.",
            "ai_keywords": [
                "multimodal questions",
                "reasoning types",
                "sub-domains",
                "core physics domains",
                "thermodynamics",
                "electromagnetism",
                "mechanics",
                "modern physics",
                "optics",
                "wave\\&acoustics",
                "fine-grained statistics",
                "case studies",
                "evaluation paradigms",
                "VLMEvalKit"
            ]
        },
        "publishedAt": "2025-05-21T14:33:50.000Z",
        "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
        "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
            "fullname": "Taki WU",
            "name": "taki555",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17618",
            "authors": [
                {
                    "_id": "6833eeaf98515618764fc204",
                    "user": {
                        "_id": "6672937ceac0fb1b9e516595",
                        "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
                        "isPro": false,
                        "fullname": "haoran he",
                        "user": "haoranhe",
                        "type": "user"
                    },
                    "name": "Haoran He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:19.593Z",
                    "hidden": false
                },
                {
                    "_id": "6833eeaf98515618764fc205",
                    "name": "Jiajun Liang",
                    "hidden": false
                },
                {
                    "_id": "6833eeaf98515618764fc206",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "6833eeaf98515618764fc207",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "6833eeaf98515618764fc208",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833eeaf98515618764fc209",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "6833eeaf98515618764fc20a",
                    "name": "Ling Pan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T08:25:46.000Z",
            "submittedOnDailyAt": "2025-05-26T04:32:07.257Z",
            "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
            "submittedOnDailyBy": {
                "_id": "667187ba9ab144eb3ac43a1b",
                "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
                "isPro": false,
                "fullname": "Runze Liu",
                "user": "RyanLiu112",
                "type": "user"
            },
            "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
            "upvotes": 32,
            "discussionId": "6833eeb198515618764fc277",
            "projectPage": "https://tinnerhrhe.github.io/evosearch/",
            "githubRepo": "https://github.com/tinnerhrhe/EvoSearch-codes",
            "ai_summary": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.",
            "ai_keywords": [
                "test-time scaling",
                "TTS",
                "image generation",
                "video generation",
                "diffusion models",
                "flow-based models",
                "denoising trajectory",
                "stochastic differential equation",
                "selection",
                "mutation",
                "EvoSearch"
            ]
        },
        "publishedAt": "2025-05-23T04:25:46.000Z",
        "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
        "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667187ba9ab144eb3ac43a1b",
            "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
            "fullname": "Runze Liu",
            "name": "RyanLiu112",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17873",
            "authors": [
                {
                    "_id": "68341f661d53989a8ecb685d",
                    "user": {
                        "_id": "6684b284dc7b0ae2cc67660c",
                        "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
                        "isPro": false,
                        "fullname": "liuwanhao",
                        "user": "wanhaoliu",
                        "type": "user"
                    },
                    "name": "Wanhao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:54.880Z",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb685e",
                    "user": {
                        "_id": "646a11791556443f24b582e9",
                        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
                        "isPro": false,
                        "fullname": "Zonglin Yang",
                        "user": "ZonglinY",
                        "type": "user"
                    },
                    "name": "Zonglin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:08:35.046Z",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb685f",
                    "name": "Jue Wang",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6860",
                    "name": "Lidong Bing",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6861",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "di-zhang-fdu",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:40:05.397Z",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6862",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6863",
                    "name": "Yuqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6864",
                    "name": "Houqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6865",
                    "name": "Erik Cambria",
                    "hidden": false
                },
                {
                    "_id": "68341f661d53989a8ecb6866",
                    "name": "Wanli Ouyang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T13:24:50.000Z",
            "submittedOnDailyAt": "2025-05-26T06:33:21.775Z",
            "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
            "submittedOnDailyBy": {
                "_id": "646a11791556443f24b582e9",
                "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
                "isPro": false,
                "fullname": "Zonglin Yang",
                "user": "ZonglinY",
                "type": "user"
            },
            "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
            "upvotes": 25,
            "discussionId": "68341f671d53989a8ecb68b8",
            "ai_summary": "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.",
            "ai_keywords": [
                "hypothesis ranking",
                "automated scientific discovery",
                "natural sciences",
                "wet-lab experiments",
                "large language model",
                "pre-experiment ranking",
                "experiment-guided ranking",
                "hypothesis performance",
                "similarity",
                "noise",
                "dataset",
                "pseudo experiment-guided ranking",
                "clustering",
                "functional characteristics",
                "simulated experimental feedback"
            ]
        },
        "publishedAt": "2025-05-23T09:24:50.000Z",
        "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
        "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17873.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "fullname": "Zonglin Yang",
            "name": "ZonglinY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17561",
            "authors": [
                {
                    "_id": "6833cb9030cd9df52a117557",
                    "name": "Kwanyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "6833cb9030cd9df52a117558",
                    "name": "Sanghyun Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T07:09:10.000Z",
            "submittedOnDailyAt": "2025-05-26T00:33:24.403Z",
            "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "63973ee44e7b4959dc98028f",
                "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
                "isPro": false,
                "fullname": "Kwanyoung",
                "user": "kwanyoung",
                "type": "user"
            },
            "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
            "upvotes": 24,
            "discussionId": "6833cb9430cd9df52a11765d",
            "projectPage": "https://anse-project.github.io/anse-project/",
            "ai_summary": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.",
            "ai_keywords": [
                "video diffusion models",
                "noise seeds",
                "prompt alignment",
                "external priors",
                "frequency filters",
                "inter-frame smoothing",
                "ANSE",
                "Active Noise Selection for Generation",
                "BANSA",
                "Bayesian Active Noise Selection via Attention",
                "acquisition function",
                "entropy disagreement",
                "stochastic attention samples",
                "score estimation",
                "diffusion step",
                "temporal coherence"
            ]
        },
        "publishedAt": "2025-05-23T03:09:10.000Z",
        "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
        "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63973ee44e7b4959dc98028f",
            "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
            "fullname": "Kwanyoung",
            "name": "kwanyoung",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17941",
            "authors": [
                {
                    "_id": "6833cc35015eb19058ed83d9",
                    "user": {
                        "_id": "65811eeaa2284a018e51f1ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
                        "isPro": true,
                        "fullname": "Zigeng Chen",
                        "user": "Zigeng",
                        "type": "user"
                    },
                    "name": "Zigeng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:12.285Z",
                    "hidden": false
                },
                {
                    "_id": "6833cc35015eb19058ed83da",
                    "name": "Xinyin Ma",
                    "hidden": false
                },
                {
                    "_id": "6833cc35015eb19058ed83db",
                    "name": "Gongfan Fang",
                    "hidden": false
                },
                {
                    "_id": "6833cc35015eb19058ed83dc",
                    "name": "Ruonan Yu",
                    "hidden": false
                },
                {
                    "_id": "6833cc35015eb19058ed83dd",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T14:17:56.000Z",
            "submittedOnDailyAt": "2025-05-26T00:36:09.618Z",
            "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
            "submittedOnDailyBy": {
                "_id": "65811eeaa2284a018e51f1ba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
                "isPro": true,
                "fullname": "Zigeng Chen",
                "user": "Zigeng",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
            "upvotes": 22,
            "discussionId": "6833cc36015eb19058ed8419",
            "githubRepo": "https://github.com/czg1225/VeriThinker",
            "ai_summary": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.",
            "ai_keywords": [
                "Large Reasoning Models (LRMs)",
                "Chain-of-Thought (CoT) reasoning",
                "CoT compression",
                "verification task",
                "reasoning chain lengths",
                "reasoning tokens",
                "accuracy",
                "DeepSeek-R1-Distill-Qwen-7B",
                "MATH500",
                "AIME25",
                "speculative reasoning"
            ]
        },
        "publishedAt": "2025-05-23T10:17:56.000Z",
        "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
        "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65811eeaa2284a018e51f1ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
            "fullname": "Zigeng Chen",
            "name": "Zigeng",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16211",
            "authors": [
                {
                    "_id": "6833d9cfdf7cbb5c087cb9cd",
                    "name": "Kai Li",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9ce",
                    "name": "Can Shen",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9cf",
                    "name": "Yile Liu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d0",
                    "name": "Jirui Han",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d1",
                    "name": "Kelong Zheng",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d2",
                    "name": "Xuechao Zou",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d3",
                    "name": "Zhe Wang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d4",
                    "name": "Xingjian Du",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d5",
                    "name": "Shun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d6",
                    "name": "Hanjun Luo",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d7",
                    "name": "Yingbin Jin",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d8",
                    "name": "Xinxin Xing",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9d9",
                    "name": "Ziyang Ma",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9da",
                    "name": "Yue Liu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9db",
                    "user": {
                        "_id": "64c6627d5671d42e0adfad56",
                        "avatarUrl": "/avatars/8b98054b2911b86dcc4856a15306e60f.svg",
                        "isPro": false,
                        "fullname": "jiaxiaojunQAQ",
                        "user": "jiaxiaojunQAQ",
                        "type": "user"
                    },
                    "name": "Xiaojun Jia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:37.847Z",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9dc",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9dd",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9de",
                    "name": "Kun Wang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9df",
                    "name": "Yibo Yan",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e0",
                    "name": "Haoyang Li",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e1",
                    "name": "Yiming Li",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e2",
                    "name": "Xiaobin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e3",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e4",
                    "name": "Haibo Hu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e5",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e6",
                    "name": "Zhizheng Wu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e7",
                    "name": "Xiaolin Hu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e8",
                    "name": "Eng-Siong Chng",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9e9",
                    "name": "XiaoFeng Wang",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9ea",
                    "name": "Wenyuan Xu",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9eb",
                    "name": "Wei Dong",
                    "hidden": false
                },
                {
                    "_id": "6833d9cfdf7cbb5c087cb9ec",
                    "name": "Xinfeng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T04:27:46.000Z",
            "submittedOnDailyAt": "2025-05-26T01:33:43.107Z",
            "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "6387676c23da90491eb9fb16",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
                "isPro": false,
                "fullname": "Kai Li",
                "user": "JusperLee",
                "type": "user"
            },
            "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
            "upvotes": 17,
            "discussionId": "6833d9d1df7cbb5c087cba85",
            "githubRepo": "https://github.com/JusperLee/AudioTrust",
            "ai_summary": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.",
            "ai_keywords": [
                "Audio Large Language Models",
                "ALLMs",
                "trustworthiness",
                "fairness",
                "hallucination",
                "safety",
                "privacy",
                "robustness",
                "authentication",
                "AudioTrust",
                "experimental setups",
                "audio-specific evaluation metrics",
                "automated pipeline"
            ]
        },
        "publishedAt": "2025-05-22T00:27:46.000Z",
        "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
        "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6387676c23da90491eb9fb16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
            "fullname": "Kai Li",
            "name": "JusperLee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.16134",
            "authors": [
                {
                    "_id": "6834833c672ae46abcdc244d",
                    "name": "Menschikov Mikhail",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc244e",
                    "name": "Alexander Kharitonov",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc244f",
                    "name": "Maiia Kotyga",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc2450",
                    "name": "Vadim Porvatov",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc2451",
                    "name": "Anna Zhukovskaya",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc2452",
                    "name": "David Kagramanyan",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc2453",
                    "name": "Egor Shvetsov",
                    "hidden": false
                },
                {
                    "_id": "6834833c672ae46abcdc2454",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65afde6ba0b4bf3b0e95b4e8/Px9Evs7LAVBe17tuAHAAz.png"
            ],
            "publishedAt": "2025-05-22T02:23:00.000Z",
            "submittedOnDailyAt": "2025-05-26T13:37:28.092Z",
            "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "65afde6ba0b4bf3b0e95b4e8",
                "avatarUrl": "/avatars/e9b97040b0a619bf6609465d1678705c.svg",
                "isPro": false,
                "fullname": "Egor Shvetsov",
                "user": "dalime",
                "type": "user"
            },
            "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi.",
            "upvotes": 14,
            "discussionId": "6834833e672ae46abcdc24a2",
            "ai_summary": "LLMs display positional bias across different languages, which affects model uncertainty, syntax, and prompting, revealing that explicit positional guidance can reduce accuracy.",
            "ai_keywords": [
                "positional bias",
                "model uncertainty",
                "syntax",
                "prompting",
                "positional guidance",
                "entropy",
                "dominant word order",
                "free-word-order languages"
            ]
        },
        "publishedAt": "2025-05-21T22:23:00.000Z",
        "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in\n  Large Language Models",
        "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65afde6ba0b4bf3b0e95b4e8/Px9Evs7LAVBe17tuAHAAz.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16134.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65afde6ba0b4bf3b0e95b4e8",
            "avatarUrl": "/avatars/e9b97040b0a619bf6609465d1678705c.svg",
            "fullname": "Egor Shvetsov",
            "name": "dalime",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17955",
            "authors": [
                {
                    "_id": "68343fcab624d4711d816434",
                    "user": {
                        "_id": "63a14dd9b5515dccd425275e",
                        "avatarUrl": "/avatars/791342bd41112bc97b5f8164d761d259.svg",
                        "isPro": false,
                        "fullname": "Jeong",
                        "user": "eugene6923",
                        "type": "user"
                    },
                    "name": "Yujin Jeong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T13:18:42.290Z",
                    "hidden": false
                },
                {
                    "_id": "68343fcab624d4711d816435",
                    "user": {
                        "_id": "6520898f7bf8cc2dd28b7a9c",
                        "avatarUrl": "/avatars/87a29ba95b71ee2dce18e97aa85e17a1.svg",
                        "isPro": false,
                        "fullname": "Arnas Uselis",
                        "user": "Gigglingface",
                        "type": "user"
                    },
                    "name": "Arnas Uselis",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T11:59:06.709Z",
                    "hidden": false
                },
                {
                    "_id": "68343fcab624d4711d816436",
                    "name": "Seong Joon Oh",
                    "hidden": false
                },
                {
                    "_id": "68343fcab624d4711d816437",
                    "name": "Anna Rohrbach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T14:29:52.000Z",
            "submittedOnDailyAt": "2025-05-26T10:39:38.324Z",
            "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply",
            "submittedOnDailyBy": {
                "_id": "6520898f7bf8cc2dd28b7a9c",
                "avatarUrl": "/avatars/87a29ba95b71ee2dce18e97aa85e17a1.svg",
                "isPro": false,
                "fullname": "Arnas Uselis",
                "user": "Gigglingface",
                "type": "user"
            },
            "summary": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.",
            "upvotes": 13,
            "discussionId": "68343fcfb624d4711d81656f",
            "githubRepo": "https://github.com/eugene6923/Diffusion-Classifiers-Compositionality",
            "ai_summary": "A study of diffusion classifiers across multiple datasets and tasks reveals their compositional understanding, highlighting domain-specific performance effects and timestep weighting importance.",
            "ai_keywords": [
                "discriminative models",
                "generative text-to-image diffusion models",
                "zero-shot diffusion classifiers",
                "compositional tasks",
                "SD 1.5",
                "SD 2.0",
                "SD3-m",
                "Self-Bench",
                "diagnostic benchmark",
                "timestep weighting",
                "domain gap",
                "timestep sensitivity"
            ]
        },
        "publishedAt": "2025-05-23T10:29:52.000Z",
        "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply",
        "summary": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17955.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6520898f7bf8cc2dd28b7a9c",
            "avatarUrl": "/avatars/87a29ba95b71ee2dce18e97aa85e17a1.svg",
            "fullname": "Arnas Uselis",
            "name": "Gigglingface",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17558",
            "authors": [
                {
                    "_id": "6833c8af029c4a53a60a5dfa",
                    "user": {
                        "_id": "648749094dea003c6dae810f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
                        "isPro": false,
                        "fullname": "Shrey Pandit",
                        "user": "SP2001",
                        "type": "user"
                    },
                    "name": "Shrey Pandit",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-26T01:49:36.568Z",
                    "hidden": false
                },
                {
                    "_id": "6833c8af029c4a53a60a5dfb",
                    "user": {
                        "_id": "62fa7294363251ee40a41dba",
                        "avatarUrl": "/avatars/869c6de9a1cb2ded690ae56559916cae.svg",
                        "isPro": false,
                        "fullname": "Ashwin V",
                        "user": "ashwinnv",
                        "type": "user"
                    },
                    "name": "Ashwin Vinod",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:22.347Z",
                    "hidden": false
                },
                {
                    "_id": "6833c8af029c4a53a60a5dfc",
                    "name": "Liu Leqi",
                    "hidden": false
                },
                {
                    "_id": "6833c8af029c4a53a60a5dfd",
                    "name": "Ying Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T07:05:09.000Z",
            "submittedOnDailyAt": "2025-05-26T00:20:35.511Z",
            "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
            "submittedOnDailyBy": {
                "_id": "648749094dea003c6dae810f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
                "isPro": false,
                "fullname": "Shrey Pandit",
                "user": "SP2001",
                "type": "user"
            },
            "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
            "upvotes": 13,
            "discussionId": "6833c8b0029c4a53a60a5e3a",
            "ai_summary": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.",
            "ai_keywords": [
                "LLMs",
                "hallucinations",
                "DPO alignment procedure",
                "curriculum learning",
                "probability scores",
                "fact checking models",
                "HaluCheck models",
                "MedHallu",
                "HaluEval",
                "zero-shot settings"
            ]
        },
        "publishedAt": "2025-05-23T03:05:09.000Z",
        "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
        "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "fullname": "Shrey Pandit",
            "name": "SP2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17412",
            "authors": [
                {
                    "_id": "6833e93697966d18e7c1e4d7",
                    "name": "Shuang Wu",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4d8",
                    "name": "Youtian Lin",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4d9",
                    "name": "Feihu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4da",
                    "name": "Yifei Zeng",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4db",
                    "name": "Yikang Yang",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4dc",
                    "name": "Yajie Bao",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4dd",
                    "name": "Jiachen Qian",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4de",
                    "name": "Siyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4df",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4e0",
                    "name": "Xun Cao",
                    "hidden": false
                },
                {
                    "_id": "6833e93697966d18e7c1e4e1",
                    "name": "Yao Yao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T02:58:01.000Z",
            "submittedOnDailyAt": "2025-05-26T03:12:06.518Z",
            "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
            "submittedOnDailyBy": {
                "_id": "645a24779f06c5897254d14b",
                "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
                "isPro": false,
                "fullname": "Youtian Lin",
                "user": "LoYoT",
                "type": "user"
            },
            "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
            "upvotes": 13,
            "discussionId": "6833e93b97966d18e7c1e676",
            "projectPage": "https://nju-3dv.github.io/projects/Direct3D-S2/",
            "githubRepo": "https://github.com/DreamTechAI/Direct3D-S2",
            "ai_summary": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.",
            "ai_keywords": [
                "Signed Distance Functions",
                "sparse volumes",
                "Spatial Sparse Attention",
                "Diffusion Transformer",
                "variational autoencoder",
                "gigascale 3D generation"
            ]
        },
        "publishedAt": "2025-05-22T22:58:01.000Z",
        "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
        "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645a24779f06c5897254d14b",
            "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
            "fullname": "Youtian Lin",
            "name": "LoYoT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17399",
            "authors": [
                {
                    "_id": "6833fd69fe87d9433d098068",
                    "user": {
                        "_id": "63a2a51ef30c464227924fc6",
                        "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg",
                        "isPro": false,
                        "fullname": "Haoyu Sun",
                        "user": "Mikivis",
                        "type": "user"
                    },
                    "name": "Haoyu Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-26T13:19:50.579Z",
                    "hidden": false
                },
                {
                    "_id": "6833fd69fe87d9433d098069",
                    "name": "Huichen Will Wang",
                    "hidden": false
                },
                {
                    "_id": "6833fd69fe87d9433d09806a",
                    "user": {
                        "_id": "645b4819f9d4ec91fdd54852",
                        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
                        "isPro": false,
                        "fullname": "Jiawei Gu",
                        "user": "Kuvvi",
                        "type": "user"
                    },
                    "name": "Jiawei Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:01.542Z",
                    "hidden": false
                },
                {
                    "_id": "6833fd69fe87d9433d09806b",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "6833fd69fe87d9433d09806c",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T02:16:11.000Z",
            "submittedOnDailyAt": "2025-05-26T08:05:22.618Z",
            "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
            "submittedOnDailyBy": {
                "_id": "645b4819f9d4ec91fdd54852",
                "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
                "isPro": false,
                "fullname": "Jiawei Gu",
                "user": "Kuvvi",
                "type": "user"
            },
            "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) across the full front-end\ndevelopment pipeline. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
            "upvotes": 13,
            "discussionId": "6833fd6bfe87d9433d0980c2",
            "githubRepo": "https://github.com/Mikivishy/FullFront",
            "ai_summary": "FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "Webpage Design",
                "Webpage Perception QA",
                "Webpage Code Generation",
                "front-end engineering"
            ]
        },
        "publishedAt": "2025-05-22T22:16:11.000Z",
        "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
        "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) across the full front-end\ndevelopment pipeline. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17399.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "fullname": "Jiawei Gu",
            "name": "Kuvvi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.15692",
            "authors": [
                {
                    "_id": "68306ffdff038ca6400a153a",
                    "user": {
                        "_id": "6747de57f8cab58c22ec94a2",
                        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                        "isPro": false,
                        "fullname": "Jinyang Wu",
                        "user": "Jinyang23",
                        "type": "user"
                    },
                    "name": "Jinyang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:13:57.397Z",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a153b",
                    "user": {
                        "_id": "667fdaee20ee9ac417c7708c",
                        "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
                        "isPro": false,
                        "fullname": "Chonghua Liao",
                        "user": "ChonghuaLiao",
                        "type": "user"
                    },
                    "name": "Chonghua Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:16:00.056Z",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a153c",
                    "name": "Mingkuan Feng",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a153d",
                    "name": "Shuai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a153e",
                    "name": "Zhengqi Wen",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a153f",
                    "name": "Pengpeng Shao",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a1540",
                    "name": "Huazhe Xu",
                    "hidden": false
                },
                {
                    "_id": "68306ffdff038ca6400a1541",
                    "name": "Jianhua Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T16:06:10.000Z",
            "submittedOnDailyAt": "2025-05-26T01:19:22.736Z",
            "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
            "submittedOnDailyBy": {
                "_id": "6747de57f8cab58c22ec94a2",
                "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
                "isPro": false,
                "fullname": "Jinyang Wu",
                "user": "Jinyang23",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
            "upvotes": 12,
            "discussionId": "68306ffeff038ca6400a1569",
            "ai_summary": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.",
            "ai_keywords": [
                "reinforcement learning",
                "TAPO",
                "Thought-Augmented Policy Optimization",
                "high-level guidance",
                "thought patterns",
                "model exploration",
                "AIME",
                "AMC",
                "Minerva Math",
                "reasoning models",
                "explainability",
                "output readability"
            ]
        },
        "publishedAt": "2025-05-21T12:06:10.000Z",
        "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
        "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "fullname": "Jinyang Wu",
            "name": "Jinyang23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16479",
            "authors": [
                {
                    "_id": "682fdc63bf762029ddcad451",
                    "user": {
                        "_id": "6640c647acae6bb179eedff5",
                        "avatarUrl": "/avatars/bcaafaaa1d4b4c241d72a886401772e3.svg",
                        "isPro": false,
                        "fullname": "Yuetong Liu",
                        "user": "YuetongLiu",
                        "type": "user"
                    },
                    "name": "Yuetong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:16:39.979Z",
                    "hidden": false
                },
                {
                    "_id": "682fdc63bf762029ddcad452",
                    "user": {
                        "_id": "646c77911ee398a4e9404b8b",
                        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
                        "isPro": false,
                        "fullname": "Yunqiu Xu",
                        "user": "Yunqiu",
                        "type": "user"
                    },
                    "name": "Yunqiu Xu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-23T04:01:10.107Z",
                    "hidden": false
                },
                {
                    "_id": "682fdc63bf762029ddcad453",
                    "name": "Yang Wei",
                    "hidden": false
                },
                {
                    "_id": "682fdc63bf762029ddcad454",
                    "name": "Xiuli Bi",
                    "hidden": false
                },
                {
                    "_id": "682fdc63bf762029ddcad455",
                    "name": "Bin Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T10:06:35.000Z",
            "submittedOnDailyAt": "2025-05-26T05:13:10.671Z",
            "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
            "submittedOnDailyBy": {
                "_id": "646c77911ee398a4e9404b8b",
                "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
                "isPro": false,
                "fullname": "Yunqiu Xu",
                "user": "Yunqiu",
                "type": "user"
            },
            "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
            "upvotes": 11,
            "discussionId": "682fdc67bf762029ddcad58c",
            "projectPage": "https://henlyta.github.io/ClearNight/mainpage.html",
            "githubRepo": "https://github.com/henlyta/ClearNight",
            "ai_summary": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.",
            "ai_keywords": [
                "Retinex-based dual priors",
                "illumination-aware degradation generation",
                "weather-aware dynamic specific-commonality collaboration",
                "nighttime image restoration"
            ]
        },
        "publishedAt": "2025-05-22T06:06:35.000Z",
        "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
        "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "fullname": "Yunqiu Xu",
            "name": "Yunqiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14146",
            "authors": [
                {
                    "_id": "68343b73f0b7aba41a360e8f",
                    "user": {
                        "_id": "63724cfada3183d9d53f2009",
                        "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg",
                        "isPro": false,
                        "fullname": "Patrick Jiang",
                        "user": "pat-jj",
                        "type": "user"
                    },
                    "name": "Pengcheng Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T13:18:29.181Z",
                    "hidden": false
                },
                {
                    "_id": "68343b73f0b7aba41a360e90",
                    "user": {
                        "_id": "66a3f1c4c38ce500371fd8d4",
                        "avatarUrl": "/avatars/381de938091f1a5c179eef72aa247bbf.svg",
                        "isPro": false,
                        "fullname": "Xueqiang Xu",
                        "user": "XueqiangXu",
                        "type": "user"
                    },
                    "name": "Xueqiang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T14:07:25.830Z",
                    "hidden": false
                },
                {
                    "_id": "68343b73f0b7aba41a360e91",
                    "name": "Jiacheng Lin",
                    "hidden": false
                },
                {
                    "_id": "68343b73f0b7aba41a360e92",
                    "name": "Jinfeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "68343b73f0b7aba41a360e93",
                    "name": "Zifeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68343b73f0b7aba41a360e94",
                    "name": "Jimeng Sun",
                    "hidden": false
                },
                {
                    "_id": "68343b73f0b7aba41a360e95",
                    "name": "Jiawei Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T09:53:56.000Z",
            "submittedOnDailyAt": "2025-05-26T08:52:33.999Z",
            "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
            "submittedOnDailyBy": {
                "_id": "63724cfada3183d9d53f2009",
                "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg",
                "isPro": false,
                "fullname": "Patrick Jiang",
                "user": "pat-jj",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.",
            "upvotes": 11,
            "discussionId": "68343b74f0b7aba41a360eaf",
            "githubRepo": "https://github.com/pat-jj/s3",
            "ai_summary": "A lightweight, model-agnostic framework decouples the retrieval and generation processes in RAG systems, enhancing performance with minimal training data.",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "large language models (LLMs)",
                "reinforcement learning (RL)",
                "retrieval engines",
                "NDCG",
                "Gain Beyond RAG",
                "search utility",
                "model-agnostic framework",
                "QA benchmarks"
            ]
        },
        "publishedAt": "2025-05-20T05:53:56.000Z",
        "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL",
        "summary": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14146.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63724cfada3183d9d53f2009",
            "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg",
            "fullname": "Patrick Jiang",
            "name": "pat-jj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13508",
            "authors": [
                {
                    "_id": "683148c0018bba5b656c94e3",
                    "user": {
                        "_id": "65d188a4aa309d842e438ef1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
                        "isPro": false,
                        "fullname": "Zijia Liu",
                        "user": "m-serious",
                        "type": "user"
                    },
                    "name": "Zijia Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:12:18.318Z",
                    "hidden": false
                },
                {
                    "_id": "683148c0018bba5b656c94e4",
                    "name": "Peixuan Han",
                    "hidden": false
                },
                {
                    "_id": "683148c0018bba5b656c94e5",
                    "name": "Haofei Yu",
                    "hidden": false
                },
                {
                    "_id": "683148c0018bba5b656c94e6",
                    "name": "Haoru Li",
                    "hidden": false
                },
                {
                    "_id": "683148c0018bba5b656c94e7",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T13:46:28.000Z",
            "submittedOnDailyAt": "2025-05-26T06:47:47.579Z",
            "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
            "submittedOnDailyBy": {
                "_id": "65d188a4aa309d842e438ef1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
                "isPro": false,
                "fullname": "Zijia Liu",
                "user": "m-serious",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\nTime-R1, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a reinforcement learning (RL)\ncurriculum driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release Time-Bench,\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of Time-R1 checkpoints.",
            "upvotes": 11,
            "discussionId": "683148c1018bba5b656c9511",
            "githubRepo": "https://github.com/ulab-uiuc/Time-R1",
            "ai_summary": "A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "reinforcement learning",
                "RL curriculum",
                "rule-based reward system",
                "temporal understanding",
                "event-time mappings",
                "future event prediction",
                "creative scenario generation",
                "Time-Bench",
                "Time-R1 checkpoints"
            ]
        },
        "publishedAt": "2025-05-16T09:46:28.000Z",
        "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
        "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\nTime-R1, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a reinforcement learning (RL)\ncurriculum driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release Time-Bench,\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of Time-R1 checkpoints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13508.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "fullname": "Zijia Liu",
            "name": "m-serious",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16770",
            "authors": [
                {
                    "_id": "68308e2697d9a81c8521bc6a",
                    "user": {
                        "_id": "62145614b670cb63a38075ba",
                        "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
                        "isPro": false,
                        "fullname": "MenghaoGuo",
                        "user": "MenghaoGuo",
                        "type": "user"
                    },
                    "name": "Meng-Hao Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:12:57.462Z",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc6b",
                    "user": {
                        "_id": "66b711f9512dac2ac08bc5e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b711f9512dac2ac08bc5e5/n2kSqNhg-TE56iN_V0xHm.png",
                        "isPro": false,
                        "fullname": "Xuanyu Chu",
                        "user": "CXY07",
                        "type": "user"
                    },
                    "name": "Xuanyu Chu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:12:48.661Z",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc6c",
                    "name": "Qianrui Yang",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc6d",
                    "user": {
                        "_id": "6816bd8e0499f6c7c7b89601",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6816bd8e0499f6c7c7b89601/-dkIPxjOGbdwDZFxhkBMC.jpeg",
                        "isPro": false,
                        "fullname": "Zhe-Han Mo",
                        "user": "Mo-ZheHan",
                        "type": "user"
                    },
                    "name": "Zhe-Han Mo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:12:52.048Z",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc6e",
                    "name": "Yiqing Shen",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc6f",
                    "name": "Pei-lin Li",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc70",
                    "name": "Xinjie Lin",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc71",
                    "name": "Jinnian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc72",
                    "name": "Xin-Sheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc73",
                    "user": {
                        "_id": "63b2efb5922f26a27e76381c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
                        "isPro": false,
                        "fullname": "Yi Zhang",
                        "user": "uyzhang",
                        "type": "user"
                    },
                    "name": "Yi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:12:54.379Z",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc74",
                    "name": "Kiyohiro Nakayama",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc75",
                    "name": "Zhengyang Geng",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc76",
                    "name": "Houwen Peng",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc77",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "68308e2697d9a81c8521bc78",
                    "name": "Shi-Nin Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T15:11:57.000Z",
            "submittedOnDailyAt": "2025-05-26T06:55:20.321Z",
            "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
            "submittedOnDailyBy": {
                "_id": "62145614b670cb63a38075ba",
                "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
                "isPro": false,
                "fullname": "MenghaoGuo",
                "user": "MenghaoGuo",
                "type": "user"
            },
            "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv",
            "upvotes": 10,
            "discussionId": "68308e2797d9a81c8521bca5",
            "ai_summary": "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.",
            "ai_keywords": [
                "multi-modal models",
                "omni-models",
                "GPT-4o",
                "Gemini",
                "o3",
                "multi-modal chain of thought",
                "M-CoT",
                "RBench-V",
                "image manipulation",
                "auxiliary lines"
            ]
        },
        "publishedAt": "2025-05-22T11:11:57.000Z",
        "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
        "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16770.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62145614b670cb63a38075ba",
            "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
            "fullname": "MenghaoGuo",
            "name": "MenghaoGuo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16483",
            "authors": [
                {
                    "_id": "6833cb27e10e89e250a6d9ae",
                    "name": "Shuzheng Si",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9af",
                    "user": {
                        "_id": "63ff09a02098b9ad105a09f6",
                        "avatarUrl": "/avatars/4409ca5d320050cf4c3df05962c7ff58.svg",
                        "isPro": false,
                        "fullname": "Hans Zhao",
                        "user": "BleachNick",
                        "type": "user"
                    },
                    "name": "Haozhe Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:14.872Z",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b0",
                    "name": "Cheng Gao",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b1",
                    "name": "Yuzhuo Bai",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b2",
                    "name": "Zhitong Wang",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b3",
                    "name": "Bofei Gao",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b4",
                    "name": "Kangyang Luo",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b5",
                    "name": "Wenhao Li",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b6",
                    "name": "Yufei Huang",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b7",
                    "name": "Gang Chen",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b8",
                    "name": "Fanchao Qi",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9b9",
                    "name": "Minjia Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9ba",
                    "name": "Baobao Chang",
                    "hidden": false
                },
                {
                    "_id": "6833cb27e10e89e250a6d9bb",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T10:10:07.000Z",
            "submittedOnDailyAt": "2025-05-26T00:36:30.930Z",
            "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "637c99bbfe115289cfedfb44",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
                "isPro": false,
                "fullname": "ssz",
                "user": "ssz1111",
                "type": "user"
            },
            "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
            "upvotes": 10,
            "discussionId": "6833cb28e10e89e250a6da0a",
            "projectPage": "https://github.com/S1s-Z/CANOE",
            "githubRepo": "https://github.com/S1s-Z/CANOE",
            "ai_summary": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.",
            "ai_keywords": [
                "teaching large language models",
                "faithfulness",
                "context",
                "CANOE",
                "short-form generation",
                "long-form generation",
                "question-answering",
                "Dual-GRPO",
                "rule-based reinforcement learning",
                "preference data",
                "reward models"
            ]
        },
        "publishedAt": "2025-05-22T06:10:07.000Z",
        "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
        "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "637c99bbfe115289cfedfb44",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
            "fullname": "ssz",
            "name": "ssz1111",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17417",
            "authors": [
                {
                    "_id": "6833d2df73bebebe5cd6604e",
                    "user": {
                        "_id": "62d7b2339b629105a5d6888a",
                        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                        "isPro": false,
                        "fullname": "Alan Dao",
                        "user": "alandao",
                        "type": "user"
                    },
                    "name": "Alan Dao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-26T02:38:31.379Z",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd6604f",
                    "name": "Dinh Bach Vu",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66050",
                    "name": "Huy Hoang Ha",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66051",
                    "name": "Tuan Le Duc Anh",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66052",
                    "name": "Shreyas Gopal",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66053",
                    "name": "Yue Heng Yeo",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66054",
                    "name": "Warren Keng Hoong Low",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66055",
                    "name": "Eng Siong Chng",
                    "hidden": false
                },
                {
                    "_id": "6833d2df73bebebe5cd66056",
                    "name": "Jia Qi Yip",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T03:05:47.000Z",
            "submittedOnDailyAt": "2025-05-26T01:03:18.385Z",
            "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
            "submittedOnDailyBy": {
                "_id": "62d7b2339b629105a5d6888a",
                "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
                "isPro": false,
                "fullname": "Alan Dao",
                "user": "alandao",
                "type": "user"
            },
            "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
            "upvotes": 9,
            "discussionId": "6833d2df73bebebe5cd66074",
            "githubRepo": "https://github.com/menloresearch/ichigo",
            "ai_summary": "A method bypasses the need for TTS models by aligning semantic representations with a Whisper encoder, enabling LLMs to understand both text and spoken instructions for low-resource languages.",
            "ai_keywords": [
                "large language models",
                "LLM",
                "speech instruction data",
                "TTS",
                "semantic representation",
                "Whisper encoder",
                "fine-tuning",
                "low-resource languages"
            ]
        },
        "publishedAt": "2025-05-22T23:05:47.000Z",
        "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
        "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "fullname": "Alan Dao",
            "name": "alandao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17295",
            "authors": [
                {
                    "_id": "6833ed0f924393051ae89d09",
                    "user": {
                        "_id": "661b42ee2b14565c7af4ff73",
                        "avatarUrl": "/avatars/a58ced9fd5f3e86ef5ebb43ce1a7af29.svg",
                        "isPro": false,
                        "fullname": "zhiling chen",
                        "user": "ed1son",
                        "type": "user"
                    },
                    "name": "Zhiling Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:23.482Z",
                    "hidden": false
                },
                {
                    "_id": "6833ed0f924393051ae89d0a",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833ed0f924393051ae89d0b",
                    "name": "Fardin Jalil Piran",
                    "hidden": false
                },
                {
                    "_id": "6833ed0f924393051ae89d0c",
                    "name": "Qianyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6833ed0f924393051ae89d0d",
                    "name": "Jiong Tang",
                    "hidden": false
                },
                {
                    "_id": "6833ed0f924393051ae89d0e",
                    "user": {
                        "_id": "6834ab8de2a34a2f37676c4b",
                        "avatarUrl": "/avatars/aa95bbf53a9c55e9148fdfb8d62e57e8.svg",
                        "isPro": false,
                        "fullname": "Farhad Imani",
                        "user": "FarhadImani1",
                        "type": "user"
                    },
                    "name": "Farhad Imani",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-26T17:59:31.038Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T21:22:50.000Z",
            "submittedOnDailyAt": "2025-05-26T15:52:22.149Z",
            "title": "ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic\n  Systems",
            "submittedOnDailyBy": {
                "_id": "661b42ee2b14565c7af4ff73",
                "avatarUrl": "/avatars/a58ced9fd5f3e86ef5ebb43ce1a7af29.svg",
                "isPro": false,
                "fullname": "zhiling chen",
                "user": "ed1son",
                "type": "user"
            },
            "summary": "We introduce ScanBot, a novel dataset designed for instruction-conditioned,\nhigh-precision surface scanning in robotic systems. In contrast to existing\nrobot learning datasets that focus on coarse tasks such as grasping,\nnavigation, or dialogue, ScanBot targets the high-precision demands of\nindustrial laser scanning, where sub-millimeter path continuity and parameter\nstability are critical. The dataset covers laser scanning trajectories executed\nby a robot across 12 diverse objects and 6 task types, including full-surface\nscans, geometry-focused regions, spatially referenced parts, functionally\nrelevant structures, defect inspection, and comparative analysis. Each scan is\nguided by natural language instructions and paired with synchronized RGB,\ndepth, and laser profiles, as well as robot pose and joint states. Despite\nrecent progress, existing vision-language action (VLA) models still fail to\ngenerate stable scanning trajectories under fine-grained instructions and\nreal-world precision demands. To investigate this limitation, we benchmark a\nrange of multimodal large language models (MLLMs) across the full\nperception-planning-execution loop, revealing persistent challenges in\ninstruction-following under realistic constraints.",
            "upvotes": 9,
            "discussionId": "6833ed11924393051ae89da3",
            "projectPage": "https://ed1sonchen.github.io/ScanBot/",
            "githubRepo": "https://github.com/Ed1sonChen/Scan-Bot",
            "ai_summary": "The ScanBot dataset, focusing on instruction-conditioned high-precision robotic surface scanning, showcases challenges for vision-language action models in achieving precise scanning trajectories under real-world constraints.",
            "ai_keywords": [
                "ScanBot",
                "instruction-conditioned",
                "high-precision",
                "surface scanning",
                "robotic systems",
                "laser scanning",
                "sub-millimeter path continuity",
                "parameter stability",
                "multimodal large language models",
                "vision-language action models",
                "perception-planning-execution loop"
            ]
        },
        "publishedAt": "2025-05-22T17:22:50.000Z",
        "title": "ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic\n  Systems",
        "summary": "We introduce ScanBot, a novel dataset designed for instruction-conditioned,\nhigh-precision surface scanning in robotic systems. In contrast to existing\nrobot learning datasets that focus on coarse tasks such as grasping,\nnavigation, or dialogue, ScanBot targets the high-precision demands of\nindustrial laser scanning, where sub-millimeter path continuity and parameter\nstability are critical. The dataset covers laser scanning trajectories executed\nby a robot across 12 diverse objects and 6 task types, including full-surface\nscans, geometry-focused regions, spatially referenced parts, functionally\nrelevant structures, defect inspection, and comparative analysis. Each scan is\nguided by natural language instructions and paired with synchronized RGB,\ndepth, and laser profiles, as well as robot pose and joint states. Despite\nrecent progress, existing vision-language action (VLA) models still fail to\ngenerate stable scanning trajectories under fine-grained instructions and\nreal-world precision demands. To investigate this limitation, we benchmark a\nrange of multimodal large language models (MLLMs) across the full\nperception-planning-execution loop, revealing persistent challenges in\ninstruction-following under realistic constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661b42ee2b14565c7af4ff73",
            "avatarUrl": "/avatars/a58ced9fd5f3e86ef5ebb43ce1a7af29.svg",
            "fullname": "zhiling chen",
            "name": "ed1son",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17826",
            "authors": [
                {
                    "_id": "6833ce1bd5c438959f750d57",
                    "name": "Xuchen Pan",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d58",
                    "name": "Yanxi Chen",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d59",
                    "name": "Yushuo Chen",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d5a",
                    "name": "Yuchang Sun",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d5b",
                    "name": "Daoyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d5c",
                    "name": "Wenhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d5d",
                    "name": "Yuexiang Xie",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d5e",
                    "name": "Yilun Huang",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d5f",
                    "name": "Yilei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d60",
                    "name": "Dawei Gao",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d61",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d62",
                    "name": "Bolin Ding",
                    "hidden": false
                },
                {
                    "_id": "6833ce1bd5c438959f750d63",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
            ],
            "publishedAt": "2025-05-23T12:41:09.000Z",
            "submittedOnDailyAt": "2025-05-26T00:50:50.296Z",
            "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6576f9f4654561a1b345610b",
                "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
                "isPro": false,
                "fullname": "Yanxi Chen",
                "user": "yanxi-chen",
                "type": "user"
            },
            "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
            "upvotes": 8,
            "discussionId": "6833ce1cd5c438959f750dab",
            "projectPage": "https://github.com/modelscope/Trinity-RFT",
            "githubRepo": "https://github.com/modelscope/Trinity-RFT",
            "ai_summary": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.",
            "ai_keywords": [
                "reinforcement fine-tuning",
                "RFT-core",
                "synchronous/asynchronous",
                "on-policy/off-policy",
                "online/offline",
                "agent-environment interaction",
                "data pipelines",
                "reinforcement learning paradigms"
            ]
        },
        "publishedAt": "2025-05-23T08:41:09.000Z",
        "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
        "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576f9f4654561a1b345610b",
            "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
            "fullname": "Yanxi Chen",
            "name": "yanxi-chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.15389",
            "authors": [
                {
                    "_id": "682f518184a99219c4b3090c",
                    "user": {
                        "_id": "6540fbf9cb7fffd683942b43",
                        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
                        "isPro": false,
                        "fullname": "DongGeon Lee",
                        "user": "oneonlee",
                        "type": "user"
                    },
                    "name": "DongGeon Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:17:11.519Z",
                    "hidden": false
                },
                {
                    "_id": "682f518184a99219c4b3090d",
                    "name": "Joonwon Jang",
                    "hidden": false
                },
                {
                    "_id": "682f518184a99219c4b3090e",
                    "name": "Jihae Jeong",
                    "hidden": false
                },
                {
                    "_id": "682f518184a99219c4b3090f",
                    "name": "Hwanjo Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T11:26:40.000Z",
            "submittedOnDailyAt": "2025-05-26T00:35:59.051Z",
            "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
            "submittedOnDailyBy": {
                "_id": "6540fbf9cb7fffd683942b43",
                "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
                "isPro": false,
                "fullname": "DongGeon Lee",
                "user": "oneonlee",
                "type": "user"
            },
            "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
            "upvotes": 7,
            "discussionId": "682f518184a99219c4b30956",
            "ai_summary": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "MemeSafetyBench",
                "safety taxonomy",
                "LLM-based instruction generation",
                "single and multi-turn interactions"
            ]
        },
        "publishedAt": "2025-05-21T07:26:40.000Z",
        "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
        "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "fullname": "DongGeon Lee",
            "name": "oneonlee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17063",
            "authors": [
                {
                    "_id": "6833e65bf9ae3819ea4c568e",
                    "name": "Yiduo Guo",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c568f",
                    "user": {
                        "_id": "638e4e66629b4d0a62ce1bf3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
                        "isPro": false,
                        "fullname": "Zhen Guo",
                        "user": "zguo0525",
                        "type": "user"
                    },
                    "name": "Zhen Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:28.399Z",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c5690",
                    "name": "Chuanwei Huang",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c5691",
                    "name": "Zi-Ang Wang",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c5692",
                    "name": "Zekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c5693",
                    "name": "Haofei Yu",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c5694",
                    "name": "Huishuai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6833e65bf9ae3819ea4c5695",
                    "name": "Yikang Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-18T05:35:13.000Z",
            "submittedOnDailyAt": "2025-05-26T02:26:50.238Z",
            "title": "Synthetic Data RL: Task Definition Is All You Need",
            "submittedOnDailyBy": {
                "_id": "638e4e66629b4d0a62ce1bf3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
                "isPro": false,
                "fullname": "Zhen Guo",
                "user": "zguo0525",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
            "upvotes": 7,
            "discussionId": "6833e65cf9ae3819ea4c56c9",
            "projectPage": "https://github.com/gydpku/Data_Synthesis_RL",
            "githubRepo": "https://github.com/gydpku/Data_Synthesis_RL",
            "ai_summary": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "synthetic data",
                "reinforcement fine-tuning",
                "question and answer pairs",
                "model solvability",
                "average pass rate",
                "data budget",
                "supervised fine-tuning"
            ]
        },
        "publishedAt": "2025-05-18T01:35:13.000Z",
        "title": "Synthetic Data RL: Task Definition Is All You Need",
        "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638e4e66629b4d0a62ce1bf3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
            "fullname": "Zhen Guo",
            "name": "zguo0525",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16270",
            "authors": [
                {
                    "_id": "6833d08edf7cbb5c087a8bf1",
                    "user": {
                        "_id": "65c288280aa2d53135734a42",
                        "avatarUrl": "/avatars/960422a1482ac8b4a52dd08c02d901f6.svg",
                        "isPro": false,
                        "fullname": "Jiaru Zou",
                        "user": "jiaruz2",
                        "type": "user"
                    },
                    "name": "Jiaru Zou",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-26T02:41:41.249Z",
                    "hidden": false
                },
                {
                    "_id": "6833d08edf7cbb5c087a8bf2",
                    "name": "Yikun Ban",
                    "hidden": false
                },
                {
                    "_id": "6833d08edf7cbb5c087a8bf3",
                    "name": "Zihao Li",
                    "hidden": false
                },
                {
                    "_id": "6833d08edf7cbb5c087a8bf4",
                    "name": "Yunzhe Qi",
                    "hidden": false
                },
                {
                    "_id": "6833d08edf7cbb5c087a8bf5",
                    "name": "Ruizhong Qiu",
                    "hidden": false
                },
                {
                    "_id": "6833d08edf7cbb5c087a8bf6",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "6833d08edf7cbb5c087a8bf7",
                    "name": "Jingrui He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T06:00:45.000Z",
            "submittedOnDailyAt": "2025-05-26T00:53:39.701Z",
            "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
            "upvotes": 6,
            "discussionId": "6833d08fdf7cbb5c087a8c29",
            "ai_summary": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.",
            "ai_keywords": [
                "large language models",
                "supervised fine-tuning",
                "domain-specific data",
                "generation loss",
                "model parameters",
                "learning signals",
                "Mistake Log",
                "transformer-based model",
                "Copilot model",
                "logits rectification",
                "joint training paradigm",
                "fused inference paradigm",
                "performance improvements",
                "computational overhead",
                "scalability",
                "transferability"
            ]
        },
        "publishedAt": "2025-05-22T02:00:45.000Z",
        "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
        "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17540",
            "authors": [
                {
                    "_id": "683409de1869c47bd0c423a4",
                    "name": "Mingrui Wu",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423a5",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423a6",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423a7",
                    "name": "Fangkai Yang",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423a8",
                    "name": "Jianjin Zhang",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423a9",
                    "name": "Jianfeng Liu",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423aa",
                    "name": "Yuefeng Zhan",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423ab",
                    "name": "Weihao Han",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423ac",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423ad",
                    "name": "Jiayi Ji",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423ae",
                    "name": "Xiaoshuai Sun",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423af",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423b0",
                    "name": "Weiwei Deng",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423b1",
                    "name": "Dongmei Zhang",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423b2",
                    "name": "Feng Sun",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423b3",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683409de1869c47bd0c423b4",
                    "name": "Rongrong Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T06:44:26.000Z",
            "submittedOnDailyAt": "2025-05-26T04:59:40.648Z",
            "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6416d0b2058f65de43191027",
                "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
                "isPro": false,
                "fullname": "Mingrui Wu",
                "user": "mrwu",
                "type": "user"
            },
            "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
            "upvotes": 5,
            "discussionId": "683409e21869c47bd0c4248e",
            "ai_summary": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.",
            "ai_keywords": [
                "text-to-image",
                "T2I",
                "large language models",
                "LLMs",
                "reinforcement learning",
                "structured prompts",
                "self-reflective prompts",
                "reward models",
                "human preference",
                "semantic alignment",
                "visual composition",
                "GenEval",
                "T2I-Compbench",
                "spatial layout fidelity",
                "compositional generalization"
            ]
        },
        "publishedAt": "2025-05-23T02:44:26.000Z",
        "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
        "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6416d0b2058f65de43191027",
            "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
            "fullname": "Mingrui Wu",
            "name": "mrwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17508",
            "authors": [
                {
                    "_id": "6833cf5a2d728e2330d572e3",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:09.930Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf5a2d728e2330d572e4",
                    "user": {
                        "_id": "653d276681f52ceb4d12bd85",
                        "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
                        "isPro": false,
                        "fullname": "Yifeng Liu",
                        "user": "Lewis-Lau",
                        "type": "user"
                    },
                    "name": "Yifeng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:06.913Z",
                    "hidden": false
                },
                {
                    "_id": "6833cf5a2d728e2330d572e5",
                    "name": "Huizhuo Yuan",
                    "hidden": false
                },
                {
                    "_id": "6833cf5a2d728e2330d572e6",
                    "name": "Yang Yuan",
                    "hidden": false
                },
                {
                    "_id": "6833cf5a2d728e2330d572e7",
                    "name": "Quanquan Gu",
                    "hidden": false
                },
                {
                    "_id": "6833cf5a2d728e2330d572e8",
                    "name": "Andrew C Yao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
            ],
            "publishedAt": "2025-05-23T06:01:21.000Z",
            "submittedOnDailyAt": "2025-05-26T00:50:14.655Z",
            "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "647bf082aba7062fe5c51ca9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                "isPro": false,
                "fullname": "Yifan Zhang",
                "user": "yifAI",
                "type": "user"
            },
            "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
            "upvotes": 5,
            "discussionId": "6833cf5b2d728e2330d57313",
            "projectPage": "https://complex-reasoning.github.io/RPG",
            "githubRepo": "https://github.com/complex-reasoning/RPG",
            "ai_summary": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.",
            "ai_keywords": [
                "policy gradient algorithms",
                "KL regularization",
                "KL divergence",
                "surrogate loss functions",
                "online reinforcement learning",
                "full differentiable loss functions",
                "REINFORCE-style gradient estimators",
                "GRPO",
                "REINFORCE++",
                "DAPO",
                "regularized policy gradient (RPG)",
                "forward KL divergence",
                "reverse KL divergence",
                "normalized policy distributions",
                "unnormalized policy distributions"
            ]
        },
        "publishedAt": "2025-05-23T02:01:21.000Z",
        "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
        "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "fullname": "Yifan Zhang",
            "name": "yifAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17016",
            "authors": [
                {
                    "_id": "6833f7847e0c637c71de0ec6",
                    "user": {
                        "_id": "64b64debeb9a833e08d079fd",
                        "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
                        "isPro": false,
                        "fullname": "Shuhan Tan",
                        "user": "tanshh97",
                        "type": "user"
                    },
                    "name": "Shuhan Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:10.881Z",
                    "hidden": false
                },
                {
                    "_id": "6833f7847e0c637c71de0ec7",
                    "name": "Kairan Dou",
                    "hidden": false
                },
                {
                    "_id": "6833f7847e0c637c71de0ec8",
                    "name": "Yue Zhao",
                    "hidden": false
                },
                {
                    "_id": "6833f7847e0c637c71de0ec9",
                    "name": "Philipp Krähenbühl",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T17:59:45.000Z",
            "submittedOnDailyAt": "2025-05-26T03:40:36.660Z",
            "title": "Interactive Post-Training for Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "64b64debeb9a833e08d079fd",
                "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
                "isPro": false,
                "fullname": "Shuhan Tan",
                "user": "tanshh97",
                "type": "user"
            },
            "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
            "upvotes": 5,
            "discussionId": "6833f7857e0c637c71de0f07",
            "projectPage": "https://ariostgx.github.io/ript_vla/",
            "githubRepo": "https://github.com/Ariostgx/ript-vla",
            "ai_summary": "RIPT-VLA is a reinforcement learning-based interactive post-training paradigm that enhances pretrained Vision-Language-Action models using sparse binary success rewards, improving adaptability and generalization.",
            "ai_keywords": [
                "reinforcement-learning-based",
                "interactive post-training",
                "Vision-Language-Action (VLA) models",
                "sparse binary success rewards",
                "offline expert demonstration",
                "supervised imitation",
                "dynamic rollout sampling",
                "leave-one-out advantage estimation",
                "policy optimization",
                "lightweight QueST model",
                "OpenVLA-OFT model",
                "success rate",
                "computational efficiency",
                "data-efficient",
                "policy learned",
                "generalization",
                "initial state context"
            ]
        },
        "publishedAt": "2025-05-22T13:59:45.000Z",
        "title": "Interactive Post-Training for Vision-Language-Action Models",
        "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b64debeb9a833e08d079fd",
            "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
            "fullname": "Shuhan Tan",
            "name": "tanshh97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.15182",
            "authors": [
                {
                    "_id": "68305a53b687672ae37437df",
                    "user": {
                        "_id": "63e48f6d9db5da2dc1f6288e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676046878664-63e48f6d9db5da2dc1f6288e.png",
                        "isPro": false,
                        "fullname": "JeonghyeKim",
                        "user": "beanie00",
                        "type": "user"
                    },
                    "name": "Jeonghye Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:14:07.810Z",
                    "hidden": false
                },
                {
                    "_id": "68305a53b687672ae37437e0",
                    "name": "Sojeong Rhee",
                    "hidden": false
                },
                {
                    "_id": "68305a53b687672ae37437e1",
                    "name": "Minbeom Kim",
                    "hidden": false
                },
                {
                    "_id": "68305a53b687672ae37437e2",
                    "name": "Dohyung Kim",
                    "hidden": false
                },
                {
                    "_id": "68305a53b687672ae37437e3",
                    "name": "Sangmook Lee",
                    "hidden": false
                },
                {
                    "_id": "68305a53b687672ae37437e4",
                    "name": "Youngchul Sung",
                    "hidden": false
                },
                {
                    "_id": "68305a53b687672ae37437e5",
                    "name": "Kyomin Jung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T06:57:39.000Z",
            "submittedOnDailyAt": "2025-05-26T10:53:42.482Z",
            "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State\n  Reflection",
            "submittedOnDailyBy": {
                "_id": "63e48f6d9db5da2dc1f6288e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676046878664-63e48f6d9db5da2dc1f6288e.png",
                "isPro": false,
                "fullname": "JeonghyeKim",
                "user": "beanie00",
                "type": "user"
            },
            "summary": "Recent advances in LLM agents have largely built on reasoning backbones like\nReAct, which interleave thought and action in complex environments. However,\nReAct often produces ungrounded or incoherent reasoning steps, leading to\nmisalignment between the agent's actual state and goal. Our analysis finds that\nthis stems from ReAct's inability to maintain consistent internal beliefs and\ngoal alignment, causing compounding errors and hallucinations. To address this,\nwe introduce ReflAct, a novel backbone that shifts reasoning from merely\nplanning next actions to continuously reflecting on the agent's state relative\nto its goal. By explicitly grounding decisions in states and enforcing ongoing\ngoal alignment, ReflAct dramatically improves strategic reliability. This\ndesign delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%\non average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even\noutperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),\nshowing that strengthening the core reasoning backbone is key to reliable agent\nperformance.",
            "upvotes": 5,
            "discussionId": "68305a54b687672ae374384e",
            "ai_summary": "ReflAct, a new reasoning backbone for LLM agents, improves goal alignment and reduces hallucinations by continuously reflecting on the agent's state, surpassing ReAct and other enhanced variants.",
            "ai_keywords": [
                "ReAct",
                "ReflAct",
                "reasoning backbone",
                "thought-action interweaving",
                "goal alignment",
                "internal beliefs",
                "state reflection",
                "strategic reliability",
                "ALFWorld",
                "Reflexion",
                "WKM"
            ]
        },
        "publishedAt": "2025-05-21T02:57:39.000Z",
        "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State\n  Reflection",
        "summary": "Recent advances in LLM agents have largely built on reasoning backbones like\nReAct, which interleave thought and action in complex environments. However,\nReAct often produces ungrounded or incoherent reasoning steps, leading to\nmisalignment between the agent's actual state and goal. Our analysis finds that\nthis stems from ReAct's inability to maintain consistent internal beliefs and\ngoal alignment, causing compounding errors and hallucinations. To address this,\nwe introduce ReflAct, a novel backbone that shifts reasoning from merely\nplanning next actions to continuously reflecting on the agent's state relative\nto its goal. By explicitly grounding decisions in states and enforcing ongoing\ngoal alignment, ReflAct dramatically improves strategic reliability. This\ndesign delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%\non average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even\noutperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),\nshowing that strengthening the core reasoning backbone is key to reliable agent\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15182.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e48f6d9db5da2dc1f6288e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676046878664-63e48f6d9db5da2dc1f6288e.png",
            "fullname": "JeonghyeKim",
            "name": "beanie00",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17091",
            "authors": [
                {
                    "_id": "6833cb25fe87d9433dfd2b1c",
                    "name": "Prateek Verma",
                    "hidden": false
                },
                {
                    "_id": "6833cb25fe87d9433dfd2b1d",
                    "name": "Mert Pilanci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T22:20:16.000Z",
            "submittedOnDailyAt": "2025-05-26T00:30:41.320Z",
            "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
            "submittedOnDailyBy": {
                "_id": "62d7f1119b629105a5d84aad",
                "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
                "isPro": false,
                "fullname": "Prateek Verma",
                "user": "prateekv",
                "type": "user"
            },
            "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
            "upvotes": 5,
            "discussionId": "6833cb26fe87d9433dfd2b64",
            "ai_summary": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.",
            "ai_keywords": [
                "auto-regressive",
                "LLM",
                "text tokens",
                "audio",
                "visual",
                "embeddings",
                "category labels",
                "classification",
                "FSD-50K",
                "GTZAN",
                "CIFAR-10",
                "Fashion-MNIST",
                "image patches"
            ]
        },
        "publishedAt": "2025-05-20T18:20:16.000Z",
        "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
        "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62d7f1119b629105a5d84aad",
            "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
            "fullname": "Prateek Verma",
            "name": "prateekv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18078",
            "authors": [
                {
                    "_id": "683463235a49ee2881ade255",
                    "user": {
                        "_id": "652b80d4703b3743c25c87d5",
                        "avatarUrl": "/avatars/3d9c0429fc6082cfb28e2dcd05845733.svg",
                        "isPro": false,
                        "fullname": "Dream How Chen",
                        "user": "yisuanwang",
                        "type": "user"
                    },
                    "name": "Junhao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T13:18:31.745Z",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade256",
                    "name": "Mingjin Chen",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade257",
                    "name": "Jianjin Xu",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade258",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade259",
                    "name": "Junting Dong",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade25a",
                    "name": "Mingze Sun",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade25b",
                    "name": "Puhua Jiang",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade25c",
                    "name": "Hongxiang Li",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade25d",
                    "name": "Yuhang Yang",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade25e",
                    "name": "Hao Zhao",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade25f",
                    "name": "Xiaoxiao Long",
                    "hidden": false
                },
                {
                    "_id": "683463235a49ee2881ade260",
                    "name": "Ruqi Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T16:37:14.000Z",
            "submittedOnDailyAt": "2025-05-26T11:20:08.646Z",
            "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "652b80d4703b3743c25c87d5",
                "avatarUrl": "/avatars/3d9c0429fc6082cfb28e2dcd05845733.svg",
                "isPro": false,
                "fullname": "Dream How Chen",
                "user": "yisuanwang",
                "type": "user"
            },
            "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.",
            "upvotes": 4,
            "discussionId": "683463265a49ee2881ade311",
            "ai_summary": "DanceTogether, an end-to-end diffusion framework, generates long, photorealistic multi-actor interaction videos from single reference images and pose-mask streams, outperforming existing systems.",
            "ai_keywords": [
                "diffusion framework",
                "MaskPoseAdapter",
                "denoising",
                "identity drift",
                "appearance bleeding",
                "PairFS-4K",
                "HumanRob-300",
                "TogetherVideoBench",
                "DanceTogEval-100",
                "embodied-AI",
                "HRI tasks",
                "persistent identity-action binding"
            ]
        },
        "publishedAt": "2025-05-23T12:37:14.000Z",
        "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation",
        "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18078.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652b80d4703b3743c25c87d5",
            "avatarUrl": "/avatars/3d9c0429fc6082cfb28e2dcd05845733.svg",
            "fullname": "Dream How Chen",
            "name": "yisuanwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17373",
            "authors": [
                {
                    "_id": "6834ab9e6b152a062eb05453",
                    "name": "Kaiwen Wang",
                    "hidden": false
                },
                {
                    "_id": "6834ab9e6b152a062eb05454",
                    "name": "Jin Peng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6834ab9e6b152a062eb05455",
                    "name": "Jonathan Chang",
                    "hidden": false
                },
                {
                    "_id": "6834ab9e6b152a062eb05456",
                    "name": "Zhaolin Gao",
                    "hidden": false
                },
                {
                    "_id": "6834ab9e6b152a062eb05457",
                    "name": "Nathan Kallus",
                    "hidden": false
                },
                {
                    "_id": "6834ab9e6b152a062eb05458",
                    "name": "Kianté Brantley",
                    "hidden": false
                },
                {
                    "_id": "6834ab9e6b152a062eb05459",
                    "name": "Wen Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T01:05:07.000Z",
            "submittedOnDailyAt": "2025-05-26T16:28:56.134Z",
            "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
            "submittedOnDailyBy": {
                "_id": "655b7d6ffb74ac37d3e0f43b",
                "avatarUrl": "/avatars/fc525bd38e660134f8a9cd4c9b0ba197.svg",
                "isPro": false,
                "fullname": "Kaiwen Wang",
                "user": "kaiwenw",
                "type": "user"
            },
            "summary": "In this paper, we propose a simple and efficient method for value model\ntraining on long-context reasoning traces. Compared to existing process reward\nmodels (PRMs), our method does not require a fine-grained notion of \"step,\"\nwhich is difficult to define for long-context reasoning models. By collecting a\ndataset of 2.5 million reasoning traces, we train a 1.5B token-level value\nmodel and apply it to DeepSeek models for improved performance with test-time\ncompute scaling. We find that block-wise value-guided search (VGS) with a final\nweighted majority vote achieves better test-time scaling than standard methods\nsuch as majority voting or best-of-n. With an inference budget of 64\ngenerations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of\n45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024\n& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly\nreduces the inference FLOPs required to achieve the same performance of\nmajority voting. Our dataset, model and codebase are open-sourced.",
            "upvotes": 3,
            "discussionId": "6834ab9f6b152a062eb05483",
            "projectPage": "https://huggingface.co/VGS-AI",
            "githubRepo": "https://github.com/kaiwenw/value-guided-search",
            "ai_summary": "A simple and efficient method for value model training on long-context reasoning traces improves test-time performance and reduces computational cost compared to existing methods.",
            "ai_keywords": [
                "value model",
                "training",
                "long-context reasoning",
                "process reward models",
                "token-level value model",
                "block-wise value-guided search",
                "VGS",
                "inference FLOPs"
            ]
        },
        "publishedAt": "2025-05-22T21:05:07.000Z",
        "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
        "summary": "In this paper, we propose a simple and efficient method for value model\ntraining on long-context reasoning traces. Compared to existing process reward\nmodels (PRMs), our method does not require a fine-grained notion of \"step,\"\nwhich is difficult to define for long-context reasoning models. By collecting a\ndataset of 2.5 million reasoning traces, we train a 1.5B token-level value\nmodel and apply it to DeepSeek models for improved performance with test-time\ncompute scaling. We find that block-wise value-guided search (VGS) with a final\nweighted majority vote achieves better test-time scaling than standard methods\nsuch as majority voting or best-of-n. With an inference budget of 64\ngenerations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of\n45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024\n& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly\nreduces the inference FLOPs required to achieve the same performance of\nmajority voting. Our dataset, model and codebase are open-sourced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17373.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655b7d6ffb74ac37d3e0f43b",
            "avatarUrl": "/avatars/fc525bd38e660134f8a9cd4c9b0ba197.svg",
            "fullname": "Kaiwen Wang",
            "name": "kaiwenw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.16056",
            "authors": [
                {
                    "_id": "6830894db51948863e05b68c",
                    "user": {
                        "_id": "64bfa1401d40292dd32f93d7",
                        "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
                        "isPro": false,
                        "fullname": "Leo Liang",
                        "user": "ljcleo",
                        "type": "user"
                    },
                    "name": "Jingcong Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:13:21.361Z",
                    "hidden": false
                },
                {
                    "_id": "6830894db51948863e05b68d",
                    "name": "Siyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6830894db51948863e05b68e",
                    "name": "Miren Tian",
                    "hidden": false
                },
                {
                    "_id": "6830894db51948863e05b68f",
                    "name": "Yitong Li",
                    "hidden": false
                },
                {
                    "_id": "6830894db51948863e05b690",
                    "name": "Duyu Tang",
                    "hidden": false
                },
                {
                    "_id": "6830894db51948863e05b691",
                    "name": "Zhongyu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T22:13:09.000Z",
            "submittedOnDailyAt": "2025-05-26T07:53:13.385Z",
            "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
            "submittedOnDailyBy": {
                "_id": "64bfa1401d40292dd32f93d7",
                "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
                "isPro": false,
                "fullname": "Leo Liang",
                "user": "ljcleo",
                "type": "user"
            },
            "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
            "upvotes": 3,
            "discussionId": "6830894eb51948863e05b6e8",
            "ai_summary": "MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "large language models (LLMs)",
                "expert offloading",
                "fast memory",
                "slow memory",
                "local routing consistency",
                "Segment Routing Best Performance (SRP)",
                "Segment Cache Best Hit Rate (SCH)",
                "domain-specialized experts",
                "vocabulary-specialized experts"
            ]
        },
        "publishedAt": "2025-05-21T18:13:09.000Z",
        "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
        "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16056.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bfa1401d40292dd32f93d7",
            "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
            "fullname": "Leo Liang",
            "name": "ljcleo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.15805",
            "authors": [
                {
                    "_id": "682eeb06720821973d643576",
                    "user": {
                        "_id": "647c4a2692182942d7c2e698",
                        "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
                        "isPro": false,
                        "fullname": "HWANCHANG",
                        "user": "HwanChang0106",
                        "type": "user"
                    },
                    "name": "Hwan Chang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:18:42.752Z",
                    "hidden": false
                },
                {
                    "_id": "682eeb06720821973d643577",
                    "user": {
                        "_id": "64aac6984135aae75f3b99c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yVR88SbW9TlJIrRHlJby-.jpeg",
                        "isPro": false,
                        "fullname": "Yumin Kim",
                        "user": "YuminKim",
                        "type": "user"
                    },
                    "name": "Yumin Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T14:07:44.617Z",
                    "hidden": false
                },
                {
                    "_id": "682eeb06720821973d643578",
                    "name": "Yonghyun Jun",
                    "hidden": false
                },
                {
                    "_id": "682eeb06720821973d643579",
                    "name": "Hwanhee Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T17:58:11.000Z",
            "submittedOnDailyAt": "2025-05-26T07:18:19.930Z",
            "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
            "submittedOnDailyBy": {
                "_id": "647c4a2692182942d7c2e698",
                "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
                "isPro": false,
                "fullname": "HWANCHANG",
                "user": "HwanChang0106",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
            "upvotes": 3,
            "discussionId": "682eeb07720821973d6435ec",
            "githubRepo": "https://github.com/hwanchang00/CoPriva",
            "ai_summary": "LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "security policies",
                "information non-disclosure",
                "CoPriva",
                "contextual security preservation",
                "question answering",
                "explicit policies",
                "indirect attacks",
                "virus",
                "policy constraints",
                "output revision"
            ]
        },
        "publishedAt": "2025-05-21T13:58:11.000Z",
        "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
        "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15805.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647c4a2692182942d7c2e698",
            "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
            "fullname": "HWANCHANG",
            "name": "HwanChang0106",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16409",
            "authors": [
                {
                    "_id": "6833d7f5ba63298d8869d305",
                    "user": {
                        "_id": "614c9487cbb5e52274a4024d",
                        "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
                        "isPro": false,
                        "fullname": "Chaeeun Kim",
                        "user": "Chaeeun-Kim",
                        "type": "user"
                    },
                    "name": "Chaeeun Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:09:41.154Z",
                    "hidden": false
                },
                {
                    "_id": "6833d7f5ba63298d8869d306",
                    "name": "Seungone Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T09:00:08.000Z",
            "submittedOnDailyAt": "2025-05-26T13:28:30.257Z",
            "title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via\n  Corpus-Traversing MCTS",
            "submittedOnDailyBy": {
                "_id": "614c9487cbb5e52274a4024d",
                "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
                "isPro": false,
                "fullname": "Chaeeun Kim",
                "user": "Chaeeun-Kim",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\nmulti-step reasoning and calling search engines at appropriate steps. However,\nexisting retrieval-augmented reasoning approaches rely on separate retrieval\nmodels, limiting the LRM's role in retrieval to deciding when to retrieve and\nhow to query. This separation not only increases hardware and operational costs\nbut also leads to errors in the retrieval process due to the representation\nbottleneck, a phenomenon where the retriever's embedding space is not\nexpressive enough to meet the generator's requirements. To address this, we\nshift our perspective from sequence-to-sequence matching to locating the\nanswer-containing paths within the corpus, and propose a novel framework called\nFREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables\nLRMs to retrieve relevant knowledge on their own by acting as both a generator\nand retriever. To achieve this, we introduce a variant of the MCTS algorithm\nspecialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing\nMonte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus\ntoward answer-containing regions. Our results on five open-domain QA\nbenchmarks, including single-hop and multi-hop questions, show that FREESON\nachieves an average improvement of 14.4% in EM and F1 over four multi-step\nreasoning models with a separate retriever, and it also performs comparably to\nthe strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.",
            "upvotes": 2,
            "discussionId": "6833d7f7ba63298d8869d357",
            "ai_summary": "FREESON, a novel framework that integrates retrieval and reasoning roles within LRMs using CT-MCTS, improves the performance of multistep reasoning models in QA tasks by reducing representation bottlenecks.",
            "ai_keywords": [
                "LREMS",
                "multi-step reasoning",
                "retrieval-augmented reasoning",
                "sequence-to-sequence matching",
                "MCTS",
                "CT-MCTS",
                "Corpus-Traversing Monte Carlo Tree Search",
                "representation bottleneck",
                "open-domain QA",
                "single-hop and multi-hop questions",
                "EM",
                "F1",
                "PopQA",
                "2WikiMultihopQA"
            ]
        },
        "publishedAt": "2025-05-22T05:00:08.000Z",
        "title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via\n  Corpus-Traversing MCTS",
        "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\nmulti-step reasoning and calling search engines at appropriate steps. However,\nexisting retrieval-augmented reasoning approaches rely on separate retrieval\nmodels, limiting the LRM's role in retrieval to deciding when to retrieve and\nhow to query. This separation not only increases hardware and operational costs\nbut also leads to errors in the retrieval process due to the representation\nbottleneck, a phenomenon where the retriever's embedding space is not\nexpressive enough to meet the generator's requirements. To address this, we\nshift our perspective from sequence-to-sequence matching to locating the\nanswer-containing paths within the corpus, and propose a novel framework called\nFREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables\nLRMs to retrieve relevant knowledge on their own by acting as both a generator\nand retriever. To achieve this, we introduce a variant of the MCTS algorithm\nspecialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing\nMonte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus\ntoward answer-containing regions. Our results on five open-domain QA\nbenchmarks, including single-hop and multi-hop questions, show that FREESON\nachieves an average improvement of 14.4% in EM and F1 over four multi-step\nreasoning models with a separate retriever, and it also performs comparably to\nthe strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16409.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "614c9487cbb5e52274a4024d",
            "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
            "fullname": "Chaeeun Kim",
            "name": "Chaeeun-Kim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16293",
            "authors": [
                {
                    "_id": "683400b5231225ee202c20b7",
                    "user": {
                        "_id": "645c26d423ed9b7788d5e24b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
                        "isPro": false,
                        "fullname": "Rishabh Maheshwary",
                        "user": "rmahesh",
                        "type": "user"
                    },
                    "name": "Rishabh Maheshwary",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:08:47.558Z",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20b8",
                    "name": "Masoud Hashemi",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20b9",
                    "name": "Khyati Mahajan",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20ba",
                    "name": "Shiva Krishna Reddy Malay",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20bb",
                    "name": "Sai Rajeswar",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20bc",
                    "name": "Sathwik Tejaswi Madhusudhan",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20bd",
                    "name": "Spandana Gella",
                    "hidden": false
                },
                {
                    "_id": "683400b5231225ee202c20be",
                    "name": "Vikas Yadav",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T06:45:05.000Z",
            "submittedOnDailyAt": "2025-05-26T04:22:12.337Z",
            "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
            "submittedOnDailyBy": {
                "_id": "645c26d423ed9b7788d5e24b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
                "isPro": false,
                "fullname": "Rishabh Maheshwary",
                "user": "rmahesh",
                "type": "user"
            },
            "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
            "upvotes": 2,
            "discussionId": "683400b6231225ee202c20e3",
            "ai_summary": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.",
            "ai_keywords": [
                "Iterative RAG",
                "multi-hop question answering",
                "context length",
                "irrelevant information",
                "dimensionality reduction",
                "Notes Writing",
                "Large Language Models",
                "LLMs",
                "framework agnostic",
                "evaluation datasets"
            ]
        },
        "publishedAt": "2025-05-22T02:45:05.000Z",
        "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
        "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645c26d423ed9b7788d5e24b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
            "fullname": "Rishabh Maheshwary",
            "name": "rmahesh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16022",
            "authors": [
                {
                    "_id": "68342cb2924393051af84722",
                    "user": {
                        "_id": "66e2932e5c100c12aa2def39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
                        "isPro": false,
                        "fullname": "weiliu",
                        "user": "thinkwee",
                        "type": "user"
                    },
                    "name": "Wei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T14:07:36.205Z",
                    "hidden": false
                },
                {
                    "_id": "68342cb2924393051af84723",
                    "name": "Siya Qi",
                    "hidden": false
                },
                {
                    "_id": "68342cb2924393051af84724",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68342cb2924393051af84725",
                    "name": "Chen Qian",
                    "hidden": false
                },
                {
                    "_id": "68342cb2924393051af84726",
                    "name": "Yali Du",
                    "hidden": false
                },
                {
                    "_id": "68342cb2924393051af84727",
                    "name": "Yulan He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T21:12:35.000Z",
            "submittedOnDailyAt": "2025-05-26T07:38:18.450Z",
            "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "66e2932e5c100c12aa2def39",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
                "isPro": false,
                "fullname": "weiliu",
                "user": "thinkwee",
                "type": "user"
            },
            "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
            "upvotes": 2,
            "discussionId": "68342cb3924393051af8476b",
            "githubRepo": "https://github.com/thinkwee/NOVER",
            "ai_summary": "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.",
            "ai_keywords": [
                "incentive training",
                "reinforcement learning",
                "NOVER",
                "NO-VERifier Reinforcement Learning",
                "DeepSeek R1-Zero",
                "DeepSeek R1 671B",
                "inverse incentive training"
            ]
        },
        "publishedAt": "2025-05-21T17:12:35.000Z",
        "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
        "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16022.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66e2932e5c100c12aa2def39",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
            "fullname": "weiliu",
            "name": "thinkwee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18383",
            "authors": [
                {
                    "_id": "6835043630e39ad8b0fa3f01",
                    "name": "Abdellah El Mekki",
                    "hidden": false
                },
                {
                    "_id": "6835043630e39ad8b0fa3f02",
                    "name": "Houdaifa Atou",
                    "hidden": false
                },
                {
                    "_id": "6835043630e39ad8b0fa3f03",
                    "name": "Omer Nacar",
                    "hidden": false
                },
                {
                    "_id": "6835043630e39ad8b0fa3f04",
                    "name": "Shady Shehata",
                    "hidden": false
                },
                {
                    "_id": "6835043630e39ad8b0fa3f05",
                    "name": "Muhammad Abdul-Mageed",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/631650cb894404e2506aac43/xte8tG1b_zlWdHT9faJ0r.png"
            ],
            "publishedAt": "2025-05-23T21:18:40.000Z",
            "submittedOnDailyAt": "2025-05-26T23:06:52.851Z",
            "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for\n  Local Communities",
            "submittedOnDailyBy": {
                "_id": "631650cb894404e2506aac43",
                "avatarUrl": "/avatars/d47134b29a968b71c5c9c2d65093945c.svg",
                "isPro": false,
                "fullname": "Abdellah EL MEKKI",
                "user": "3ebdola",
                "type": "user"
            },
            "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development.",
            "upvotes": 1,
            "discussionId": "6835043830e39ad8b0fa3f56",
            "githubRepo": "https://github.com/UBC-NLP/nilechat",
            "ai_summary": "A methodology is proposed to create pre-training data tailored to low-resource languages and cultures, demonstrated through NileChat, a 3B parameter LLM for Egyptian and Moroccan dialects, which outperforms existing similar-sized Arabic-aware models.",
            "ai_keywords": [
                "Large Language Models",
                "synthetic data",
                "retrieval-based pre-training",
                "cultural heritage",
                "cultural values",
                "NileChat"
            ]
        },
        "publishedAt": "2025-05-23T17:18:40.000Z",
        "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for\n  Local Communities",
        "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/631650cb894404e2506aac43/xte8tG1b_zlWdHT9faJ0r.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18383.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631650cb894404e2506aac43",
            "avatarUrl": "/avatars/d47134b29a968b71c5c9c2d65093945c.svg",
            "fullname": "Abdellah EL MEKKI",
            "name": "3ebdola",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14256",
            "authors": [
                {
                    "_id": "683330f7e10e89e250807ec2",
                    "name": "Shaolin Zhu",
                    "hidden": false
                },
                {
                    "_id": "683330f7e10e89e250807ec3",
                    "name": "Tianyu Dong",
                    "hidden": false
                },
                {
                    "_id": "683330f7e10e89e250807ec4",
                    "user": {
                        "_id": "6582c482f3006507ea10302a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
                        "isPro": false,
                        "fullname": "Bo Li",
                        "user": "liboaccn",
                        "type": "user"
                    },
                    "name": "Bo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:10:37.787Z",
                    "hidden": false
                },
                {
                    "_id": "683330f7e10e89e250807ec5",
                    "name": "Deyi Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T12:09:17.000Z",
            "submittedOnDailyAt": "2025-05-26T16:22:40.786Z",
            "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation",
            "submittedOnDailyBy": {
                "_id": "6582c482f3006507ea10302a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
                "isPro": false,
                "fullname": "Bo Li",
                "user": "liboaccn",
                "type": "user"
            },
            "summary": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.",
            "upvotes": 1,
            "discussionId": "683330f8e10e89e250807ef5",
            "ai_summary": "FuxiMT, a Chinese-centric multilingual machine translation model utilizing a sparsified large language model, demonstrates superior performance in low-resource scenarios and strong zero-shot capabilities across 65 languages.",
            "ai_keywords": [
                "sparsified large language model",
                "Mixture-of-Experts (MoEs)",
                "curriculum learning",
                "zero-shot translation"
            ]
        },
        "publishedAt": "2025-05-20T08:09:17.000Z",
        "title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation",
        "summary": "In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14256.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6582c482f3006507ea10302a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6582c482f3006507ea10302a/KbgSsq0FnbMngBcWPhIXi.jpeg",
            "fullname": "Bo Li",
            "name": "liboaccn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.12891",
            "authors": [
                {
                    "_id": "683059e8e2f446ed653e8512",
                    "name": "Shaohang Wei",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8513",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8514",
                    "user": {
                        "_id": "6447ca6ca478b20f1755b294",
                        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
                        "isPro": false,
                        "fullname": "Feifan Song",
                        "user": "songff",
                        "type": "user"
                    },
                    "name": "Feifan Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:14:11.572Z",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8515",
                    "name": "Wen Luo",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8516",
                    "name": "Tianyi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8517",
                    "name": "Haochen Tan",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8518",
                    "name": "Zhijiang Guo",
                    "hidden": false
                },
                {
                    "_id": "683059e8e2f446ed653e8519",
                    "name": "Houfeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T09:22:02.000Z",
            "submittedOnDailyAt": "2025-05-26T03:54:28.256Z",
            "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
            "submittedOnDailyBy": {
                "_id": "6447ca6ca478b20f1755b294",
                "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
                "isPro": false,
                "fullname": "Feifan Song",
                "user": "songff",
                "type": "user"
            },
            "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
            "upvotes": 1,
            "discussionId": "683059eae2f446ed653e85d7",
            "ai_summary": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.",
            "ai_keywords": [
                "Temporal reasoning",
                "Large Language Models (LLMs)",
                "QA pairs",
                "benchmark",
                "TIME-Wiki",
                "TIME-News",
                "TIME-Dial",
                "reasoning models",
                "non-reasoning models",
                "TIME-Lite"
            ]
        },
        "publishedAt": "2025-05-19T05:22:02.000Z",
        "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
        "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "fullname": "Feifan Song",
            "name": "songff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11881",
            "authors": [
                {
                    "_id": "6833ebdb142b0e50399413d3",
                    "name": "Giyeong Oh",
                    "hidden": false
                },
                {
                    "_id": "6833ebdb142b0e50399413d4",
                    "name": "Woohyun Cho",
                    "hidden": false
                },
                {
                    "_id": "6833ebdb142b0e50399413d5",
                    "name": "Siyeol Kim",
                    "hidden": false
                },
                {
                    "_id": "6833ebdb142b0e50399413d6",
                    "name": "Suhwan Choi",
                    "hidden": false
                },
                {
                    "_id": "6833ebdb142b0e50399413d7",
                    "name": "Younjae Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-17T07:16:11.000Z",
            "submittedOnDailyAt": "2025-05-26T02:52:51.028Z",
            "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
            "submittedOnDailyBy": {
                "_id": "63d93667255ef6add20f9272",
                "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
                "isPro": false,
                "fullname": "Giyeong Oh",
                "user": "BootsofLagrangian",
                "type": "user"
            },
            "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
            "upvotes": 1,
            "discussionId": "6833ebdc142b0e5039941420",
            "ai_summary": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.",
            "ai_keywords": [
                "residual connections",
                "vanishing gradients",
                "orthogonal update",
                "ResNetV2",
                "Vision Transformers",
                "CIFARs",
                "TinyImageNet",
                "ImageNet-1k",
                "top-1 accuracy"
            ]
        },
        "publishedAt": "2025-05-17T03:16:11.000Z",
        "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
        "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d93667255ef6add20f9272",
            "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
            "fullname": "Giyeong Oh",
            "name": "BootsofLagrangian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17552",
            "authors": [
                {
                    "_id": "6834b5158c6b65c0506d12c6",
                    "name": "Zijie Qiu",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12c7",
                    "name": "Jiaqi Wei",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12c8",
                    "name": "Xiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12c9",
                    "name": "Sheng Xu",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12ca",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12cb",
                    "name": "Zhi Jin",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12cc",
                    "name": "Zhiqiang Gao",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12cd",
                    "name": "Nanqing Dong",
                    "hidden": false
                },
                {
                    "_id": "6834b5158c6b65c0506d12ce",
                    "name": "Siqi Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T06:56:55.000Z",
            "submittedOnDailyAt": "2025-05-26T17:08:38.265Z",
            "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide\n  Sequencing",
            "submittedOnDailyBy": {
                "_id": "656553d89bf6665f10e3a92d",
                "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                "isPro": false,
                "fullname": "xiang wyatt zhang",
                "user": "Wyattz23",
                "type": "user"
            },
            "summary": "De novo peptide sequencing is a critical task in proteomics. However, the\nperformance of current deep learning-based methods is limited by the inherent\ncomplexity of mass spectrometry data and the heterogeneous distribution of\nnoise signals, leading to data-specific biases. We present RankNovo, the first\ndeep reranking framework that enhances de novo peptide sequencing by leveraging\nthe complementary strengths of multiple sequencing models. RankNovo employs a\nlist-wise reranking approach, modeling candidate peptides as multiple sequence\nalignments and utilizing axial attention to extract informative features across\ncandidates. Additionally, we introduce two new metrics, PMD (Peptide Mass\nDeviation) and RMD (residual Mass Deviation), which offer delicate supervision\nby quantifying mass differences between peptides at both the sequence and\nresidue levels. Extensive experiments demonstrate that RankNovo not only\nsurpasses its base models used to generate training candidates for reranking\npre-training, but also sets a new state-of-the-art benchmark. Moreover,\nRankNovo exhibits strong zero-shot generalization to unseen models whose\ngenerations were not exposed during training, highlighting its robustness and\npotential as a universal reranking framework for peptide sequencing. Our work\npresents a novel reranking strategy that fundamentally challenges existing\nsingle-model paradigms and advances the frontier of accurate de novo\nsequencing. Our source code is provided on GitHub.",
            "upvotes": 0,
            "discussionId": "6834b5168c6b65c0506d12f8",
            "ai_summary": "RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.",
            "ai_keywords": [
                "RankNovo",
                "deep reranking framework",
                "de novo peptide sequencing",
                "axial attention",
                "PMD",
                "RMD",
                "zero-shot generalization"
            ]
        },
        "publishedAt": "2025-05-23T02:56:55.000Z",
        "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide\n  Sequencing",
        "summary": "De novo peptide sequencing is a critical task in proteomics. However, the\nperformance of current deep learning-based methods is limited by the inherent\ncomplexity of mass spectrometry data and the heterogeneous distribution of\nnoise signals, leading to data-specific biases. We present RankNovo, the first\ndeep reranking framework that enhances de novo peptide sequencing by leveraging\nthe complementary strengths of multiple sequencing models. RankNovo employs a\nlist-wise reranking approach, modeling candidate peptides as multiple sequence\nalignments and utilizing axial attention to extract informative features across\ncandidates. Additionally, we introduce two new metrics, PMD (Peptide Mass\nDeviation) and RMD (residual Mass Deviation), which offer delicate supervision\nby quantifying mass differences between peptides at both the sequence and\nresidue levels. Extensive experiments demonstrate that RankNovo not only\nsurpasses its base models used to generate training candidates for reranking\npre-training, but also sets a new state-of-the-art benchmark. Moreover,\nRankNovo exhibits strong zero-shot generalization to unseen models whose\ngenerations were not exposed during training, highlighting its robustness and\npotential as a universal reranking framework for peptide sequencing. Our work\npresents a novel reranking strategy that fundamentally challenges existing\nsingle-model paradigms and advances the frontier of accurate de novo\nsequencing. Our source code is provided on GitHub.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17552.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]